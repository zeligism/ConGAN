{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConsistentGAN",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeligism/ConGAN/blob/main/ConsistentGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxx3Jy_8qsPE"
      },
      "source": [
        "### Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MFx20xTNkpQ",
        "outputId": "055826a3-f127-4190-9e1b-9a4e12be075b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-QNzdq01hSb"
      },
      "source": [
        "# Header"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSlF68ff2K8L"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf_Qrpq7z3iJ"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import glob\n",
        "import random\n",
        "import datetime\n",
        "import yaml\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "import torchvision\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.tensorboard as tensorboard\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "from math import log2\n",
        "from pprint import pformat\n",
        "from collections import defaultdict"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USDduLe1Qkd9"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiRxrufxw1cm"
      },
      "source": [
        "### Report Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmEyNG58w2kJ"
      },
      "source": [
        "def plot_lines(losses_dict, filename=None, title=\"\"):\n",
        "    \"\"\"\n",
        "    Plots the losses of the discriminator and the generator.\n",
        "\n",
        "    Args:\n",
        "        filename: The plot's filename. If None, plot won't be saved.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(title)\n",
        "    for label, losses in losses_dict.items():\n",
        "        plt.plot(losses, label=label)\n",
        "    plt.xlabel(\"t\")\n",
        "    plt.legend()\n",
        "    \n",
        "    if filename is not None:\n",
        "        plt.savefig(filename)\n",
        "    \n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def create_progress_animation(frames, filename):\n",
        "    \"\"\"\n",
        "    Creates a video of the progress of the generator on a fixed latent vector.\n",
        "\n",
        "    Args:\n",
        "        filename: The animation's filename.\n",
        "    \"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    plt.axis(\"off\")\n",
        "    ims = [[plt.imshow(img.permute(1,2,0), animated=True)]\n",
        "           for img in frames]\n",
        "    ani = animation.ArtistAnimation(fig, ims, blit=True)\n",
        "    \n",
        "    ani.save(filename)\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def generate_grid(generator, latent):\n",
        "    \"\"\"\n",
        "    Check generator's output on latent vectors and return it.\n",
        "\n",
        "    Args:\n",
        "        generator: The generator.\n",
        "        latent: Latent vector from which an image grid will be generated.\n",
        "\n",
        "    Returns:\n",
        "        A grid of images generated by `generator` from `latent`.\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake = generator(latent).detach()\n",
        "\n",
        "    image_grid = vutils.make_grid(fake.cpu(), padding=2, normalize=True, range=(-1,1))\n",
        "\n",
        "    return image_grid\n",
        "\n"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUzwGurc1qOx"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPhc2oS53G4e"
      },
      "source": [
        "## PyTorch Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU7HFc6t5N8w"
      },
      "source": [
        "### DCGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJHPo8w13JmH"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Default stride and padding half the size of features,\n",
        "    e.g. if input is [in_channels, 64, 64], output will be [out_channels, 32, 32].\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1,\n",
        "                 use_batchnorm=True, use_spectralnorm=False, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                              stride=stride, padding=padding, bias=False)\n",
        "        if use_spectralnorm:\n",
        "            self.conv = nn.utils.spectral_norm(self.conv)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels) if use_batchnorm else None\n",
        "        self.activation = nn.LeakyReLU(0.2, inplace=True) if activation is None else activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.batchnorm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Default stride and padding double the size of features,\n",
        "    e.g. if input is [in_channels, 32, 32], output will be [out_channels, 64, 64].\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1,\n",
        "                 use_batchnorm=True, use_spectralnorm=False, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convT = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                                        stride=stride, padding=padding, bias=False)\n",
        "        if use_spectralnorm:\n",
        "            self.convT = nn.utils.spectral_norm(self.convT)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels) if use_batchnorm else None\n",
        "        self.activation = nn.ReLU(inplace=True) if activation is None else activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convT(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.batchnorm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCGAN_Discriminator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_latents=16,\n",
        "                 num_features=64,\n",
        "                 image_channels=3,\n",
        "                 image_size=64,\n",
        "                 feature_multiplier=2,\n",
        "                 max_features=512,\n",
        "                 gan_type=\"gan\",\n",
        "                 fully_convolutional=True,\n",
        "                 activation=None,\n",
        "                 use_batchnorm=True,\n",
        "                 use_spectralnorm=False,\n",
        "                 output_sigmoid=True,\n",
        "                 D_block=ConvBlock):\n",
        "        super().__init__()\n",
        "\n",
        "        using_grad_penalty = gan_type in (\"gan-gp\", \"wgan-gp\")\n",
        "        output_sigmoid = output_sigmoid and gan_type in (\"gan\", \"gan-gp\")\n",
        "\n",
        "        block_config = {\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": use_batchnorm and not using_grad_penalty,\n",
        "            \"use_spectralnorm\": use_spectralnorm,\n",
        "        }\n",
        "\n",
        "        # Calculate intermediate image sizes\n",
        "        image_sizes = [image_size]\n",
        "        while image_sizes[-1] > 5:\n",
        "            image_sizes.append(image_sizes[-1] // 2)\n",
        "        latent_kernel = image_sizes[-1]  # should be either 3, 4, or 5\n",
        "        num_layers = len(image_sizes) - 1\n",
        "\n",
        "        # Calculate feature sizes\n",
        "        features = [min(max_features, round(num_features * feature_multiplier**i))\n",
        "                    for i in range(num_layers)]\n",
        "\n",
        "        # Input layer\n",
        "        self.input_layer = D_block(image_channels, features[0], **block_config)\n",
        "\n",
        "        # Intermediate layers\n",
        "        self.main_layers = nn.Sequential(*[\n",
        "            D_block(in_features, out_features, **block_config)\n",
        "            for in_features, out_features in zip(features, features[1:])\n",
        "        ])\n",
        "\n",
        "        # Output layer (feature_size = 3, 4, or 5 -> 1)\n",
        "        if fully_convolutional:\n",
        "            self.output_layer = nn.Sequential(\n",
        "                nn.Conv2d(features[-1], num_latents, latent_kernel, bias=False),\n",
        "                nn.Flatten(),\n",
        "            )\n",
        "        else:\n",
        "            self.output_layer = nn.Sequential(\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(features[-1] * latent_kernel**2, num_latents, bias=False)\n",
        "            )\n",
        "\n",
        "        # Add sigmoid activation if using regular GAN loss\n",
        "        self.output_activation = nn.Sigmoid() if output_sigmoid else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.main_layers(x)\n",
        "        x = self.output_layer(x)\n",
        "        if self.output_activation:\n",
        "            x = self.output_activation(x)\n",
        "        # Remove H and W dimensions, infer channels dim (remove if 1)\n",
        "        x = x.view(x.size(0), -1).squeeze(1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCGAN_Generator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_latents=100,\n",
        "                 num_features=64,\n",
        "                 image_channels=3,\n",
        "                 image_size=64,\n",
        "                 feature_multiplier=2,\n",
        "                 max_features=512,\n",
        "                 gan_type=\"gan\",\n",
        "                 fully_convolutional=True,\n",
        "                 activation=None,\n",
        "                 use_batchnorm=True,\n",
        "                 use_spectralnorm=False,\n",
        "                 G_block=ConvTBlock):\n",
        "        super().__init__()\n",
        "\n",
        "        block_config = {\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": use_batchnorm,\n",
        "            \"use_spectralnorm\": use_spectralnorm\n",
        "        }\n",
        "\n",
        "        # Calculate intermediate image sizes\n",
        "        image_sizes = [image_size]\n",
        "        while image_sizes[-1] > 5:\n",
        "            image_sizes.append(image_sizes[-1] // 2)\n",
        "        latent_kernel = image_sizes[-1]  # should be either 3, 4, or 5\n",
        "        num_layers = len(image_sizes) - 1\n",
        "\n",
        "        # Calculate feature sizes\n",
        "        features = [min(max_features, round(num_features * feature_multiplier**i))\n",
        "                    for i in range(num_layers)]\n",
        "\n",
        "        # Reverse order of image sizes and features for generator\n",
        "        image_sizes = image_sizes[::-1]\n",
        "        features = features[::-1]\n",
        "\n",
        "        # Input layer\n",
        "        if fully_convolutional:\n",
        "            self.input_layer = G_block(num_latents, features[0], kernel_size=latent_kernel,\n",
        "                                       stride=1, padding=0, **block_config)\n",
        "        else:\n",
        "            self.input_layer = nn.Sequential(\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(num_latents, features[0] * image_sizes[0]**2, bias=False),\n",
        "                View(features[0], image_sizes[0], image_sizes[0])\n",
        "            )\n",
        "\n",
        "        # Intermediate layers\n",
        "        self.main_layers = nn.Sequential(*[\n",
        "            G_block(in_features, out_features, kernel_size=4+(expected_size%2), **block_config)\n",
        "            for in_features, out_features, expected_size in zip(features, features[1:], image_sizes[1:])\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.ConvTranspose2d(features[-1], image_channels, kernel_size=4+(image_size%2),\n",
        "                                               stride=2, padding=1, bias=False)\n",
        "        self.output_activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add H and W dimensions, infer channels dim (add if none)\n",
        "        x = x.view(x.size(0), -1, 1, 1)\n",
        "        x = self.input_layer(x)\n",
        "        x = self.main_layers(x)\n",
        "        x = self.output_layer(x)\n",
        "        x = self.output_activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCGAN(nn.Module):\n",
        "    \"\"\"Deep Convolutional Generative Adversarial Network\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_latents=100,\n",
        "                 D_num_features=64,\n",
        "                 G_num_features=64,\n",
        "                 image_channels=3,\n",
        "                 image_size=64,\n",
        "                 feature_multiplier=2,\n",
        "                 gan_type=\"gan\",\n",
        "                 fully_convolutional=True,\n",
        "                 activation=None,\n",
        "                 use_batchnorm=True,\n",
        "                 use_spectralnorm=False,\n",
        "                 output_sigmoid=True,):\n",
        "        \"\"\"\n",
        "        Initializes DCGAN.\n",
        "\n",
        "        Args:\n",
        "            num_latents: Number of latent factors.\n",
        "            num_features: Number of features in the convolutions.\n",
        "            image_channels: Number of channels in the input image.\n",
        "            image_size: Size (i.e. height or width) of image.\n",
        "            gan_type: Type of GAN (e.g. \"gan\" or \"wgan-gp\").\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_latents = num_latents\n",
        "        self.D_num_features = D_num_features\n",
        "        self.G_num_features = G_num_features\n",
        "        self.image_channels = image_channels\n",
        "        self.image_size = image_size\n",
        "        self.feature_multiplier = feature_multiplier\n",
        "        self.gan_type = gan_type\n",
        "        self.fully_convolutional = fully_convolutional\n",
        "        self.activation = activation\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        self.use_spectralnorm = use_spectralnorm\n",
        "\n",
        "        D_params = {\n",
        "            \"num_latents\": 1,  # XXX\n",
        "            \"num_features\": D_num_features,\n",
        "            \"image_channels\": image_channels,\n",
        "            \"image_size\": image_size,\n",
        "            \"feature_multiplier\": feature_multiplier,\n",
        "            \"gan_type\": gan_type,\n",
        "            \"fully_convolutional\": fully_convolutional,\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": use_batchnorm,\n",
        "            \"use_spectralnorm\": use_spectralnorm,\n",
        "            \"output_sigmoid\": output_sigmoid,\n",
        "        }\n",
        "        G_params = {\n",
        "            \"num_latents\": num_latents,\n",
        "            \"num_features\": G_num_features,\n",
        "            \"image_channels\": image_channels,\n",
        "            \"image_size\": image_size,\n",
        "            \"feature_multiplier\": feature_multiplier,\n",
        "            \"gan_type\": gan_type,\n",
        "            \"fully_convolutional\": fully_convolutional,\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": True,\n",
        "            \"use_spectralnorm\": False,  # XXX\n",
        "        }\n",
        "\n",
        "        self.D = DCGAN_Discriminator(**D_params)\n",
        "        self.G = DCGAN_Generator(**G_params)\n",
        "\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self, *shape, including_batch=False):\n",
        "        super().__init__()\n",
        "        self.shape = shape\n",
        "        self.including_batch = including_batch\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.including_batch:\n",
        "            return x.view(*self.shape)\n",
        "        else:\n",
        "            return x.view(x.size(0), *self.shape)\n",
        "\n",
        "class ChannelNoise(nn.Module):\n",
        "    \"\"\"\n",
        "    Channel noise injection module.\n",
        "    Adds a linearly transformed noise to a convolution layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, std=0.02):\n",
        "        super().__init__()\n",
        "        self.std = std\n",
        "        self.scale = nn.Parameter(torch.ones(1, num_channels, 1, 1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        noise_size = [x.size()[0], 1, *x.size()[2:]]  # single channel\n",
        "        noise = self.std * torch.randn(noise_size).to(x)\n",
        "\n",
        "        return x + self.scale * noise"
      ],
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYujBEzC7EOO"
      },
      "source": [
        "### SimSiam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-YcNut27F-v"
      },
      "source": [
        "class SimSiam(nn.Module):\n",
        "    \"\"\"\n",
        "    Build a SimSiam model.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_encoder, dim=2048, pred_dim=512):\n",
        "        \"\"\"\n",
        "        dim: feature dimension (default: 2048)\n",
        "        pred_dim: hidden dimension of the predictor (default: 512)\n",
        "        \"\"\"\n",
        "        super(SimSiam, self).__init__()\n",
        "\n",
        "        # create the encoder\n",
        "        # num_classes is the output fc dimension, zero-initialize last BNs\n",
        "        self.encoder = base_encoder(num_classes=dim, zero_init_residual=True)\n",
        "\n",
        "        # build a 3-layer projector\n",
        "        prev_dim = self.encoder.fc.weight.shape[1]\n",
        "        self.encoder.fc = nn.Sequential(nn.Linear(prev_dim, prev_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(prev_dim),\n",
        "                                        nn.ReLU(inplace=True), # first layer\n",
        "                                        nn.Linear(prev_dim, prev_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(prev_dim),\n",
        "                                        nn.ReLU(inplace=True), # second layer\n",
        "                                        self.encoder.fc,\n",
        "                                        nn.BatchNorm1d(dim, affine=False)) # output layer\n",
        "        self.encoder.fc[6].bias.requires_grad = False # hack: not use bias as it is followed by BN\n",
        "\n",
        "        # build a 2-layer predictor\n",
        "        self.predictor = nn.Sequential(nn.Linear(dim, pred_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(pred_dim),\n",
        "                                        nn.ReLU(inplace=True), # hidden layer\n",
        "                                        nn.Linear(pred_dim, dim)) # output layer\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            x1: first views of images\n",
        "            x2: second views of images\n",
        "        Output:\n",
        "            p1, p2, z1, z2: predictors and targets of the network\n",
        "            See Sec. 3 of https://arxiv.org/abs/2011.10566 for detailed notations\n",
        "        \"\"\"\n",
        "\n",
        "        # compute features for one view\n",
        "        z1 = self.encoder(x1) # NxC\n",
        "        z2 = self.encoder(x2) # NxC\n",
        "\n",
        "        p1 = self.predictor(z1) # NxC\n",
        "        p2 = self.predictor(z2) # NxC\n",
        "\n",
        "        return p1, p2, z1.detach(), z2.detach()"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQcvNDLQ5niT"
      },
      "source": [
        "### ConsistentGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeUqwjQ95mMV"
      },
      "source": [
        "class ConsistentGAN(nn.Module):\n",
        "    def __init__(self, repr_dim, latent_dim,\n",
        "                 D_batchnorm=True, image_size=64, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.repr_dim = repr_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        \n",
        "        # Make D's architecture kinda similar to predictor\n",
        "        D_hidden_dim = repr_dim // 10\n",
        "        if D_batchnorm:\n",
        "            self.D = nn.Sequential(nn.Linear(repr_dim, D_hidden_dim, bias=False),\n",
        "                                   nn.LeakyReLU(0.2, inplace=True),\n",
        "                                   nn.Linear(D_hidden_dim, D_hidden_dim, bias=False),\n",
        "                                   nn.BatchNorm1d(D_hidden_dim),\n",
        "                                   nn.LeakyReLU(0.2, inplace=True),\n",
        "                                   nn.Linear(D_hidden_dim, 1),\n",
        "                                   nn.Sigmoid())\n",
        "        else:\n",
        "            self.D = nn.Sequential(nn.Linear(repr_dim, D_hidden_dim, bias=False),\n",
        "                                   nn.LeakyReLU(0.2, inplace=True),\n",
        "                                   nn.Linear(D_hidden_dim, D_hidden_dim, bias=False),\n",
        "                                   nn.LeakyReLU(0.2, inplace=True),\n",
        "                                   nn.Linear(D_hidden_dim, 1),\n",
        "                                   nn.Sigmoid())\n",
        "\n",
        "\n",
        "        # Same for generator (latent -> representations)\n",
        "        G_hidden_dim = repr_dim // 10\n",
        "        self.G = nn.Sequential(nn.Linear(latent_dim, G_hidden_dim, bias=False),\n",
        "                               nn.BatchNorm1d(G_hidden_dim),\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               nn.Linear(G_hidden_dim, G_hidden_dim, bias=False),\n",
        "                               nn.BatchNorm1d(G_hidden_dim),\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               nn.Linear(G_hidden_dim, G_hidden_dim, bias=False),\n",
        "                               nn.BatchNorm1d(G_hidden_dim),\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               nn.Linear(G_hidden_dim, G_hidden_dim, bias=False),\n",
        "                               nn.BatchNorm1d(G_hidden_dim),\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               nn.Linear(G_hidden_dim, G_hidden_dim, bias=False),\n",
        "                               nn.BatchNorm1d(G_hidden_dim),\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               nn.Linear(G_hidden_dim, G_hidden_dim, bias=False),\n",
        "                               nn.BatchNorm1d(G_hidden_dim),\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               nn.Linear(G_hidden_dim, repr_dim, bias=False),\n",
        "                               nn.BatchNorm1d(repr_dim, affine=False),\n",
        "                               Normalize2()\n",
        "                               )\n",
        "\n",
        "        # Encodes x to context @XXX\n",
        "        ctx_dim = 256\n",
        "        self.ctx_encoder = DCGAN_Discriminator(num_latents=ctx_dim, image_size=image_size)\n",
        "\n",
        "        # Projections to decoding space\n",
        "        # Decodes representation + context projection to an image\n",
        "        num_latents = 512\n",
        "        self.repr_proj = nn.Sequential(nn.Linear(repr_dim, num_latents // 2), nn.Sigmoid())\n",
        "        self.ctx_proj = nn.Sequential(nn.Linear(ctx_dim, num_latents // 2), nn.Sigmoid())\n",
        "        self.decoder = DCGAN_Generator(num_latents=num_latents, image_size=image_size)\n",
        "\n",
        "        # To check progress of G\n",
        "        self.fixed_latent = self.sample_latent(8*8)\n",
        "    \n",
        "    # XXX temp function\n",
        "    def proj(self, repr, ctx, return_all=False):\n",
        "        repr_proj = self.repr_proj(repr)\n",
        "        ctx_proj = self.ctx_proj(ctx)\n",
        "        proj = torch.cat([repr_proj, ctx_proj], dim=1)\n",
        "        if return_all:\n",
        "            return proj, repr_proj, ctx_proj\n",
        "        else:\n",
        "            return proj\n",
        "\n",
        "    def sample_latent(self, batch_size):\n",
        "        latent_size = [batch_size, self.latent_dim]\n",
        "        latent = torch.randn(latent_size)\n",
        "        return latent\n",
        "\n",
        "\n",
        "class Normalize2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return F.normalize(x, p=2, dim=1)\n"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yRivPV9BwFk"
      },
      "source": [
        "# Training v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5rpQp2E9rE5"
      },
      "source": [
        "### Imports and globals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51zFNn509xLz"
      },
      "source": [
        "import argparse\n",
        "import builtins\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.multiprocessing as mp\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "GANSIAM_DIR = \"/content/drive/My Drive/gansiam/\"\n",
        "SIMSIAM_PATH = os.path.join(GANSIAM_DIR, \"pretrained_batch256.tar\")\n",
        "TINYIMAGENET_DIR = \"tiny-imagenet-200\""
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9X_JYE2Vwxd"
      },
      "source": [
        "### Download Tiny Imagenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "559H2an_V03M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5700bc-9c34-48cf-c13e-6c58174d49cf"
      },
      "source": [
        "%%bash\n",
        "if [[ -d  \"tiny-imagenet-200\" ]]; then\n",
        "    echo \"Tiny Imagenet exists.\"\n",
        "else\n",
        "    wget -q \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "    unzip -qq \"tiny-imagenet-200.zip\" && rm \"tiny-imagenet-200.zip\"\n",
        "    echo \"Downloaded Tiny Imagenet.\"\n",
        "fi"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiny Imagenet exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwyRrBEGq9jh"
      },
      "source": [
        "### Utility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlbiU7QhrETa"
      },
      "source": [
        "#### GAN Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-TnvLE0q-1U"
      },
      "source": [
        "def get_D_loss(gan_type=\"gan\"):\n",
        "    if gan_type in (\"gan\", \"gan-gp\"):\n",
        "        return D_loss_GAN\n",
        "    elif gan_type in (\"wgan\", \"wgan-gp\"):\n",
        "        return D_loss_WGAN\n",
        "    else:\n",
        "        raise ValueError(f\"gan_type {gan_type} not supported\")\n",
        "\n",
        "\n",
        "def get_G_loss(gan_type=\"gan\"):\n",
        "    if gan_type in (\"gan\", \"gan-gp\"):\n",
        "        return G_loss_GAN\n",
        "    elif gan_type in (\"wgan\", \"wgan-gp\"):\n",
        "        return G_loss_WGAN\n",
        "    else:\n",
        "        raise ValueError(f\"gan_type {gan_type} not supported\")\n",
        "\n",
        "\n",
        "def D_loss_GAN(D_real, D_fake, label_smoothing=True):\n",
        "    \n",
        "    # Create (noisy) real and fake labels XXX\n",
        "    if label_smoothing:\n",
        "        real_label = 0.7 + 0.5 * torch.rand_like(D_real)\n",
        "    else:\n",
        "        real_label = torch.ones_like(D_real) - 0.1\n",
        "    fake_label = torch.zeros_like(D_fake)\n",
        "\n",
        "    # Calculate binary cross entropy loss\n",
        "    D_loss_real = F.binary_cross_entropy(D_real, real_label)\n",
        "    D_loss_fake = F.binary_cross_entropy(D_fake, fake_label)\n",
        "\n",
        "    # Loss is: - log(D(x)) - log(1 - D(x_g)),\n",
        "    # which is equiv. to maximizing: log(D(x)) + log(1 - D(x_g))\n",
        "    D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "    return D_loss.mean()\n",
        "\n",
        "\n",
        "def D_loss_WGAN(D_real, D_fake):\n",
        "\n",
        "    # Maximize: D(x) - D(x_g) - const * (|| grad of D(x_i) wrt x_i || - 1)^2,\n",
        "    # where x_i <- eps * x + (1 - eps) * x_g, and eps ~ rand(0,1)\n",
        "    D_loss = -1 * (D_real - D_fake)\n",
        "\n",
        "    return D_loss.mean()\n",
        "\n",
        "\n",
        "def G_loss_GAN(D_fake):\n",
        "\n",
        "    # Calculate binary cross entropy loss with a fake binary label\n",
        "    fake_label = torch.zeros_like(D_fake)\n",
        "\n",
        "    # Loss is: -log(D(G(z))), which is equiv. to minimizing log(1-D(G(z)))\n",
        "    # We use this loss vs. the original one for stability only.\n",
        "    G_loss = F.binary_cross_entropy(D_fake, 1 - fake_label)\n",
        "\n",
        "    return G_loss.mean()\n",
        "\n",
        "\n",
        "def G_loss_WGAN(D_fake):\n",
        "\n",
        "    # Minimize: -D(G(z))\n",
        "    G_loss = -D_fake\n",
        "    \n",
        "    return G_loss.mean()\n",
        "\n",
        "\n",
        "def interpolate(real, fake):\n",
        "    eps_size = [1] * len(real.size())\n",
        "    eps_size[0] = real.size(0)\n",
        "    eps = torch.rand(eps_size).to(real)\n",
        "    return eps * real + (1 - eps) * fake\n",
        "\n",
        "def simple_gradient_penalty(D, x, center=0.):\n",
        "    x.requires_grad_()\n",
        "    D_x = D(x)\n",
        "    D_grad = torch.autograd.grad(D_x, x, torch.ones_like(D_x), create_graph=True)\n",
        "    D_grad_norm = D_grad[0].view(x.size(0), -1).norm(dim=1)\n",
        "    return (D_grad_norm - center).pow(2).mean()\n"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esZGRUlerHNX"
      },
      "source": [
        "#### Data Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-K3c9WlrJDD"
      },
      "source": [
        "from PIL import ImageFilter\n",
        "import random\n",
        "\n",
        "\n",
        "class TwoCropsTransform:\n",
        "    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n",
        "\n",
        "    def __init__(self, base_transform):\n",
        "        self.base_transform = base_transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        q = self.base_transform(x)\n",
        "        k = self.base_transform(x)\n",
        "        return [q, k]\n",
        "\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
        "\n",
        "    def __init__(self, sigma=[.1, 2.]):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, x):\n",
        "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
        "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
        "        return x"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyzHmINsryyh"
      },
      "source": [
        "#### Train Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8KJUUeWr1dI"
      },
      "source": [
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    filename = os.path.join(GANSIAM_DIR, \"results\", filename)\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, init_lr, epoch, args):\n",
        "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
        "    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / args.epochs))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        if 'fix_lr' in param_group and param_group['fix_lr']:\n",
        "            param_group['lr'] = init_lr\n",
        "        else:\n",
        "            param_group['lr'] = cur_lr\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbo7T6blPVTc"
      },
      "source": [
        "### Load pre-trained SimSiam model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nr8clgi_AzY",
        "outputId": "0ccaa409-75ec-4cfb-94b8-8c2c56bdd90c"
      },
      "source": [
        "checkpoint = torch.load(SIMSIAM_PATH, map_location=\"cuda:0\")\n",
        "# remove 'module.' from dict keys\n",
        "model_dict = OrderedDict((k[7:], v) for k, v in checkpoint[\"state_dict\"].items())\n",
        "\n",
        "# Load model\n",
        "simsiam = SimSiam(models.__dict__[\"resnet50\"])\n",
        "simsiam.load_state_dict(model_dict)\n",
        "#print(simsiam)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kckB_xSVX8kB"
      },
      "source": [
        "# Training v3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nouxldrYb0r"
      },
      "source": [
        "## Args"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUP5vn8OX--p"
      },
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.data = TINYIMAGENET_DIR\n",
        "        self.workers = 2\n",
        "        self.epochs = 100\n",
        "        self.batch_size = 256\n",
        "        self.D_lr = 2e-4\n",
        "        self.G_lr = 2e-4\n",
        "        self.betas = (0., 0.9)\n",
        "        #self.momentum = 0.9\n",
        "        #self.weight_decay = 1e-4\n",
        "        self.print_freq = 10\n",
        "        self.seed = None\n",
        "        self.gpu = 0\n",
        "\n",
        "        # SimSiam (don't change if loading pre-trained)\n",
        "        self.dim = 2048\n",
        "        self.pred_dim = 512\n",
        "\n",
        "        # GAN\n",
        "        self.gan_type = \"gan\"\n",
        "        self.repr_dim = self.dim  # don't change\n",
        "        self.latent_dim = 128\n",
        "        self.num_features = 64\n",
        "        self.D_iters = 5\n",
        "        self.grad_penalty = 0.\n",
        "        self.grad_center = 1.\n",
        "        self.generate_grid_interval = 100\n",
        "\n",
        "        # noise is assumed to be proportional to 1e-3 * sd(data)\n",
        "        self.im_noise = 1e-3  # image sd is about 1.0\n",
        "        self.repr_noise = 1e-6  # repr sd is about 0.001\n",
        "        self.repr_consistency = 0.1  # how much should G be consistent in repr XXX\n",
        "        self.latent_transform_lr = self.G_lr\n",
        "\n",
        "\n",
        "GENERATED_GRIDS = []\n",
        "IMAGE_SIZE = 64\n",
        "args = Args()"
      ],
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xihRlU0PYhiJ"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW_P-JfeYiOe",
        "outputId": "fcb9d60a-348e-43a7-98ee-3bf05161a87b"
      },
      "source": [
        "# image normalization\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "normalize = transforms.Normalize(mean=mean, std=std)\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean= [-m/s for m, s in zip(mean, std)],\n",
        "   std= [1/s for s in std]\n",
        ")\n",
        "\n",
        "\n",
        "# MoCo v2's aug: similar to SimCLR https://arxiv.org/abs/2002.05709\n",
        "\"\"\"\n",
        "augmentation = [\n",
        "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.2, 1.)),\n",
        "    transforms.RandomApply([\n",
        "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
        "    ], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "augmentation = [\n",
        "    #transforms.RandomResizedCrop(IMAGE_SIZE),\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "]\n",
        "\n",
        "DATASET = \"CIFAR10\"\n",
        "\n",
        "if DATASET == \"MNIST\":\n",
        "    augmentation = [transforms.Grayscale(3)] + augmentation\n",
        "    train_dataset = datasets.MNIST(\n",
        "        root=os.path.join(GANSIAM_DIR, \"mnist/train\"), train=True, download=True,\n",
        "        transform=transforms.Compose(augmentation))\n",
        "elif DATASET == \"CelebA\":\n",
        "    train_dataset = datasets.CelebA(\n",
        "        root=os.path.join(GANSIAM_DIR, \"celeba\"), download=True,\n",
        "        transform=transforms.Compose(augmentation))\n",
        "elif DATASET == \"CIFAR10\":\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=os.path.join(GANSIAM_DIR, \"cifar10/train\"), train=True, download=True,\n",
        "        transform=transforms.Compose(augmentation))\n",
        "        #transform=TwoCropsTransform(transforms.Compose(augmentation)))\n",
        "elif DATASET == \"Tiny Imagenet\":\n",
        "    train_dataset = datasets.ImageFolder(\n",
        "        root=os.path.join(TINYIMAGENET_DIR, 'train'),\n",
        "        transform=transforms.Compose(augmentation))\n",
        "else:\n",
        "    raise Exception(f\"Dataset '{DATASET}' not found\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "    num_workers=args.workers, pin_memory=True, sampler=None, drop_last=True)"
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0Z4Ke6DYkDg"
      },
      "source": [
        "## Model + Opt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmLiHFvBimb3"
      },
      "source": [
        "def D_criterion_NS(D_real, D_fake):\n",
        "    d_loss = F.softplus(-D_real) + F.softplus(D_fake)\n",
        "    return d_loss.mean()\n",
        "\n",
        "def G_criterion_NS(D_fake):\n",
        "    return F.softplus(-D_fake).mean()\n",
        "\n",
        "def D_criterion_LS(D_real, D_fake):\n",
        "    d_loss = 0.5 * (D_real - torch.ones_like(D_real))**2 + 0.5 * (D_fake)**2\n",
        "    return d_loss.mean()\n",
        "\n",
        "def G_criterion_LS(D_fake):\n",
        "    gen_loss = 0.5 * (D_fake - torch.ones_like(D_fake))**2\n",
        "    return gen_loss.mean()\n",
        "\n",
        "def D_criterion_hinge(D_real, D_fake):\n",
        "    return torch.mean(F.relu(1. - D_real)) + torch.mean(F.relu(1. + D_fake))\n",
        "\n",
        "def G_criterion_hinge(D_fake):\n",
        "    return -torch.mean(D_fake)"
      ],
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rO_MvGzYldx",
        "outputId": "02e681b2-07f5-4dbf-8906-ab125e6a06ba"
      },
      "source": [
        "\n",
        "load = False\n",
        "\n",
        "if args.seed is not None:\n",
        "    random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    cudnn.deterministic = True\n",
        "\n",
        "torch.cuda.set_device(args.gpu)\n",
        "\n",
        "hidden_latent_dim = 2*args.latent_dim  # XXX\n",
        "latent_transform = nn.Sequential(\n",
        "    nn.Linear(args.repr_dim+args.latent_dim, hidden_latent_dim, bias=False),\n",
        "    #nn.Linear(hidden_latent_dim, hidden_latent_dim, bias=False),\n",
        "    #nn.Linear(hidden_latent_dim, hidden_latent_dim, bias=False),\n",
        "    #nn.Linear(hidden_latent_dim, hidden_latent_dim, bias=False),\n",
        "    #nn.Linear(hidden_latent_dim, hidden_latent_dim, bias=False),\n",
        ")\n",
        "\n",
        "model = DCGAN(num_latents=hidden_latent_dim,\n",
        "              image_size=IMAGE_SIZE,\n",
        "              gan_type=args.gan_type,\n",
        "              use_batchnorm=False,\n",
        "              use_spectralnorm=True,\n",
        "              output_sigmoid=False,\n",
        "              D_num_features=args.num_features,\n",
        "              G_num_features=args.num_features*2,\n",
        "              )\n",
        "\n",
        "latent_transform = latent_transform.cuda(args.gpu)\n",
        "model = model.cuda(args.gpu)\n",
        "simsiam = simsiam.cuda(args.gpu)\n",
        "\n",
        "# Define D and G loss functions\n",
        "D_criterion = D_criterion_LS\n",
        "G_criterion = G_criterion_LS\n",
        "\n",
        "# Optimizers\n",
        "D_optimizer = torch.optim.Adam(\n",
        "    model.D.parameters(),\n",
        "    args.D_lr, betas=args.betas)\n",
        "\n",
        "G_optimizer = torch.optim.Adam(\n",
        "    [{\"params\": model.G.parameters()},\n",
        "     {\"params\": latent_transform.parameters(), \"lr\": args.latent_transform_lr}],\n",
        "     args.G_lr, betas=args.betas)\n",
        "\n",
        "D_sched = torch.optim.lr_scheduler.ExponentialLR(D_optimizer, 0.99)\n",
        "G_sched = torch.optim.lr_scheduler.ExponentialLR(G_optimizer, 0.99)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "print(\"Num of params in D:\", sum(map(torch.numel, model.D.parameters())))\n",
        "print(\"Num of params in G:\", sum(map(torch.numel, model.G.parameters())))\n",
        "\n",
        "if load:\n",
        "    model.load_state_dict(torch.load(f\"{GANSIAM_DIR}/results/model.pth.tar\"))\n",
        "    latent_transform.load_state_dict(torch.load(f\"{GANSIAM_DIR}/results/latent_transform.pth.tar\"))\n",
        "    D_sched.load_state_dict(torch.load(f\"{GANSIAM_DIR}/results/D_sched.pth.tar\"))\n",
        "    G_sched.load_state_dict(torch.load(f\"{GANSIAM_DIR}/results/G_sched.pth.tar\"))"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of params in D: 2763776\n",
            "Num of params in G: 8921856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeDicP6QZNQ2"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA8BoUEjNrlR"
      },
      "source": [
        "def sample_latent(num_samples):\n",
        "    return torch.randn(num_samples, args.latent_dim)\n",
        "\n",
        "def check_G_progress(G):\n",
        "    with torch.no_grad():\n",
        "        fake_progress = G(latent_transform(fixed_latent))\n",
        "    im_grid = torch.cat([fixed_x, fake_progress], dim=0)\n",
        "    grid = vutils.make_grid(im_grid.cpu(), padding=2, normalize=True, range=(-1,1))\n",
        "    return grid"
      ],
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckj9x-2fNtnp"
      },
      "source": [
        "# Sample a global latent for reuse\n",
        "fixed_x, _ = next(iter(train_loader))\n",
        "fixed_x = fixed_x[:32].cuda(args.gpu)\n",
        "with torch.no_grad():\n",
        "    fixed_repr = simsiam.encoder(fixed_x)\n",
        "    fixed_repr = F.normalize(fixed_repr)\n",
        "fixed_noise = sample_latent(32).cuda(args.gpu)\n",
        "fixed_latent = torch.cat([fixed_repr, fixed_noise], dim=1)"
      ],
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7gU4I8OZOer"
      },
      "source": [
        "def train(train_loader, model, simsiam,\n",
        "          D_criterion, G_criterion, D_optimizer, G_optimizer, epoch, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    data_time = AverageMeter('Data', ':6.3f')\n",
        "    D_on_reals = AverageMeter('D(real)', ':.4f')\n",
        "    D_on_fakes = AverageMeter('D(fake)', ':.4f')\n",
        "    D_grads = AverageMeter('grad(D)', ':.4f')\n",
        "    repr_losses = AverageMeter('repr loss', ':.4f')\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader),\n",
        "        [batch_time, data_time,\n",
        "         D_on_reals, D_on_fakes, D_grads, repr_losses],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    D_grad_penalty = torch.zeros(1).cuda(args.gpu)\n",
        "    repr_loss = torch.zeros(1).cuda(args.gpu)\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        x = x.cuda(args.gpu, non_blocking=True)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # compute output and loss\n",
        "        with torch.no_grad():\n",
        "            repr = simsiam.encoder(x)\n",
        "            repr = F.normalize(repr + args.repr_noise * torch.randn_like(repr))\n",
        "\n",
        "        ### train GAN\n",
        "        #if (i+1) % (args.D_iters+1) > 0:\n",
        "        for _ in range(args.D_iters):\n",
        "            # Add noise to real sample\n",
        "            real = x + args.im_noise * torch.randn_like(x)\n",
        "\n",
        "            # Sample from generator\n",
        "            with torch.no_grad():\n",
        "                # Sample latent\n",
        "                latent_noise = sample_latent(batch_size).cuda(args.gpu)\n",
        "                latent = torch.cat([repr, latent_noise], dim=1)\n",
        "                fake = model.G(latent_transform(latent))\n",
        "                # Add noise to fake sample as well\n",
        "                fake = fake + args.im_noise * torch.randn_like(fake)\n",
        "\n",
        "            # Classify real and fake data\n",
        "            D_real = model.D(real)\n",
        "            D_fake = model.D(fake)\n",
        "\n",
        "            # Calculate loss\n",
        "            D_loss = D_criterion(D_real, D_fake)\n",
        "            # Gradient penalty\n",
        "            if args.grad_penalty != 0.:\n",
        "                D_grad_penalty = simple_gradient_penalty(\n",
        "                    model.D, interpolate(real, fake), center=args.grad_center)\n",
        "                D_loss += args.grad_penalty * D_grad_penalty\n",
        "\n",
        "            # Calculate gradient and minimize\n",
        "            D_optimizer.zero_grad()\n",
        "            D_loss.backward()\n",
        "            D_optimizer.step()\n",
        "\n",
        "            # Save data\n",
        "            D_on_reals.update(D_real.mean().item(), batch_size)\n",
        "            D_on_fakes.update(D_fake.mean().item(), batch_size)\n",
        "            D_grads.update(D_grad_penalty.mean().item(), batch_size)\n",
        "\n",
        "        else:\n",
        "            # Sample from generator\n",
        "            latent_noise = sample_latent(batch_size).cuda(args.gpu)\n",
        "            latent = torch.cat([repr, latent_noise], dim=1)\n",
        "            fake = model.G(latent_transform(latent))\n",
        "            fake = fake + args.im_noise * torch.randn_like(fake)\n",
        "            # Classify fake images\n",
        "            D_fake = model.D(fake)\n",
        "            # Calculate loss\n",
        "            G_loss = G_criterion(D_fake)\n",
        "            if args.repr_consistency != 0.:\n",
        "                # Representation consistency loss\n",
        "                fake_repr = simsiam.encoder(fake)\n",
        "                repr_loss = -F.cosine_similarity(fake_repr, repr).mean()\n",
        "            # Calculate gradient and minimize\n",
        "            G_optimizer.zero_grad()\n",
        "            (G_loss + args.repr_consistency * repr_loss).backward()\n",
        "            G_optimizer.step()\n",
        "\n",
        "            # Save data\n",
        "            D_on_fakes.update(D_fake.mean().item(), batch_size)\n",
        "            repr_losses.update(repr_loss.mean().item(), batch_size)\n",
        "\n",
        "        # Check generator's progress by recording its output on a fixed input\n",
        "        if i % args.generate_grid_interval == 0:\n",
        "            grid = check_G_progress(model.G)\n",
        "            GENERATED_GRIDS.append(grid)\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            progress.display(i)\n"
      ],
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v24ES94Re55W"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDMPwe-ae6q-",
        "outputId": "fa2dff2b-a02b-42d9-e05c-530220cc6fc3"
      },
      "source": [
        "for epoch in range(args.epochs):\n",
        "\n",
        "    if epoch == 0:\n",
        "        args.repr_consistency = 0.1\n",
        "    if epoch == 100:\n",
        "        args.repr_consistency = 10.\n",
        "\n",
        "    # train for one epoch\n",
        "    train(train_loader, model, simsiam,\n",
        "          D_criterion, G_criterion, D_optimizer, G_optimizer, epoch, args)\n",
        "\n",
        "    D_sched.step()\n",
        "    G_sched.step()\n",
        "    \n",
        "    # Check G's progress evey epoch by generating an image\n",
        "    grid = check_G_progress(model.G)\n",
        "    imname = f'{GANSIAM_DIR}/results/progress/grid_{epoch:04d}.png'\n",
        "    plt.imsave(imname, grid.permute(1,2,0).numpy())\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save({'state_dict': model.state_dict()},\n",
        "                f\"{GANSIAM_DIR}/results/model.pth.tar\")\n",
        "        torch.save({'state_dict': latent_transform.state_dict()},\n",
        "                f\"{GANSIAM_DIR}/results/latent_transform.pth.tar\")\n",
        "        torch.save({'state_dict': D_sched.state_dict()},\n",
        "                f\"{GANSIAM_DIR}/results/D_sched.pth.tar\")\n",
        "        torch.save({'state_dict': G_sched.state_dict()},\n",
        "                f\"{GANSIAM_DIR}/results/G_sched.pth.tar\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][  0/195]\tTime  1.406 ( 1.406)\tData  0.342 ( 0.342)\tD(real) 0.6454 (0.5813)\tD(fake) 0.3982 (0.1623)\tgrad(D) 0.0000 (0.0000)\trepr loss 0.0175 (0.0175)\n",
            "Epoch: [0][ 10/195]\tTime  0.982 ( 1.023)\tData  0.000 ( 0.031)\tD(real) 0.3250 (0.8035)\tD(fake) -0.0086 (0.0378)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.2935 (-0.1188)\n",
            "Epoch: [0][ 20/195]\tTime  0.995 ( 1.008)\tData  0.000 ( 0.017)\tD(real) 0.8023 (0.8356)\tD(fake) 0.0093 (0.0212)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6414 (-0.3316)\n",
            "Epoch: [0][ 30/195]\tTime  0.976 ( 1.002)\tData  0.000 ( 0.011)\tD(real) 0.7366 (0.8570)\tD(fake) 0.0141 (0.0162)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6216 (-0.4236)\n",
            "Epoch: [0][ 40/195]\tTime  0.989 ( 0.998)\tData  0.000 ( 0.009)\tD(real) 0.7616 (0.8719)\tD(fake) 0.0169 (0.0133)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6901 (-0.4791)\n",
            "Epoch: [0][ 50/195]\tTime  0.997 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 1.0765 (0.8820)\tD(fake) -0.0028 (0.0141)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6724 (-0.5150)\n",
            "Epoch: [0][ 60/195]\tTime  0.980 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0785 (0.8898)\tD(fake) -0.0005 (0.0134)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7020 (-0.5424)\n",
            "Epoch: [0][ 70/195]\tTime  1.006 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 0.7650 (0.8946)\tD(fake) 0.0093 (0.0143)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7389 (-0.5616)\n",
            "Epoch: [0][ 80/195]\tTime  0.998 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9568 (0.8983)\tD(fake) 0.0012 (0.0156)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7010 (-0.5790)\n",
            "Epoch: [0][ 90/195]\tTime  0.982 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.9470 (0.8997)\tD(fake) 0.0169 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7363 (-0.5954)\n",
            "Epoch: [0][100/195]\tTime  1.012 ( 0.993)\tData  0.001 ( 0.004)\tD(real) 1.0071 (0.9003)\tD(fake) 0.0049 (0.0220)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6156 (-0.6018)\n",
            "Epoch: [0][110/195]\tTime  0.996 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0651 (0.9019)\tD(fake) 0.0003 (0.0236)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6829 (-0.6102)\n",
            "Epoch: [0][120/195]\tTime  0.987 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0014 (0.9025)\tD(fake) -0.0039 (0.0257)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7076 (-0.6161)\n",
            "Epoch: [0][130/195]\tTime  0.999 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0049 (0.9012)\tD(fake) 0.0233 (0.0299)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6102 (-0.6143)\n",
            "Epoch: [0][140/195]\tTime  0.985 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0466 (0.9008)\tD(fake) -0.0098 (0.0323)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7337 (-0.6155)\n",
            "Epoch: [0][150/195]\tTime  0.995 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9772 (0.8995)\tD(fake) 0.0310 (0.0365)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6324 (-0.6165)\n",
            "Epoch: [0][160/195]\tTime  0.976 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0983 (0.8987)\tD(fake) -0.0575 (0.0393)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6785 (-0.6166)\n",
            "Epoch: [0][170/195]\tTime  0.973 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.6608 (0.8974)\tD(fake) 0.1986 (0.0424)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6676 (-0.6183)\n",
            "Epoch: [0][180/195]\tTime  1.002 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0397 (0.8962)\tD(fake) -0.0565 (0.0451)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6405 (-0.6184)\n",
            "Epoch: [0][190/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0766 (0.8956)\tD(fake) -0.0032 (0.0459)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6777 (-0.6197)\n",
            "Epoch: [1][  0/195]\tTime  1.382 ( 1.382)\tData  0.347 ( 0.347)\tD(real) 0.7511 (0.8648)\tD(fake) 0.1998 (0.0836)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6322 (-0.6322)\n",
            "Epoch: [1][ 10/195]\tTime  0.996 ( 1.026)\tData  0.000 ( 0.032)\tD(real) 1.1440 (0.8786)\tD(fake) -0.0646 (0.0959)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6607 (-0.6374)\n",
            "Epoch: [1][ 20/195]\tTime  0.998 ( 1.006)\tData  0.000 ( 0.017)\tD(real) 1.0632 (0.8766)\tD(fake) -0.0158 (0.0938)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5923 (-0.6295)\n",
            "Epoch: [1][ 30/195]\tTime  0.996 ( 1.000)\tData  0.000 ( 0.012)\tD(real) 1.0596 (0.8784)\tD(fake) -0.0296 (0.0920)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6202 (-0.6210)\n",
            "Epoch: [1][ 40/195]\tTime  1.008 ( 0.997)\tData  0.000 ( 0.009)\tD(real) 1.1155 (0.8766)\tD(fake) -0.0259 (0.0965)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6192 (-0.6239)\n",
            "Epoch: [1][ 50/195]\tTime  0.983 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 0.6516 (0.8760)\tD(fake) 0.1215 (0.0957)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6386 (-0.6220)\n",
            "Epoch: [1][ 60/195]\tTime  0.992 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 0.9311 (0.8758)\tD(fake) 0.0419 (0.0943)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6086 (-0.6121)\n",
            "Epoch: [1][ 70/195]\tTime  0.999 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.7084 (0.8755)\tD(fake) 0.2164 (0.0951)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6124 (-0.6131)\n",
            "Epoch: [1][ 80/195]\tTime  1.004 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9228 (0.8763)\tD(fake) 0.0465 (0.0959)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6528 (-0.6157)\n",
            "Epoch: [1][ 90/195]\tTime  0.985 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9702 (0.8766)\tD(fake) -0.0412 (0.0962)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5476 (-0.6132)\n",
            "Epoch: [1][100/195]\tTime  1.012 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0105 (0.8771)\tD(fake) 0.0053 (0.0956)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5779 (-0.6078)\n",
            "Epoch: [1][110/195]\tTime  0.985 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.6439 (0.8773)\tD(fake) 0.1181 (0.0955)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5287 (-0.6056)\n",
            "Epoch: [1][120/195]\tTime  0.984 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.8915 (0.8785)\tD(fake) 0.0706 (0.0957)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6638 (-0.6042)\n",
            "Epoch: [1][130/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.8282 (0.8797)\tD(fake) 0.1880 (0.0953)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5934 (-0.6008)\n",
            "Epoch: [1][140/195]\tTime  0.985 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9031 (0.8802)\tD(fake) 0.0875 (0.0958)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5269 (-0.5990)\n",
            "Epoch: [1][150/195]\tTime  0.993 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9799 (0.8814)\tD(fake) 0.0168 (0.0957)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6045 (-0.5976)\n",
            "Epoch: [1][160/195]\tTime  0.985 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.1002 (0.8825)\tD(fake) -0.0004 (0.0952)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5054 (-0.5980)\n",
            "Epoch: [1][170/195]\tTime  0.991 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0592 (0.8829)\tD(fake) -0.0239 (0.0957)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6073 (-0.5984)\n",
            "Epoch: [1][180/195]\tTime  0.976 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.7960 (0.8831)\tD(fake) 0.1045 (0.0963)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6268 (-0.5990)\n",
            "Epoch: [1][190/195]\tTime  0.980 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9356 (0.8835)\tD(fake) 0.0476 (0.0970)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5909 (-0.5979)\n",
            "Epoch: [2][  0/195]\tTime  1.406 ( 1.406)\tData  0.341 ( 0.341)\tD(real) 0.9512 (0.8816)\tD(fake) 0.0272 (0.1215)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5685 (-0.5685)\n",
            "Epoch: [2][ 10/195]\tTime  0.983 ( 1.028)\tData  0.000 ( 0.031)\tD(real) 0.9818 (0.8875)\tD(fake) -0.0014 (0.1083)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5928 (-0.5794)\n",
            "Epoch: [2][ 20/195]\tTime  0.992 ( 1.009)\tData  0.000 ( 0.017)\tD(real) 1.0809 (0.8877)\tD(fake) 0.0152 (0.1066)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5915 (-0.5694)\n",
            "Epoch: [2][ 30/195]\tTime  0.993 ( 1.002)\tData  0.000 ( 0.011)\tD(real) 0.9875 (0.8870)\tD(fake) 0.0062 (0.1084)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5685 (-0.5784)\n",
            "Epoch: [2][ 40/195]\tTime  0.997 ( 1.000)\tData  0.000 ( 0.009)\tD(real) 0.8044 (0.8892)\tD(fake) 0.1191 (0.1065)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6019 (-0.5780)\n",
            "Epoch: [2][ 50/195]\tTime  0.990 ( 0.998)\tData  0.000 ( 0.007)\tD(real) 0.8023 (0.8905)\tD(fake) 0.0732 (0.1058)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5177 (-0.5778)\n",
            "Epoch: [2][ 60/195]\tTime  0.977 ( 0.996)\tData  0.000 ( 0.006)\tD(real) 0.7135 (0.8899)\tD(fake) 0.1724 (0.1067)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.4715 (-0.5741)\n",
            "Epoch: [2][ 70/195]\tTime  0.989 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 0.8529 (0.8898)\tD(fake) 0.1397 (0.1078)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5672 (-0.5749)\n",
            "Epoch: [2][ 80/195]\tTime  0.996 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 0.9235 (0.8896)\tD(fake) 0.0539 (0.1095)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6659 (-0.5779)\n",
            "Epoch: [2][ 90/195]\tTime  0.988 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.6959 (0.8896)\tD(fake) 0.1829 (0.1099)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.4929 (-0.5776)\n",
            "Epoch: [2][100/195]\tTime  1.005 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9814 (0.8899)\tD(fake) 0.0508 (0.1102)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6009 (-0.5777)\n",
            "Epoch: [2][110/195]\tTime  0.993 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.7519 (0.8888)\tD(fake) 0.1603 (0.1120)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.4608 (-0.5772)\n",
            "Epoch: [2][120/195]\tTime  0.973 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.8330 (0.8883)\tD(fake) 0.1359 (0.1128)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6138 (-0.5772)\n",
            "Epoch: [2][130/195]\tTime  0.981 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.7813 (0.8885)\tD(fake) 0.1832 (0.1130)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5562 (-0.5772)\n",
            "Epoch: [2][140/195]\tTime  0.975 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9761 (0.8884)\tD(fake) 0.0209 (0.1138)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6028 (-0.5761)\n",
            "Epoch: [2][150/195]\tTime  0.999 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0464 (0.8891)\tD(fake) 0.0008 (0.1136)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6438 (-0.5767)\n",
            "Epoch: [2][160/195]\tTime  1.000 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.8761 (0.8889)\tD(fake) 0.1418 (0.1140)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6066 (-0.5783)\n",
            "Epoch: [2][170/195]\tTime  0.992 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0339 (0.8892)\tD(fake) -0.0070 (0.1144)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5917 (-0.5782)\n",
            "Epoch: [2][180/195]\tTime  0.990 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.7059 (0.8891)\tD(fake) 0.1636 (0.1150)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5649 (-0.5796)\n",
            "Epoch: [2][190/195]\tTime  0.982 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9522 (0.8893)\tD(fake) 0.0609 (0.1155)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5873 (-0.5813)\n",
            "Epoch: [3][  0/195]\tTime  1.415 ( 1.415)\tData  0.359 ( 0.359)\tD(real) 1.0159 (0.9211)\tD(fake) 0.0228 (0.1211)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5620 (-0.5620)\n",
            "Epoch: [3][ 10/195]\tTime  0.996 ( 1.025)\tData  0.000 ( 0.033)\tD(real) 0.9372 (0.9010)\tD(fake) 0.0535 (0.1125)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5572 (-0.5840)\n",
            "Epoch: [3][ 20/195]\tTime  0.978 ( 1.007)\tData  0.000 ( 0.017)\tD(real) 1.1910 (0.9002)\tD(fake) -0.0554 (0.1168)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5689 (-0.5896)\n",
            "Epoch: [3][ 30/195]\tTime  0.989 ( 0.999)\tData  0.000 ( 0.012)\tD(real) 0.7569 (0.8977)\tD(fake) 0.2021 (0.1158)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6268 (-0.5970)\n",
            "Epoch: [3][ 40/195]\tTime  0.989 ( 0.997)\tData  0.000 ( 0.009)\tD(real) 0.7034 (0.8962)\tD(fake) 0.1844 (0.1179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5977 (-0.5928)\n",
            "Epoch: [3][ 50/195]\tTime  0.981 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 0.9886 (0.8959)\tD(fake) 0.0168 (0.1179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5558 (-0.5881)\n",
            "Epoch: [3][ 60/195]\tTime  0.987 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 0.8822 (0.8944)\tD(fake) 0.1652 (0.1202)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6308 (-0.5908)\n",
            "Epoch: [3][ 70/195]\tTime  0.984 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9368 (0.8946)\tD(fake) 0.0719 (0.1198)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5685 (-0.5884)\n",
            "Epoch: [3][ 80/195]\tTime  0.999 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9892 (0.8949)\tD(fake) 0.0154 (0.1207)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5184 (-0.5901)\n",
            "Epoch: [3][ 90/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.6864 (0.8945)\tD(fake) 0.2221 (0.1204)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5633 (-0.5883)\n",
            "Epoch: [3][100/195]\tTime  1.002 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0096 (0.8951)\tD(fake) -0.0083 (0.1205)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5803 (-0.5875)\n",
            "Epoch: [3][110/195]\tTime  1.003 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9812 (0.8953)\tD(fake) 0.0756 (0.1207)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6228 (-0.5897)\n",
            "Epoch: [3][120/195]\tTime  1.001 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.7479 (0.8955)\tD(fake) 0.2400 (0.1204)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5898 (-0.5914)\n",
            "Epoch: [3][130/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9709 (0.8961)\tD(fake) 0.0269 (0.1200)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6144 (-0.5917)\n",
            "Epoch: [3][140/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.8335 (0.8965)\tD(fake) 0.0384 (0.1195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6172 (-0.5931)\n",
            "Epoch: [3][150/195]\tTime  1.000 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0390 (0.8966)\tD(fake) -0.0210 (0.1198)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6333 (-0.5954)\n",
            "Epoch: [3][160/195]\tTime  0.992 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9164 (0.8973)\tD(fake) 0.0927 (0.1188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6206 (-0.5965)\n",
            "Epoch: [3][170/195]\tTime  0.989 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9853 (0.8978)\tD(fake) 0.0714 (0.1184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6299 (-0.5990)\n",
            "Epoch: [3][180/195]\tTime  0.995 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0236 (0.8983)\tD(fake) 0.0049 (0.1180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6217 (-0.5998)\n",
            "Epoch: [3][190/195]\tTime  0.986 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9665 (0.8983)\tD(fake) 0.0892 (0.1185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5808 (-0.6008)\n",
            "Epoch: [4][  0/195]\tTime  1.385 ( 1.385)\tData  0.332 ( 0.332)\tD(real) 1.0040 (0.9163)\tD(fake) -0.0200 (0.1081)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6445 (-0.6445)\n",
            "Epoch: [4][ 10/195]\tTime  0.997 ( 1.025)\tData  0.000 ( 0.031)\tD(real) 0.7686 (0.9076)\tD(fake) 0.1338 (0.1095)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5906 (-0.6146)\n",
            "Epoch: [4][ 20/195]\tTime  1.004 ( 1.008)\tData  0.000 ( 0.016)\tD(real) 0.8649 (0.9088)\tD(fake) 0.1398 (0.1091)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5906 (-0.6025)\n",
            "Epoch: [4][ 30/195]\tTime  0.991 ( 1.003)\tData  0.000 ( 0.011)\tD(real) 0.7996 (0.9061)\tD(fake) 0.1223 (0.1110)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6389 (-0.6108)\n",
            "Epoch: [4][ 40/195]\tTime  0.999 ( 0.999)\tData  0.000 ( 0.008)\tD(real) 0.9987 (0.9061)\tD(fake) -0.0219 (0.1120)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6393 (-0.6141)\n",
            "Epoch: [4][ 50/195]\tTime  0.999 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 0.7607 (0.9067)\tD(fake) 0.2153 (0.1121)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6198 (-0.6166)\n",
            "Epoch: [4][ 60/195]\tTime  0.980 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0357 (0.9066)\tD(fake) -0.0407 (0.1132)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6314 (-0.6201)\n",
            "Epoch: [4][ 70/195]\tTime  0.982 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 0.8715 (0.9057)\tD(fake) 0.1271 (0.1154)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.4965 (-0.6193)\n",
            "Epoch: [4][ 80/195]\tTime  0.985 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0641 (0.9067)\tD(fake) -0.0217 (0.1141)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6383 (-0.6175)\n",
            "Epoch: [4][ 90/195]\tTime  0.981 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0220 (0.9068)\tD(fake) -0.0518 (0.1138)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5509 (-0.6123)\n",
            "Epoch: [4][100/195]\tTime  1.011 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9808 (0.9067)\tD(fake) 0.0211 (0.1137)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5635 (-0.6097)\n",
            "Epoch: [4][110/195]\tTime  0.975 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9715 (0.9070)\tD(fake) 0.0461 (0.1139)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6119 (-0.6098)\n",
            "Epoch: [4][120/195]\tTime  0.986 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0065 (0.9072)\tD(fake) 0.0923 (0.1136)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6171 (-0.6102)\n",
            "Epoch: [4][130/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.6949 (0.9072)\tD(fake) 0.1360 (0.1136)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6319 (-0.6102)\n",
            "Epoch: [4][140/195]\tTime  1.005 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9148 (0.9075)\tD(fake) 0.0611 (0.1134)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6512 (-0.6116)\n",
            "Epoch: [4][150/195]\tTime  0.991 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0044 (0.9080)\tD(fake) 0.0897 (0.1129)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6650 (-0.6119)\n",
            "Epoch: [4][160/195]\tTime  1.000 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9931 (0.9083)\tD(fake) 0.0289 (0.1127)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6161 (-0.6117)\n",
            "Epoch: [4][170/195]\tTime  0.985 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.8630 (0.9086)\tD(fake) 0.0812 (0.1122)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6155 (-0.6103)\n",
            "Epoch: [4][180/195]\tTime  0.980 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.8703 (0.9091)\tD(fake) 0.1604 (0.1114)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6683 (-0.6102)\n",
            "Epoch: [4][190/195]\tTime  0.993 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9681 (0.9092)\tD(fake) 0.0419 (0.1113)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5545 (-0.6105)\n",
            "Epoch: [5][  0/195]\tTime  1.350 ( 1.350)\tData  0.327 ( 0.327)\tD(real) 0.8724 (0.9074)\tD(fake) 0.0721 (0.0637)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6469 (-0.6469)\n",
            "Epoch: [5][ 10/195]\tTime  0.981 ( 1.017)\tData  0.000 ( 0.030)\tD(real) 0.8532 (0.9122)\tD(fake) 0.1542 (0.1056)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6539 (-0.6216)\n",
            "Epoch: [5][ 20/195]\tTime  0.979 ( 1.003)\tData  0.000 ( 0.016)\tD(real) 0.8470 (0.9129)\tD(fake) 0.0633 (0.1058)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5652 (-0.6020)\n",
            "Epoch: [5][ 30/195]\tTime  0.979 ( 0.998)\tData  0.000 ( 0.011)\tD(real) 0.7748 (0.9134)\tD(fake) 0.0781 (0.1065)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6279 (-0.6126)\n",
            "Epoch: [5][ 40/195]\tTime  1.002 ( 0.996)\tData  0.000 ( 0.008)\tD(real) 1.0202 (0.9150)\tD(fake) -0.0147 (0.1036)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6120 (-0.6177)\n",
            "Epoch: [5][ 50/195]\tTime  0.989 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 1.0012 (0.9154)\tD(fake) 0.0038 (0.1041)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6209 (-0.6201)\n",
            "Epoch: [5][ 60/195]\tTime  0.991 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 1.0644 (0.9159)\tD(fake) -0.0078 (0.1034)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6418 (-0.6223)\n",
            "Epoch: [5][ 70/195]\tTime  0.991 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9363 (0.9158)\tD(fake) 0.0428 (0.1025)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6357 (-0.6203)\n",
            "Epoch: [5][ 80/195]\tTime  0.994 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.7763 (0.9150)\tD(fake) 0.1494 (0.1047)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6217 (-0.6190)\n",
            "Epoch: [5][ 90/195]\tTime  1.006 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0626 (0.9154)\tD(fake) -0.0379 (0.1049)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6107 (-0.6162)\n",
            "Epoch: [5][100/195]\tTime  1.002 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9664 (0.9157)\tD(fake) 0.0103 (0.1042)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5916 (-0.6161)\n",
            "Epoch: [5][110/195]\tTime  0.995 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.8728 (0.9164)\tD(fake) 0.1531 (0.1027)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5609 (-0.6161)\n",
            "Epoch: [5][120/195]\tTime  0.980 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9968 (0.9161)\tD(fake) 0.0312 (0.1038)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6292 (-0.6160)\n",
            "Epoch: [5][130/195]\tTime  1.006 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9363 (0.9165)\tD(fake) 0.0555 (0.1029)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6030 (-0.6151)\n",
            "Epoch: [5][140/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.1066 (0.9168)\tD(fake) -0.0010 (0.1032)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6210 (-0.6149)\n",
            "Epoch: [5][150/195]\tTime  0.985 ( 0.991)\tData  0.001 ( 0.003)\tD(real) 1.0591 (0.9173)\tD(fake) 0.0148 (0.1023)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6173 (-0.6154)\n",
            "Epoch: [5][160/195]\tTime  1.008 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 1.0104 (0.9176)\tD(fake) -0.0521 (0.1020)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5633 (-0.6151)\n",
            "Epoch: [5][170/195]\tTime  0.981 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.8421 (0.9176)\tD(fake) 0.0754 (0.1015)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6155 (-0.6156)\n",
            "Epoch: [5][180/195]\tTime  0.980 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9366 (0.9178)\tD(fake) 0.0626 (0.1014)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6374 (-0.6163)\n",
            "Epoch: [5][190/195]\tTime  0.984 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 1.0128 (0.9182)\tD(fake) -0.0418 (0.1006)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6853 (-0.6163)\n",
            "Epoch: [6][  0/195]\tTime  1.393 ( 1.393)\tData  0.355 ( 0.355)\tD(real) 0.8263 (0.9339)\tD(fake) 0.2130 (0.0703)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6331 (-0.6331)\n",
            "Epoch: [6][ 10/195]\tTime  0.986 ( 1.024)\tData  0.000 ( 0.033)\tD(real) 1.0911 (0.9278)\tD(fake) -0.0220 (0.0952)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6122 (-0.6200)\n",
            "Epoch: [6][ 20/195]\tTime  0.973 ( 1.008)\tData  0.000 ( 0.017)\tD(real) 0.7863 (0.9248)\tD(fake) 0.1681 (0.0957)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6511 (-0.6277)\n",
            "Epoch: [6][ 30/195]\tTime  0.985 ( 0.999)\tData  0.000 ( 0.012)\tD(real) 0.8264 (0.9243)\tD(fake) 0.0938 (0.0940)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6225 (-0.6261)\n",
            "Epoch: [6][ 40/195]\tTime  0.982 ( 0.997)\tData  0.000 ( 0.009)\tD(real) 1.0870 (0.9241)\tD(fake) -0.0481 (0.0954)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6207 (-0.6277)\n",
            "Epoch: [6][ 50/195]\tTime  0.981 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 0.9188 (0.9236)\tD(fake) 0.0685 (0.0942)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6829 (-0.6307)\n",
            "Epoch: [6][ 60/195]\tTime  0.991 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 0.9876 (0.9232)\tD(fake) 0.0407 (0.0955)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6429 (-0.6307)\n",
            "Epoch: [6][ 70/195]\tTime  0.997 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 1.0078 (0.9233)\tD(fake) -0.0015 (0.0951)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6100 (-0.6302)\n",
            "Epoch: [6][ 80/195]\tTime  0.997 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.8667 (0.9239)\tD(fake) 0.1552 (0.0940)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6270 (-0.6312)\n",
            "Epoch: [6][ 90/195]\tTime  0.988 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0767 (0.9241)\tD(fake) -0.0829 (0.0943)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6489 (-0.6340)\n",
            "Epoch: [6][100/195]\tTime  0.997 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9687 (0.9243)\tD(fake) 0.0014 (0.0933)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6132 (-0.6344)\n",
            "Epoch: [6][110/195]\tTime  0.978 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9966 (0.9240)\tD(fake) 0.0028 (0.0932)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6119 (-0.6344)\n",
            "Epoch: [6][120/195]\tTime  0.979 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9642 (0.9243)\tD(fake) 0.0203 (0.0925)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6283 (-0.6342)\n",
            "Epoch: [6][130/195]\tTime  0.982 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0402 (0.9241)\tD(fake) 0.0302 (0.0929)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6535 (-0.6355)\n",
            "Epoch: [6][140/195]\tTime  0.986 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0324 (0.9245)\tD(fake) -0.0220 (0.0922)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6594 (-0.6363)\n",
            "Epoch: [6][150/195]\tTime  0.989 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0314 (0.9243)\tD(fake) -0.0127 (0.0926)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6231 (-0.6347)\n",
            "Epoch: [6][160/195]\tTime  0.977 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0487 (0.9243)\tD(fake) -0.0555 (0.0925)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7307 (-0.6348)\n",
            "Epoch: [6][170/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0133 (0.9246)\tD(fake) -0.0084 (0.0918)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5654 (-0.6344)\n",
            "Epoch: [6][180/195]\tTime  0.992 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.9248)\tD(fake) 0.1682 (0.0915)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6319 (-0.6351)\n",
            "Epoch: [6][190/195]\tTime  0.983 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.8612 (0.9249)\tD(fake) 0.0589 (0.0915)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5886 (-0.6350)\n",
            "Epoch: [7][  0/195]\tTime  1.393 ( 1.393)\tData  0.329 ( 0.329)\tD(real) 1.0269 (0.9354)\tD(fake) -0.0453 (0.1172)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6693 (-0.6693)\n",
            "Epoch: [7][ 10/195]\tTime  0.984 ( 1.024)\tData  0.000 ( 0.030)\tD(real) 0.9821 (0.9294)\tD(fake) 0.0383 (0.0894)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5478 (-0.6311)\n",
            "Epoch: [7][ 20/195]\tTime  0.989 ( 1.006)\tData  0.000 ( 0.016)\tD(real) 1.0586 (0.9302)\tD(fake) 0.0404 (0.0863)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6078 (-0.6297)\n",
            "Epoch: [7][ 30/195]\tTime  0.992 ( 0.999)\tData  0.000 ( 0.011)\tD(real) 0.9116 (0.9288)\tD(fake) 0.0936 (0.0872)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7182 (-0.6341)\n",
            "Epoch: [7][ 40/195]\tTime  1.004 ( 0.997)\tData  0.000 ( 0.008)\tD(real) 0.8409 (0.9291)\tD(fake) 0.1443 (0.0871)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6608 (-0.6349)\n",
            "Epoch: [7][ 50/195]\tTime  0.979 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 1.0195 (0.9301)\tD(fake) 0.0563 (0.0861)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5507 (-0.6344)\n",
            "Epoch: [7][ 60/195]\tTime  0.975 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 0.8443 (0.9291)\tD(fake) 0.0844 (0.0867)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6524 (-0.6313)\n",
            "Epoch: [7][ 70/195]\tTime  0.985 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9333 (0.9288)\tD(fake) 0.0592 (0.0865)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6818 (-0.6333)\n",
            "Epoch: [7][ 80/195]\tTime  0.981 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9610 (0.9290)\tD(fake) 0.0455 (0.0858)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6742 (-0.6336)\n",
            "Epoch: [7][ 90/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9433 (0.9297)\tD(fake) 0.0388 (0.0844)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6435 (-0.6346)\n",
            "Epoch: [7][100/195]\tTime  1.000 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9787 (0.9297)\tD(fake) 0.0169 (0.0847)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6486 (-0.6339)\n",
            "Epoch: [7][110/195]\tTime  0.996 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.1078 (0.9300)\tD(fake) -0.0244 (0.0844)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6239 (-0.6342)\n",
            "Epoch: [7][120/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9822 (0.9297)\tD(fake) 0.0195 (0.0844)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6714 (-0.6345)\n",
            "Epoch: [7][130/195]\tTime  1.000 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9411 (0.9297)\tD(fake) 0.0390 (0.0845)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6469 (-0.6360)\n",
            "Epoch: [7][140/195]\tTime  0.972 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9261 (0.9296)\tD(fake) 0.0732 (0.0846)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6299 (-0.6351)\n",
            "Epoch: [7][150/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9457 (0.9299)\tD(fake) 0.0433 (0.0840)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6166 (-0.6365)\n",
            "Epoch: [7][160/195]\tTime  1.011 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.8539 (0.9301)\tD(fake) 0.1220 (0.0834)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6618 (-0.6386)\n",
            "Epoch: [7][170/195]\tTime  0.991 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0297 (0.9302)\tD(fake) 0.0045 (0.0835)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6454 (-0.6392)\n",
            "Epoch: [7][180/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.7218 (0.9302)\tD(fake) 0.1380 (0.0830)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6329 (-0.6398)\n",
            "Epoch: [7][190/195]\tTime  0.982 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.8332 (0.9303)\tD(fake) 0.1126 (0.0830)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6124 (-0.6401)\n",
            "Epoch: [8][  0/195]\tTime  1.415 ( 1.415)\tData  0.380 ( 0.380)\tD(real) 0.8747 (0.9014)\tD(fake) 0.1385 (0.0758)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6667 (-0.6667)\n",
            "Epoch: [8][ 10/195]\tTime  0.975 ( 1.027)\tData  0.000 ( 0.035)\tD(real) 0.8840 (0.9269)\tD(fake) 0.1938 (0.0857)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6583 (-0.6682)\n",
            "Epoch: [8][ 20/195]\tTime  0.996 ( 1.006)\tData  0.000 ( 0.018)\tD(real) 1.0142 (0.9270)\tD(fake) -0.0438 (0.0886)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6217 (-0.6526)\n",
            "Epoch: [8][ 30/195]\tTime  0.989 ( 1.000)\tData  0.000 ( 0.013)\tD(real) 0.7790 (0.9289)\tD(fake) 0.2064 (0.0836)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6121 (-0.6510)\n",
            "Epoch: [8][ 40/195]\tTime  0.982 ( 0.997)\tData  0.000 ( 0.010)\tD(real) 0.8292 (0.9307)\tD(fake) 0.1727 (0.0817)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6762 (-0.6473)\n",
            "Epoch: [8][ 50/195]\tTime  0.994 ( 0.996)\tData  0.000 ( 0.008)\tD(real) 0.8190 (0.9296)\tD(fake) 0.2255 (0.0839)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6120 (-0.6490)\n",
            "Epoch: [8][ 60/195]\tTime  0.979 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 0.8774 (0.9306)\tD(fake) 0.1106 (0.0825)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6478 (-0.6508)\n",
            "Epoch: [8][ 70/195]\tTime  0.985 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 0.9044 (0.9302)\tD(fake) 0.1547 (0.0835)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6813 (-0.6513)\n",
            "Epoch: [8][ 80/195]\tTime  0.977 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9006 (0.9291)\tD(fake) 0.0891 (0.0855)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6848 (-0.6519)\n",
            "Epoch: [8][ 90/195]\tTime  0.982 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9059 (0.9289)\tD(fake) 0.1158 (0.0858)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6397 (-0.6502)\n",
            "Epoch: [8][100/195]\tTime  1.010 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0110 (0.9292)\tD(fake) -0.0492 (0.0857)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6738 (-0.6501)\n",
            "Epoch: [8][110/195]\tTime  0.987 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.1104 (0.9288)\tD(fake) 0.1371 (0.0866)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6075 (-0.6487)\n",
            "Epoch: [8][120/195]\tTime  0.987 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9903 (0.9294)\tD(fake) -0.0039 (0.0852)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6706 (-0.6482)\n",
            "Epoch: [8][130/195]\tTime  0.984 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.8923 (0.9297)\tD(fake) 0.1048 (0.0848)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6296 (-0.6477)\n",
            "Epoch: [8][140/195]\tTime  0.984 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0752 (0.9301)\tD(fake) -0.0722 (0.0844)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6504 (-0.6475)\n",
            "Epoch: [8][150/195]\tTime  0.992 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9875 (0.9297)\tD(fake) 0.0231 (0.0851)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6456 (-0.6483)\n",
            "Epoch: [8][160/195]\tTime  0.987 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.7700 (0.9294)\tD(fake) 0.2424 (0.0856)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6823 (-0.6487)\n",
            "Epoch: [8][170/195]\tTime  0.985 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0748 (0.9296)\tD(fake) 0.0790 (0.0858)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6428 (-0.6488)\n",
            "Epoch: [8][180/195]\tTime  0.993 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0306 (0.9294)\tD(fake) -0.0184 (0.0860)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6512 (-0.6498)\n",
            "Epoch: [8][190/195]\tTime  1.000 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9961 (0.9295)\tD(fake) 0.0177 (0.0859)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6500 (-0.6505)\n",
            "Epoch: [9][  0/195]\tTime  1.399 ( 1.399)\tData  0.368 ( 0.368)\tD(real) 1.0030 (0.9541)\tD(fake) -0.0115 (0.0492)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6498 (-0.6498)\n",
            "Epoch: [9][ 10/195]\tTime  0.976 ( 1.022)\tData  0.000 ( 0.034)\tD(real) 1.0429 (0.9324)\tD(fake) 0.0245 (0.0845)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6515 (-0.6618)\n",
            "Epoch: [9][ 20/195]\tTime  0.981 ( 1.004)\tData  0.000 ( 0.018)\tD(real) 1.0129 (0.9305)\tD(fake) 0.0305 (0.0845)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6449 (-0.6597)\n",
            "Epoch: [9][ 30/195]\tTime  0.979 ( 0.998)\tData  0.000 ( 0.012)\tD(real) 0.9440 (0.9306)\tD(fake) 0.0010 (0.0838)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6639 (-0.6613)\n",
            "Epoch: [9][ 40/195]\tTime  0.985 ( 0.994)\tData  0.000 ( 0.009)\tD(real) 1.0313 (0.9294)\tD(fake) 0.0051 (0.0871)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7014 (-0.6680)\n",
            "Epoch: [9][ 50/195]\tTime  0.999 ( 0.994)\tData  0.000 ( 0.008)\tD(real) 0.8641 (0.9304)\tD(fake) 0.0960 (0.0855)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7064 (-0.6673)\n",
            "Epoch: [9][ 60/195]\tTime  0.989 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 1.0478 (0.9304)\tD(fake) -0.0072 (0.0853)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7105 (-0.6695)\n",
            "Epoch: [9][ 70/195]\tTime  0.984 ( 0.992)\tData  0.000 ( 0.006)\tD(real) 1.0600 (0.9311)\tD(fake) -0.0519 (0.0847)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6342 (-0.6688)\n",
            "Epoch: [9][ 80/195]\tTime  0.990 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 1.0252 (0.9305)\tD(fake) -0.0175 (0.0851)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6850 (-0.6686)\n",
            "Epoch: [9][ 90/195]\tTime  1.004 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9056 (0.9306)\tD(fake) 0.0114 (0.0838)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7091 (-0.6693)\n",
            "Epoch: [9][100/195]\tTime  0.995 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.8574 (0.9304)\tD(fake) 0.1094 (0.0841)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6838 (-0.6686)\n",
            "Epoch: [9][110/195]\tTime  0.973 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9213 (0.9307)\tD(fake) 0.0812 (0.0839)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6683 (-0.6683)\n",
            "Epoch: [9][120/195]\tTime  1.004 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0318 (0.9311)\tD(fake) 0.0107 (0.0837)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6743 (-0.6679)\n",
            "Epoch: [9][130/195]\tTime  0.991 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0656 (0.9314)\tD(fake) -0.0471 (0.0834)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6571 (-0.6683)\n",
            "Epoch: [9][140/195]\tTime  0.991 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9792 (0.9307)\tD(fake) 0.0566 (0.0848)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7237 (-0.6696)\n",
            "Epoch: [9][150/195]\tTime  0.990 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9950 (0.9308)\tD(fake) 0.0029 (0.0848)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7068 (-0.6714)\n",
            "Epoch: [9][160/195]\tTime  1.006 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9552 (0.9306)\tD(fake) 0.0393 (0.0847)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5565 (-0.6701)\n",
            "Epoch: [9][170/195]\tTime  1.000 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9859 (0.9306)\tD(fake) 0.0596 (0.0850)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6493 (-0.6709)\n",
            "Epoch: [9][180/195]\tTime  0.976 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9689 (0.9306)\tD(fake) 0.1172 (0.0850)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6758 (-0.6706)\n",
            "Epoch: [9][190/195]\tTime  0.995 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9999 (0.9305)\tD(fake) -0.0198 (0.0855)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6734 (-0.6702)\n",
            "Epoch: [10][  0/195]\tTime  1.373 ( 1.373)\tData  0.327 ( 0.327)\tD(real) 1.0549 (0.9535)\tD(fake) 0.0092 (0.0936)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7479 (-0.7479)\n",
            "Epoch: [10][ 10/195]\tTime  0.992 ( 1.026)\tData  0.000 ( 0.030)\tD(real) 1.0182 (0.9340)\tD(fake) 0.0169 (0.0871)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6807 (-0.6889)\n",
            "Epoch: [10][ 20/195]\tTime  0.990 ( 1.010)\tData  0.000 ( 0.016)\tD(real) 0.9799 (0.9325)\tD(fake) -0.0165 (0.0865)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6894 (-0.6882)\n",
            "Epoch: [10][ 30/195]\tTime  0.977 ( 1.003)\tData  0.000 ( 0.011)\tD(real) 1.1157 (0.9348)\tD(fake) -0.0079 (0.0832)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6666 (-0.6865)\n",
            "Epoch: [10][ 40/195]\tTime  0.983 ( 0.999)\tData  0.000 ( 0.008)\tD(real) 0.9650 (0.9331)\tD(fake) 0.0170 (0.0832)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6961 (-0.6876)\n",
            "Epoch: [10][ 50/195]\tTime  1.002 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 1.0062 (0.9328)\tD(fake) -0.0021 (0.0829)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7024 (-0.6840)\n",
            "Epoch: [10][ 60/195]\tTime  0.980 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 0.9541 (0.9327)\tD(fake) 0.0351 (0.0829)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6727 (-0.6829)\n",
            "Epoch: [10][ 70/195]\tTime  0.982 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 0.8404 (0.9323)\tD(fake) 0.1328 (0.0830)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6999 (-0.6824)\n",
            "Epoch: [10][ 80/195]\tTime  0.977 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.8460 (0.9329)\tD(fake) 0.1005 (0.0820)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6928 (-0.6798)\n",
            "Epoch: [10][ 90/195]\tTime  1.003 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0560 (0.9329)\tD(fake) -0.0794 (0.0823)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6764 (-0.6774)\n",
            "Epoch: [10][100/195]\tTime  0.999 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9318 (0.9330)\tD(fake) 0.0200 (0.0813)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6268 (-0.6780)\n",
            "Epoch: [10][110/195]\tTime  0.988 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0516 (0.9328)\tD(fake) -0.0652 (0.0821)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7046 (-0.6796)\n",
            "Epoch: [10][120/195]\tTime  0.977 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0006 (0.9325)\tD(fake) 0.0467 (0.0827)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6218 (-0.6778)\n",
            "Epoch: [10][130/195]\tTime  0.991 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9593 (0.9331)\tD(fake) 0.0089 (0.0812)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7255 (-0.6800)\n",
            "Epoch: [10][140/195]\tTime  0.997 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9954 (0.9331)\tD(fake) 0.0242 (0.0811)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6839 (-0.6796)\n",
            "Epoch: [10][150/195]\tTime  1.001 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0346 (0.9328)\tD(fake) 0.0207 (0.0820)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7175 (-0.6806)\n",
            "Epoch: [10][160/195]\tTime  0.994 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 0.8997 (0.9329)\tD(fake) 0.0466 (0.0814)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6384 (-0.6806)\n",
            "Epoch: [10][170/195]\tTime  0.977 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0061 (0.9329)\tD(fake) -0.0555 (0.0819)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7037 (-0.6807)\n",
            "Epoch: [10][180/195]\tTime  0.986 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9195 (0.9327)\tD(fake) 0.0634 (0.0819)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6959 (-0.6804)\n",
            "Epoch: [10][190/195]\tTime  0.978 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9555 (0.9327)\tD(fake) 0.0162 (0.0820)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6318 (-0.6808)\n",
            "Epoch: [11][  0/195]\tTime  1.400 ( 1.400)\tData  0.346 ( 0.346)\tD(real) 1.0436 (0.9306)\tD(fake) -0.0025 (0.1250)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6947 (-0.6947)\n",
            "Epoch: [11][ 10/195]\tTime  0.996 ( 1.022)\tData  0.000 ( 0.032)\tD(real) 0.9547 (0.9318)\tD(fake) 0.0382 (0.0851)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7066 (-0.6892)\n",
            "Epoch: [11][ 20/195]\tTime  0.979 ( 1.004)\tData  0.000 ( 0.017)\tD(real) 0.9173 (0.9318)\tD(fake) 0.0087 (0.0858)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6952 (-0.6896)\n",
            "Epoch: [11][ 30/195]\tTime  0.996 ( 0.998)\tData  0.000 ( 0.012)\tD(real) 0.9482 (0.9322)\tD(fake) 0.0497 (0.0857)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6679 (-0.6847)\n",
            "Epoch: [11][ 40/195]\tTime  1.002 ( 0.995)\tData  0.000 ( 0.009)\tD(real) 1.0149 (0.9317)\tD(fake) -0.0323 (0.0868)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7154 (-0.6861)\n",
            "Epoch: [11][ 50/195]\tTime  0.972 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 1.0332 (0.9328)\tD(fake) -0.0439 (0.0834)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7077 (-0.6858)\n",
            "Epoch: [11][ 60/195]\tTime  0.980 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 0.9257 (0.9321)\tD(fake) 0.0435 (0.0839)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7083 (-0.6828)\n",
            "Epoch: [11][ 70/195]\tTime  0.992 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9354 (0.9328)\tD(fake) 0.0149 (0.0826)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6986 (-0.6828)\n",
            "Epoch: [11][ 80/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0750 (0.9325)\tD(fake) -0.0530 (0.0840)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6949 (-0.6829)\n",
            "Epoch: [11][ 90/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9191 (0.9325)\tD(fake) 0.1874 (0.0841)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7145 (-0.6844)\n",
            "Epoch: [11][100/195]\tTime  1.008 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9832 (0.9325)\tD(fake) 0.0778 (0.0839)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7201 (-0.6848)\n",
            "Epoch: [11][110/195]\tTime  0.981 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.1155 (0.9327)\tD(fake) 0.0844 (0.0838)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7193 (-0.6844)\n",
            "Epoch: [11][120/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9864 (0.9324)\tD(fake) 0.0634 (0.0840)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6680 (-0.6852)\n",
            "Epoch: [11][130/195]\tTime  0.977 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0229 (0.9325)\tD(fake) 0.0448 (0.0841)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6894 (-0.6847)\n",
            "Epoch: [11][140/195]\tTime  1.001 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.7788 (0.9324)\tD(fake) 0.2287 (0.0838)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6597 (-0.6840)\n",
            "Epoch: [11][150/195]\tTime  1.002 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0641 (0.9324)\tD(fake) -0.0243 (0.0843)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6511 (-0.6844)\n",
            "Epoch: [11][160/195]\tTime  0.982 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9094 (0.9327)\tD(fake) 0.0091 (0.0833)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6275 (-0.6839)\n",
            "Epoch: [11][170/195]\tTime  0.988 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9585 (0.9327)\tD(fake) 0.0595 (0.0839)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6728 (-0.6840)\n",
            "Epoch: [11][180/195]\tTime  1.002 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9772 (0.9327)\tD(fake) 0.0535 (0.0838)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6781 (-0.6841)\n",
            "Epoch: [11][190/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9649 (0.9330)\tD(fake) 0.0173 (0.0833)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7089 (-0.6841)\n",
            "Epoch: [12][  0/195]\tTime  1.371 ( 1.371)\tData  0.334 ( 0.334)\tD(real) 1.0411 (0.9297)\tD(fake) 0.0220 (0.1243)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6526 (-0.6526)\n",
            "Epoch: [12][ 10/195]\tTime  0.998 ( 1.025)\tData  0.000 ( 0.031)\tD(real) 0.9877 (0.9289)\tD(fake) 0.0884 (0.0953)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7018 (-0.6692)\n",
            "Epoch: [12][ 20/195]\tTime  0.982 ( 1.008)\tData  0.000 ( 0.016)\tD(real) 0.8699 (0.9305)\tD(fake) 0.1226 (0.0899)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6905 (-0.6762)\n",
            "Epoch: [12][ 30/195]\tTime  0.981 ( 1.002)\tData  0.000 ( 0.011)\tD(real) 1.0079 (0.9312)\tD(fake) 0.0306 (0.0875)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6753 (-0.6779)\n",
            "Epoch: [12][ 40/195]\tTime  0.987 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 0.9610 (0.9315)\tD(fake) 0.0300 (0.0854)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6722 (-0.6802)\n",
            "Epoch: [12][ 50/195]\tTime  1.000 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 0.9705 (0.9311)\tD(fake) 0.0378 (0.0876)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6723 (-0.6818)\n",
            "Epoch: [12][ 60/195]\tTime  0.977 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 1.0485 (0.9312)\tD(fake) 0.0708 (0.0881)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6905 (-0.6843)\n",
            "Epoch: [12][ 70/195]\tTime  0.991 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0547 (0.9306)\tD(fake) 0.0912 (0.0888)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7027 (-0.6822)\n",
            "Epoch: [12][ 80/195]\tTime  0.984 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0516 (0.9310)\tD(fake) 0.0032 (0.0889)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6729 (-0.6829)\n",
            "Epoch: [12][ 90/195]\tTime  0.989 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9051 (0.9308)\tD(fake) 0.0448 (0.0886)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7211 (-0.6830)\n",
            "Epoch: [12][100/195]\tTime  1.017 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0179 (0.9307)\tD(fake) 0.0171 (0.0887)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6130 (-0.6819)\n",
            "Epoch: [12][110/195]\tTime  1.009 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9399 (0.9311)\tD(fake) 0.0128 (0.0875)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7314 (-0.6821)\n",
            "Epoch: [12][120/195]\tTime  0.994 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.8078 (0.9307)\tD(fake) 0.1999 (0.0883)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6843 (-0.6826)\n",
            "Epoch: [12][130/195]\tTime  0.978 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.7496 (0.9302)\tD(fake) 0.2085 (0.0889)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7305 (-0.6827)\n",
            "Epoch: [12][140/195]\tTime  0.997 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0030 (0.9303)\tD(fake) 0.0458 (0.0895)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6511 (-0.6821)\n",
            "Epoch: [12][150/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0537 (0.9303)\tD(fake) 0.0477 (0.0897)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6896 (-0.6824)\n",
            "Epoch: [12][160/195]\tTime  0.991 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9565 (0.9306)\tD(fake) 0.0468 (0.0886)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7190 (-0.6814)\n",
            "Epoch: [12][170/195]\tTime  0.978 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9602 (0.9305)\tD(fake) 0.0432 (0.0891)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6766 (-0.6812)\n",
            "Epoch: [12][180/195]\tTime  0.987 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.8226 (0.9302)\tD(fake) 0.1562 (0.0896)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6830 (-0.6801)\n",
            "Epoch: [12][190/195]\tTime  1.015 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0853 (0.9301)\tD(fake) 0.0500 (0.0902)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6431 (-0.6796)\n",
            "Epoch: [13][  0/195]\tTime  1.403 ( 1.403)\tData  0.365 ( 0.365)\tD(real) 0.9253 (0.9520)\tD(fake) 0.0741 (0.0117)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6778 (-0.6778)\n",
            "Epoch: [13][ 10/195]\tTime  0.996 ( 1.028)\tData  0.000 ( 0.033)\tD(real) 1.0235 (0.9337)\tD(fake) 0.0907 (0.0830)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7002 (-0.7006)\n",
            "Epoch: [13][ 20/195]\tTime  0.975 ( 1.011)\tData  0.000 ( 0.018)\tD(real) 1.0554 (0.9348)\tD(fake) -0.0588 (0.0805)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5725 (-0.6915)\n",
            "Epoch: [13][ 30/195]\tTime  0.985 ( 1.004)\tData  0.000 ( 0.012)\tD(real) 0.9699 (0.9323)\tD(fake) 0.0471 (0.0841)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6227 (-0.6828)\n",
            "Epoch: [13][ 40/195]\tTime  0.980 ( 1.000)\tData  0.000 ( 0.009)\tD(real) 1.0464 (0.9329)\tD(fake) 0.0219 (0.0825)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6855 (-0.6833)\n",
            "Epoch: [13][ 50/195]\tTime  1.000 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 0.9854 (0.9335)\tD(fake) 0.0275 (0.0828)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6923 (-0.6842)\n",
            "Epoch: [13][ 60/195]\tTime  0.976 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 0.9546 (0.9338)\tD(fake) 0.0332 (0.0826)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6882 (-0.6827)\n",
            "Epoch: [13][ 70/195]\tTime  0.995 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 1.0273 (0.9330)\tD(fake) 0.0842 (0.0854)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6914 (-0.6826)\n",
            "Epoch: [13][ 80/195]\tTime  0.981 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0113 (0.9328)\tD(fake) 0.0300 (0.0857)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6664 (-0.6828)\n",
            "Epoch: [13][ 90/195]\tTime  0.983 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.8680 (0.9323)\tD(fake) 0.2032 (0.0871)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6835 (-0.6838)\n",
            "Epoch: [13][100/195]\tTime  1.002 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.9961 (0.9327)\tD(fake) 0.0577 (0.0862)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7484 (-0.6834)\n",
            "Epoch: [13][110/195]\tTime  0.989 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0724 (0.9328)\tD(fake) 0.0493 (0.0860)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6426 (-0.6837)\n",
            "Epoch: [13][120/195]\tTime  0.993 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0299 (0.9329)\tD(fake) 0.0061 (0.0860)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6939 (-0.6832)\n",
            "Epoch: [13][130/195]\tTime  0.996 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0062 (0.9328)\tD(fake) 0.0265 (0.0859)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6582 (-0.6840)\n",
            "Epoch: [13][140/195]\tTime  0.982 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0572 (0.9323)\tD(fake) 0.0242 (0.0869)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6862 (-0.6842)\n",
            "Epoch: [13][150/195]\tTime  0.978 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9813 (0.9322)\tD(fake) -0.0023 (0.0869)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7093 (-0.6841)\n",
            "Epoch: [13][160/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0630 (0.9322)\tD(fake) 0.0520 (0.0870)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6849 (-0.6846)\n",
            "Epoch: [13][170/195]\tTime  0.982 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0451 (0.9322)\tD(fake) 0.0453 (0.0869)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7298 (-0.6842)\n",
            "Epoch: [13][180/195]\tTime  0.977 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0158 (0.9323)\tD(fake) 0.0232 (0.0870)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6745 (-0.6851)\n",
            "Epoch: [13][190/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0403 (0.9324)\tD(fake) -0.0337 (0.0870)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6578 (-0.6843)\n",
            "Epoch: [14][  0/195]\tTime  1.390 ( 1.390)\tData  0.347 ( 0.347)\tD(real) 0.9783 (0.9154)\tD(fake) 0.0373 (0.1699)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6999 (-0.6999)\n",
            "Epoch: [14][ 10/195]\tTime  0.997 ( 1.026)\tData  0.000 ( 0.032)\tD(real) 0.8961 (0.9317)\tD(fake) 0.0955 (0.0904)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6613 (-0.6761)\n",
            "Epoch: [14][ 20/195]\tTime  0.996 ( 1.007)\tData  0.000 ( 0.017)\tD(real) 0.8681 (0.9315)\tD(fake) 0.1252 (0.0898)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6853 (-0.6852)\n",
            "Epoch: [14][ 30/195]\tTime  0.983 ( 1.000)\tData  0.000 ( 0.012)\tD(real) 0.9931 (0.9323)\tD(fake) 0.0429 (0.0890)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6786 (-0.6847)\n",
            "Epoch: [14][ 40/195]\tTime  0.980 ( 0.997)\tData  0.000 ( 0.009)\tD(real) 1.1037 (0.9334)\tD(fake) 0.0090 (0.0873)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7336 (-0.6816)\n",
            "Epoch: [14][ 50/195]\tTime  0.978 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 0.9144 (0.9343)\tD(fake) 0.1266 (0.0849)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6536 (-0.6858)\n",
            "Epoch: [14][ 60/195]\tTime  0.991 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 0.9135 (0.9354)\tD(fake) 0.0442 (0.0825)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6981 (-0.6871)\n",
            "Epoch: [14][ 70/195]\tTime  0.982 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0435 (0.9347)\tD(fake) 0.0562 (0.0844)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6754 (-0.6891)\n",
            "Epoch: [14][ 80/195]\tTime  0.976 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9842 (0.9350)\tD(fake) -0.0301 (0.0830)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7154 (-0.6909)\n",
            "Epoch: [14][ 90/195]\tTime  0.977 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9011 (0.9342)\tD(fake) 0.1072 (0.0843)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6950 (-0.6900)\n",
            "Epoch: [14][100/195]\tTime  1.004 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0719 (0.9346)\tD(fake) -0.0530 (0.0840)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6875 (-0.6900)\n",
            "Epoch: [14][110/195]\tTime  0.974 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0161 (0.9349)\tD(fake) -0.0279 (0.0831)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6962 (-0.6912)\n",
            "Epoch: [14][120/195]\tTime  0.977 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9954 (0.9349)\tD(fake) -0.0188 (0.0832)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7292 (-0.6918)\n",
            "Epoch: [14][130/195]\tTime  0.986 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0181 (0.9349)\tD(fake) 0.0314 (0.0834)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6663 (-0.6899)\n",
            "Epoch: [14][140/195]\tTime  0.992 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0459 (0.9343)\tD(fake) 0.0938 (0.0848)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6957 (-0.6900)\n",
            "Epoch: [14][150/195]\tTime  0.973 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.8723 (0.9337)\tD(fake) 0.1908 (0.0859)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6605 (-0.6903)\n",
            "Epoch: [14][160/195]\tTime  0.986 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.7455 (0.9340)\tD(fake) 0.1714 (0.0852)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7004 (-0.6901)\n",
            "Epoch: [14][170/195]\tTime  1.002 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.8447 (0.9339)\tD(fake) 0.1217 (0.0850)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6228 (-0.6895)\n",
            "Epoch: [14][180/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9418 (0.9340)\tD(fake) 0.0659 (0.0848)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7209 (-0.6890)\n",
            "Epoch: [14][190/195]\tTime  0.990 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0290 (0.9340)\tD(fake) -0.0088 (0.0849)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7406 (-0.6887)\n",
            "Epoch: [15][  0/195]\tTime  1.406 ( 1.406)\tData  0.359 ( 0.359)\tD(real) 1.0220 (0.9000)\tD(fake) 0.0459 (0.1643)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7030 (-0.7030)\n",
            "Epoch: [15][ 10/195]\tTime  1.005 ( 1.029)\tData  0.000 ( 0.033)\tD(real) 1.0046 (0.9267)\tD(fake) 0.0749 (0.1033)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7064 (-0.6805)\n",
            "Epoch: [15][ 20/195]\tTime  0.976 ( 1.010)\tData  0.000 ( 0.017)\tD(real) 0.9943 (0.9299)\tD(fake) -0.0056 (0.0940)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7179 (-0.6861)\n",
            "Epoch: [15][ 30/195]\tTime  0.989 ( 1.002)\tData  0.000 ( 0.012)\tD(real) 0.9048 (0.9302)\tD(fake) 0.1123 (0.0920)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7368 (-0.6881)\n",
            "Epoch: [15][ 40/195]\tTime  0.998 ( 1.000)\tData  0.000 ( 0.009)\tD(real) 1.0536 (0.9313)\tD(fake) -0.0043 (0.0907)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6788 (-0.6869)\n",
            "Epoch: [15][ 50/195]\tTime  0.995 ( 1.000)\tData  0.000 ( 0.007)\tD(real) 0.9444 (0.9315)\tD(fake) 0.0466 (0.0906)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6847 (-0.6870)\n",
            "Epoch: [15][ 60/195]\tTime  0.991 ( 0.998)\tData  0.000 ( 0.006)\tD(real) 0.8261 (0.9312)\tD(fake) 0.1734 (0.0908)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7108 (-0.6883)\n",
            "Epoch: [15][ 70/195]\tTime  0.994 ( 0.997)\tData  0.000 ( 0.005)\tD(real) 0.9905 (0.9321)\tD(fake) 0.0242 (0.0895)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7037 (-0.6861)\n",
            "Epoch: [15][ 80/195]\tTime  0.981 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 1.0005 (0.9322)\tD(fake) 0.0246 (0.0892)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7110 (-0.6866)\n",
            "Epoch: [15][ 90/195]\tTime  0.989 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 1.0260 (0.9325)\tD(fake) -0.0151 (0.0883)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6642 (-0.6871)\n",
            "Epoch: [15][100/195]\tTime  0.994 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.8753 (0.9326)\tD(fake) 0.1718 (0.0884)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6964 (-0.6873)\n",
            "Epoch: [15][110/195]\tTime  0.991 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.9242 (0.9327)\tD(fake) 0.0784 (0.0881)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7090 (-0.6876)\n",
            "Epoch: [15][120/195]\tTime  0.985 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9515 (0.9328)\tD(fake) 0.1520 (0.0880)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6218 (-0.6857)\n",
            "Epoch: [15][130/195]\tTime  0.982 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0003 (0.9336)\tD(fake) 0.0047 (0.0867)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6816 (-0.6871)\n",
            "Epoch: [15][140/195]\tTime  0.987 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0008 (0.9334)\tD(fake) 0.0451 (0.0867)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6584 (-0.6864)\n",
            "Epoch: [15][150/195]\tTime  0.985 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9925 (0.9337)\tD(fake) 0.0914 (0.0863)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7382 (-0.6871)\n",
            "Epoch: [15][160/195]\tTime  0.992 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9313 (0.9335)\tD(fake) 0.0875 (0.0864)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7115 (-0.6881)\n",
            "Epoch: [15][170/195]\tTime  1.012 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9293 (0.9337)\tD(fake) 0.0762 (0.0863)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6676 (-0.6880)\n",
            "Epoch: [15][180/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9647 (0.9338)\tD(fake) 0.0058 (0.0862)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7103 (-0.6891)\n",
            "Epoch: [15][190/195]\tTime  0.982 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 1.0009 (0.9336)\tD(fake) 0.0817 (0.0868)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6834 (-0.6886)\n",
            "Epoch: [16][  0/195]\tTime  1.384 ( 1.384)\tData  0.336 ( 0.336)\tD(real) 0.9493 (0.9545)\tD(fake) 0.0527 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6388 (-0.6388)\n",
            "Epoch: [16][ 10/195]\tTime  0.982 ( 1.024)\tData  0.000 ( 0.031)\tD(real) 1.0417 (0.9397)\tD(fake) 0.1329 (0.0790)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7143 (-0.6931)\n",
            "Epoch: [16][ 20/195]\tTime  0.991 ( 1.007)\tData  0.000 ( 0.016)\tD(real) 0.8731 (0.9372)\tD(fake) 0.1276 (0.0826)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7159 (-0.6980)\n",
            "Epoch: [16][ 30/195]\tTime  0.994 ( 1.003)\tData  0.000 ( 0.011)\tD(real) 0.9174 (0.9367)\tD(fake) 0.0919 (0.0843)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7182 (-0.6987)\n",
            "Epoch: [16][ 40/195]\tTime  0.988 ( 1.000)\tData  0.000 ( 0.009)\tD(real) 0.9880 (0.9364)\tD(fake) 0.0140 (0.0846)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7034 (-0.6941)\n",
            "Epoch: [16][ 50/195]\tTime  0.998 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 1.0288 (0.9369)\tD(fake) 0.0512 (0.0818)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6915 (-0.6940)\n",
            "Epoch: [16][ 60/195]\tTime  0.984 ( 0.997)\tData  0.000 ( 0.006)\tD(real) 0.9281 (0.9365)\tD(fake) 0.0372 (0.0817)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7179 (-0.6971)\n",
            "Epoch: [16][ 70/195]\tTime  0.977 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 0.9541 (0.9361)\tD(fake) 0.0697 (0.0829)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6955 (-0.6977)\n",
            "Epoch: [16][ 80/195]\tTime  0.984 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9520 (0.9356)\tD(fake) 0.0159 (0.0839)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7287 (-0.6991)\n",
            "Epoch: [16][ 90/195]\tTime  0.988 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0679 (0.9353)\tD(fake) 0.0124 (0.0853)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7098 (-0.6994)\n",
            "Epoch: [16][100/195]\tTime  1.009 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.1033 (0.9352)\tD(fake) -0.0633 (0.0850)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7079 (-0.6995)\n",
            "Epoch: [16][110/195]\tTime  0.984 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0046 (0.9349)\tD(fake) 0.0328 (0.0849)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7051 (-0.6986)\n",
            "Epoch: [16][120/195]\tTime  0.981 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0707 (0.9352)\tD(fake) -0.0344 (0.0842)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6582 (-0.6981)\n",
            "Epoch: [16][130/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.8640 (0.9350)\tD(fake) 0.1335 (0.0843)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7125 (-0.6992)\n",
            "Epoch: [16][140/195]\tTime  0.998 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9773 (0.9352)\tD(fake) 0.0356 (0.0844)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6373 (-0.6988)\n",
            "Epoch: [16][150/195]\tTime  0.985 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9934 (0.9352)\tD(fake) 0.0273 (0.0846)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7361 (-0.7000)\n",
            "Epoch: [16][160/195]\tTime  0.985 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9207 (0.9350)\tD(fake) 0.0342 (0.0850)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7184 (-0.7001)\n",
            "Epoch: [16][170/195]\tTime  0.996 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9965 (0.9347)\tD(fake) 0.1347 (0.0857)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6881 (-0.7006)\n",
            "Epoch: [16][180/195]\tTime  0.978 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 0.9860 (0.9345)\tD(fake) 0.0113 (0.0861)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7252 (-0.7008)\n",
            "Epoch: [16][190/195]\tTime  0.985 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 0.8883 (0.9346)\tD(fake) 0.0561 (0.0858)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5764 (-0.6995)\n",
            "Epoch: [17][  0/195]\tTime  1.374 ( 1.374)\tData  0.334 ( 0.334)\tD(real) 1.0264 (0.9571)\tD(fake) -0.0216 (0.0571)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6540 (-0.6540)\n",
            "Epoch: [17][ 10/195]\tTime  0.991 ( 1.022)\tData  0.000 ( 0.031)\tD(real) 0.8187 (0.9274)\tD(fake) 0.2212 (0.0990)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6811 (-0.7038)\n",
            "Epoch: [17][ 20/195]\tTime  1.002 ( 1.006)\tData  0.000 ( 0.016)\tD(real) 0.9280 (0.9290)\tD(fake) 0.0860 (0.0991)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6924 (-0.6975)\n",
            "Epoch: [17][ 30/195]\tTime  0.981 ( 1.000)\tData  0.000 ( 0.011)\tD(real) 0.9535 (0.9329)\tD(fake) 0.0063 (0.0917)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6515 (-0.6980)\n",
            "Epoch: [17][ 40/195]\tTime  0.985 ( 0.997)\tData  0.000 ( 0.008)\tD(real) 1.0550 (0.9332)\tD(fake) 0.0817 (0.0911)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6898 (-0.7000)\n",
            "Epoch: [17][ 50/195]\tTime  0.997 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 0.9551 (0.9333)\tD(fake) 0.0452 (0.0910)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6842 (-0.6993)\n",
            "Epoch: [17][ 60/195]\tTime  0.971 ( 0.996)\tData  0.000 ( 0.006)\tD(real) 0.9541 (0.9343)\tD(fake) 0.0192 (0.0885)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7027 (-0.7012)\n",
            "Epoch: [17][ 70/195]\tTime  0.978 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 1.0131 (0.9338)\tD(fake) 0.0503 (0.0900)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6784 (-0.7024)\n",
            "Epoch: [17][ 80/195]\tTime  0.994 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 1.0267 (0.9346)\tD(fake) 0.0053 (0.0884)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7053 (-0.7036)\n",
            "Epoch: [17][ 90/195]\tTime  0.994 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 1.0017 (0.9345)\tD(fake) 0.0099 (0.0879)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7699 (-0.7034)\n",
            "Epoch: [17][100/195]\tTime  1.010 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0100 (0.9345)\tD(fake) 0.0009 (0.0880)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6670 (-0.7041)\n",
            "Epoch: [17][110/195]\tTime  0.987 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.8548 (0.9348)\tD(fake) 0.1165 (0.0872)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7228 (-0.7056)\n",
            "Epoch: [17][120/195]\tTime  1.000 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0203 (0.9349)\tD(fake) 0.0396 (0.0874)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6179 (-0.7046)\n",
            "Epoch: [17][130/195]\tTime  1.012 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9610 (0.9346)\tD(fake) 0.0734 (0.0878)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7133 (-0.7034)\n",
            "Epoch: [17][140/195]\tTime  0.987 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0452 (0.9348)\tD(fake) -0.0339 (0.0870)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6897 (-0.7035)\n",
            "Epoch: [17][150/195]\tTime  0.981 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9725 (0.9349)\tD(fake) 0.0750 (0.0871)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7081 (-0.7029)\n",
            "Epoch: [17][160/195]\tTime  1.014 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 1.0196 (0.9349)\tD(fake) -0.0424 (0.0870)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7332 (-0.7031)\n",
            "Epoch: [17][170/195]\tTime  0.985 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 0.9590 (0.9348)\tD(fake) 0.0885 (0.0872)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7021 (-0.7031)\n",
            "Epoch: [17][180/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9986 (0.9346)\tD(fake) -0.0523 (0.0876)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7272 (-0.7040)\n",
            "Epoch: [17][190/195]\tTime  0.988 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 1.1163 (0.9348)\tD(fake) -0.0014 (0.0871)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7550 (-0.7039)\n",
            "Epoch: [18][  0/195]\tTime  1.409 ( 1.409)\tData  0.351 ( 0.351)\tD(real) 0.9443 (0.9481)\tD(fake) 0.0474 (0.0112)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6590 (-0.6590)\n",
            "Epoch: [18][ 10/195]\tTime  0.998 ( 1.032)\tData  0.000 ( 0.032)\tD(real) 0.9220 (0.9356)\tD(fake) 0.1173 (0.0847)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7387 (-0.7019)\n",
            "Epoch: [18][ 20/195]\tTime  0.988 ( 1.010)\tData  0.000 ( 0.017)\tD(real) 1.0281 (0.9362)\tD(fake) 0.0174 (0.0882)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7174 (-0.6981)\n",
            "Epoch: [18][ 30/195]\tTime  0.997 ( 1.004)\tData  0.000 ( 0.012)\tD(real) 0.9760 (0.9375)\tD(fake) 0.0181 (0.0818)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7320 (-0.7016)\n",
            "Epoch: [18][ 40/195]\tTime  0.990 ( 1.000)\tData  0.000 ( 0.009)\tD(real) 0.9795 (0.9381)\tD(fake) 0.0008 (0.0824)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7077 (-0.7004)\n",
            "Epoch: [18][ 50/195]\tTime  0.987 ( 0.998)\tData  0.000 ( 0.007)\tD(real) 0.9575 (0.9380)\tD(fake) 0.0177 (0.0831)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6844 (-0.7012)\n",
            "Epoch: [18][ 60/195]\tTime  0.981 ( 0.997)\tData  0.000 ( 0.006)\tD(real) 0.9764 (0.9381)\tD(fake) 0.0870 (0.0833)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7017 (-0.6996)\n",
            "Epoch: [18][ 70/195]\tTime  0.983 ( 0.996)\tData  0.000 ( 0.005)\tD(real) 0.9465 (0.9382)\tD(fake) 0.0129 (0.0827)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7091 (-0.6980)\n",
            "Epoch: [18][ 80/195]\tTime  1.004 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 1.0167 (0.9382)\tD(fake) -0.0594 (0.0831)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7690 (-0.6999)\n",
            "Epoch: [18][ 90/195]\tTime  0.988 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 0.9647 (0.9380)\tD(fake) 0.0115 (0.0831)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6849 (-0.7011)\n",
            "Epoch: [18][100/195]\tTime  0.999 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 0.9477 (0.9384)\tD(fake) 0.0177 (0.0826)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6746 (-0.7026)\n",
            "Epoch: [18][110/195]\tTime  0.982 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 0.9668 (0.9378)\tD(fake) 0.0064 (0.0841)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7233 (-0.7031)\n",
            "Epoch: [18][120/195]\tTime  0.987 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0180 (0.9374)\tD(fake) -0.0117 (0.0849)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6570 (-0.7037)\n",
            "Epoch: [18][130/195]\tTime  1.001 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.8963 (0.9371)\tD(fake) 0.1124 (0.0849)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5780 (-0.7023)\n",
            "Epoch: [18][140/195]\tTime  0.977 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9944 (0.9368)\tD(fake) 0.0206 (0.0855)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7107 (-0.7025)\n",
            "Epoch: [18][150/195]\tTime  0.977 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9610 (0.9373)\tD(fake) 0.0008 (0.0843)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7337 (-0.7038)\n",
            "Epoch: [18][160/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0483 (0.9373)\tD(fake) 0.1163 (0.0845)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7118 (-0.7045)\n",
            "Epoch: [18][170/195]\tTime  0.991 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9519 (0.9374)\tD(fake) 0.0620 (0.0842)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7492 (-0.7056)\n",
            "Epoch: [18][180/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 1.0196 (0.9373)\tD(fake) 0.0797 (0.0845)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7427 (-0.7059)\n",
            "Epoch: [18][190/195]\tTime  0.979 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 0.8918 (0.9375)\tD(fake) 0.1201 (0.0839)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7426 (-0.7056)\n",
            "Epoch: [19][  0/195]\tTime  1.457 ( 1.457)\tData  0.391 ( 0.391)\tD(real) 1.0074 (0.9157)\tD(fake) 0.0365 (0.1452)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6862 (-0.6862)\n",
            "Epoch: [19][ 10/195]\tTime  0.985 ( 1.032)\tData  0.000 ( 0.036)\tD(real) 1.0076 (0.9356)\tD(fake) 0.0103 (0.0894)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7042 (-0.7047)\n",
            "Epoch: [19][ 20/195]\tTime  0.979 ( 1.011)\tData  0.000 ( 0.019)\tD(real) 0.9615 (0.9365)\tD(fake) 0.0407 (0.0885)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7026 (-0.7156)\n",
            "Epoch: [19][ 30/195]\tTime  0.990 ( 1.004)\tData  0.000 ( 0.013)\tD(real) 0.9700 (0.9364)\tD(fake) 0.0059 (0.0876)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7105 (-0.7175)\n",
            "Epoch: [19][ 40/195]\tTime  1.004 ( 1.000)\tData  0.000 ( 0.010)\tD(real) 0.9379 (0.9363)\tD(fake) 0.0178 (0.0872)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7138 (-0.7186)\n",
            "Epoch: [19][ 50/195]\tTime  0.979 ( 0.997)\tData  0.000 ( 0.008)\tD(real) 1.0055 (0.9356)\tD(fake) 0.1272 (0.0890)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7416 (-0.7176)\n",
            "Epoch: [19][ 60/195]\tTime  1.000 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 0.9618 (0.9363)\tD(fake) 0.0213 (0.0872)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7190 (-0.7171)\n",
            "Epoch: [19][ 70/195]\tTime  0.977 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0182 (0.9361)\tD(fake) 0.0229 (0.0882)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7150 (-0.7150)\n",
            "Epoch: [19][ 80/195]\tTime  0.983 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 0.8689 (0.9357)\tD(fake) 0.1424 (0.0882)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7197 (-0.7147)\n",
            "Epoch: [19][ 90/195]\tTime  0.974 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 0.9390 (0.9361)\tD(fake) 0.0594 (0.0881)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7459 (-0.7167)\n",
            "Epoch: [19][100/195]\tTime  0.995 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.9270 (0.9364)\tD(fake) 0.0489 (0.0869)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7095 (-0.7153)\n",
            "Epoch: [19][110/195]\tTime  1.003 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0358 (0.9368)\tD(fake) 0.0210 (0.0864)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7318 (-0.7162)\n",
            "Epoch: [19][120/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9866 (0.9366)\tD(fake) 0.0331 (0.0863)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6992 (-0.7161)\n",
            "Epoch: [19][130/195]\tTime  1.001 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9385 (0.9370)\tD(fake) 0.0560 (0.0852)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7061 (-0.7150)\n",
            "Epoch: [19][140/195]\tTime  0.971 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0074 (0.9371)\tD(fake) 0.0894 (0.0851)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7202 (-0.7149)\n",
            "Epoch: [19][150/195]\tTime  0.995 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9964 (0.9371)\tD(fake) 0.0666 (0.0852)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6808 (-0.7129)\n",
            "Epoch: [19][160/195]\tTime  0.988 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9991 (0.9374)\tD(fake) 0.0136 (0.0847)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7275 (-0.7124)\n",
            "Epoch: [19][170/195]\tTime  0.982 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.8944 (0.9376)\tD(fake) 0.1431 (0.0840)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7512 (-0.7124)\n",
            "Epoch: [19][180/195]\tTime  0.981 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0203 (0.9373)\tD(fake) 0.0664 (0.0852)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7198 (-0.7125)\n",
            "Epoch: [19][190/195]\tTime  0.982 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 1.0250 (0.9375)\tD(fake) 0.0404 (0.0849)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7331 (-0.7125)\n",
            "Epoch: [20][  0/195]\tTime  1.412 ( 1.412)\tData  0.351 ( 0.351)\tD(real) 1.0197 (0.9597)\tD(fake) -0.0456 (0.0584)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7068 (-0.7068)\n",
            "Epoch: [20][ 10/195]\tTime  0.998 ( 1.022)\tData  0.000 ( 0.032)\tD(real) 0.9121 (0.9424)\tD(fake) 0.0506 (0.0731)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7241 (-0.7028)\n",
            "Epoch: [20][ 20/195]\tTime  0.993 ( 1.005)\tData  0.000 ( 0.017)\tD(real) 1.0421 (0.9421)\tD(fake) -0.0174 (0.0771)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7244 (-0.7061)\n",
            "Epoch: [20][ 30/195]\tTime  0.980 ( 0.999)\tData  0.000 ( 0.012)\tD(real) 1.0113 (0.9411)\tD(fake) -0.0032 (0.0792)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6887 (-0.7084)\n",
            "Epoch: [20][ 40/195]\tTime  0.990 ( 0.996)\tData  0.000 ( 0.009)\tD(real) 0.9935 (0.9409)\tD(fake) -0.0188 (0.0797)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7406 (-0.7100)\n",
            "Epoch: [20][ 50/195]\tTime  0.997 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 0.9930 (0.9400)\tD(fake) 0.0187 (0.0811)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7081 (-0.7119)\n",
            "Epoch: [20][ 60/195]\tTime  0.981 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 0.9522 (0.9399)\tD(fake) 0.0056 (0.0813)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7081 (-0.7140)\n",
            "Epoch: [20][ 70/195]\tTime  0.978 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 0.9592 (0.9400)\tD(fake) 0.0034 (0.0810)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7292 (-0.7149)\n",
            "Epoch: [20][ 80/195]\tTime  0.990 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 0.9645 (0.9393)\tD(fake) 0.0883 (0.0829)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6669 (-0.7126)\n",
            "Epoch: [20][ 90/195]\tTime  1.004 ( 0.991)\tData  0.001 ( 0.004)\tD(real) 1.0273 (0.9392)\tD(fake) -0.0604 (0.0833)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6623 (-0.7126)\n",
            "Epoch: [20][100/195]\tTime  1.008 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9599 (0.9392)\tD(fake) 0.0208 (0.0825)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6823 (-0.7118)\n",
            "Epoch: [20][110/195]\tTime  0.987 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0406 (0.9393)\tD(fake) -0.0327 (0.0829)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6838 (-0.7117)\n",
            "Epoch: [20][120/195]\tTime  0.976 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9496 (0.9393)\tD(fake) 0.0219 (0.0828)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6098 (-0.7111)\n",
            "Epoch: [20][130/195]\tTime  0.998 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0611 (0.9392)\tD(fake) 0.0832 (0.0832)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7275 (-0.7131)\n",
            "Epoch: [20][140/195]\tTime  0.982 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9578 (0.9391)\tD(fake) 0.0942 (0.0833)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6859 (-0.7128)\n",
            "Epoch: [20][150/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9926 (0.9388)\tD(fake) 0.1406 (0.0841)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7137 (-0.7130)\n",
            "Epoch: [20][160/195]\tTime  1.005 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9124 (0.9386)\tD(fake) 0.0619 (0.0840)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7399 (-0.7134)\n",
            "Epoch: [20][170/195]\tTime  0.995 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9618 (0.9385)\tD(fake) 0.0142 (0.0839)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7338 (-0.7135)\n",
            "Epoch: [20][180/195]\tTime  0.994 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.7763 (0.9381)\tD(fake) 0.2061 (0.0849)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7569 (-0.7133)\n",
            "Epoch: [20][190/195]\tTime  0.985 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0270 (0.9382)\tD(fake) -0.0027 (0.0851)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7260 (-0.7141)\n",
            "Epoch: [21][  0/195]\tTime  1.416 ( 1.416)\tData  0.340 ( 0.340)\tD(real) 1.0095 (0.9171)\tD(fake) 0.0641 (0.1404)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6859 (-0.6859)\n",
            "Epoch: [21][ 10/195]\tTime  0.985 ( 1.029)\tData  0.000 ( 0.031)\tD(real) 1.0298 (0.9369)\tD(fake) 0.0251 (0.0913)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7262 (-0.7075)\n",
            "Epoch: [21][ 20/195]\tTime  0.997 ( 1.011)\tData  0.000 ( 0.016)\tD(real) 0.9357 (0.9394)\tD(fake) 0.0539 (0.0823)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6602 (-0.7116)\n",
            "Epoch: [21][ 30/195]\tTime  0.981 ( 1.002)\tData  0.000 ( 0.011)\tD(real) 1.0287 (0.9380)\tD(fake) 0.0928 (0.0856)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7254 (-0.7123)\n",
            "Epoch: [21][ 40/195]\tTime  0.984 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 0.8850 (0.9382)\tD(fake) 0.0968 (0.0845)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6928 (-0.7091)\n",
            "Epoch: [21][ 50/195]\tTime  0.979 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 1.0140 (0.9386)\tD(fake) -0.0645 (0.0849)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6424 (-0.7117)\n",
            "Epoch: [21][ 60/195]\tTime  0.986 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 0.9391 (0.9385)\tD(fake) 0.0858 (0.0852)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7894 (-0.7138)\n",
            "Epoch: [21][ 70/195]\tTime  0.988 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 1.0268 (0.9389)\tD(fake) 0.0813 (0.0850)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7315 (-0.7134)\n",
            "Epoch: [21][ 80/195]\tTime  0.978 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9761 (0.9388)\tD(fake) -0.0010 (0.0848)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7016 (-0.7145)\n",
            "Epoch: [21][ 90/195]\tTime  0.987 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0283 (0.9386)\tD(fake) 0.0322 (0.0853)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7627 (-0.7169)\n",
            "Epoch: [21][100/195]\tTime  0.997 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0293 (0.9390)\tD(fake) 0.0152 (0.0843)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6867 (-0.7157)\n",
            "Epoch: [21][110/195]\tTime  0.991 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9805 (0.9386)\tD(fake) 0.1137 (0.0853)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7058 (-0.7155)\n",
            "Epoch: [21][120/195]\tTime  0.987 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9401 (0.9384)\tD(fake) 0.1000 (0.0859)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6844 (-0.7152)\n",
            "Epoch: [21][130/195]\tTime  0.982 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0234 (0.9388)\tD(fake) 0.0627 (0.0852)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7399 (-0.7158)\n",
            "Epoch: [21][140/195]\tTime  0.996 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9654 (0.9391)\tD(fake) -0.0028 (0.0844)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6912 (-0.7151)\n",
            "Epoch: [21][150/195]\tTime  0.986 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0133 (0.9387)\tD(fake) 0.1253 (0.0851)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7232 (-0.7151)\n",
            "Epoch: [21][160/195]\tTime  0.969 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.8925 (0.9383)\tD(fake) 0.1514 (0.0857)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6302 (-0.7148)\n",
            "Epoch: [21][170/195]\tTime  0.982 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9716 (0.9388)\tD(fake) 0.0196 (0.0845)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7626 (-0.7149)\n",
            "Epoch: [21][180/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9459 (0.9389)\tD(fake) 0.0205 (0.0843)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7156 (-0.7145)\n",
            "Epoch: [21][190/195]\tTime  0.988 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9546 (0.9385)\tD(fake) 0.0675 (0.0850)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7334 (-0.7150)\n",
            "Epoch: [22][  0/195]\tTime  1.445 ( 1.445)\tData  0.392 ( 0.392)\tD(real) 1.0267 (0.9367)\tD(fake) 0.0280 (0.1119)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6966 (-0.6966)\n",
            "Epoch: [22][ 10/195]\tTime  0.984 ( 1.033)\tData  0.000 ( 0.036)\tD(real) 0.8645 (0.9395)\tD(fake) 0.1427 (0.0830)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7245 (-0.7026)\n",
            "Epoch: [22][ 20/195]\tTime  0.973 ( 1.012)\tData  0.000 ( 0.019)\tD(real) 1.0331 (0.9435)\tD(fake) -0.0369 (0.0764)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7335 (-0.7079)\n",
            "Epoch: [22][ 30/195]\tTime  0.977 ( 1.005)\tData  0.000 ( 0.013)\tD(real) 1.0194 (0.9408)\tD(fake) 0.0671 (0.0818)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6773 (-0.7052)\n",
            "Epoch: [22][ 40/195]\tTime  0.993 ( 1.002)\tData  0.000 ( 0.010)\tD(real) 0.9966 (0.9387)\tD(fake) 0.0748 (0.0857)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6533 (-0.7060)\n",
            "Epoch: [22][ 50/195]\tTime  0.997 ( 0.998)\tData  0.000 ( 0.008)\tD(real) 1.0140 (0.9393)\tD(fake) 0.0074 (0.0838)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7040 (-0.7081)\n",
            "Epoch: [22][ 60/195]\tTime  1.003 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 1.0425 (0.9378)\tD(fake) 0.0487 (0.0880)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7140 (-0.7094)\n",
            "Epoch: [22][ 70/195]\tTime  0.980 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0445 (0.9384)\tD(fake) 0.0262 (0.0866)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6864 (-0.7089)\n",
            "Epoch: [22][ 80/195]\tTime  0.982 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 1.0125 (0.9386)\tD(fake) 0.0656 (0.0854)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7160 (-0.7092)\n",
            "Epoch: [22][ 90/195]\tTime  0.992 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0278 (0.9385)\tD(fake) 0.0779 (0.0859)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7280 (-0.7118)\n",
            "Epoch: [22][100/195]\tTime  0.996 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9319 (0.9392)\tD(fake) 0.0828 (0.0843)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7285 (-0.7129)\n",
            "Epoch: [22][110/195]\tTime  0.982 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0349 (0.9395)\tD(fake) 0.0935 (0.0838)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7108 (-0.7109)\n",
            "Epoch: [22][120/195]\tTime  0.976 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9900 (0.9395)\tD(fake) 0.0124 (0.0837)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7291 (-0.7118)\n",
            "Epoch: [22][130/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0322 (0.9393)\tD(fake) -0.0067 (0.0845)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7392 (-0.7125)\n",
            "Epoch: [22][140/195]\tTime  0.989 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0025 (0.9391)\tD(fake) 0.1092 (0.0848)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7455 (-0.7138)\n",
            "Epoch: [22][150/195]\tTime  0.988 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0552 (0.9394)\tD(fake) 0.0805 (0.0843)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7130 (-0.7145)\n",
            "Epoch: [22][160/195]\tTime  0.989 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0059 (0.9393)\tD(fake) 0.0751 (0.0842)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6474 (-0.7146)\n",
            "Epoch: [22][170/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9988 (0.9394)\tD(fake) 0.0820 (0.0841)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7719 (-0.7141)\n",
            "Epoch: [22][180/195]\tTime  0.986 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.8322 (0.9392)\tD(fake) 0.2224 (0.0844)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7485 (-0.7152)\n",
            "Epoch: [22][190/195]\tTime  1.000 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9935 (0.9393)\tD(fake) 0.0232 (0.0843)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7023 (-0.7148)\n",
            "Epoch: [23][  0/195]\tTime  1.425 ( 1.425)\tData  0.386 ( 0.386)\tD(real) 0.9841 (0.9339)\tD(fake) 0.0428 (0.1358)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7006 (-0.7006)\n",
            "Epoch: [23][ 10/195]\tTime  0.984 ( 1.030)\tData  0.000 ( 0.035)\tD(real) 0.9451 (0.9406)\tD(fake) 0.0885 (0.0899)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7407 (-0.7173)\n",
            "Epoch: [23][ 20/195]\tTime  0.973 ( 1.009)\tData  0.000 ( 0.019)\tD(real) 0.9722 (0.9394)\tD(fake) 0.0677 (0.0892)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6554 (-0.7236)\n",
            "Epoch: [23][ 30/195]\tTime  0.983 ( 1.003)\tData  0.000 ( 0.013)\tD(real) 0.7808 (0.9372)\tD(fake) 0.1876 (0.0929)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7410 (-0.7220)\n",
            "Epoch: [23][ 40/195]\tTime  0.996 ( 1.000)\tData  0.000 ( 0.010)\tD(real) 1.0419 (0.9396)\tD(fake) -0.0304 (0.0883)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7246 (-0.7208)\n",
            "Epoch: [23][ 50/195]\tTime  0.981 ( 0.997)\tData  0.000 ( 0.008)\tD(real) 0.9802 (0.9395)\tD(fake) -0.0410 (0.0874)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7188 (-0.7217)\n",
            "Epoch: [23][ 60/195]\tTime  0.978 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 0.9741 (0.9396)\tD(fake) 0.0150 (0.0872)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7237 (-0.7222)\n",
            "Epoch: [23][ 70/195]\tTime  0.988 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 0.8673 (0.9398)\tD(fake) 0.1544 (0.0864)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7188 (-0.7236)\n",
            "Epoch: [23][ 80/195]\tTime  0.999 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9898 (0.9397)\tD(fake) 0.0314 (0.0867)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7482 (-0.7242)\n",
            "Epoch: [23][ 90/195]\tTime  0.978 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0363 (0.9398)\tD(fake) 0.0805 (0.0868)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7278 (-0.7245)\n",
            "Epoch: [23][100/195]\tTime  1.002 ( 0.992)\tData  0.001 ( 0.004)\tD(real) 1.0263 (0.9397)\tD(fake) -0.0441 (0.0867)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7434 (-0.7234)\n",
            "Epoch: [23][110/195]\tTime  0.991 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0405 (0.9400)\tD(fake) -0.0178 (0.0862)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7423 (-0.7232)\n",
            "Epoch: [23][120/195]\tTime  0.994 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0369 (0.9404)\tD(fake) 0.0555 (0.0851)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5881 (-0.7216)\n",
            "Epoch: [23][130/195]\tTime  0.998 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9140 (0.9405)\tD(fake) 0.0681 (0.0842)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6843 (-0.7210)\n",
            "Epoch: [23][140/195]\tTime  1.002 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0433 (0.9408)\tD(fake) 0.1146 (0.0838)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6804 (-0.7199)\n",
            "Epoch: [23][150/195]\tTime  0.978 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.8503 (0.9406)\tD(fake) 0.2006 (0.0841)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7352 (-0.7196)\n",
            "Epoch: [23][160/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9746 (0.9406)\tD(fake) 0.0732 (0.0843)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7251 (-0.7189)\n",
            "Epoch: [23][170/195]\tTime  0.988 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9466 (0.9409)\tD(fake) -0.0044 (0.0835)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7308 (-0.7188)\n",
            "Epoch: [23][180/195]\tTime  0.978 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9989 (0.9407)\tD(fake) -0.0154 (0.0840)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7235 (-0.7187)\n",
            "Epoch: [23][190/195]\tTime  0.989 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9432 (0.9405)\tD(fake) 0.0109 (0.0840)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7281 (-0.7189)\n",
            "Epoch: [24][  0/195]\tTime  1.398 ( 1.398)\tData  0.359 ( 0.359)\tD(real) 1.0157 (0.9358)\tD(fake) 0.0475 (0.1222)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7702 (-0.7702)\n",
            "Epoch: [24][ 10/195]\tTime  0.980 ( 1.027)\tData  0.000 ( 0.033)\tD(real) 1.0589 (0.9395)\tD(fake) 0.0884 (0.0894)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7656 (-0.7427)\n",
            "Epoch: [24][ 20/195]\tTime  1.000 ( 1.007)\tData  0.000 ( 0.018)\tD(real) 0.9296 (0.9397)\tD(fake) 0.0696 (0.0853)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7119 (-0.7328)\n",
            "Epoch: [24][ 30/195]\tTime  0.980 ( 0.999)\tData  0.000 ( 0.012)\tD(real) 1.0153 (0.9393)\tD(fake) 0.0492 (0.0863)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7372 (-0.7286)\n",
            "Epoch: [24][ 40/195]\tTime  0.980 ( 0.995)\tData  0.000 ( 0.009)\tD(real) 0.9174 (0.9413)\tD(fake) 0.0772 (0.0819)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7437 (-0.7233)\n",
            "Epoch: [24][ 50/195]\tTime  0.994 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 1.0062 (0.9412)\tD(fake) -0.0472 (0.0815)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7211 (-0.7257)\n",
            "Epoch: [24][ 60/195]\tTime  0.979 ( 0.992)\tData  0.000 ( 0.006)\tD(real) 1.0310 (0.9409)\tD(fake) 0.0602 (0.0826)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7144 (-0.7242)\n",
            "Epoch: [24][ 70/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 1.0185 (0.9414)\tD(fake) -0.0272 (0.0815)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7093 (-0.7240)\n",
            "Epoch: [24][ 80/195]\tTime  0.984 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 1.0120 (0.9406)\tD(fake) 0.0435 (0.0827)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7384 (-0.7212)\n",
            "Epoch: [24][ 90/195]\tTime  0.977 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9078 (0.9409)\tD(fake) 0.1290 (0.0820)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7187 (-0.7217)\n",
            "Epoch: [24][100/195]\tTime  0.996 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0227 (0.9409)\tD(fake) 0.0331 (0.0822)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7465 (-0.7215)\n",
            "Epoch: [24][110/195]\tTime  1.009 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0271 (0.9412)\tD(fake) -0.0774 (0.0814)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6687 (-0.7209)\n",
            "Epoch: [24][120/195]\tTime  0.989 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9387 (0.9416)\tD(fake) 0.0241 (0.0807)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6610 (-0.7215)\n",
            "Epoch: [24][130/195]\tTime  0.985 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0167 (0.9416)\tD(fake) -0.0828 (0.0810)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6799 (-0.7213)\n",
            "Epoch: [24][140/195]\tTime  0.976 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0361 (0.9421)\tD(fake) -0.0101 (0.0799)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7271 (-0.7215)\n",
            "Epoch: [24][150/195]\tTime  0.979 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0029 (0.9422)\tD(fake) -0.0158 (0.0798)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7229 (-0.7219)\n",
            "Epoch: [24][160/195]\tTime  0.994 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9831 (0.9426)\tD(fake) 0.0496 (0.0791)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6707 (-0.7227)\n",
            "Epoch: [24][170/195]\tTime  0.999 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9640 (0.9428)\tD(fake) 0.0143 (0.0785)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7757 (-0.7239)\n",
            "Epoch: [24][180/195]\tTime  0.994 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0275 (0.9429)\tD(fake) -0.0037 (0.0783)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7247 (-0.7239)\n",
            "Epoch: [24][190/195]\tTime  0.987 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9635 (0.9426)\tD(fake) 0.0475 (0.0791)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7477 (-0.7235)\n",
            "Epoch: [25][  0/195]\tTime  1.392 ( 1.392)\tData  0.354 ( 0.354)\tD(real) 1.0315 (0.9280)\tD(fake) -0.0407 (0.1261)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7252 (-0.7252)\n",
            "Epoch: [25][ 10/195]\tTime  0.972 ( 1.025)\tData  0.000 ( 0.033)\tD(real) 0.8721 (0.9432)\tD(fake) 0.1534 (0.0801)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7150 (-0.7076)\n",
            "Epoch: [25][ 20/195]\tTime  0.982 ( 1.006)\tData  0.000 ( 0.017)\tD(real) 0.9983 (0.9426)\tD(fake) -0.0016 (0.0829)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7435 (-0.7209)\n",
            "Epoch: [25][ 30/195]\tTime  0.981 ( 1.001)\tData  0.000 ( 0.012)\tD(real) 1.0340 (0.9426)\tD(fake) -0.0640 (0.0791)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7223 (-0.7164)\n",
            "Epoch: [25][ 40/195]\tTime  0.990 ( 0.997)\tData  0.000 ( 0.009)\tD(real) 0.9702 (0.9436)\tD(fake) 0.0029 (0.0765)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6913 (-0.7139)\n",
            "Epoch: [25][ 50/195]\tTime  0.977 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 0.9079 (0.9423)\tD(fake) 0.1526 (0.0797)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7086 (-0.7144)\n",
            "Epoch: [25][ 60/195]\tTime  0.995 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 1.0351 (0.9430)\tD(fake) -0.0315 (0.0787)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7249 (-0.7154)\n",
            "Epoch: [25][ 70/195]\tTime  0.999 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9689 (0.9422)\tD(fake) 0.0134 (0.0802)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7469 (-0.7150)\n",
            "Epoch: [25][ 80/195]\tTime  0.983 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9743 (0.9422)\tD(fake) 0.0106 (0.0804)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7276 (-0.7170)\n",
            "Epoch: [25][ 90/195]\tTime  0.996 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0137 (0.9415)\tD(fake) 0.0690 (0.0823)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7227 (-0.7177)\n",
            "Epoch: [25][100/195]\tTime  0.995 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9851 (0.9418)\tD(fake) -0.0309 (0.0810)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7342 (-0.7184)\n",
            "Epoch: [25][110/195]\tTime  0.978 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0184 (0.9419)\tD(fake) 0.0514 (0.0809)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7201 (-0.7188)\n",
            "Epoch: [25][120/195]\tTime  0.994 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9872 (0.9418)\tD(fake) 0.0912 (0.0813)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7397 (-0.7193)\n",
            "Epoch: [25][130/195]\tTime  0.981 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0659 (0.9421)\tD(fake) -0.0266 (0.0808)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6723 (-0.7181)\n",
            "Epoch: [25][140/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.8652 (0.9418)\tD(fake) 0.1129 (0.0811)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6643 (-0.7180)\n",
            "Epoch: [25][150/195]\tTime  0.976 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9837 (0.9421)\tD(fake) 0.0267 (0.0803)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7136 (-0.7181)\n",
            "Epoch: [25][160/195]\tTime  0.983 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.8012 (0.9415)\tD(fake) 0.2024 (0.0818)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7288 (-0.7186)\n",
            "Epoch: [25][170/195]\tTime  0.980 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9114 (0.9417)\tD(fake) 0.1203 (0.0814)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7396 (-0.7193)\n",
            "Epoch: [25][180/195]\tTime  0.992 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9560 (0.9417)\tD(fake) 0.0238 (0.0815)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7085 (-0.7193)\n",
            "Epoch: [25][190/195]\tTime  0.976 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9568 (0.9417)\tD(fake) 0.0533 (0.0817)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7649 (-0.7195)\n",
            "Epoch: [26][  0/195]\tTime  1.420 ( 1.420)\tData  0.366 ( 0.366)\tD(real) 1.0695 (0.9230)\tD(fake) 0.0730 (0.1413)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6484 (-0.6484)\n",
            "Epoch: [26][ 10/195]\tTime  0.993 ( 1.030)\tData  0.000 ( 0.034)\tD(real) 0.9944 (0.9453)\tD(fake) 0.0665 (0.0778)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7075 (-0.7030)\n",
            "Epoch: [26][ 20/195]\tTime  0.998 ( 1.012)\tData  0.000 ( 0.018)\tD(real) 0.9830 (0.9445)\tD(fake) 0.1189 (0.0789)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7218 (-0.7164)\n",
            "Epoch: [26][ 30/195]\tTime  0.992 ( 1.004)\tData  0.000 ( 0.012)\tD(real) 1.0101 (0.9450)\tD(fake) 0.0215 (0.0768)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7586 (-0.7212)\n",
            "Epoch: [26][ 40/195]\tTime  0.994 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 1.0054 (0.9444)\tD(fake) 0.0967 (0.0764)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7396 (-0.7156)\n",
            "Epoch: [26][ 50/195]\tTime  1.002 ( 0.997)\tData  0.000 ( 0.008)\tD(real) 0.9684 (0.9441)\tD(fake) 0.0232 (0.0770)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7383 (-0.7160)\n",
            "Epoch: [26][ 60/195]\tTime  0.980 ( 0.996)\tData  0.001 ( 0.006)\tD(real) 0.9527 (0.9444)\tD(fake) 0.0264 (0.0766)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7188 (-0.7191)\n",
            "Epoch: [26][ 70/195]\tTime  0.977 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0168 (0.9441)\tD(fake) 0.1179 (0.0776)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7617 (-0.7203)\n",
            "Epoch: [26][ 80/195]\tTime  0.985 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0520 (0.9442)\tD(fake) -0.0066 (0.0779)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6870 (-0.7191)\n",
            "Epoch: [26][ 90/195]\tTime  0.978 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9427 (0.9443)\tD(fake) 0.0385 (0.0774)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7630 (-0.7196)\n",
            "Epoch: [26][100/195]\tTime  1.019 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0125 (0.9435)\tD(fake) -0.0301 (0.0794)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7382 (-0.7195)\n",
            "Epoch: [26][110/195]\tTime  0.988 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.8739 (0.9434)\tD(fake) 0.1848 (0.0792)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7390 (-0.7207)\n",
            "Epoch: [26][120/195]\tTime  0.984 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0468 (0.9437)\tD(fake) 0.0305 (0.0791)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6715 (-0.7210)\n",
            "Epoch: [26][130/195]\tTime  0.975 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0178 (0.9438)\tD(fake) -0.0257 (0.0786)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7088 (-0.7212)\n",
            "Epoch: [26][140/195]\tTime  0.976 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0155 (0.9438)\tD(fake) -0.0420 (0.0784)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7127 (-0.7207)\n",
            "Epoch: [26][150/195]\tTime  0.974 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9591 (0.9436)\tD(fake) -0.0076 (0.0786)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7243 (-0.7221)\n",
            "Epoch: [26][160/195]\tTime  1.002 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9873 (0.9438)\tD(fake) 0.0015 (0.0782)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6518 (-0.7219)\n",
            "Epoch: [26][170/195]\tTime  0.992 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0115 (0.9437)\tD(fake) -0.0255 (0.0783)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7430 (-0.7225)\n",
            "Epoch: [26][180/195]\tTime  0.987 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0303 (0.9437)\tD(fake) 0.0950 (0.0786)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7789 (-0.7232)\n",
            "Epoch: [26][190/195]\tTime  0.973 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0232 (0.9437)\tD(fake) 0.0015 (0.0787)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6917 (-0.7220)\n",
            "Epoch: [27][  0/195]\tTime  1.377 ( 1.377)\tData  0.347 ( 0.347)\tD(real) 0.9844 (0.9791)\tD(fake) 0.0077 (0.0125)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7493 (-0.7493)\n",
            "Epoch: [27][ 10/195]\tTime  0.984 ( 1.023)\tData  0.000 ( 0.032)\tD(real) 0.8910 (0.9454)\tD(fake) 0.2005 (0.0780)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7514 (-0.7492)\n",
            "Epoch: [27][ 20/195]\tTime  0.986 ( 1.006)\tData  0.000 ( 0.017)\tD(real) 0.8951 (0.9450)\tD(fake) 0.0534 (0.0770)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7334 (-0.7308)\n",
            "Epoch: [27][ 30/195]\tTime  0.982 ( 1.000)\tData  0.000 ( 0.012)\tD(real) 0.8434 (0.9422)\tD(fake) 0.2030 (0.0851)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7338 (-0.7341)\n",
            "Epoch: [27][ 40/195]\tTime  0.995 ( 0.996)\tData  0.000 ( 0.009)\tD(real) 1.0144 (0.9420)\tD(fake) 0.0731 (0.0851)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7317 (-0.7256)\n",
            "Epoch: [27][ 50/195]\tTime  0.996 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 0.9402 (0.9421)\tD(fake) 0.1141 (0.0838)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6896 (-0.7230)\n",
            "Epoch: [27][ 60/195]\tTime  0.989 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 1.0100 (0.9422)\tD(fake) 0.0346 (0.0829)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6479 (-0.7213)\n",
            "Epoch: [27][ 70/195]\tTime  0.980 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0085 (0.9424)\tD(fake) 0.0926 (0.0826)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7612 (-0.7223)\n",
            "Epoch: [27][ 80/195]\tTime  0.987 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 1.0007 (0.9422)\tD(fake) 0.0385 (0.0825)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7331 (-0.7218)\n",
            "Epoch: [27][ 90/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0302 (0.9426)\tD(fake) 0.1102 (0.0820)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7000 (-0.7222)\n",
            "Epoch: [27][100/195]\tTime  1.001 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9398 (0.9426)\tD(fake) 0.0685 (0.0818)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6870 (-0.7227)\n",
            "Epoch: [27][110/195]\tTime  0.985 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0473 (0.9430)\tD(fake) 0.0373 (0.0809)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7708 (-0.7207)\n",
            "Epoch: [27][120/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9812 (0.9431)\tD(fake) 0.0544 (0.0802)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7468 (-0.7217)\n",
            "Epoch: [27][130/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9623 (0.9433)\tD(fake) 0.0906 (0.0799)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7116 (-0.7227)\n",
            "Epoch: [27][140/195]\tTime  0.975 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9404 (0.9434)\tD(fake) 0.1053 (0.0796)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6349 (-0.7218)\n",
            "Epoch: [27][150/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9716 (0.9437)\tD(fake) -0.0089 (0.0787)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7099 (-0.7211)\n",
            "Epoch: [27][160/195]\tTime  0.972 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0527 (0.9437)\tD(fake) 0.0107 (0.0789)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6716 (-0.7203)\n",
            "Epoch: [27][170/195]\tTime  0.980 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.8381 (0.9434)\tD(fake) 0.1720 (0.0793)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7333 (-0.7203)\n",
            "Epoch: [27][180/195]\tTime  0.977 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0253 (0.9438)\tD(fake) -0.0230 (0.0787)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7548 (-0.7202)\n",
            "Epoch: [27][190/195]\tTime  0.979 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9014 (0.9437)\tD(fake) 0.1391 (0.0787)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7233 (-0.7205)\n",
            "Epoch: [28][  0/195]\tTime  1.379 ( 1.379)\tData  0.341 ( 0.341)\tD(real) 0.8140 (0.9120)\tD(fake) 0.1382 (0.1101)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7131 (-0.7131)\n",
            "Epoch: [28][ 10/195]\tTime  0.998 ( 1.024)\tData  0.000 ( 0.031)\tD(real) 1.0467 (0.9407)\tD(fake) 0.0461 (0.0842)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7121 (-0.7036)\n",
            "Epoch: [28][ 20/195]\tTime  0.995 ( 1.009)\tData  0.000 ( 0.017)\tD(real) 1.0094 (0.9405)\tD(fake) 0.1096 (0.0852)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6823 (-0.7166)\n",
            "Epoch: [28][ 30/195]\tTime  0.984 ( 1.002)\tData  0.000 ( 0.011)\tD(real) 0.9866 (0.9413)\tD(fake) -0.0097 (0.0842)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7348 (-0.7248)\n",
            "Epoch: [28][ 40/195]\tTime  0.986 ( 0.998)\tData  0.000 ( 0.009)\tD(real) 1.0596 (0.9428)\tD(fake) -0.0609 (0.0820)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5882 (-0.7244)\n",
            "Epoch: [28][ 50/195]\tTime  1.001 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 0.9397 (0.9433)\tD(fake) 0.0001 (0.0796)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7244 (-0.7232)\n",
            "Epoch: [28][ 60/195]\tTime  0.995 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 1.0002 (0.9429)\tD(fake) 0.0008 (0.0815)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7324 (-0.7236)\n",
            "Epoch: [28][ 70/195]\tTime  0.997 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9773 (0.9428)\tD(fake) -0.0056 (0.0808)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7618 (-0.7228)\n",
            "Epoch: [28][ 80/195]\tTime  0.989 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9939 (0.9421)\tD(fake) 0.0707 (0.0828)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6586 (-0.7230)\n",
            "Epoch: [28][ 90/195]\tTime  1.003 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.8147 (0.9414)\tD(fake) 0.2608 (0.0836)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7187 (-0.7217)\n",
            "Epoch: [28][100/195]\tTime  1.003 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9668 (0.9409)\tD(fake) 0.0323 (0.0853)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7639 (-0.7240)\n",
            "Epoch: [28][110/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9689 (0.9408)\tD(fake) 0.0612 (0.0854)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7211 (-0.7250)\n",
            "Epoch: [28][120/195]\tTime  0.978 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0479 (0.9411)\tD(fake) 0.0131 (0.0850)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6920 (-0.7248)\n",
            "Epoch: [28][130/195]\tTime  0.995 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9448 (0.9408)\tD(fake) 0.0056 (0.0853)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6906 (-0.7246)\n",
            "Epoch: [28][140/195]\tTime  0.978 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9392 (0.9410)\tD(fake) 0.0654 (0.0851)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7012 (-0.7249)\n",
            "Epoch: [28][150/195]\tTime  0.978 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9864 (0.9410)\tD(fake) 0.1058 (0.0854)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7614 (-0.7254)\n",
            "Epoch: [28][160/195]\tTime  1.009 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9929 (0.9412)\tD(fake) 0.1137 (0.0850)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6425 (-0.7246)\n",
            "Epoch: [28][170/195]\tTime  0.993 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9891 (0.9412)\tD(fake) 0.0599 (0.0849)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7440 (-0.7252)\n",
            "Epoch: [28][180/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0041 (0.9413)\tD(fake) -0.0503 (0.0846)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7804 (-0.7250)\n",
            "Epoch: [28][190/195]\tTime  0.980 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9439 (0.9413)\tD(fake) 0.1201 (0.0843)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7158 (-0.7253)\n",
            "Epoch: [29][  0/195]\tTime  1.416 ( 1.416)\tData  0.362 ( 0.362)\tD(real) 0.9144 (0.9341)\tD(fake) 0.1272 (0.0725)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7201 (-0.7201)\n",
            "Epoch: [29][ 10/195]\tTime  0.996 ( 1.027)\tData  0.000 ( 0.033)\tD(real) 1.0312 (0.9397)\tD(fake) -0.0042 (0.0849)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7379 (-0.7235)\n",
            "Epoch: [29][ 20/195]\tTime  1.008 ( 1.011)\tData  0.000 ( 0.018)\tD(real) 0.9621 (0.9390)\tD(fake) 0.0479 (0.0852)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.5977 (-0.7135)\n",
            "Epoch: [29][ 30/195]\tTime  0.976 ( 1.002)\tData  0.000 ( 0.012)\tD(real) 1.0398 (0.9401)\tD(fake) 0.0696 (0.0867)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6872 (-0.7140)\n",
            "Epoch: [29][ 40/195]\tTime  0.985 ( 0.998)\tData  0.000 ( 0.009)\tD(real) 0.9433 (0.9399)\tD(fake) 0.0638 (0.0869)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7288 (-0.7177)\n",
            "Epoch: [29][ 50/195]\tTime  0.980 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 0.9802 (0.9403)\tD(fake) 0.0407 (0.0862)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7289 (-0.7148)\n",
            "Epoch: [29][ 60/195]\tTime  0.995 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0362 (0.9401)\tD(fake) -0.0560 (0.0871)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7191 (-0.7167)\n",
            "Epoch: [29][ 70/195]\tTime  0.983 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 0.9068 (0.9400)\tD(fake) 0.1540 (0.0863)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7540 (-0.7177)\n",
            "Epoch: [29][ 80/195]\tTime  0.979 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9477 (0.9411)\tD(fake) 0.0206 (0.0846)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7090 (-0.7184)\n",
            "Epoch: [29][ 90/195]\tTime  0.986 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9256 (0.9410)\tD(fake) 0.1465 (0.0848)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7152 (-0.7173)\n",
            "Epoch: [29][100/195]\tTime  1.003 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0303 (0.9416)\tD(fake) -0.0329 (0.0844)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7440 (-0.7191)\n",
            "Epoch: [29][110/195]\tTime  0.998 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0455 (0.9420)\tD(fake) 0.0102 (0.0839)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7015 (-0.7204)\n",
            "Epoch: [29][120/195]\tTime  0.976 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0512 (0.9420)\tD(fake) 0.0348 (0.0836)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7088 (-0.7213)\n",
            "Epoch: [29][130/195]\tTime  0.997 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0128 (0.9418)\tD(fake) -0.0004 (0.0840)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7311 (-0.7218)\n",
            "Epoch: [29][140/195]\tTime  0.972 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9652 (0.9421)\tD(fake) 0.0085 (0.0833)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6951 (-0.7213)\n",
            "Epoch: [29][150/195]\tTime  0.978 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9554 (0.9418)\tD(fake) 0.0414 (0.0842)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7274 (-0.7215)\n",
            "Epoch: [29][160/195]\tTime  0.990 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9260 (0.9419)\tD(fake) 0.0996 (0.0840)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7351 (-0.7219)\n",
            "Epoch: [29][170/195]\tTime  0.974 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0162 (0.9422)\tD(fake) 0.0138 (0.0838)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7549 (-0.7224)\n",
            "Epoch: [29][180/195]\tTime  0.987 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9820 (0.9422)\tD(fake) -0.0115 (0.0831)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7440 (-0.7231)\n",
            "Epoch: [29][190/195]\tTime  0.995 ( 0.990)\tData  0.001 ( 0.002)\tD(real) 1.0170 (0.9426)\tD(fake) -0.0391 (0.0824)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7288 (-0.7239)\n",
            "Epoch: [30][  0/195]\tTime  1.373 ( 1.373)\tData  0.329 ( 0.329)\tD(real) 0.9832 (0.9324)\tD(fake) 0.0191 (0.1399)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7235 (-0.7235)\n",
            "Epoch: [30][ 10/195]\tTime  0.979 ( 1.022)\tData  0.000 ( 0.030)\tD(real) 0.9766 (0.9455)\tD(fake) 0.0912 (0.0802)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7883 (-0.7391)\n",
            "Epoch: [30][ 20/195]\tTime  0.991 ( 1.004)\tData  0.000 ( 0.016)\tD(real) 1.0352 (0.9487)\tD(fake) 0.0276 (0.0751)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8031 (-0.7413)\n",
            "Epoch: [30][ 30/195]\tTime  0.979 ( 1.000)\tData  0.000 ( 0.011)\tD(real) 1.0089 (0.9504)\tD(fake) -0.0273 (0.0711)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7849 (-0.7484)\n",
            "Epoch: [30][ 40/195]\tTime  0.972 ( 0.996)\tData  0.000 ( 0.008)\tD(real) 0.9772 (0.9509)\tD(fake) -0.0002 (0.0694)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7847 (-0.7555)\n",
            "Epoch: [30][ 50/195]\tTime  0.977 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 1.0449 (0.9510)\tD(fake) 0.0217 (0.0701)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7814 (-0.7556)\n",
            "Epoch: [30][ 60/195]\tTime  0.990 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 0.9094 (0.9519)\tD(fake) 0.1012 (0.0673)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7819 (-0.7589)\n",
            "Epoch: [30][ 70/195]\tTime  0.980 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 0.9229 (0.9525)\tD(fake) 0.0809 (0.0661)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8067 (-0.7601)\n",
            "Epoch: [30][ 80/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0167 (0.9528)\tD(fake) -0.0285 (0.0655)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7760 (-0.7604)\n",
            "Epoch: [30][ 90/195]\tTime  0.977 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.8706 (0.9529)\tD(fake) 0.1974 (0.0650)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6837 (-0.7604)\n",
            "Epoch: [30][100/195]\tTime  0.995 ( 0.989)\tData  0.000 ( 0.004)\tD(real) 1.0196 (0.9535)\tD(fake) 0.0530 (0.0647)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7383 (-0.7608)\n",
            "Epoch: [30][110/195]\tTime  1.001 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0096 (0.9534)\tD(fake) 0.0553 (0.0649)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7751 (-0.7612)\n",
            "Epoch: [30][120/195]\tTime  0.986 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0148 (0.9534)\tD(fake) 0.0906 (0.0652)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7923 (-0.7626)\n",
            "Epoch: [30][130/195]\tTime  0.978 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9771 (0.9537)\tD(fake) 0.0205 (0.0642)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7993 (-0.7638)\n",
            "Epoch: [30][140/195]\tTime  0.978 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9691 (0.9539)\tD(fake) 0.1016 (0.0637)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7937 (-0.7639)\n",
            "Epoch: [30][150/195]\tTime  0.990 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9633 (0.9541)\tD(fake) 0.0002 (0.0632)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7772 (-0.7647)\n",
            "Epoch: [30][160/195]\tTime  0.982 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0197 (0.9542)\tD(fake) -0.0460 (0.0631)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7701 (-0.7646)\n",
            "Epoch: [30][170/195]\tTime  0.981 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9196 (0.9546)\tD(fake) -0.0028 (0.0623)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8101 (-0.7647)\n",
            "Epoch: [30][180/195]\tTime  0.979 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0137 (0.9546)\tD(fake) -0.0155 (0.0625)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8092 (-0.7654)\n",
            "Epoch: [30][190/195]\tTime  0.991 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0613 (0.9547)\tD(fake) 0.0899 (0.0622)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7645 (-0.7659)\n",
            "Epoch: [31][  0/195]\tTime  1.368 ( 1.368)\tData  0.323 ( 0.323)\tD(real) 0.9034 (0.9457)\tD(fake) 0.1813 (0.0463)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8083 (-0.8083)\n",
            "Epoch: [31][ 10/195]\tTime  0.985 ( 1.020)\tData  0.000 ( 0.030)\tD(real) 0.9766 (0.9589)\tD(fake) -0.0073 (0.0480)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7617 (-0.7813)\n",
            "Epoch: [31][ 20/195]\tTime  0.987 ( 1.006)\tData  0.000 ( 0.016)\tD(real) 0.9152 (0.9579)\tD(fake) 0.0333 (0.0525)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7660 (-0.7724)\n",
            "Epoch: [31][ 30/195]\tTime  1.001 ( 1.000)\tData  0.000 ( 0.011)\tD(real) 0.9144 (0.9578)\tD(fake) 0.0317 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7799 (-0.7720)\n",
            "Epoch: [31][ 40/195]\tTime  1.001 ( 0.998)\tData  0.000 ( 0.008)\tD(real) 1.0103 (0.9576)\tD(fake) 0.0195 (0.0566)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7708 (-0.7710)\n",
            "Epoch: [31][ 50/195]\tTime  0.987 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 0.9625 (0.9580)\tD(fake) 0.1040 (0.0562)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8009 (-0.7729)\n",
            "Epoch: [31][ 60/195]\tTime  0.983 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 0.9788 (0.9579)\tD(fake) 0.0599 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7786 (-0.7741)\n",
            "Epoch: [31][ 70/195]\tTime  0.982 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 0.9250 (0.9578)\tD(fake) 0.0660 (0.0571)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7467 (-0.7733)\n",
            "Epoch: [31][ 80/195]\tTime  0.985 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.8361 (0.9571)\tD(fake) 0.2117 (0.0583)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7766 (-0.7739)\n",
            "Epoch: [31][ 90/195]\tTime  0.978 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0747 (0.9575)\tD(fake) 0.0111 (0.0584)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7934 (-0.7759)\n",
            "Epoch: [31][100/195]\tTime  1.008 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0312 (0.9576)\tD(fake) 0.0477 (0.0581)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7798 (-0.7762)\n",
            "Epoch: [31][110/195]\tTime  0.982 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0527 (0.9578)\tD(fake) -0.0214 (0.0576)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7070 (-0.7750)\n",
            "Epoch: [31][120/195]\tTime  0.987 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.8816 (0.9574)\tD(fake) 0.1005 (0.0581)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7881 (-0.7747)\n",
            "Epoch: [31][130/195]\tTime  0.989 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0217 (0.9575)\tD(fake) 0.0321 (0.0583)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7994 (-0.7751)\n",
            "Epoch: [31][140/195]\tTime  0.985 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0239 (0.9573)\tD(fake) 0.0377 (0.0587)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7611 (-0.7755)\n",
            "Epoch: [31][150/195]\tTime  0.981 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 0.8481 (0.9573)\tD(fake) 0.1181 (0.0583)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7546 (-0.7751)\n",
            "Epoch: [31][160/195]\tTime  0.982 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9814 (0.9571)\tD(fake) -0.0065 (0.0590)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7587 (-0.7748)\n",
            "Epoch: [31][170/195]\tTime  0.995 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9800 (0.9572)\tD(fake) 0.0045 (0.0588)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7750 (-0.7743)\n",
            "Epoch: [31][180/195]\tTime  0.993 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9054 (0.9570)\tD(fake) 0.0846 (0.0593)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7876 (-0.7750)\n",
            "Epoch: [31][190/195]\tTime  0.978 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.8226 (0.9570)\tD(fake) 0.2126 (0.0592)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8010 (-0.7750)\n",
            "Epoch: [32][  0/195]\tTime  1.386 ( 1.386)\tData  0.335 ( 0.335)\tD(real) 1.0201 (0.9753)\tD(fake) 0.0413 (0.0338)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6586 (-0.6586)\n",
            "Epoch: [32][ 10/195]\tTime  0.992 ( 1.023)\tData  0.000 ( 0.031)\tD(real) 1.0263 (0.9589)\tD(fake) -0.0352 (0.0604)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7909 (-0.7726)\n",
            "Epoch: [32][ 20/195]\tTime  0.981 ( 1.007)\tData  0.000 ( 0.016)\tD(real) 0.9692 (0.9586)\tD(fake) 0.0719 (0.0593)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7635 (-0.7702)\n",
            "Epoch: [32][ 30/195]\tTime  0.989 ( 0.999)\tData  0.000 ( 0.011)\tD(real) 0.9842 (0.9579)\tD(fake) 0.0385 (0.0600)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7893 (-0.7703)\n",
            "Epoch: [32][ 40/195]\tTime  0.989 ( 0.996)\tData  0.000 ( 0.009)\tD(real) 1.0316 (0.9574)\tD(fake) 0.0332 (0.0609)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7707 (-0.7750)\n",
            "Epoch: [32][ 50/195]\tTime  0.980 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 1.0529 (0.9577)\tD(fake) -0.0213 (0.0597)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7804 (-0.7762)\n",
            "Epoch: [32][ 60/195]\tTime  0.990 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 0.9447 (0.9570)\tD(fake) 0.0200 (0.0595)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8074 (-0.7745)\n",
            "Epoch: [32][ 70/195]\tTime  0.982 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0124 (0.9572)\tD(fake) -0.0599 (0.0590)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7909 (-0.7742)\n",
            "Epoch: [32][ 80/195]\tTime  0.984 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0277 (0.9572)\tD(fake) 0.0020 (0.0592)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7651 (-0.7761)\n",
            "Epoch: [32][ 90/195]\tTime  0.995 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0040 (0.9570)\tD(fake) 0.0506 (0.0598)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7926 (-0.7757)\n",
            "Epoch: [32][100/195]\tTime  1.023 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0782 (0.9573)\tD(fake) -0.0059 (0.0593)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7659 (-0.7761)\n",
            "Epoch: [32][110/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9482 (0.9575)\tD(fake) -0.0321 (0.0582)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7793 (-0.7765)\n",
            "Epoch: [32][120/195]\tTime  0.987 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.8684 (0.9574)\tD(fake) 0.0602 (0.0582)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7931 (-0.7763)\n",
            "Epoch: [32][130/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9686 (0.9571)\tD(fake) 0.0375 (0.0591)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7869 (-0.7771)\n",
            "Epoch: [32][140/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9438 (0.9573)\tD(fake) 0.0007 (0.0586)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7515 (-0.7777)\n",
            "Epoch: [32][150/195]\tTime  0.978 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.8968 (0.9573)\tD(fake) 0.0776 (0.0590)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8012 (-0.7790)\n",
            "Epoch: [32][160/195]\tTime  0.974 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0684 (0.9575)\tD(fake) 0.0017 (0.0589)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7929 (-0.7792)\n",
            "Epoch: [32][170/195]\tTime  0.984 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0117 (0.9574)\tD(fake) 0.0614 (0.0592)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7475 (-0.7797)\n",
            "Epoch: [32][180/195]\tTime  0.975 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0414 (0.9574)\tD(fake) 0.0733 (0.0591)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7783 (-0.7792)\n",
            "Epoch: [32][190/195]\tTime  0.987 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0329 (0.9574)\tD(fake) 0.0214 (0.0592)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8136 (-0.7799)\n",
            "Epoch: [33][  0/195]\tTime  1.399 ( 1.399)\tData  0.365 ( 0.365)\tD(real) 0.9427 (0.9691)\tD(fake) 0.0192 (0.0087)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8035 (-0.8035)\n",
            "Epoch: [33][ 10/195]\tTime  0.996 ( 1.022)\tData  0.000 ( 0.034)\tD(real) 1.0331 (0.9598)\tD(fake) -0.0109 (0.0551)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7533 (-0.7880)\n",
            "Epoch: [33][ 20/195]\tTime  0.986 ( 1.004)\tData  0.000 ( 0.018)\tD(real) 1.0699 (0.9608)\tD(fake) 0.0719 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7960 (-0.7900)\n",
            "Epoch: [33][ 30/195]\tTime  0.980 ( 0.999)\tData  0.000 ( 0.012)\tD(real) 0.9927 (0.9583)\tD(fake) 0.0149 (0.0601)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7449 (-0.7896)\n",
            "Epoch: [33][ 40/195]\tTime  0.994 ( 0.996)\tData  0.000 ( 0.009)\tD(real) 0.9081 (0.9580)\tD(fake) 0.1030 (0.0602)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7109 (-0.7888)\n",
            "Epoch: [33][ 50/195]\tTime  0.982 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 1.0296 (0.9583)\tD(fake) 0.0028 (0.0608)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8027 (-0.7896)\n",
            "Epoch: [33][ 60/195]\tTime  0.991 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 0.8861 (0.9578)\tD(fake) 0.1453 (0.0610)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7950 (-0.7897)\n",
            "Epoch: [33][ 70/195]\tTime  0.993 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9382 (0.9583)\tD(fake) 0.0190 (0.0595)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8011 (-0.7897)\n",
            "Epoch: [33][ 80/195]\tTime  0.993 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9826 (0.9584)\tD(fake) 0.0203 (0.0594)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7424 (-0.7862)\n",
            "Epoch: [33][ 90/195]\tTime  0.985 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9861 (0.9581)\tD(fake) -0.0363 (0.0599)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7774 (-0.7847)\n",
            "Epoch: [33][100/195]\tTime  1.001 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9406 (0.9582)\tD(fake) 0.0565 (0.0595)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7568 (-0.7847)\n",
            "Epoch: [33][110/195]\tTime  0.987 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0079 (0.9581)\tD(fake) 0.0413 (0.0600)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8205 (-0.7858)\n",
            "Epoch: [33][120/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0448 (0.9584)\tD(fake) 0.0172 (0.0595)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7790 (-0.7850)\n",
            "Epoch: [33][130/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0159 (0.9583)\tD(fake) 0.0218 (0.0596)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7863 (-0.7849)\n",
            "Epoch: [33][140/195]\tTime  0.998 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0361 (0.9583)\tD(fake) 0.0914 (0.0595)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7781 (-0.7841)\n",
            "Epoch: [33][150/195]\tTime  0.986 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0031 (0.9582)\tD(fake) 0.0237 (0.0595)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8007 (-0.7839)\n",
            "Epoch: [33][160/195]\tTime  0.971 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0137 (0.9581)\tD(fake) 0.0254 (0.0593)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8192 (-0.7843)\n",
            "Epoch: [33][170/195]\tTime  0.987 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9626 (0.9581)\tD(fake) 0.0789 (0.0590)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7061 (-0.7840)\n",
            "Epoch: [33][180/195]\tTime  0.997 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0062 (0.9581)\tD(fake) 0.0072 (0.0590)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7916 (-0.7841)\n",
            "Epoch: [33][190/195]\tTime  0.977 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.8970 (0.9580)\tD(fake) 0.1072 (0.0592)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7862 (-0.7842)\n",
            "Epoch: [34][  0/195]\tTime  1.386 ( 1.386)\tData  0.340 ( 0.340)\tD(real) 0.9357 (0.9511)\tD(fake) 0.0485 (0.0181)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7730 (-0.7730)\n",
            "Epoch: [34][ 10/195]\tTime  0.997 ( 1.027)\tData  0.000 ( 0.031)\tD(real) 0.9144 (0.9575)\tD(fake) 0.0748 (0.0558)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8041 (-0.7725)\n",
            "Epoch: [34][ 20/195]\tTime  0.990 ( 1.008)\tData  0.000 ( 0.017)\tD(real) 1.0138 (0.9579)\tD(fake) 0.0260 (0.0584)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7327 (-0.7662)\n",
            "Epoch: [34][ 30/195]\tTime  0.984 ( 1.003)\tData  0.000 ( 0.011)\tD(real) 0.9093 (0.9579)\tD(fake) 0.0801 (0.0575)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7482 (-0.7665)\n",
            "Epoch: [34][ 40/195]\tTime  0.997 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 0.9859 (0.9582)\tD(fake) 0.1010 (0.0580)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8034 (-0.7711)\n",
            "Epoch: [34][ 50/195]\tTime  1.000 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 0.9104 (0.9577)\tD(fake) 0.0941 (0.0591)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7715 (-0.7754)\n",
            "Epoch: [34][ 60/195]\tTime  0.991 ( 0.996)\tData  0.000 ( 0.006)\tD(real) 0.9713 (0.9581)\tD(fake) -0.0102 (0.0581)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8153 (-0.7768)\n",
            "Epoch: [34][ 70/195]\tTime  0.983 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 0.9303 (0.9582)\tD(fake) 0.0465 (0.0575)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7762 (-0.7774)\n",
            "Epoch: [34][ 80/195]\tTime  0.992 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0005 (0.9571)\tD(fake) -0.0085 (0.0606)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7589 (-0.7775)\n",
            "Epoch: [34][ 90/195]\tTime  0.995 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0090 (0.9573)\tD(fake) 0.0540 (0.0601)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7436 (-0.7772)\n",
            "Epoch: [34][100/195]\tTime  0.998 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.9316 (0.9575)\tD(fake) 0.0229 (0.0593)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7854 (-0.7774)\n",
            "Epoch: [34][110/195]\tTime  1.009 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9671 (0.9576)\tD(fake) 0.0189 (0.0592)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7765 (-0.7776)\n",
            "Epoch: [34][120/195]\tTime  0.978 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9546 (0.9578)\tD(fake) 0.0227 (0.0592)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7918 (-0.7789)\n",
            "Epoch: [34][130/195]\tTime  0.976 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0049 (0.9576)\tD(fake) 0.0177 (0.0603)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8023 (-0.7791)\n",
            "Epoch: [34][140/195]\tTime  0.985 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0157 (0.9574)\tD(fake) 0.0583 (0.0605)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7802 (-0.7800)\n",
            "Epoch: [34][150/195]\tTime  1.002 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0273 (0.9575)\tD(fake) 0.0104 (0.0600)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7490 (-0.7801)\n",
            "Epoch: [34][160/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.8873 (0.9576)\tD(fake) 0.0596 (0.0598)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7636 (-0.7801)\n",
            "Epoch: [34][170/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0303 (0.9577)\tD(fake) 0.0132 (0.0599)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8302 (-0.7799)\n",
            "Epoch: [34][180/195]\tTime  1.000 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9413 (0.9578)\tD(fake) 0.0605 (0.0593)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7970 (-0.7800)\n",
            "Epoch: [34][190/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9080 (0.9577)\tD(fake) 0.1129 (0.0596)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7664 (-0.7800)\n",
            "Epoch: [35][  0/195]\tTime  1.393 ( 1.393)\tData  0.326 ( 0.326)\tD(real) 0.9848 (0.9543)\tD(fake) 0.0171 (0.0845)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8024 (-0.8024)\n",
            "Epoch: [35][ 10/195]\tTime  1.002 ( 1.028)\tData  0.000 ( 0.030)\tD(real) 0.9813 (0.9631)\tD(fake) 0.0391 (0.0518)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7716 (-0.7892)\n",
            "Epoch: [35][ 20/195]\tTime  0.974 ( 1.010)\tData  0.000 ( 0.016)\tD(real) 0.9943 (0.9599)\tD(fake) 0.0285 (0.0572)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8156 (-0.7918)\n",
            "Epoch: [35][ 30/195]\tTime  0.983 ( 1.004)\tData  0.000 ( 0.011)\tD(real) 1.0399 (0.9607)\tD(fake) -0.0225 (0.0553)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7534 (-0.7925)\n",
            "Epoch: [35][ 40/195]\tTime  0.993 ( 1.000)\tData  0.000 ( 0.008)\tD(real) 0.9800 (0.9598)\tD(fake) 0.0142 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7968 (-0.7876)\n",
            "Epoch: [35][ 50/195]\tTime  0.991 ( 0.998)\tData  0.000 ( 0.007)\tD(real) 1.0087 (0.9597)\tD(fake) -0.0094 (0.0575)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8125 (-0.7885)\n",
            "Epoch: [35][ 60/195]\tTime  0.989 ( 0.996)\tData  0.000 ( 0.006)\tD(real) 0.9003 (0.9595)\tD(fake) 0.1302 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8160 (-0.7895)\n",
            "Epoch: [35][ 70/195]\tTime  0.980 ( 0.996)\tData  0.000 ( 0.005)\tD(real) 1.0156 (0.9593)\tD(fake) 0.0263 (0.0577)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7859 (-0.7901)\n",
            "Epoch: [35][ 80/195]\tTime  0.986 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 0.9365 (0.9590)\tD(fake) 0.0678 (0.0578)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8133 (-0.7891)\n",
            "Epoch: [35][ 90/195]\tTime  0.998 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 1.0125 (0.9589)\tD(fake) 0.0051 (0.0579)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7791 (-0.7887)\n",
            "Epoch: [35][100/195]\tTime  1.023 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 1.0282 (0.9585)\tD(fake) 0.0594 (0.0589)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8245 (-0.7857)\n",
            "Epoch: [35][110/195]\tTime  0.987 ( 0.995)\tData  0.000 ( 0.003)\tD(real) 0.9688 (0.9589)\tD(fake) 0.0071 (0.0577)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7740 (-0.7856)\n",
            "Epoch: [35][120/195]\tTime  0.999 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 1.0558 (0.9592)\tD(fake) -0.0243 (0.0578)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7947 (-0.7857)\n",
            "Epoch: [35][130/195]\tTime  0.987 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 0.9362 (0.9588)\tD(fake) 0.0490 (0.0581)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7995 (-0.7850)\n",
            "Epoch: [35][140/195]\tTime  0.984 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0576 (0.9590)\tD(fake) 0.0574 (0.0577)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8221 (-0.7843)\n",
            "Epoch: [35][150/195]\tTime  1.000 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.8257 (0.9589)\tD(fake) 0.1072 (0.0578)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7706 (-0.7829)\n",
            "Epoch: [35][160/195]\tTime  0.984 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 1.0026 (0.9590)\tD(fake) -0.0438 (0.0575)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7745 (-0.7826)\n",
            "Epoch: [35][170/195]\tTime  0.989 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 1.0196 (0.9590)\tD(fake) -0.0106 (0.0576)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8102 (-0.7834)\n",
            "Epoch: [35][180/195]\tTime  0.975 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9608 (0.9590)\tD(fake) 0.0177 (0.0574)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7876 (-0.7841)\n",
            "Epoch: [35][190/195]\tTime  0.990 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9823 (0.9592)\tD(fake) -0.0229 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7977 (-0.7841)\n",
            "Epoch: [36][  0/195]\tTime  1.419 ( 1.419)\tData  0.367 ( 0.367)\tD(real) 1.0313 (0.9749)\tD(fake) -0.0157 (0.0412)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8170 (-0.8170)\n",
            "Epoch: [36][ 10/195]\tTime  0.994 ( 1.027)\tData  0.000 ( 0.034)\tD(real) 1.0544 (0.9615)\tD(fake) 0.0454 (0.0562)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7777 (-0.7895)\n",
            "Epoch: [36][ 20/195]\tTime  1.000 ( 1.011)\tData  0.000 ( 0.018)\tD(real) 0.9612 (0.9603)\tD(fake) -0.0056 (0.0566)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7874 (-0.7853)\n",
            "Epoch: [36][ 30/195]\tTime  1.002 ( 1.004)\tData  0.000 ( 0.012)\tD(real) 1.0097 (0.9594)\tD(fake) 0.0027 (0.0591)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7634 (-0.7876)\n",
            "Epoch: [36][ 40/195]\tTime  0.986 ( 1.000)\tData  0.000 ( 0.009)\tD(real) 1.0143 (0.9594)\tD(fake) 0.0014 (0.0584)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7796 (-0.7866)\n",
            "Epoch: [36][ 50/195]\tTime  0.988 ( 0.998)\tData  0.000 ( 0.008)\tD(real) 0.9434 (0.9594)\tD(fake) 0.0326 (0.0574)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7851 (-0.7869)\n",
            "Epoch: [36][ 60/195]\tTime  0.975 ( 0.996)\tData  0.000 ( 0.006)\tD(real) 1.0444 (0.9596)\tD(fake) 0.0597 (0.0574)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8150 (-0.7851)\n",
            "Epoch: [36][ 70/195]\tTime  0.990 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0528 (0.9596)\tD(fake) -0.0628 (0.0570)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8257 (-0.7851)\n",
            "Epoch: [36][ 80/195]\tTime  0.998 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 0.9779 (0.9595)\tD(fake) -0.0331 (0.0570)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7727 (-0.7836)\n",
            "Epoch: [36][ 90/195]\tTime  0.981 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 1.0157 (0.9597)\tD(fake) 0.0658 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7769 (-0.7832)\n",
            "Epoch: [36][100/195]\tTime  1.014 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 1.0623 (0.9600)\tD(fake) 0.0349 (0.0557)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7872 (-0.7834)\n",
            "Epoch: [36][110/195]\tTime  0.979 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0092 (0.9599)\tD(fake) -0.0010 (0.0559)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7988 (-0.7824)\n",
            "Epoch: [36][120/195]\tTime  0.992 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 1.0306 (0.9601)\tD(fake) 0.0559 (0.0555)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7965 (-0.7816)\n",
            "Epoch: [36][130/195]\tTime  0.980 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0506 (0.9602)\tD(fake) 0.0470 (0.0554)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7676 (-0.7826)\n",
            "Epoch: [36][140/195]\tTime  0.978 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9746 (0.9603)\tD(fake) 0.0472 (0.0553)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7947 (-0.7828)\n",
            "Epoch: [36][150/195]\tTime  1.003 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.8759 (0.9602)\tD(fake) 0.1571 (0.0557)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7612 (-0.7828)\n",
            "Epoch: [36][160/195]\tTime  0.999 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9482 (0.9603)\tD(fake) 0.0833 (0.0560)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7937 (-0.7829)\n",
            "Epoch: [36][170/195]\tTime  0.993 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9418 (0.9602)\tD(fake) -0.0147 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7954 (-0.7828)\n",
            "Epoch: [36][180/195]\tTime  0.991 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9633 (0.9603)\tD(fake) 0.0066 (0.0558)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7631 (-0.7828)\n",
            "Epoch: [36][190/195]\tTime  0.980 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9203 (0.9603)\tD(fake) 0.0751 (0.0558)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8030 (-0.7827)\n",
            "Epoch: [37][  0/195]\tTime  1.432 ( 1.432)\tData  0.382 ( 0.382)\tD(real) 0.9396 (0.9632)\tD(fake) 0.0667 (0.0224)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7538 (-0.7538)\n",
            "Epoch: [37][ 10/195]\tTime  1.002 ( 1.033)\tData  0.000 ( 0.035)\tD(real) 0.9277 (0.9583)\tD(fake) 0.0541 (0.0566)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8191 (-0.7797)\n",
            "Epoch: [37][ 20/195]\tTime  0.979 ( 1.012)\tData  0.000 ( 0.019)\tD(real) 0.9010 (0.9598)\tD(fake) 0.0503 (0.0549)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7898 (-0.7752)\n",
            "Epoch: [37][ 30/195]\tTime  0.996 ( 1.003)\tData  0.000 ( 0.013)\tD(real) 0.9711 (0.9600)\tD(fake) -0.0032 (0.0582)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8009 (-0.7781)\n",
            "Epoch: [37][ 40/195]\tTime  0.990 ( 0.999)\tData  0.000 ( 0.010)\tD(real) 1.0325 (0.9607)\tD(fake) 0.0474 (0.0576)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8184 (-0.7813)\n",
            "Epoch: [37][ 50/195]\tTime  0.984 ( 0.996)\tData  0.000 ( 0.008)\tD(real) 1.0257 (0.9609)\tD(fake) 0.0095 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7112 (-0.7746)\n",
            "Epoch: [37][ 60/195]\tTime  0.993 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 0.9760 (0.9608)\tD(fake) -0.0122 (0.0559)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8009 (-0.7760)\n",
            "Epoch: [37][ 70/195]\tTime  0.977 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 1.0404 (0.9610)\tD(fake) -0.0094 (0.0554)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8024 (-0.7761)\n",
            "Epoch: [37][ 80/195]\tTime  0.989 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0493 (0.9604)\tD(fake) 0.0587 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7694 (-0.7768)\n",
            "Epoch: [37][ 90/195]\tTime  1.006 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0486 (0.9604)\tD(fake) 0.0367 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7960 (-0.7794)\n",
            "Epoch: [37][100/195]\tTime  1.000 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9519 (0.9602)\tD(fake) 0.0317 (0.0569)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7861 (-0.7797)\n",
            "Epoch: [37][110/195]\tTime  0.988 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.8903 (0.9598)\tD(fake) 0.0490 (0.0577)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8109 (-0.7803)\n",
            "Epoch: [37][120/195]\tTime  0.986 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0169 (0.9596)\tD(fake) 0.0087 (0.0584)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7952 (-0.7803)\n",
            "Epoch: [37][130/195]\tTime  0.982 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0128 (0.9594)\tD(fake) -0.0696 (0.0585)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7904 (-0.7810)\n",
            "Epoch: [37][140/195]\tTime  0.984 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9272 (0.9592)\tD(fake) 0.1318 (0.0585)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7753 (-0.7809)\n",
            "Epoch: [37][150/195]\tTime  0.987 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0331 (0.9591)\tD(fake) 0.0032 (0.0591)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7541 (-0.7810)\n",
            "Epoch: [37][160/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9465 (0.9591)\tD(fake) 0.0291 (0.0591)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7974 (-0.7823)\n",
            "Epoch: [37][170/195]\tTime  0.994 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.8826 (0.9592)\tD(fake) 0.0773 (0.0585)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7773 (-0.7810)\n",
            "Epoch: [37][180/195]\tTime  0.979 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 0.9634 (0.9591)\tD(fake) 0.0137 (0.0590)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7152 (-0.7809)\n",
            "Epoch: [37][190/195]\tTime  0.982 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 0.9967 (0.9592)\tD(fake) -0.0171 (0.0590)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7974 (-0.7814)\n",
            "Epoch: [38][  0/195]\tTime  1.412 ( 1.412)\tData  0.372 ( 0.372)\tD(real) 0.9536 (0.9574)\tD(fake) 0.0167 (0.0087)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7968 (-0.7968)\n",
            "Epoch: [38][ 10/195]\tTime  0.983 ( 1.027)\tData  0.000 ( 0.034)\tD(real) 0.9479 (0.9596)\tD(fake) 0.0218 (0.0552)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7959 (-0.7926)\n",
            "Epoch: [38][ 20/195]\tTime  0.985 ( 1.011)\tData  0.000 ( 0.018)\tD(real) 0.9462 (0.9595)\tD(fake) 0.1035 (0.0600)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8192 (-0.7860)\n",
            "Epoch: [38][ 30/195]\tTime  0.991 ( 1.005)\tData  0.000 ( 0.012)\tD(real) 1.0066 (0.9600)\tD(fake) -0.0096 (0.0592)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7964 (-0.7837)\n",
            "Epoch: [38][ 40/195]\tTime  0.988 ( 1.002)\tData  0.000 ( 0.009)\tD(real) 1.0100 (0.9592)\tD(fake) -0.0030 (0.0601)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7899 (-0.7872)\n",
            "Epoch: [38][ 50/195]\tTime  0.992 ( 1.000)\tData  0.000 ( 0.008)\tD(real) 0.9969 (0.9593)\tD(fake) -0.0263 (0.0598)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7576 (-0.7854)\n",
            "Epoch: [38][ 60/195]\tTime  0.987 ( 0.998)\tData  0.000 ( 0.006)\tD(real) 0.8495 (0.9589)\tD(fake) 0.1711 (0.0595)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7674 (-0.7871)\n",
            "Epoch: [38][ 70/195]\tTime  1.001 ( 0.997)\tData  0.000 ( 0.006)\tD(real) 1.0745 (0.9597)\tD(fake) 0.0355 (0.0593)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8054 (-0.7879)\n",
            "Epoch: [38][ 80/195]\tTime  0.987 ( 0.996)\tData  0.000 ( 0.005)\tD(real) 1.0406 (0.9598)\tD(fake) -0.0503 (0.0584)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7972 (-0.7889)\n",
            "Epoch: [38][ 90/195]\tTime  0.978 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 1.0382 (0.9598)\tD(fake) -0.0015 (0.0583)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7540 (-0.7872)\n",
            "Epoch: [38][100/195]\tTime  1.002 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 0.8804 (0.9596)\tD(fake) 0.0673 (0.0583)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8022 (-0.7879)\n",
            "Epoch: [38][110/195]\tTime  0.979 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.9302 (0.9598)\tD(fake) 0.0392 (0.0575)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8119 (-0.7886)\n",
            "Epoch: [38][120/195]\tTime  0.979 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0227 (0.9596)\tD(fake) -0.0086 (0.0583)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8111 (-0.7898)\n",
            "Epoch: [38][130/195]\tTime  0.981 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9810 (0.9594)\tD(fake) 0.0091 (0.0585)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7966 (-0.7892)\n",
            "Epoch: [38][140/195]\tTime  0.992 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.8883 (0.9591)\tD(fake) 0.1090 (0.0591)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7869 (-0.7895)\n",
            "Epoch: [38][150/195]\tTime  0.987 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0681 (0.9593)\tD(fake) -0.0094 (0.0590)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8217 (-0.7895)\n",
            "Epoch: [38][160/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0105 (0.9593)\tD(fake) -0.0371 (0.0587)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7914 (-0.7893)\n",
            "Epoch: [38][170/195]\tTime  0.977 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9342 (0.9591)\tD(fake) 0.0571 (0.0589)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8259 (-0.7898)\n",
            "Epoch: [38][180/195]\tTime  0.992 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 1.0099 (0.9591)\tD(fake) 0.0087 (0.0590)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7085 (-0.7892)\n",
            "Epoch: [38][190/195]\tTime  0.974 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 1.0036 (0.9592)\tD(fake) -0.0270 (0.0590)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7888 (-0.7898)\n",
            "Epoch: [39][  0/195]\tTime  1.400 ( 1.400)\tData  0.340 ( 0.340)\tD(real) 0.9250 (0.9626)\tD(fake) 0.0200 (0.0050)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6947 (-0.6947)\n",
            "Epoch: [39][ 10/195]\tTime  0.984 ( 1.028)\tData  0.000 ( 0.031)\tD(real) 0.9407 (0.9611)\tD(fake) 0.0326 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7628 (-0.7651)\n",
            "Epoch: [39][ 20/195]\tTime  0.985 ( 1.010)\tData  0.000 ( 0.017)\tD(real) 1.0427 (0.9595)\tD(fake) 0.0667 (0.0628)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8157 (-0.7766)\n",
            "Epoch: [39][ 30/195]\tTime  0.979 ( 1.003)\tData  0.000 ( 0.011)\tD(real) 0.9672 (0.9595)\tD(fake) 0.0331 (0.0596)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7656 (-0.7789)\n",
            "Epoch: [39][ 40/195]\tTime  0.994 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 1.0530 (0.9607)\tD(fake) -0.0375 (0.0577)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8126 (-0.7832)\n",
            "Epoch: [39][ 50/195]\tTime  0.984 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 0.9754 (0.9599)\tD(fake) 0.0093 (0.0587)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8003 (-0.7847)\n",
            "Epoch: [39][ 60/195]\tTime  0.982 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 0.9712 (0.9593)\tD(fake) 0.0224 (0.0604)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8391 (-0.7878)\n",
            "Epoch: [39][ 70/195]\tTime  0.997 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9509 (0.9589)\tD(fake) 0.0298 (0.0613)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7838 (-0.7865)\n",
            "Epoch: [39][ 80/195]\tTime  0.990 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0445 (0.9590)\tD(fake) 0.0583 (0.0614)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8071 (-0.7872)\n",
            "Epoch: [39][ 90/195]\tTime  0.977 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.8798 (0.9590)\tD(fake) 0.1366 (0.0610)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7524 (-0.7876)\n",
            "Epoch: [39][100/195]\tTime  1.019 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0214 (0.9592)\tD(fake) 0.0118 (0.0605)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8181 (-0.7900)\n",
            "Epoch: [39][110/195]\tTime  0.981 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0056 (0.9597)\tD(fake) -0.0232 (0.0591)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7820 (-0.7906)\n",
            "Epoch: [39][120/195]\tTime  0.989 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9226 (0.9597)\tD(fake) 0.0649 (0.0586)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8261 (-0.7908)\n",
            "Epoch: [39][130/195]\tTime  0.990 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9774 (0.9598)\tD(fake) -0.0083 (0.0586)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7812 (-0.7905)\n",
            "Epoch: [39][140/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9246 (0.9597)\tD(fake) 0.0330 (0.0586)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7714 (-0.7911)\n",
            "Epoch: [39][150/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9980 (0.9597)\tD(fake) 0.0352 (0.0589)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8021 (-0.7905)\n",
            "Epoch: [39][160/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.8918 (0.9596)\tD(fake) 0.1458 (0.0588)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7841 (-0.7908)\n",
            "Epoch: [39][170/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0697 (0.9599)\tD(fake) 0.0417 (0.0583)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7781 (-0.7893)\n",
            "Epoch: [39][180/195]\tTime  0.979 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9356 (0.9600)\tD(fake) 0.1210 (0.0580)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7962 (-0.7881)\n",
            "Epoch: [39][190/195]\tTime  0.994 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0055 (0.9599)\tD(fake) 0.0193 (0.0581)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7455 (-0.7879)\n",
            "Epoch: [40][  0/195]\tTime  1.367 ( 1.367)\tData  0.337 ( 0.337)\tD(real) 0.9154 (0.9362)\tD(fake) 0.0745 (0.0500)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7686 (-0.7686)\n",
            "Epoch: [40][ 10/195]\tTime  0.996 ( 1.020)\tData  0.000 ( 0.031)\tD(real) 0.9575 (0.9583)\tD(fake) 0.0093 (0.0581)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7649 (-0.7779)\n",
            "Epoch: [40][ 20/195]\tTime  0.977 ( 1.005)\tData  0.000 ( 0.016)\tD(real) 0.9416 (0.9592)\tD(fake) 0.0418 (0.0571)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8153 (-0.7815)\n",
            "Epoch: [40][ 30/195]\tTime  0.979 ( 0.999)\tData  0.000 ( 0.011)\tD(real) 0.9596 (0.9593)\tD(fake) 0.0769 (0.0596)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7888 (-0.7855)\n",
            "Epoch: [40][ 40/195]\tTime  0.984 ( 0.996)\tData  0.000 ( 0.009)\tD(real) 0.9554 (0.9590)\tD(fake) 0.0120 (0.0599)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8255 (-0.7822)\n",
            "Epoch: [40][ 50/195]\tTime  0.990 ( 0.994)\tData  0.001 ( 0.007)\tD(real) 1.0120 (0.9589)\tD(fake) -0.0203 (0.0604)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7892 (-0.7835)\n",
            "Epoch: [40][ 60/195]\tTime  0.984 ( 0.992)\tData  0.000 ( 0.006)\tD(real) 0.9038 (0.9592)\tD(fake) 0.0978 (0.0600)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8099 (-0.7864)\n",
            "Epoch: [40][ 70/195]\tTime  0.985 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 1.0027 (0.9591)\tD(fake) 0.0455 (0.0600)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8295 (-0.7872)\n",
            "Epoch: [40][ 80/195]\tTime  0.983 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9097 (0.9592)\tD(fake) 0.0986 (0.0591)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8438 (-0.7887)\n",
            "Epoch: [40][ 90/195]\tTime  0.980 ( 0.989)\tData  0.000 ( 0.004)\tD(real) 0.9446 (0.9599)\tD(fake) 0.0248 (0.0574)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7981 (-0.7894)\n",
            "Epoch: [40][100/195]\tTime  1.013 ( 0.989)\tData  0.000 ( 0.004)\tD(real) 0.9641 (0.9600)\tD(fake) 0.0313 (0.0575)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8314 (-0.7892)\n",
            "Epoch: [40][110/195]\tTime  0.975 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 1.0471 (0.9603)\tD(fake) -0.0082 (0.0571)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8227 (-0.7904)\n",
            "Epoch: [40][120/195]\tTime  0.983 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 0.8959 (0.9601)\tD(fake) 0.0821 (0.0573)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7764 (-0.7908)\n",
            "Epoch: [40][130/195]\tTime  0.987 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 0.9466 (0.9600)\tD(fake) 0.0374 (0.0573)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7917 (-0.7913)\n",
            "Epoch: [40][140/195]\tTime  0.987 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 0.9124 (0.9599)\tD(fake) 0.0612 (0.0576)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7861 (-0.7914)\n",
            "Epoch: [40][150/195]\tTime  0.985 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 1.0041 (0.9598)\tD(fake) 0.0624 (0.0579)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7621 (-0.7901)\n",
            "Epoch: [40][160/195]\tTime  0.982 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9913 (0.9597)\tD(fake) 0.0168 (0.0585)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7839 (-0.7906)\n",
            "Epoch: [40][170/195]\tTime  0.994 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0480 (0.9600)\tD(fake) -0.0252 (0.0580)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8120 (-0.7902)\n",
            "Epoch: [40][180/195]\tTime  0.983 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9767 (0.9599)\tD(fake) 0.0346 (0.0583)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8272 (-0.7905)\n",
            "Epoch: [40][190/195]\tTime  0.981 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9647 (0.9599)\tD(fake) 0.0183 (0.0583)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8120 (-0.7905)\n",
            "Epoch: [41][  0/195]\tTime  1.387 ( 1.387)\tData  0.327 ( 0.327)\tD(real) 1.0533 (0.9601)\tD(fake) 0.0656 (0.1032)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8292 (-0.8292)\n",
            "Epoch: [41][ 10/195]\tTime  0.998 ( 1.028)\tData  0.000 ( 0.030)\tD(real) 0.8805 (0.9639)\tD(fake) 0.0814 (0.0497)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7823 (-0.7873)\n",
            "Epoch: [41][ 20/195]\tTime  0.976 ( 1.006)\tData  0.000 ( 0.016)\tD(real) 0.9055 (0.9611)\tD(fake) 0.1301 (0.0584)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7999 (-0.7865)\n",
            "Epoch: [41][ 30/195]\tTime  0.980 ( 0.998)\tData  0.000 ( 0.011)\tD(real) 0.9476 (0.9610)\tD(fake) 0.0629 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8078 (-0.7832)\n",
            "Epoch: [41][ 40/195]\tTime  0.994 ( 0.996)\tData  0.000 ( 0.008)\tD(real) 0.9279 (0.9620)\tD(fake) 0.0763 (0.0546)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8049 (-0.7879)\n",
            "Epoch: [41][ 50/195]\tTime  0.975 ( 0.993)\tData  0.000 ( 0.007)\tD(real) 0.9753 (0.9617)\tD(fake) -0.0193 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8197 (-0.7893)\n",
            "Epoch: [41][ 60/195]\tTime  0.986 ( 0.992)\tData  0.000 ( 0.006)\tD(real) 1.0397 (0.9620)\tD(fake) 0.0727 (0.0564)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7680 (-0.7900)\n",
            "Epoch: [41][ 70/195]\tTime  0.988 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0039 (0.9615)\tD(fake) 0.0420 (0.0572)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7849 (-0.7910)\n",
            "Epoch: [41][ 80/195]\tTime  0.982 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0327 (0.9618)\tD(fake) -0.0187 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6705 (-0.7867)\n",
            "Epoch: [41][ 90/195]\tTime  1.000 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0181 (0.9618)\tD(fake) 0.0275 (0.0560)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8173 (-0.7870)\n",
            "Epoch: [41][100/195]\tTime  1.013 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0006 (0.9615)\tD(fake) -0.0131 (0.0565)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7814 (-0.7883)\n",
            "Epoch: [41][110/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0259 (0.9615)\tD(fake) -0.0088 (0.0565)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7780 (-0.7888)\n",
            "Epoch: [41][120/195]\tTime  0.998 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0462 (0.9613)\tD(fake) 0.0289 (0.0572)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8206 (-0.7890)\n",
            "Epoch: [41][130/195]\tTime  0.999 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0430 (0.9609)\tD(fake) 0.0668 (0.0578)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7791 (-0.7894)\n",
            "Epoch: [41][140/195]\tTime  0.995 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9917 (0.9609)\tD(fake) -0.0281 (0.0574)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8074 (-0.7879)\n",
            "Epoch: [41][150/195]\tTime  0.982 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9702 (0.9607)\tD(fake) 0.0146 (0.0574)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7096 (-0.7872)\n",
            "Epoch: [41][160/195]\tTime  0.978 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0004 (0.9607)\tD(fake) -0.0218 (0.0574)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7941 (-0.7874)\n",
            "Epoch: [41][170/195]\tTime  0.991 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0091 (0.9605)\tD(fake) -0.0355 (0.0578)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7931 (-0.7872)\n",
            "Epoch: [41][180/195]\tTime  0.993 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0228 (0.9606)\tD(fake) -0.0178 (0.0576)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8070 (-0.7873)\n",
            "Epoch: [41][190/195]\tTime  1.000 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0548 (0.9606)\tD(fake) -0.0020 (0.0576)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7073 (-0.7873)\n",
            "Epoch: [42][  0/195]\tTime  1.370 ( 1.370)\tData  0.328 ( 0.328)\tD(real) 1.0261 (0.9620)\tD(fake) -0.0081 (0.0542)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7903 (-0.7903)\n",
            "Epoch: [42][ 10/195]\tTime  0.978 ( 1.023)\tData  0.000 ( 0.030)\tD(real) 1.0296 (0.9627)\tD(fake) 0.1063 (0.0533)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8073 (-0.8027)\n",
            "Epoch: [42][ 20/195]\tTime  0.990 ( 1.006)\tData  0.000 ( 0.016)\tD(real) 1.0585 (0.9609)\tD(fake) 0.0267 (0.0605)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7587 (-0.7970)\n",
            "Epoch: [42][ 30/195]\tTime  0.984 ( 1.000)\tData  0.000 ( 0.011)\tD(real) 0.9993 (0.9612)\tD(fake) -0.0091 (0.0581)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7824 (-0.7904)\n",
            "Epoch: [42][ 40/195]\tTime  0.986 ( 0.997)\tData  0.000 ( 0.008)\tD(real) 1.0596 (0.9608)\tD(fake) 0.0141 (0.0594)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7827 (-0.7872)\n",
            "Epoch: [42][ 50/195]\tTime  0.980 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 0.9780 (0.9610)\tD(fake) -0.0115 (0.0578)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7789 (-0.7851)\n",
            "Epoch: [42][ 60/195]\tTime  0.985 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 1.0434 (0.9612)\tD(fake) -0.0131 (0.0576)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8012 (-0.7862)\n",
            "Epoch: [42][ 70/195]\tTime  1.000 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9280 (0.9606)\tD(fake) 0.1252 (0.0586)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7981 (-0.7870)\n",
            "Epoch: [42][ 80/195]\tTime  0.987 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9963 (0.9603)\tD(fake) 0.0159 (0.0596)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7930 (-0.7882)\n",
            "Epoch: [42][ 90/195]\tTime  0.975 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9649 (0.9604)\tD(fake) 0.0170 (0.0593)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8077 (-0.7882)\n",
            "Epoch: [42][100/195]\tTime  0.992 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9288 (0.9606)\tD(fake) 0.0945 (0.0587)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7778 (-0.7891)\n",
            "Epoch: [42][110/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9664 (0.9603)\tD(fake) 0.0060 (0.0590)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7142 (-0.7878)\n",
            "Epoch: [42][120/195]\tTime  0.988 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0244 (0.9606)\tD(fake) -0.0103 (0.0584)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8016 (-0.7873)\n",
            "Epoch: [42][130/195]\tTime  0.983 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0585 (0.9606)\tD(fake) 0.0051 (0.0584)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7468 (-0.7879)\n",
            "Epoch: [42][140/195]\tTime  0.983 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 0.9505 (0.9604)\tD(fake) 0.0328 (0.0584)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7960 (-0.7885)\n",
            "Epoch: [42][150/195]\tTime  0.993 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 1.0336 (0.9607)\tD(fake) -0.0149 (0.0579)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8071 (-0.7889)\n",
            "Epoch: [42][160/195]\tTime  0.985 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0065 (0.9607)\tD(fake) 0.0355 (0.0580)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7997 (-0.7891)\n",
            "Epoch: [42][170/195]\tTime  0.985 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9529 (0.9607)\tD(fake) 0.0999 (0.0579)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8069 (-0.7904)\n",
            "Epoch: [42][180/195]\tTime  0.997 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9593 (0.9607)\tD(fake) 0.1018 (0.0578)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7724 (-0.7906)\n",
            "Epoch: [42][190/195]\tTime  0.985 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0363 (0.9607)\tD(fake) 0.0018 (0.0580)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6500 (-0.7896)\n",
            "Epoch: [43][  0/195]\tTime  1.381 ( 1.381)\tData  0.333 ( 0.333)\tD(real) 0.9475 (0.9626)\tD(fake) 0.0086 (0.0096)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7406 (-0.7406)\n",
            "Epoch: [43][ 10/195]\tTime  0.982 ( 1.028)\tData  0.000 ( 0.031)\tD(real) 0.9355 (0.9645)\tD(fake) 0.0456 (0.0440)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8137 (-0.7698)\n",
            "Epoch: [43][ 20/195]\tTime  0.986 ( 1.006)\tData  0.000 ( 0.016)\tD(real) 1.0644 (0.9639)\tD(fake) 0.0396 (0.0493)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7982 (-0.7765)\n",
            "Epoch: [43][ 30/195]\tTime  0.999 ( 1.000)\tData  0.000 ( 0.011)\tD(real) 1.0087 (0.9629)\tD(fake) 0.0406 (0.0517)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8450 (-0.7806)\n",
            "Epoch: [43][ 40/195]\tTime  1.001 ( 0.999)\tData  0.000 ( 0.008)\tD(real) 0.9544 (0.9628)\tD(fake) 0.0655 (0.0518)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7423 (-0.7827)\n",
            "Epoch: [43][ 50/195]\tTime  0.986 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 0.9691 (0.9622)\tD(fake) 0.0195 (0.0541)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7054 (-0.7855)\n",
            "Epoch: [43][ 60/195]\tTime  0.973 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 0.8736 (0.9620)\tD(fake) 0.1773 (0.0548)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8098 (-0.7861)\n",
            "Epoch: [43][ 70/195]\tTime  0.980 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 0.9833 (0.9617)\tD(fake) -0.0062 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7794 (-0.7869)\n",
            "Epoch: [43][ 80/195]\tTime  0.981 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 0.9258 (0.9618)\tD(fake) 0.0708 (0.0560)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7482 (-0.7869)\n",
            "Epoch: [43][ 90/195]\tTime  0.984 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0372 (0.9616)\tD(fake) -0.0196 (0.0566)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7894 (-0.7872)\n",
            "Epoch: [43][100/195]\tTime  0.994 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0459 (0.9619)\tD(fake) -0.0272 (0.0558)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7909 (-0.7877)\n",
            "Epoch: [43][110/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9335 (0.9617)\tD(fake) 0.1472 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7989 (-0.7883)\n",
            "Epoch: [43][120/195]\tTime  0.978 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9458 (0.9616)\tD(fake) 0.0351 (0.0565)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8084 (-0.7897)\n",
            "Epoch: [43][130/195]\tTime  1.006 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9293 (0.9616)\tD(fake) 0.1196 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8221 (-0.7889)\n",
            "Epoch: [43][140/195]\tTime  0.994 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0733 (0.9615)\tD(fake) 0.0210 (0.0569)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7994 (-0.7899)\n",
            "Epoch: [43][150/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0891 (0.9614)\tD(fake) 0.0274 (0.0572)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7975 (-0.7900)\n",
            "Epoch: [43][160/195]\tTime  0.992 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0444 (0.9614)\tD(fake) -0.0265 (0.0570)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7800 (-0.7896)\n",
            "Epoch: [43][170/195]\tTime  0.983 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.8566 (0.9611)\tD(fake) 0.1646 (0.0575)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7848 (-0.7888)\n",
            "Epoch: [43][180/195]\tTime  0.986 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0249 (0.9611)\tD(fake) 0.0628 (0.0577)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7993 (-0.7890)\n",
            "Epoch: [43][190/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9856 (0.9610)\tD(fake) 0.0741 (0.0578)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7853 (-0.7899)\n",
            "Epoch: [44][  0/195]\tTime  1.364 ( 1.364)\tData  0.328 ( 0.328)\tD(real) 1.0677 (0.9696)\tD(fake) -0.0110 (0.0613)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7500 (-0.7500)\n",
            "Epoch: [44][ 10/195]\tTime  0.977 ( 1.022)\tData  0.000 ( 0.030)\tD(real) 0.9748 (0.9606)\tD(fake) -0.0047 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7789 (-0.7802)\n",
            "Epoch: [44][ 20/195]\tTime  1.000 ( 1.004)\tData  0.000 ( 0.016)\tD(real) 1.0067 (0.9617)\tD(fake) -0.0017 (0.0544)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8022 (-0.7860)\n",
            "Epoch: [44][ 30/195]\tTime  0.978 ( 0.997)\tData  0.000 ( 0.011)\tD(real) 0.8632 (0.9603)\tD(fake) 0.1304 (0.0583)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8447 (-0.7947)\n",
            "Epoch: [44][ 40/195]\tTime  0.976 ( 0.995)\tData  0.000 ( 0.008)\tD(real) 1.0148 (0.9613)\tD(fake) -0.0035 (0.0571)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7680 (-0.7932)\n",
            "Epoch: [44][ 50/195]\tTime  0.980 ( 0.992)\tData  0.000 ( 0.007)\tD(real) 1.0216 (0.9614)\tD(fake) 0.0060 (0.0570)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8042 (-0.7948)\n",
            "Epoch: [44][ 60/195]\tTime  0.985 ( 0.991)\tData  0.000 ( 0.006)\tD(real) 1.0018 (0.9613)\tD(fake) -0.0090 (0.0577)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7904 (-0.7967)\n",
            "Epoch: [44][ 70/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.005)\tD(real) 1.0030 (0.9615)\tD(fake) -0.0149 (0.0573)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8047 (-0.7944)\n",
            "Epoch: [44][ 80/195]\tTime  0.994 ( 0.989)\tData  0.000 ( 0.004)\tD(real) 0.9336 (0.9616)\tD(fake) 0.0772 (0.0571)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7637 (-0.7934)\n",
            "Epoch: [44][ 90/195]\tTime  1.000 ( 0.989)\tData  0.000 ( 0.004)\tD(real) 0.9447 (0.9617)\tD(fake) 0.0737 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7821 (-0.7930)\n",
            "Epoch: [44][100/195]\tTime  1.010 ( 0.989)\tData  0.000 ( 0.004)\tD(real) 0.9944 (0.9614)\tD(fake) 0.0037 (0.0578)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7650 (-0.7921)\n",
            "Epoch: [44][110/195]\tTime  0.997 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0344 (0.9611)\tD(fake) -0.0037 (0.0586)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7930 (-0.7921)\n",
            "Epoch: [44][120/195]\tTime  0.998 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9081 (0.9607)\tD(fake) 0.1258 (0.0588)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7997 (-0.7925)\n",
            "Epoch: [44][130/195]\tTime  0.986 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 1.0270 (0.9609)\tD(fake) -0.0204 (0.0585)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7654 (-0.7924)\n",
            "Epoch: [44][140/195]\tTime  0.982 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 1.0281 (0.9606)\tD(fake) 0.0418 (0.0589)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7992 (-0.7920)\n",
            "Epoch: [44][150/195]\tTime  0.981 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9570 (0.9605)\tD(fake) 0.0269 (0.0584)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6991 (-0.7908)\n",
            "Epoch: [44][160/195]\tTime  1.002 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0577 (0.9606)\tD(fake) -0.0254 (0.0586)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8075 (-0.7909)\n",
            "Epoch: [44][170/195]\tTime  0.987 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0131 (0.9604)\tD(fake) -0.0137 (0.0589)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7962 (-0.7918)\n",
            "Epoch: [44][180/195]\tTime  0.980 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9334 (0.9604)\tD(fake) 0.0981 (0.0587)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8309 (-0.7915)\n",
            "Epoch: [44][190/195]\tTime  0.985 ( 0.987)\tData  0.000 ( 0.002)\tD(real) 0.9452 (0.9603)\tD(fake) 0.0367 (0.0590)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7787 (-0.7918)\n",
            "Epoch: [45][  0/195]\tTime  1.385 ( 1.385)\tData  0.342 ( 0.342)\tD(real) 0.9674 (0.9639)\tD(fake) 0.0273 (0.0732)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8204 (-0.8204)\n",
            "Epoch: [45][ 10/195]\tTime  0.989 ( 1.021)\tData  0.000 ( 0.031)\tD(real) 0.9886 (0.9642)\tD(fake) -0.0197 (0.0520)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7934 (-0.8011)\n",
            "Epoch: [45][ 20/195]\tTime  0.969 ( 1.004)\tData  0.000 ( 0.017)\tD(real) 1.0553 (0.9649)\tD(fake) -0.0321 (0.0523)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7808 (-0.7929)\n",
            "Epoch: [45][ 30/195]\tTime  0.976 ( 0.997)\tData  0.000 ( 0.011)\tD(real) 1.0664 (0.9632)\tD(fake) 0.0974 (0.0543)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7154 (-0.7905)\n",
            "Epoch: [45][ 40/195]\tTime  0.998 ( 0.995)\tData  0.000 ( 0.009)\tD(real) 1.0604 (0.9636)\tD(fake) -0.0552 (0.0528)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7939 (-0.7911)\n",
            "Epoch: [45][ 50/195]\tTime  0.978 ( 0.993)\tData  0.000 ( 0.007)\tD(real) 1.0155 (0.9628)\tD(fake) -0.0135 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7471 (-0.7877)\n",
            "Epoch: [45][ 60/195]\tTime  0.990 ( 0.992)\tData  0.000 ( 0.006)\tD(real) 1.0454 (0.9631)\tD(fake) 0.0359 (0.0542)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7903 (-0.7876)\n",
            "Epoch: [45][ 70/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.005)\tD(real) 1.0529 (0.9630)\tD(fake) 0.0580 (0.0548)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8234 (-0.7894)\n",
            "Epoch: [45][ 80/195]\tTime  0.990 ( 0.989)\tData  0.000 ( 0.005)\tD(real) 0.9740 (0.9631)\tD(fake) 0.0041 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8027 (-0.7896)\n",
            "Epoch: [45][ 90/195]\tTime  0.983 ( 0.989)\tData  0.000 ( 0.004)\tD(real) 1.0034 (0.9630)\tD(fake) 0.0123 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7842 (-0.7894)\n",
            "Epoch: [45][100/195]\tTime  0.993 ( 0.988)\tData  0.000 ( 0.004)\tD(real) 0.9510 (0.9630)\tD(fake) 0.0223 (0.0547)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7833 (-0.7897)\n",
            "Epoch: [45][110/195]\tTime  0.979 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 1.0836 (0.9633)\tD(fake) 0.0543 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8074 (-0.7890)\n",
            "Epoch: [45][120/195]\tTime  0.987 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 0.9488 (0.9632)\tD(fake) 0.0454 (0.0542)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8147 (-0.7875)\n",
            "Epoch: [45][130/195]\tTime  0.990 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 0.9810 (0.9632)\tD(fake) 0.0341 (0.0540)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7715 (-0.7866)\n",
            "Epoch: [45][140/195]\tTime  1.011 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 0.9421 (0.9634)\tD(fake) 0.0777 (0.0538)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7732 (-0.7867)\n",
            "Epoch: [45][150/195]\tTime  0.979 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 0.9182 (0.9633)\tD(fake) 0.0666 (0.0538)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8012 (-0.7857)\n",
            "Epoch: [45][160/195]\tTime  0.991 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9504 (0.9635)\tD(fake) 0.0123 (0.0535)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8213 (-0.7867)\n",
            "Epoch: [45][170/195]\tTime  0.981 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9321 (0.9632)\tD(fake) 0.0647 (0.0540)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7939 (-0.7865)\n",
            "Epoch: [45][180/195]\tTime  0.982 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9910 (0.9632)\tD(fake) 0.0369 (0.0540)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7854 (-0.7866)\n",
            "Epoch: [45][190/195]\tTime  0.977 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0099 (0.9629)\tD(fake) 0.0141 (0.0543)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7611 (-0.7861)\n",
            "Epoch: [46][  0/195]\tTime  1.395 ( 1.395)\tData  0.348 ( 0.348)\tD(real) 0.9599 (0.9624)\tD(fake) 0.0153 (0.0322)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8186 (-0.8186)\n",
            "Epoch: [46][ 10/195]\tTime  0.988 ( 1.028)\tData  0.000 ( 0.032)\tD(real) 0.9732 (0.9640)\tD(fake) 0.0832 (0.0501)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8035 (-0.8059)\n",
            "Epoch: [46][ 20/195]\tTime  0.983 ( 1.008)\tData  0.000 ( 0.017)\tD(real) 0.8859 (0.9620)\tD(fake) 0.1383 (0.0564)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7909 (-0.7989)\n",
            "Epoch: [46][ 30/195]\tTime  0.987 ( 1.000)\tData  0.000 ( 0.012)\tD(real) 0.9434 (0.9618)\tD(fake) 0.0795 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8164 (-0.7920)\n",
            "Epoch: [46][ 40/195]\tTime  0.981 ( 0.996)\tData  0.000 ( 0.009)\tD(real) 1.0294 (0.9619)\tD(fake) -0.0157 (0.0571)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8064 (-0.7954)\n",
            "Epoch: [46][ 50/195]\tTime  0.977 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 1.0309 (0.9611)\tD(fake) 0.0688 (0.0593)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7299 (-0.7948)\n",
            "Epoch: [46][ 60/195]\tTime  0.984 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 1.0364 (0.9620)\tD(fake) 0.0288 (0.0571)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7681 (-0.7938)\n",
            "Epoch: [46][ 70/195]\tTime  0.979 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 0.9970 (0.9623)\tD(fake) -0.0150 (0.0565)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7787 (-0.7926)\n",
            "Epoch: [46][ 80/195]\tTime  0.984 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 1.0365 (0.9625)\tD(fake) 0.0107 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7940 (-0.7929)\n",
            "Epoch: [46][ 90/195]\tTime  0.986 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0001 (0.9626)\tD(fake) 0.0129 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8064 (-0.7934)\n",
            "Epoch: [46][100/195]\tTime  0.999 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0297 (0.9631)\tD(fake) 0.0785 (0.0558)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7951 (-0.7925)\n",
            "Epoch: [46][110/195]\tTime  0.995 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0284 (0.9633)\tD(fake) 0.0252 (0.0553)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7608 (-0.7927)\n",
            "Epoch: [46][120/195]\tTime  0.983 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9881 (0.9634)\tD(fake) 0.0184 (0.0549)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7853 (-0.7908)\n",
            "Epoch: [46][130/195]\tTime  0.986 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0365 (0.9634)\tD(fake) -0.0181 (0.0554)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7994 (-0.7911)\n",
            "Epoch: [46][140/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9548 (0.9635)\tD(fake) 0.0254 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8250 (-0.7920)\n",
            "Epoch: [46][150/195]\tTime  0.992 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9411 (0.9632)\tD(fake) 0.1025 (0.0549)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7668 (-0.7910)\n",
            "Epoch: [46][160/195]\tTime  0.994 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9405 (0.9633)\tD(fake) 0.0772 (0.0546)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7895 (-0.7915)\n",
            "Epoch: [46][170/195]\tTime  0.986 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0151 (0.9630)\tD(fake) -0.0054 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7996 (-0.7918)\n",
            "Epoch: [46][180/195]\tTime  0.982 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9935 (0.9629)\tD(fake) -0.0282 (0.0551)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6595 (-0.7916)\n",
            "Epoch: [46][190/195]\tTime  0.991 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0285 (0.9629)\tD(fake) -0.0678 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8049 (-0.7905)\n",
            "Epoch: [47][  0/195]\tTime  1.408 ( 1.408)\tData  0.367 ( 0.367)\tD(real) 0.9802 (0.9643)\tD(fake) 0.0035 (0.0285)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8138 (-0.8138)\n",
            "Epoch: [47][ 10/195]\tTime  0.999 ( 1.029)\tData  0.000 ( 0.034)\tD(real) 0.9567 (0.9628)\tD(fake) 0.0037 (0.0487)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7763 (-0.7997)\n",
            "Epoch: [47][ 20/195]\tTime  0.999 ( 1.009)\tData  0.000 ( 0.018)\tD(real) 1.0015 (0.9641)\tD(fake) 0.0036 (0.0520)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7744 (-0.7907)\n",
            "Epoch: [47][ 30/195]\tTime  0.982 ( 1.002)\tData  0.000 ( 0.012)\tD(real) 0.9239 (0.9627)\tD(fake) 0.1072 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8173 (-0.7891)\n",
            "Epoch: [47][ 40/195]\tTime  0.985 ( 0.998)\tData  0.000 ( 0.009)\tD(real) 0.9550 (0.9631)\tD(fake) 0.0717 (0.0546)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7741 (-0.7886)\n",
            "Epoch: [47][ 50/195]\tTime  1.003 ( 0.996)\tData  0.000 ( 0.008)\tD(real) 0.9384 (0.9635)\tD(fake) 0.0887 (0.0544)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8207 (-0.7887)\n",
            "Epoch: [47][ 60/195]\tTime  0.988 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0158 (0.9639)\tD(fake) 0.0075 (0.0537)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8205 (-0.7902)\n",
            "Epoch: [47][ 70/195]\tTime  0.991 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0219 (0.9636)\tD(fake) -0.0268 (0.0543)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7875 (-0.7911)\n",
            "Epoch: [47][ 80/195]\tTime  0.994 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 1.0280 (0.9636)\tD(fake) -0.0192 (0.0542)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8222 (-0.7920)\n",
            "Epoch: [47][ 90/195]\tTime  1.005 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.9409 (0.9638)\tD(fake) 0.0391 (0.0536)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7963 (-0.7938)\n",
            "Epoch: [47][100/195]\tTime  1.003 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9996 (0.9632)\tD(fake) 0.0240 (0.0554)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7969 (-0.7946)\n",
            "Epoch: [47][110/195]\tTime  0.980 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0439 (0.9633)\tD(fake) 0.1080 (0.0553)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7886 (-0.7941)\n",
            "Epoch: [47][120/195]\tTime  0.993 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9947 (0.9634)\tD(fake) 0.0490 (0.0549)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8060 (-0.7932)\n",
            "Epoch: [47][130/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9914 (0.9633)\tD(fake) -0.0220 (0.0549)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8053 (-0.7928)\n",
            "Epoch: [47][140/195]\tTime  0.976 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0172 (0.9632)\tD(fake) 0.0182 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7987 (-0.7925)\n",
            "Epoch: [47][150/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0098 (0.9634)\tD(fake) 0.0314 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8137 (-0.7917)\n",
            "Epoch: [47][160/195]\tTime  0.987 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0136 (0.9633)\tD(fake) 0.0012 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7898 (-0.7922)\n",
            "Epoch: [47][170/195]\tTime  0.986 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0567 (0.9635)\tD(fake) -0.0171 (0.0543)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7874 (-0.7923)\n",
            "Epoch: [47][180/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0286 (0.9634)\tD(fake) 0.0538 (0.0546)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7850 (-0.7923)\n",
            "Epoch: [47][190/195]\tTime  0.977 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9490 (0.9632)\tD(fake) 0.0214 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8252 (-0.7921)\n",
            "Epoch: [48][  0/195]\tTime  1.393 ( 1.393)\tData  0.355 ( 0.355)\tD(real) 0.9877 (0.9612)\tD(fake) 0.0329 (0.0845)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8074 (-0.8074)\n",
            "Epoch: [48][ 10/195]\tTime  0.985 ( 1.025)\tData  0.000 ( 0.033)\tD(real) 0.9796 (0.9653)\tD(fake) -0.0017 (0.0503)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7991 (-0.7821)\n",
            "Epoch: [48][ 20/195]\tTime  0.993 ( 1.006)\tData  0.000 ( 0.017)\tD(real) 0.9448 (0.9636)\tD(fake) 0.0254 (0.0566)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8277 (-0.7901)\n",
            "Epoch: [48][ 30/195]\tTime  0.989 ( 1.000)\tData  0.000 ( 0.012)\tD(real) 0.9354 (0.9625)\tD(fake) 0.0380 (0.0578)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8210 (-0.7936)\n",
            "Epoch: [48][ 40/195]\tTime  0.998 ( 0.997)\tData  0.000 ( 0.009)\tD(real) 1.0321 (0.9624)\tD(fake) -0.0070 (0.0573)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7271 (-0.7930)\n",
            "Epoch: [48][ 50/195]\tTime  0.988 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 0.9973 (0.9626)\tD(fake) -0.0263 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7890 (-0.7936)\n",
            "Epoch: [48][ 60/195]\tTime  0.987 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 0.9433 (0.9630)\tD(fake) 0.0300 (0.0555)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7832 (-0.7943)\n",
            "Epoch: [48][ 70/195]\tTime  0.984 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0206 (0.9634)\tD(fake) 0.0263 (0.0551)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8034 (-0.7948)\n",
            "Epoch: [48][ 80/195]\tTime  0.993 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9918 (0.9636)\tD(fake) 0.0356 (0.0544)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7651 (-0.7922)\n",
            "Epoch: [48][ 90/195]\tTime  0.982 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0009 (0.9636)\tD(fake) 0.0217 (0.0546)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8232 (-0.7920)\n",
            "Epoch: [48][100/195]\tTime  0.997 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9426 (0.9637)\tD(fake) 0.0314 (0.0543)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7863 (-0.7909)\n",
            "Epoch: [48][110/195]\tTime  0.982 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9893 (0.9637)\tD(fake) -0.0147 (0.0543)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7882 (-0.7911)\n",
            "Epoch: [48][120/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.1019 (0.9637)\tD(fake) 0.0077 (0.0542)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8023 (-0.7911)\n",
            "Epoch: [48][130/195]\tTime  0.972 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9803 (0.9638)\tD(fake) 0.0584 (0.0537)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7985 (-0.7918)\n",
            "Epoch: [48][140/195]\tTime  0.989 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0068 (0.9635)\tD(fake) 0.0110 (0.0541)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8023 (-0.7907)\n",
            "Epoch: [48][150/195]\tTime  0.991 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0540 (0.9634)\tD(fake) 0.0754 (0.0542)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8084 (-0.7912)\n",
            "Epoch: [48][160/195]\tTime  0.999 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0119 (0.9632)\tD(fake) 0.0028 (0.0546)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7929 (-0.7908)\n",
            "Epoch: [48][170/195]\tTime  0.986 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9716 (0.9631)\tD(fake) 0.0069 (0.0546)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7929 (-0.7912)\n",
            "Epoch: [48][180/195]\tTime  0.985 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9701 (0.9631)\tD(fake) 0.0144 (0.0546)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8060 (-0.7906)\n",
            "Epoch: [48][190/195]\tTime  0.989 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.8992 (0.9630)\tD(fake) 0.1073 (0.0549)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8113 (-0.7907)\n",
            "Epoch: [49][  0/195]\tTime  1.380 ( 1.380)\tData  0.336 ( 0.336)\tD(real) 0.9944 (0.9690)\tD(fake) 0.0202 (0.0435)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7923 (-0.7923)\n",
            "Epoch: [49][ 10/195]\tTime  1.000 ( 1.026)\tData  0.000 ( 0.031)\tD(real) 0.9784 (0.9648)\tD(fake) 0.0772 (0.0511)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7693 (-0.7893)\n",
            "Epoch: [49][ 20/195]\tTime  0.990 ( 1.010)\tData  0.000 ( 0.016)\tD(real) 1.0865 (0.9648)\tD(fake) 0.0215 (0.0526)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8124 (-0.7949)\n",
            "Epoch: [49][ 30/195]\tTime  0.990 ( 1.002)\tData  0.000 ( 0.011)\tD(real) 0.9849 (0.9640)\tD(fake) 0.0072 (0.0544)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8488 (-0.7972)\n",
            "Epoch: [49][ 40/195]\tTime  0.982 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 0.9895 (0.9644)\tD(fake) -0.0022 (0.0531)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8295 (-0.7970)\n",
            "Epoch: [49][ 50/195]\tTime  0.980 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 0.9780 (0.9637)\tD(fake) 0.0129 (0.0543)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8016 (-0.7992)\n",
            "Epoch: [49][ 60/195]\tTime  1.006 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 0.9729 (0.9636)\tD(fake) 0.0146 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8116 (-0.7962)\n",
            "Epoch: [49][ 70/195]\tTime  0.981 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 1.0169 (0.9636)\tD(fake) -0.0183 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8153 (-0.7955)\n",
            "Epoch: [49][ 80/195]\tTime  1.002 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0164 (0.9634)\tD(fake) 0.0071 (0.0549)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8481 (-0.7938)\n",
            "Epoch: [49][ 90/195]\tTime  0.985 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0254 (0.9636)\tD(fake) 0.0209 (0.0548)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7852 (-0.7941)\n",
            "Epoch: [49][100/195]\tTime  1.015 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0479 (0.9636)\tD(fake) 0.0339 (0.0549)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8056 (-0.7948)\n",
            "Epoch: [49][110/195]\tTime  1.002 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0301 (0.9637)\tD(fake) -0.0228 (0.0547)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7533 (-0.7950)\n",
            "Epoch: [49][120/195]\tTime  0.982 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0028 (0.9634)\tD(fake) 0.0190 (0.0552)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8125 (-0.7957)\n",
            "Epoch: [49][130/195]\tTime  0.995 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9343 (0.9633)\tD(fake) 0.0585 (0.0555)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8119 (-0.7950)\n",
            "Epoch: [49][140/195]\tTime  0.987 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9646 (0.9633)\tD(fake) 0.0370 (0.0556)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7415 (-0.7948)\n",
            "Epoch: [49][150/195]\tTime  0.978 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0073 (0.9632)\tD(fake) 0.0553 (0.0556)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7805 (-0.7950)\n",
            "Epoch: [49][160/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0341 (0.9631)\tD(fake) -0.0335 (0.0558)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7854 (-0.7948)\n",
            "Epoch: [49][170/195]\tTime  0.976 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9146 (0.9632)\tD(fake) 0.0739 (0.0558)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7987 (-0.7940)\n",
            "Epoch: [49][180/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9509 (0.9631)\tD(fake) 0.0448 (0.0559)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7788 (-0.7938)\n",
            "Epoch: [49][190/195]\tTime  0.982 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9971 (0.9630)\tD(fake) 0.0347 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7853 (-0.7936)\n",
            "Epoch: [50][  0/195]\tTime  1.373 ( 1.373)\tData  0.334 ( 0.334)\tD(real) 1.0074 (0.9535)\tD(fake) 0.0295 (0.0754)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8161 (-0.8161)\n",
            "Epoch: [50][ 10/195]\tTime  0.976 ( 1.027)\tData  0.000 ( 0.031)\tD(real) 0.9955 (0.9651)\tD(fake) 0.0038 (0.0521)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8213 (-0.7968)\n",
            "Epoch: [50][ 20/195]\tTime  0.993 ( 1.008)\tData  0.000 ( 0.016)\tD(real) 0.9272 (0.9639)\tD(fake) 0.0833 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8007 (-0.7878)\n",
            "Epoch: [50][ 30/195]\tTime  0.982 ( 1.001)\tData  0.000 ( 0.011)\tD(real) 1.0453 (0.9651)\tD(fake) 0.0095 (0.0533)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8183 (-0.7915)\n",
            "Epoch: [50][ 40/195]\tTime  0.981 ( 0.998)\tData  0.000 ( 0.008)\tD(real) 0.9669 (0.9645)\tD(fake) -0.0148 (0.0532)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7875 (-0.7917)\n",
            "Epoch: [50][ 50/195]\tTime  0.991 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 1.0388 (0.9645)\tD(fake) 0.0668 (0.0543)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8306 (-0.7921)\n",
            "Epoch: [50][ 60/195]\tTime  0.989 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 0.9460 (0.9647)\tD(fake) 0.0364 (0.0533)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7498 (-0.7916)\n",
            "Epoch: [50][ 70/195]\tTime  0.986 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9336 (0.9643)\tD(fake) 0.0517 (0.0541)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8197 (-0.7922)\n",
            "Epoch: [50][ 80/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9321 (0.9642)\tD(fake) 0.0341 (0.0541)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6955 (-0.7913)\n",
            "Epoch: [50][ 90/195]\tTime  0.981 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9214 (0.9635)\tD(fake) 0.1074 (0.0558)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8041 (-0.7909)\n",
            "Epoch: [50][100/195]\tTime  1.005 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9355 (0.9633)\tD(fake) 0.0753 (0.0559)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8133 (-0.7909)\n",
            "Epoch: [50][110/195]\tTime  0.981 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0294 (0.9634)\tD(fake) 0.0343 (0.0559)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7984 (-0.7910)\n",
            "Epoch: [50][120/195]\tTime  0.986 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9472 (0.9635)\tD(fake) 0.0239 (0.0551)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7821 (-0.7907)\n",
            "Epoch: [50][130/195]\tTime  0.975 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9590 (0.9633)\tD(fake) 0.0164 (0.0557)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7659 (-0.7908)\n",
            "Epoch: [50][140/195]\tTime  0.995 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0374 (0.9632)\tD(fake) -0.0090 (0.0560)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7992 (-0.7913)\n",
            "Epoch: [50][150/195]\tTime  0.976 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9445 (0.9629)\tD(fake) 0.0782 (0.0565)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8095 (-0.7913)\n",
            "Epoch: [50][160/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0371 (0.9632)\tD(fake) 0.0221 (0.0559)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7721 (-0.7908)\n",
            "Epoch: [50][170/195]\tTime  0.975 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0394 (0.9631)\tD(fake) 0.0398 (0.0560)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7973 (-0.7910)\n",
            "Epoch: [50][180/195]\tTime  0.970 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9779 (0.9632)\tD(fake) 0.0285 (0.0559)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7334 (-0.7912)\n",
            "Epoch: [50][190/195]\tTime  1.002 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9601 (0.9633)\tD(fake) 0.0681 (0.0555)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7778 (-0.7907)\n",
            "Epoch: [51][  0/195]\tTime  1.402 ( 1.402)\tData  0.351 ( 0.351)\tD(real) 0.9883 (0.9686)\tD(fake) 0.0200 (0.0656)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7969 (-0.7969)\n",
            "Epoch: [51][ 10/195]\tTime  0.994 ( 1.030)\tData  0.000 ( 0.032)\tD(real) 0.9960 (0.9640)\tD(fake) 0.0678 (0.0571)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8155 (-0.7997)\n",
            "Epoch: [51][ 20/195]\tTime  0.978 ( 1.010)\tData  0.000 ( 0.017)\tD(real) 0.9447 (0.9630)\tD(fake) 0.0459 (0.0569)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8117 (-0.8070)\n",
            "Epoch: [51][ 30/195]\tTime  0.987 ( 1.002)\tData  0.000 ( 0.012)\tD(real) 1.0382 (0.9635)\tD(fake) -0.0081 (0.0578)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7828 (-0.7968)\n",
            "Epoch: [51][ 40/195]\tTime  0.984 ( 0.998)\tData  0.000 ( 0.009)\tD(real) 1.0161 (0.9630)\tD(fake) -0.0038 (0.0587)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8109 (-0.7976)\n",
            "Epoch: [51][ 50/195]\tTime  0.974 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 1.1081 (0.9637)\tD(fake) 0.0779 (0.0570)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7829 (-0.7962)\n",
            "Epoch: [51][ 60/195]\tTime  1.004 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 0.9773 (0.9645)\tD(fake) 0.0452 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8158 (-0.7970)\n",
            "Epoch: [51][ 70/195]\tTime  0.997 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 1.0062 (0.9640)\tD(fake) -0.0069 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7488 (-0.7970)\n",
            "Epoch: [51][ 80/195]\tTime  0.983 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0056 (0.9641)\tD(fake) 0.0196 (0.0555)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7966 (-0.7956)\n",
            "Epoch: [51][ 90/195]\tTime  0.999 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.9984 (0.9640)\tD(fake) 0.0347 (0.0558)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7798 (-0.7953)\n",
            "Epoch: [51][100/195]\tTime  0.991 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0711 (0.9639)\tD(fake) 0.0548 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7922 (-0.7951)\n",
            "Epoch: [51][110/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9524 (0.9641)\tD(fake) 0.0213 (0.0556)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8126 (-0.7944)\n",
            "Epoch: [51][120/195]\tTime  0.979 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9165 (0.9640)\tD(fake) 0.0652 (0.0557)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7993 (-0.7944)\n",
            "Epoch: [51][130/195]\tTime  0.994 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9629 (0.9638)\tD(fake) 0.0532 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7975 (-0.7946)\n",
            "Epoch: [51][140/195]\tTime  0.992 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0388 (0.9639)\tD(fake) -0.0192 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8005 (-0.7950)\n",
            "Epoch: [51][150/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9817 (0.9638)\tD(fake) 0.0176 (0.0562)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8286 (-0.7950)\n",
            "Epoch: [51][160/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0415 (0.9639)\tD(fake) 0.0047 (0.0560)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8077 (-0.7955)\n",
            "Epoch: [51][170/195]\tTime  0.998 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0267 (0.9637)\tD(fake) 0.0472 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7348 (-0.7947)\n",
            "Epoch: [51][180/195]\tTime  0.998 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9394 (0.9636)\tD(fake) 0.0844 (0.0562)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8012 (-0.7939)\n",
            "Epoch: [51][190/195]\tTime  0.991 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9960 (0.9637)\tD(fake) 0.0071 (0.0559)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7775 (-0.7940)\n",
            "Epoch: [52][  0/195]\tTime  1.395 ( 1.395)\tData  0.347 ( 0.347)\tD(real) 0.9074 (0.9480)\tD(fake) 0.1177 (0.0703)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8194 (-0.8194)\n",
            "Epoch: [52][ 10/195]\tTime  1.005 ( 1.023)\tData  0.000 ( 0.032)\tD(real) 0.9934 (0.9639)\tD(fake) 0.0841 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7920 (-0.8035)\n",
            "Epoch: [52][ 20/195]\tTime  0.982 ( 1.006)\tData  0.000 ( 0.017)\tD(real) 1.0309 (0.9644)\tD(fake) 0.0174 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8117 (-0.8003)\n",
            "Epoch: [52][ 30/195]\tTime  0.988 ( 1.000)\tData  0.000 ( 0.012)\tD(real) 1.0079 (0.9641)\tD(fake) -0.0168 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7998 (-0.7984)\n",
            "Epoch: [52][ 40/195]\tTime  0.989 ( 0.997)\tData  0.000 ( 0.009)\tD(real) 0.9546 (0.9640)\tD(fake) 0.0962 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7792 (-0.7952)\n",
            "Epoch: [52][ 50/195]\tTime  0.971 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 1.0044 (0.9636)\tD(fake) -0.0008 (0.0566)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6783 (-0.7903)\n",
            "Epoch: [52][ 60/195]\tTime  0.982 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 1.0550 (0.9638)\tD(fake) 0.0240 (0.0560)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7747 (-0.7893)\n",
            "Epoch: [52][ 70/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0044 (0.9637)\tD(fake) 0.0032 (0.0557)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7883 (-0.7889)\n",
            "Epoch: [52][ 80/195]\tTime  0.988 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 1.0466 (0.9641)\tD(fake) -0.0172 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7991 (-0.7906)\n",
            "Epoch: [52][ 90/195]\tTime  0.987 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0056 (0.9640)\tD(fake) -0.0065 (0.0552)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7772 (-0.7905)\n",
            "Epoch: [52][100/195]\tTime  0.991 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9891 (0.9642)\tD(fake) 0.0444 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7978 (-0.7897)\n",
            "Epoch: [52][110/195]\tTime  0.989 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9461 (0.9642)\tD(fake) 0.0556 (0.0547)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8144 (-0.7903)\n",
            "Epoch: [52][120/195]\tTime  0.991 ( 0.991)\tData  0.001 ( 0.003)\tD(real) 0.9023 (0.9642)\tD(fake) 0.1393 (0.0546)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8081 (-0.7912)\n",
            "Epoch: [52][130/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9349 (0.9644)\tD(fake) 0.0956 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8204 (-0.7900)\n",
            "Epoch: [52][140/195]\tTime  0.998 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0002 (0.9645)\tD(fake) 0.0321 (0.0544)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8293 (-0.7903)\n",
            "Epoch: [52][150/195]\tTime  0.974 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0115 (0.9643)\tD(fake) -0.0019 (0.0546)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7955 (-0.7905)\n",
            "Epoch: [52][160/195]\tTime  0.982 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9152 (0.9642)\tD(fake) 0.0786 (0.0548)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8158 (-0.7911)\n",
            "Epoch: [52][170/195]\tTime  0.982 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9960 (0.9641)\tD(fake) 0.0603 (0.0552)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7887 (-0.7911)\n",
            "Epoch: [52][180/195]\tTime  0.990 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0296 (0.9639)\tD(fake) 0.0194 (0.0560)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8074 (-0.7913)\n",
            "Epoch: [52][190/195]\tTime  0.998 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0129 (0.9638)\tD(fake) 0.0343 (0.0559)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7369 (-0.7914)\n",
            "Epoch: [53][  0/195]\tTime  1.383 ( 1.383)\tData  0.335 ( 0.335)\tD(real) 0.9531 (0.9573)\tD(fake) 0.0609 (0.0799)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7497 (-0.7497)\n",
            "Epoch: [53][ 10/195]\tTime  0.979 ( 1.028)\tData  0.001 ( 0.031)\tD(real) 0.9350 (0.9664)\tD(fake) 0.0372 (0.0487)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8358 (-0.7781)\n",
            "Epoch: [53][ 20/195]\tTime  0.983 ( 1.005)\tData  0.000 ( 0.016)\tD(real) 1.0369 (0.9661)\tD(fake) -0.0453 (0.0531)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7863 (-0.7837)\n",
            "Epoch: [53][ 30/195]\tTime  0.984 ( 0.998)\tData  0.000 ( 0.011)\tD(real) 0.9156 (0.9650)\tD(fake) 0.1093 (0.0539)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7944 (-0.7836)\n",
            "Epoch: [53][ 40/195]\tTime  0.983 ( 0.995)\tData  0.000 ( 0.009)\tD(real) 0.9452 (0.9652)\tD(fake) -0.0073 (0.0535)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8012 (-0.7880)\n",
            "Epoch: [53][ 50/195]\tTime  0.985 ( 0.992)\tData  0.000 ( 0.007)\tD(real) 0.9358 (0.9648)\tD(fake) 0.0791 (0.0546)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8117 (-0.7893)\n",
            "Epoch: [53][ 60/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.006)\tD(real) 1.0346 (0.9644)\tD(fake) -0.0171 (0.0558)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8031 (-0.7904)\n",
            "Epoch: [53][ 70/195]\tTime  0.979 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 0.9470 (0.9645)\tD(fake) 0.0217 (0.0546)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8473 (-0.7924)\n",
            "Epoch: [53][ 80/195]\tTime  1.000 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0192 (0.9645)\tD(fake) -0.0224 (0.0549)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8152 (-0.7928)\n",
            "Epoch: [53][ 90/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0123 (0.9641)\tD(fake) 0.0289 (0.0557)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8067 (-0.7926)\n",
            "Epoch: [53][100/195]\tTime  1.013 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0102 (0.9638)\tD(fake) 0.0018 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7915 (-0.7910)\n",
            "Epoch: [53][110/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0375 (0.9637)\tD(fake) 0.0107 (0.0564)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7925 (-0.7908)\n",
            "Epoch: [53][120/195]\tTime  0.982 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0029 (0.9635)\tD(fake) 0.0079 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8008 (-0.7920)\n",
            "Epoch: [53][130/195]\tTime  1.012 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0486 (0.9637)\tD(fake) 0.0316 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7904 (-0.7918)\n",
            "Epoch: [53][140/195]\tTime  0.985 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9451 (0.9636)\tD(fake) 0.0657 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8227 (-0.7917)\n",
            "Epoch: [53][150/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0198 (0.9636)\tD(fake) 0.0394 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8077 (-0.7913)\n",
            "Epoch: [53][160/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9727 (0.9636)\tD(fake) 0.0077 (0.0565)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8001 (-0.7916)\n",
            "Epoch: [53][170/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9512 (0.9634)\tD(fake) 0.0615 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7916 (-0.7918)\n",
            "Epoch: [53][180/195]\tTime  1.003 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0789 (0.9634)\tD(fake) 0.0099 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7849 (-0.7906)\n",
            "Epoch: [53][190/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0368 (0.9635)\tD(fake) -0.0291 (0.0565)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7929 (-0.7908)\n",
            "Epoch: [54][  0/195]\tTime  1.389 ( 1.389)\tData  0.345 ( 0.345)\tD(real) 0.9283 (0.9470)\tD(fake) 0.0703 (0.0982)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8267 (-0.8267)\n",
            "Epoch: [54][ 10/195]\tTime  0.996 ( 1.028)\tData  0.000 ( 0.032)\tD(real) 0.9526 (0.9626)\tD(fake) 0.0832 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7738 (-0.7846)\n",
            "Epoch: [54][ 20/195]\tTime  0.982 ( 1.008)\tData  0.000 ( 0.017)\tD(real) 0.9877 (0.9634)\tD(fake) 0.0698 (0.0552)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7911 (-0.7809)\n",
            "Epoch: [54][ 30/195]\tTime  0.992 ( 1.002)\tData  0.000 ( 0.011)\tD(real) 1.0421 (0.9644)\tD(fake) 0.0436 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7677 (-0.7849)\n",
            "Epoch: [54][ 40/195]\tTime  0.982 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 0.9617 (0.9647)\tD(fake) -0.0114 (0.0536)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8125 (-0.7872)\n",
            "Epoch: [54][ 50/195]\tTime  0.983 ( 0.998)\tData  0.000 ( 0.007)\tD(real) 1.0565 (0.9645)\tD(fake) 0.0483 (0.0544)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7618 (-0.7877)\n",
            "Epoch: [54][ 60/195]\tTime  0.987 ( 0.997)\tData  0.000 ( 0.006)\tD(real) 1.0231 (0.9647)\tD(fake) -0.0274 (0.0543)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7895 (-0.7895)\n",
            "Epoch: [54][ 70/195]\tTime  0.998 ( 0.996)\tData  0.000 ( 0.005)\tD(real) 1.0154 (0.9649)\tD(fake) 0.0107 (0.0538)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7904 (-0.7887)\n",
            "Epoch: [54][ 80/195]\tTime  1.008 ( 0.996)\tData  0.000 ( 0.005)\tD(real) 0.9871 (0.9651)\tD(fake) -0.0170 (0.0536)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6655 (-0.7872)\n",
            "Epoch: [54][ 90/195]\tTime  0.979 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 0.9461 (0.9653)\tD(fake) 0.0458 (0.0528)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8241 (-0.7878)\n",
            "Epoch: [54][100/195]\tTime  1.009 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 1.0629 (0.9657)\tD(fake) 0.0336 (0.0523)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7889 (-0.7892)\n",
            "Epoch: [54][110/195]\tTime  0.997 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0020 (0.9656)\tD(fake) 0.0236 (0.0523)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8020 (-0.7899)\n",
            "Epoch: [54][120/195]\tTime  1.003 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9851 (0.9657)\tD(fake) 0.0281 (0.0524)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8083 (-0.7909)\n",
            "Epoch: [54][130/195]\tTime  0.999 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9495 (0.9659)\tD(fake) 0.0257 (0.0517)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8056 (-0.7912)\n",
            "Epoch: [54][140/195]\tTime  0.991 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9203 (0.9659)\tD(fake) 0.0472 (0.0514)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8035 (-0.7912)\n",
            "Epoch: [54][150/195]\tTime  0.997 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0291 (0.9659)\tD(fake) -0.0185 (0.0516)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8088 (-0.7907)\n",
            "Epoch: [54][160/195]\tTime  0.996 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 0.8275 (0.9657)\tD(fake) 0.1183 (0.0520)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8005 (-0.7912)\n",
            "Epoch: [54][170/195]\tTime  0.997 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 1.0651 (0.9658)\tD(fake) 0.0105 (0.0522)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7806 (-0.7912)\n",
            "Epoch: [54][180/195]\tTime  0.989 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 0.9321 (0.9657)\tD(fake) 0.1388 (0.0523)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7647 (-0.7903)\n",
            "Epoch: [54][190/195]\tTime  0.975 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 1.0205 (0.9656)\tD(fake) 0.0144 (0.0524)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8016 (-0.7899)\n",
            "Epoch: [55][  0/195]\tTime  1.407 ( 1.407)\tData  0.354 ( 0.354)\tD(real) 1.0646 (0.9702)\tD(fake) -0.0051 (0.0456)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8096 (-0.8096)\n",
            "Epoch: [55][ 10/195]\tTime  0.988 ( 1.026)\tData  0.000 ( 0.033)\tD(real) 0.9991 (0.9640)\tD(fake) 0.1263 (0.0596)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7894 (-0.7772)\n",
            "Epoch: [55][ 20/195]\tTime  0.978 ( 1.007)\tData  0.000 ( 0.017)\tD(real) 0.9833 (0.9642)\tD(fake) 0.0392 (0.0583)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7822 (-0.7775)\n",
            "Epoch: [55][ 30/195]\tTime  0.995 ( 1.002)\tData  0.000 ( 0.012)\tD(real) 0.9723 (0.9641)\tD(fake) 0.0126 (0.0570)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8248 (-0.7827)\n",
            "Epoch: [55][ 40/195]\tTime  0.983 ( 1.001)\tData  0.000 ( 0.009)\tD(real) 0.9275 (0.9638)\tD(fake) 0.0820 (0.0577)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7741 (-0.7865)\n",
            "Epoch: [55][ 50/195]\tTime  0.993 ( 0.999)\tData  0.000 ( 0.007)\tD(real) 1.0268 (0.9642)\tD(fake) 0.0201 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8091 (-0.7876)\n",
            "Epoch: [55][ 60/195]\tTime  1.003 ( 0.998)\tData  0.000 ( 0.006)\tD(real) 0.9948 (0.9642)\tD(fake) 0.0311 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7646 (-0.7889)\n",
            "Epoch: [55][ 70/195]\tTime  0.983 ( 0.997)\tData  0.000 ( 0.005)\tD(real) 0.9800 (0.9638)\tD(fake) 0.0011 (0.0573)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7931 (-0.7903)\n",
            "Epoch: [55][ 80/195]\tTime  1.004 ( 0.996)\tData  0.000 ( 0.005)\tD(real) 1.0538 (0.9640)\tD(fake) 0.0083 (0.0566)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8090 (-0.7921)\n",
            "Epoch: [55][ 90/195]\tTime  0.989 ( 0.996)\tData  0.000 ( 0.004)\tD(real) 0.9863 (0.9638)\tD(fake) 0.0255 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7980 (-0.7933)\n",
            "Epoch: [55][100/195]\tTime  0.991 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 0.9807 (0.9641)\tD(fake) -0.0029 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8018 (-0.7927)\n",
            "Epoch: [55][110/195]\tTime  1.004 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 0.9112 (0.9639)\tD(fake) 0.1495 (0.0569)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8182 (-0.7930)\n",
            "Epoch: [55][120/195]\tTime  0.997 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 1.0081 (0.9641)\tD(fake) 0.0172 (0.0565)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7808 (-0.7925)\n",
            "Epoch: [55][130/195]\tTime  0.987 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 1.0442 (0.9641)\tD(fake) -0.0258 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7233 (-0.7929)\n",
            "Epoch: [55][140/195]\tTime  0.991 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9337 (0.9640)\tD(fake) 0.0915 (0.0569)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8012 (-0.7932)\n",
            "Epoch: [55][150/195]\tTime  0.976 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9699 (0.9641)\tD(fake) 0.0134 (0.0566)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8146 (-0.7933)\n",
            "Epoch: [55][160/195]\tTime  0.982 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9085 (0.9640)\tD(fake) 0.1200 (0.0566)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8355 (-0.7930)\n",
            "Epoch: [55][170/195]\tTime  1.004 ( 0.993)\tData  0.001 ( 0.002)\tD(real) 0.9717 (0.9642)\tD(fake) -0.0187 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8131 (-0.7934)\n",
            "Epoch: [55][180/195]\tTime  0.978 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 1.0240 (0.9642)\tD(fake) 0.0564 (0.0564)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7776 (-0.7933)\n",
            "Epoch: [55][190/195]\tTime  0.986 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 1.0067 (0.9640)\tD(fake) 0.0187 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8102 (-0.7943)\n",
            "Epoch: [56][  0/195]\tTime  1.415 ( 1.415)\tData  0.367 ( 0.367)\tD(real) 0.9470 (0.9729)\tD(fake) -0.0006 (-0.0030)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8232 (-0.8232)\n",
            "Epoch: [56][ 10/195]\tTime  1.003 ( 1.031)\tData  0.000 ( 0.034)\tD(real) 0.9554 (0.9653)\tD(fake) 0.0199 (0.0527)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8156 (-0.7848)\n",
            "Epoch: [56][ 20/195]\tTime  0.984 ( 1.013)\tData  0.000 ( 0.018)\tD(real) 0.9284 (0.9644)\tD(fake) 0.0574 (0.0565)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8122 (-0.7911)\n",
            "Epoch: [56][ 30/195]\tTime  0.982 ( 1.004)\tData  0.000 ( 0.012)\tD(real) 0.9526 (0.9646)\tD(fake) 0.0006 (0.0557)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8128 (-0.7892)\n",
            "Epoch: [56][ 40/195]\tTime  0.992 ( 1.000)\tData  0.000 ( 0.009)\tD(real) 0.9492 (0.9639)\tD(fake) 0.1222 (0.0586)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7683 (-0.7875)\n",
            "Epoch: [56][ 50/195]\tTime  0.989 ( 0.998)\tData  0.000 ( 0.008)\tD(real) 1.0508 (0.9641)\tD(fake) 0.0901 (0.0586)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8300 (-0.7916)\n",
            "Epoch: [56][ 60/195]\tTime  1.006 ( 0.998)\tData  0.000 ( 0.006)\tD(real) 1.0132 (0.9641)\tD(fake) -0.0433 (0.0586)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7748 (-0.7925)\n",
            "Epoch: [56][ 70/195]\tTime  0.997 ( 0.997)\tData  0.000 ( 0.006)\tD(real) 0.9541 (0.9637)\tD(fake) 0.0548 (0.0589)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7772 (-0.7920)\n",
            "Epoch: [56][ 80/195]\tTime  0.982 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 1.0206 (0.9636)\tD(fake) 0.0084 (0.0587)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7969 (-0.7929)\n",
            "Epoch: [56][ 90/195]\tTime  0.984 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 1.0234 (0.9638)\tD(fake) -0.0111 (0.0580)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7761 (-0.7933)\n",
            "Epoch: [56][100/195]\tTime  1.011 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 0.9151 (0.9637)\tD(fake) 0.0793 (0.0576)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7465 (-0.7938)\n",
            "Epoch: [56][110/195]\tTime  0.985 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 0.9895 (0.9640)\tD(fake) 0.0322 (0.0569)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7969 (-0.7937)\n",
            "Epoch: [56][120/195]\tTime  1.001 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 1.0066 (0.9640)\tD(fake) 0.0182 (0.0566)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7970 (-0.7930)\n",
            "Epoch: [56][130/195]\tTime  0.987 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0260 (0.9642)\tD(fake) -0.0406 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7934 (-0.7931)\n",
            "Epoch: [56][140/195]\tTime  0.991 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9892 (0.9640)\tD(fake) 0.0184 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7980 (-0.7933)\n",
            "Epoch: [56][150/195]\tTime  0.998 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0313 (0.9640)\tD(fake) 0.0048 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8088 (-0.7943)\n",
            "Epoch: [56][160/195]\tTime  0.994 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0582 (0.9640)\tD(fake) -0.0180 (0.0564)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8212 (-0.7942)\n",
            "Epoch: [56][170/195]\tTime  0.989 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 1.0065 (0.9639)\tD(fake) 0.0003 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7563 (-0.7933)\n",
            "Epoch: [56][180/195]\tTime  0.979 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9514 (0.9639)\tD(fake) 0.0329 (0.0562)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8130 (-0.7931)\n",
            "Epoch: [56][190/195]\tTime  0.990 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9549 (0.9640)\tD(fake) 0.0183 (0.0560)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7897 (-0.7930)\n",
            "Epoch: [57][  0/195]\tTime  1.399 ( 1.399)\tData  0.339 ( 0.339)\tD(real) 1.0044 (0.9603)\tD(fake) 0.0340 (0.0998)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8139 (-0.8139)\n",
            "Epoch: [57][ 10/195]\tTime  0.995 ( 1.029)\tData  0.000 ( 0.031)\tD(real) 1.0260 (0.9640)\tD(fake) 0.0161 (0.0604)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7855 (-0.8008)\n",
            "Epoch: [57][ 20/195]\tTime  0.994 ( 1.010)\tData  0.000 ( 0.016)\tD(real) 1.0166 (0.9642)\tD(fake) 0.0126 (0.0605)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7955 (-0.7996)\n",
            "Epoch: [57][ 30/195]\tTime  0.994 ( 1.004)\tData  0.000 ( 0.011)\tD(real) 0.9106 (0.9646)\tD(fake) 0.0842 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8119 (-0.7987)\n",
            "Epoch: [57][ 40/195]\tTime  0.981 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 0.8691 (0.9638)\tD(fake) 0.1446 (0.0577)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7912 (-0.7988)\n",
            "Epoch: [57][ 50/195]\tTime  0.985 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 0.9047 (0.9641)\tD(fake) 0.0732 (0.0570)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8282 (-0.7957)\n",
            "Epoch: [57][ 60/195]\tTime  0.981 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 1.0457 (0.9644)\tD(fake) 0.0832 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7666 (-0.7935)\n",
            "Epoch: [57][ 70/195]\tTime  0.987 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9416 (0.9641)\tD(fake) 0.1011 (0.0566)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7935 (-0.7930)\n",
            "Epoch: [57][ 80/195]\tTime  0.972 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0338 (0.9642)\tD(fake) 0.0300 (0.0571)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8507 (-0.7938)\n",
            "Epoch: [57][ 90/195]\tTime  0.977 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9357 (0.9640)\tD(fake) 0.0905 (0.0572)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7733 (-0.7915)\n",
            "Epoch: [57][100/195]\tTime  1.000 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0773 (0.9644)\tD(fake) 0.0089 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8049 (-0.7917)\n",
            "Epoch: [57][110/195]\tTime  0.979 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0347 (0.9644)\tD(fake) 0.0141 (0.0565)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7731 (-0.7911)\n",
            "Epoch: [57][120/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9737 (0.9645)\tD(fake) 0.0219 (0.0562)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8117 (-0.7915)\n",
            "Epoch: [57][130/195]\tTime  0.985 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0076 (0.9644)\tD(fake) 0.0231 (0.0566)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7875 (-0.7923)\n",
            "Epoch: [57][140/195]\tTime  1.002 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9416 (0.9643)\tD(fake) 0.0195 (0.0564)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8095 (-0.7928)\n",
            "Epoch: [57][150/195]\tTime  0.992 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0033 (0.9643)\tD(fake) -0.0139 (0.0563)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8185 (-0.7908)\n",
            "Epoch: [57][160/195]\tTime  0.986 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9899 (0.9645)\tD(fake) 0.0120 (0.0560)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7821 (-0.7916)\n",
            "Epoch: [57][170/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0458 (0.9647)\tD(fake) 0.0040 (0.0557)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8410 (-0.7923)\n",
            "Epoch: [57][180/195]\tTime  0.989 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9802 (0.9646)\tD(fake) 0.0651 (0.0554)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7941 (-0.7927)\n",
            "Epoch: [57][190/195]\tTime  0.974 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0155 (0.9645)\tD(fake) 0.0345 (0.0556)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7751 (-0.7928)\n",
            "Epoch: [58][  0/195]\tTime  1.389 ( 1.389)\tData  0.343 ( 0.343)\tD(real) 0.9885 (0.9593)\tD(fake) -0.0044 (0.0285)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8168 (-0.8168)\n",
            "Epoch: [58][ 10/195]\tTime  0.990 ( 1.027)\tData  0.000 ( 0.032)\tD(real) 0.9340 (0.9635)\tD(fake) 0.0785 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7776 (-0.7976)\n",
            "Epoch: [58][ 20/195]\tTime  0.983 ( 1.005)\tData  0.000 ( 0.017)\tD(real) 0.9924 (0.9642)\tD(fake) 0.0153 (0.0544)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8100 (-0.8043)\n",
            "Epoch: [58][ 30/195]\tTime  0.995 ( 1.000)\tData  0.000 ( 0.011)\tD(real) 1.0311 (0.9647)\tD(fake) -0.0428 (0.0546)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8112 (-0.8012)\n",
            "Epoch: [58][ 40/195]\tTime  0.977 ( 0.996)\tData  0.000 ( 0.009)\tD(real) 0.9352 (0.9647)\tD(fake) 0.0555 (0.0535)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7912 (-0.7957)\n",
            "Epoch: [58][ 50/195]\tTime  0.985 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 0.9580 (0.9641)\tD(fake) 0.0563 (0.0554)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8127 (-0.7975)\n",
            "Epoch: [58][ 60/195]\tTime  0.997 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 0.9925 (0.9647)\tD(fake) 0.0828 (0.0549)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7477 (-0.7974)\n",
            "Epoch: [58][ 70/195]\tTime  0.996 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0022 (0.9651)\tD(fake) 0.0485 (0.0547)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8036 (-0.7980)\n",
            "Epoch: [58][ 80/195]\tTime  0.985 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 0.9043 (0.9648)\tD(fake) 0.0962 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8037 (-0.7958)\n",
            "Epoch: [58][ 90/195]\tTime  0.980 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9182 (0.9648)\tD(fake) 0.0989 (0.0553)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8155 (-0.7966)\n",
            "Epoch: [58][100/195]\tTime  0.988 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0159 (0.9650)\tD(fake) 0.0655 (0.0551)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7905 (-0.7962)\n",
            "Epoch: [58][110/195]\tTime  0.983 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0182 (0.9651)\tD(fake) 0.0063 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8251 (-0.7960)\n",
            "Epoch: [58][120/195]\tTime  0.996 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9279 (0.9651)\tD(fake) 0.0653 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8083 (-0.7958)\n",
            "Epoch: [58][130/195]\tTime  0.986 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9583 (0.9650)\tD(fake) 0.0472 (0.0554)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7931 (-0.7959)\n",
            "Epoch: [58][140/195]\tTime  0.982 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0276 (0.9650)\tD(fake) -0.0122 (0.0553)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7556 (-0.7950)\n",
            "Epoch: [58][150/195]\tTime  0.986 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9607 (0.9651)\tD(fake) 0.0395 (0.0548)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7380 (-0.7940)\n",
            "Epoch: [58][160/195]\tTime  0.975 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9065 (0.9650)\tD(fake) 0.0660 (0.0549)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8346 (-0.7949)\n",
            "Epoch: [58][170/195]\tTime  0.992 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9501 (0.9650)\tD(fake) 0.1051 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8346 (-0.7949)\n",
            "Epoch: [58][180/195]\tTime  0.991 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0368 (0.9651)\tD(fake) 0.0099 (0.0548)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8013 (-0.7953)\n",
            "Epoch: [58][190/195]\tTime  0.979 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9946 (0.9651)\tD(fake) 0.0029 (0.0545)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6991 (-0.7949)\n",
            "Epoch: [59][  0/195]\tTime  1.420 ( 1.420)\tData  0.350 ( 0.350)\tD(real) 0.9500 (0.9573)\tD(fake) 0.0245 (0.0508)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7852 (-0.7852)\n",
            "Epoch: [59][ 10/195]\tTime  0.997 ( 1.030)\tData  0.000 ( 0.032)\tD(real) 0.9178 (0.9634)\tD(fake) 0.1112 (0.0593)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8339 (-0.8085)\n",
            "Epoch: [59][ 20/195]\tTime  1.007 ( 1.010)\tData  0.000 ( 0.017)\tD(real) 0.9614 (0.9661)\tD(fake) 0.0183 (0.0536)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7875 (-0.8011)\n",
            "Epoch: [59][ 30/195]\tTime  0.984 ( 1.003)\tData  0.000 ( 0.012)\tD(real) 0.9975 (0.9654)\tD(fake) 0.0507 (0.0550)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8042 (-0.8002)\n",
            "Epoch: [59][ 40/195]\tTime  1.007 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 0.9632 (0.9654)\tD(fake) 0.0305 (0.0539)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8034 (-0.7998)\n",
            "Epoch: [59][ 50/195]\tTime  0.981 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 0.9555 (0.9655)\tD(fake) 0.0279 (0.0536)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8032 (-0.7987)\n",
            "Epoch: [59][ 60/195]\tTime  0.999 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0279 (0.9648)\tD(fake) 0.0854 (0.0558)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7892 (-0.7961)\n",
            "Epoch: [59][ 70/195]\tTime  1.002 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 0.9712 (0.9645)\tD(fake) -0.0080 (0.0565)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8100 (-0.7976)\n",
            "Epoch: [59][ 80/195]\tTime  1.006 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9562 (0.9645)\tD(fake) 0.0544 (0.0569)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8081 (-0.7977)\n",
            "Epoch: [59][ 90/195]\tTime  0.988 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0260 (0.9647)\tD(fake) 0.0306 (0.0562)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8142 (-0.7969)\n",
            "Epoch: [59][100/195]\tTime  1.002 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9245 (0.9646)\tD(fake) 0.0100 (0.0559)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7677 (-0.7975)\n",
            "Epoch: [59][110/195]\tTime  0.991 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0081 (0.9646)\tD(fake) -0.0141 (0.0565)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8199 (-0.7964)\n",
            "Epoch: [59][120/195]\tTime  0.989 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9871 (0.9647)\tD(fake) 0.0288 (0.0562)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8114 (-0.7970)\n",
            "Epoch: [59][130/195]\tTime  0.992 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9494 (0.9647)\tD(fake) 0.0234 (0.0559)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8303 (-0.7962)\n",
            "Epoch: [59][140/195]\tTime  0.985 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0217 (0.9648)\tD(fake) 0.0357 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8154 (-0.7959)\n",
            "Epoch: [59][150/195]\tTime  1.001 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0106 (0.9648)\tD(fake) -0.0179 (0.0561)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7677 (-0.7964)\n",
            "Epoch: [59][160/195]\tTime  0.998 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0428 (0.9647)\tD(fake) -0.0000 (0.0562)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8241 (-0.7967)\n",
            "Epoch: [59][170/195]\tTime  1.012 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0366 (0.9646)\tD(fake) 0.0655 (0.0564)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8085 (-0.7968)\n",
            "Epoch: [59][180/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0222 (0.9644)\tD(fake) 0.0280 (0.0568)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7868 (-0.7967)\n",
            "Epoch: [59][190/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9252 (0.9644)\tD(fake) 0.0696 (0.0567)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8105 (-0.7960)\n",
            "Epoch: [60][  0/195]\tTime  1.363 ( 1.363)\tData  0.333 ( 0.333)\tD(real) 1.0208 (0.9707)\tD(fake) -0.0072 (0.0459)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7952 (-0.7952)\n",
            "Epoch: [60][ 10/195]\tTime  0.981 ( 1.024)\tData  0.000 ( 0.031)\tD(real) 1.0402 (0.9716)\tD(fake) -0.0199 (0.0423)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8381 (-0.8125)\n",
            "Epoch: [60][ 20/195]\tTime  0.987 ( 1.006)\tData  0.000 ( 0.016)\tD(real) 0.9277 (0.9718)\tD(fake) 0.0400 (0.0402)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8229 (-0.8142)\n",
            "Epoch: [60][ 30/195]\tTime  0.990 ( 1.002)\tData  0.000 ( 0.011)\tD(real) 0.9175 (0.9723)\tD(fake) 0.0872 (0.0391)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8163 (-0.8150)\n",
            "Epoch: [60][ 40/195]\tTime  0.988 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 1.0318 (0.9736)\tD(fake) -0.0184 (0.0372)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8537 (-0.8139)\n",
            "Epoch: [60][ 50/195]\tTime  1.005 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 1.0281 (0.9738)\tD(fake) -0.0039 (0.0363)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7594 (-0.8106)\n",
            "Epoch: [60][ 60/195]\tTime  0.995 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0106 (0.9743)\tD(fake) 0.0229 (0.0353)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7310 (-0.8090)\n",
            "Epoch: [60][ 70/195]\tTime  0.995 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9420 (0.9746)\tD(fake) 0.0277 (0.0343)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8185 (-0.8108)\n",
            "Epoch: [60][ 80/195]\tTime  0.982 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0397 (0.9753)\tD(fake) -0.0063 (0.0332)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8247 (-0.8124)\n",
            "Epoch: [60][ 90/195]\tTime  0.982 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0138 (0.9758)\tD(fake) 0.0128 (0.0320)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8288 (-0.8128)\n",
            "Epoch: [60][100/195]\tTime  1.006 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9682 (0.9761)\tD(fake) 0.0011 (0.0312)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8336 (-0.8151)\n",
            "Epoch: [60][110/195]\tTime  0.990 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9684 (0.9762)\tD(fake) 0.0139 (0.0312)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7856 (-0.8123)\n",
            "Epoch: [60][120/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0384 (0.9766)\tD(fake) -0.0337 (0.0306)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8425 (-0.8137)\n",
            "Epoch: [60][130/195]\tTime  0.975 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9370 (0.9767)\tD(fake) 0.0283 (0.0305)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8277 (-0.8152)\n",
            "Epoch: [60][140/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9662 (0.9768)\tD(fake) 0.0452 (0.0302)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8190 (-0.8155)\n",
            "Epoch: [60][150/195]\tTime  0.997 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0053 (0.9769)\tD(fake) -0.0034 (0.0302)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8597 (-0.8166)\n",
            "Epoch: [60][160/195]\tTime  0.979 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0573 (0.9771)\tD(fake) -0.0090 (0.0300)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8411 (-0.8177)\n",
            "Epoch: [60][170/195]\tTime  0.986 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0326 (0.9772)\tD(fake) -0.0039 (0.0297)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8477 (-0.8178)\n",
            "Epoch: [60][180/195]\tTime  0.977 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9240 (0.9771)\tD(fake) 0.0454 (0.0297)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7665 (-0.8181)\n",
            "Epoch: [60][190/195]\tTime  0.987 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9620 (0.9773)\tD(fake) 0.0055 (0.0291)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8565 (-0.8189)\n",
            "Epoch: [61][  0/195]\tTime  1.406 ( 1.406)\tData  0.358 ( 0.358)\tD(real) 1.0184 (0.9909)\tD(fake) -0.0031 (0.0010)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8320 (-0.8320)\n",
            "Epoch: [61][ 10/195]\tTime  0.994 ( 1.027)\tData  0.000 ( 0.033)\tD(real) 0.9098 (0.9814)\tD(fake) -0.0155 (0.0200)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8315 (-0.8334)\n",
            "Epoch: [61][ 20/195]\tTime  0.982 ( 1.008)\tData  0.000 ( 0.017)\tD(real) 0.9922 (0.9805)\tD(fake) 0.0257 (0.0249)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8482 (-0.8365)\n",
            "Epoch: [61][ 30/195]\tTime  0.974 ( 1.000)\tData  0.000 ( 0.012)\tD(real) 0.9628 (0.9803)\tD(fake) -0.0035 (0.0247)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8252 (-0.8340)\n",
            "Epoch: [61][ 40/195]\tTime  0.997 ( 0.996)\tData  0.000 ( 0.009)\tD(real) 0.9519 (0.9800)\tD(fake) 0.0468 (0.0255)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8559 (-0.8354)\n",
            "Epoch: [61][ 50/195]\tTime  0.984 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 0.9555 (0.9799)\tD(fake) 0.0470 (0.0262)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8285 (-0.8362)\n",
            "Epoch: [61][ 60/195]\tTime  0.979 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 1.0353 (0.9799)\tD(fake) 0.0278 (0.0260)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8435 (-0.8309)\n",
            "Epoch: [61][ 70/195]\tTime  0.990 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0613 (0.9798)\tD(fake) 0.0051 (0.0264)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8466 (-0.8297)\n",
            "Epoch: [61][ 80/195]\tTime  0.993 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 0.9688 (0.9798)\tD(fake) 0.0587 (0.0265)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7805 (-0.8286)\n",
            "Epoch: [61][ 90/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0050 (0.9798)\tD(fake) 0.0380 (0.0263)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8015 (-0.8274)\n",
            "Epoch: [61][100/195]\tTime  0.990 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0278 (0.9799)\tD(fake) 0.0071 (0.0264)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8480 (-0.8285)\n",
            "Epoch: [61][110/195]\tTime  0.991 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0532 (0.9800)\tD(fake) -0.0151 (0.0263)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7935 (-0.8286)\n",
            "Epoch: [61][120/195]\tTime  1.002 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9436 (0.9800)\tD(fake) 0.0294 (0.0260)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8409 (-0.8296)\n",
            "Epoch: [61][130/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9959 (0.9801)\tD(fake) -0.0093 (0.0259)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8799 (-0.8309)\n",
            "Epoch: [61][140/195]\tTime  1.002 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0230 (0.9801)\tD(fake) 0.0276 (0.0257)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7972 (-0.8299)\n",
            "Epoch: [61][150/195]\tTime  0.987 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0195 (0.9801)\tD(fake) 0.0127 (0.0258)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8498 (-0.8308)\n",
            "Epoch: [61][160/195]\tTime  0.987 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9889 (0.9802)\tD(fake) -0.0100 (0.0256)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8555 (-0.8310)\n",
            "Epoch: [61][170/195]\tTime  0.976 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9774 (0.9802)\tD(fake) 0.0186 (0.0254)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8342 (-0.8307)\n",
            "Epoch: [61][180/195]\tTime  0.976 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0033 (0.9803)\tD(fake) 0.0131 (0.0254)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8404 (-0.8310)\n",
            "Epoch: [61][190/195]\tTime  1.001 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9544 (0.9802)\tD(fake) 0.0178 (0.0254)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8475 (-0.8319)\n",
            "Epoch: [62][  0/195]\tTime  1.384 ( 1.384)\tData  0.331 ( 0.331)\tD(real) 1.0144 (0.9869)\tD(fake) 0.0052 (0.0140)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8430 (-0.8430)\n",
            "Epoch: [62][ 10/195]\tTime  0.980 ( 1.021)\tData  0.000 ( 0.030)\tD(real) 0.9256 (0.9806)\tD(fake) 0.0290 (0.0233)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8355 (-0.8245)\n",
            "Epoch: [62][ 20/195]\tTime  0.981 ( 1.002)\tData  0.000 ( 0.016)\tD(real) 0.9689 (0.9813)\tD(fake) 0.0056 (0.0225)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8452 (-0.8331)\n",
            "Epoch: [62][ 30/195]\tTime  0.980 ( 0.997)\tData  0.000 ( 0.011)\tD(real) 1.0453 (0.9816)\tD(fake) -0.0096 (0.0229)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8652 (-0.8385)\n",
            "Epoch: [62][ 40/195]\tTime  0.976 ( 0.995)\tData  0.000 ( 0.008)\tD(real) 1.0221 (0.9811)\tD(fake) -0.0226 (0.0233)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8376 (-0.8406)\n",
            "Epoch: [62][ 50/195]\tTime  1.000 ( 0.993)\tData  0.000 ( 0.007)\tD(real) 0.9621 (0.9809)\tD(fake) -0.0048 (0.0231)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8397 (-0.8396)\n",
            "Epoch: [62][ 60/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.006)\tD(real) 1.0043 (0.9809)\tD(fake) 0.0222 (0.0242)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8699 (-0.8390)\n",
            "Epoch: [62][ 70/195]\tTime  0.978 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 1.0240 (0.9812)\tD(fake) 0.0091 (0.0239)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8395 (-0.8401)\n",
            "Epoch: [62][ 80/195]\tTime  0.982 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9794 (0.9811)\tD(fake) 0.0498 (0.0240)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8344 (-0.8392)\n",
            "Epoch: [62][ 90/195]\tTime  0.994 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0299 (0.9812)\tD(fake) -0.0063 (0.0241)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8475 (-0.8389)\n",
            "Epoch: [62][100/195]\tTime  0.994 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0372 (0.9812)\tD(fake) 0.0003 (0.0239)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7102 (-0.8384)\n",
            "Epoch: [62][110/195]\tTime  0.982 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9733 (0.9811)\tD(fake) -0.0024 (0.0239)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8492 (-0.8382)\n",
            "Epoch: [62][120/195]\tTime  0.985 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9100 (0.9810)\tD(fake) 0.0472 (0.0242)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8496 (-0.8389)\n",
            "Epoch: [62][130/195]\tTime  0.982 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9838 (0.9812)\tD(fake) 0.0562 (0.0240)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8725 (-0.8396)\n",
            "Epoch: [62][140/195]\tTime  1.001 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0489 (0.9814)\tD(fake) 0.0040 (0.0238)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8442 (-0.8404)\n",
            "Epoch: [62][150/195]\tTime  0.976 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0125 (0.9813)\tD(fake) 0.0015 (0.0237)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8604 (-0.8409)\n",
            "Epoch: [62][160/195]\tTime  0.978 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9162 (0.9813)\tD(fake) -0.0128 (0.0236)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8627 (-0.8406)\n",
            "Epoch: [62][170/195]\tTime  0.985 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9204 (0.9812)\tD(fake) 0.0060 (0.0238)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8522 (-0.8410)\n",
            "Epoch: [62][180/195]\tTime  0.980 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9689 (0.9812)\tD(fake) 0.0316 (0.0240)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8503 (-0.8420)\n",
            "Epoch: [62][190/195]\tTime  0.982 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9287 (0.9812)\tD(fake) 0.0482 (0.0239)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8623 (-0.8426)\n",
            "Epoch: [63][  0/195]\tTime  1.435 ( 1.435)\tData  0.375 ( 0.375)\tD(real) 1.0056 (0.9881)\tD(fake) 0.0091 (0.0450)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8329 (-0.8329)\n",
            "Epoch: [63][ 10/195]\tTime  0.989 ( 1.028)\tData  0.000 ( 0.035)\tD(real) 1.0200 (0.9820)\tD(fake) 0.0273 (0.0235)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8870 (-0.8631)\n",
            "Epoch: [63][ 20/195]\tTime  0.978 ( 1.006)\tData  0.000 ( 0.018)\tD(real) 0.9367 (0.9817)\tD(fake) 0.0268 (0.0239)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8630 (-0.8524)\n",
            "Epoch: [63][ 30/195]\tTime  1.006 ( 0.999)\tData  0.000 ( 0.012)\tD(real) 1.0472 (0.9818)\tD(fake) -0.0224 (0.0246)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8815 (-0.8567)\n",
            "Epoch: [63][ 40/195]\tTime  0.998 ( 0.997)\tData  0.000 ( 0.010)\tD(real) 0.9444 (0.9816)\tD(fake) -0.0031 (0.0242)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8720 (-0.8523)\n",
            "Epoch: [63][ 50/195]\tTime  1.005 ( 0.995)\tData  0.000 ( 0.008)\tD(real) 1.0339 (0.9818)\tD(fake) 0.0163 (0.0242)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8525 (-0.8555)\n",
            "Epoch: [63][ 60/195]\tTime  0.977 ( 0.993)\tData  0.000 ( 0.007)\tD(real) 1.0278 (0.9818)\tD(fake) 0.0065 (0.0242)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8684 (-0.8561)\n",
            "Epoch: [63][ 70/195]\tTime  0.982 ( 0.992)\tData  0.000 ( 0.006)\tD(real) 1.0159 (0.9818)\tD(fake) -0.0100 (0.0245)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8608 (-0.8547)\n",
            "Epoch: [63][ 80/195]\tTime  0.990 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0005 (0.9820)\tD(fake) -0.0243 (0.0239)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8564 (-0.8541)\n",
            "Epoch: [63][ 90/195]\tTime  0.976 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0337 (0.9820)\tD(fake) 0.0065 (0.0238)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8560 (-0.8540)\n",
            "Epoch: [63][100/195]\tTime  0.996 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0303 (0.9820)\tD(fake) 0.0071 (0.0238)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8593 (-0.8539)\n",
            "Epoch: [63][110/195]\tTime  0.982 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9641 (0.9819)\tD(fake) 0.0017 (0.0238)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8489 (-0.8539)\n",
            "Epoch: [63][120/195]\tTime  0.999 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0354 (0.9821)\tD(fake) -0.0024 (0.0239)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8401 (-0.8543)\n",
            "Epoch: [63][130/195]\tTime  0.985 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9481 (0.9820)\tD(fake) 0.0359 (0.0237)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8547 (-0.8544)\n",
            "Epoch: [63][140/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0040 (0.9820)\tD(fake) 0.0203 (0.0239)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8541 (-0.8540)\n",
            "Epoch: [63][150/195]\tTime  0.978 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9699 (0.9821)\tD(fake) 0.0380 (0.0238)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8715 (-0.8547)\n",
            "Epoch: [63][160/195]\tTime  0.979 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 1.0295 (0.9822)\tD(fake) -0.0180 (0.0236)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8338 (-0.8543)\n",
            "Epoch: [63][170/195]\tTime  0.973 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 1.0293 (0.9821)\tD(fake) 0.0040 (0.0238)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8471 (-0.8534)\n",
            "Epoch: [63][180/195]\tTime  0.983 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9152 (0.9820)\tD(fake) 0.0761 (0.0239)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8675 (-0.8533)\n",
            "Epoch: [63][190/195]\tTime  0.992 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0237 (0.9821)\tD(fake) 0.0111 (0.0240)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8476 (-0.8528)\n",
            "Epoch: [64][  0/195]\tTime  1.374 ( 1.374)\tData  0.338 ( 0.338)\tD(real) 0.9728 (0.9772)\tD(fake) -0.0081 (0.0128)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8321 (-0.8321)\n",
            "Epoch: [64][ 10/195]\tTime  0.983 ( 1.020)\tData  0.000 ( 0.031)\tD(real) 1.0313 (0.9839)\tD(fake) -0.0083 (0.0182)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8448 (-0.8540)\n",
            "Epoch: [64][ 20/195]\tTime  0.986 ( 1.002)\tData  0.000 ( 0.016)\tD(real) 0.9767 (0.9834)\tD(fake) -0.0031 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8349 (-0.8527)\n",
            "Epoch: [64][ 30/195]\tTime  0.977 ( 0.998)\tData  0.000 ( 0.011)\tD(real) 0.9574 (0.9835)\tD(fake) 0.0421 (0.0198)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8855 (-0.8562)\n",
            "Epoch: [64][ 40/195]\tTime  0.982 ( 0.995)\tData  0.000 ( 0.009)\tD(real) 1.0405 (0.9833)\tD(fake) 0.0003 (0.0209)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8418 (-0.8595)\n",
            "Epoch: [64][ 50/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.007)\tD(real) 0.9577 (0.9832)\tD(fake) -0.0025 (0.0207)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8897 (-0.8625)\n",
            "Epoch: [64][ 60/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.006)\tD(real) 1.0235 (0.9830)\tD(fake) -0.0081 (0.0218)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8687 (-0.8628)\n",
            "Epoch: [64][ 70/195]\tTime  0.997 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 0.9883 (0.9829)\tD(fake) -0.0179 (0.0214)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8493 (-0.8613)\n",
            "Epoch: [64][ 80/195]\tTime  0.972 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 1.0300 (0.9827)\tD(fake) 0.0006 (0.0220)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8449 (-0.8613)\n",
            "Epoch: [64][ 90/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9374 (0.9828)\tD(fake) 0.0665 (0.0217)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.6994 (-0.8562)\n",
            "Epoch: [64][100/195]\tTime  1.009 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9434 (0.9828)\tD(fake) 0.0534 (0.0220)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8627 (-0.8555)\n",
            "Epoch: [64][110/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0429 (0.9829)\tD(fake) -0.0109 (0.0219)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8794 (-0.8546)\n",
            "Epoch: [64][120/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9378 (0.9828)\tD(fake) 0.0399 (0.0220)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8653 (-0.8553)\n",
            "Epoch: [64][130/195]\tTime  0.991 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9690 (0.9828)\tD(fake) 0.0319 (0.0221)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7955 (-0.8554)\n",
            "Epoch: [64][140/195]\tTime  0.988 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0672 (0.9829)\tD(fake) 0.0043 (0.0218)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8697 (-0.8547)\n",
            "Epoch: [64][150/195]\tTime  0.997 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0338 (0.9829)\tD(fake) -0.0115 (0.0218)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8419 (-0.8547)\n",
            "Epoch: [64][160/195]\tTime  1.007 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0050 (0.9828)\tD(fake) 0.0107 (0.0219)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8766 (-0.8544)\n",
            "Epoch: [64][170/195]\tTime  0.983 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0337 (0.9828)\tD(fake) -0.0067 (0.0219)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8813 (-0.8550)\n",
            "Epoch: [64][180/195]\tTime  1.011 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0255 (0.9828)\tD(fake) -0.0094 (0.0220)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8517 (-0.8554)\n",
            "Epoch: [64][190/195]\tTime  0.992 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9364 (0.9827)\tD(fake) 0.0283 (0.0219)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8495 (-0.8556)\n",
            "Epoch: [65][  0/195]\tTime  1.394 ( 1.394)\tData  0.350 ( 0.350)\tD(real) 1.0209 (0.9880)\tD(fake) 0.0027 (0.0355)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8514 (-0.8514)\n",
            "Epoch: [65][ 10/195]\tTime  0.973 ( 1.023)\tData  0.000 ( 0.032)\tD(real) 0.9672 (0.9842)\tD(fake) 0.0170 (0.0217)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8507 (-0.8601)\n",
            "Epoch: [65][ 20/195]\tTime  0.977 ( 1.005)\tData  0.000 ( 0.017)\tD(real) 0.9812 (0.9844)\tD(fake) 0.0127 (0.0212)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8471 (-0.8548)\n",
            "Epoch: [65][ 30/195]\tTime  0.986 ( 0.997)\tData  0.000 ( 0.012)\tD(real) 1.0303 (0.9850)\tD(fake) -0.0105 (0.0202)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8821 (-0.8588)\n",
            "Epoch: [65][ 40/195]\tTime  0.978 ( 0.994)\tData  0.000 ( 0.009)\tD(real) 0.9480 (0.9840)\tD(fake) 0.0229 (0.0215)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8923 (-0.8579)\n",
            "Epoch: [65][ 50/195]\tTime  0.986 ( 0.992)\tData  0.000 ( 0.007)\tD(real) 1.0459 (0.9840)\tD(fake) 0.0135 (0.0211)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8929 (-0.8598)\n",
            "Epoch: [65][ 60/195]\tTime  0.995 ( 0.991)\tData  0.000 ( 0.006)\tD(real) 1.0305 (0.9838)\tD(fake) -0.0167 (0.0217)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8617 (-0.8592)\n",
            "Epoch: [65][ 70/195]\tTime  0.985 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 1.0573 (0.9839)\tD(fake) -0.0255 (0.0216)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8480 (-0.8588)\n",
            "Epoch: [65][ 80/195]\tTime  0.997 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 1.0234 (0.9837)\tD(fake) -0.0160 (0.0216)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8627 (-0.8587)\n",
            "Epoch: [65][ 90/195]\tTime  0.981 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0207 (0.9838)\tD(fake) -0.0078 (0.0213)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9011 (-0.8582)\n",
            "Epoch: [65][100/195]\tTime  1.014 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9548 (0.9837)\tD(fake) 0.0233 (0.0211)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8849 (-0.8578)\n",
            "Epoch: [65][110/195]\tTime  0.998 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0308 (0.9839)\tD(fake) -0.0061 (0.0211)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8533 (-0.8571)\n",
            "Epoch: [65][120/195]\tTime  0.982 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9749 (0.9838)\tD(fake) -0.0010 (0.0210)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8877 (-0.8573)\n",
            "Epoch: [65][130/195]\tTime  0.976 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9666 (0.9839)\tD(fake) 0.0232 (0.0209)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8670 (-0.8559)\n",
            "Epoch: [65][140/195]\tTime  0.978 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9518 (0.9839)\tD(fake) 0.0067 (0.0209)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8708 (-0.8552)\n",
            "Epoch: [65][150/195]\tTime  0.986 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 1.0310 (0.9840)\tD(fake) 0.0149 (0.0208)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8755 (-0.8559)\n",
            "Epoch: [65][160/195]\tTime  0.987 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 1.0483 (0.9840)\tD(fake) 0.0000 (0.0208)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8380 (-0.8567)\n",
            "Epoch: [65][170/195]\tTime  0.974 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0323 (0.9840)\tD(fake) -0.0203 (0.0208)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8603 (-0.8573)\n",
            "Epoch: [65][180/195]\tTime  0.976 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9648 (0.9839)\tD(fake) 0.0501 (0.0210)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8481 (-0.8576)\n",
            "Epoch: [65][190/195]\tTime  0.986 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9326 (0.9838)\tD(fake) 0.0566 (0.0210)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8804 (-0.8585)\n",
            "Epoch: [66][  0/195]\tTime  1.377 ( 1.377)\tData  0.334 ( 0.334)\tD(real) 1.0396 (0.9924)\tD(fake) -0.0159 (0.0217)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8655 (-0.8655)\n",
            "Epoch: [66][ 10/195]\tTime  0.979 ( 1.025)\tData  0.000 ( 0.031)\tD(real) 1.0164 (0.9852)\tD(fake) 0.0064 (0.0206)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9068 (-0.8585)\n",
            "Epoch: [66][ 20/195]\tTime  0.982 ( 1.006)\tData  0.000 ( 0.016)\tD(real) 1.0690 (0.9853)\tD(fake) -0.0123 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8604 (-0.8575)\n",
            "Epoch: [66][ 30/195]\tTime  0.978 ( 1.000)\tData  0.000 ( 0.011)\tD(real) 0.9480 (0.9845)\tD(fake) 0.0246 (0.0198)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8023 (-0.8566)\n",
            "Epoch: [66][ 40/195]\tTime  0.980 ( 0.996)\tData  0.000 ( 0.009)\tD(real) 0.9209 (0.9842)\tD(fake) 0.0737 (0.0203)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8295 (-0.8555)\n",
            "Epoch: [66][ 50/195]\tTime  0.992 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 0.9673 (0.9844)\tD(fake) 0.0053 (0.0201)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8517 (-0.8563)\n",
            "Epoch: [66][ 60/195]\tTime  0.978 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 0.9659 (0.9842)\tD(fake) 0.0326 (0.0211)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8809 (-0.8559)\n",
            "Epoch: [66][ 70/195]\tTime  0.984 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9692 (0.9843)\tD(fake) 0.0102 (0.0207)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8740 (-0.8580)\n",
            "Epoch: [66][ 80/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9712 (0.9843)\tD(fake) 0.0162 (0.0210)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8212 (-0.8579)\n",
            "Epoch: [66][ 90/195]\tTime  0.980 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9925 (0.9843)\tD(fake) -0.0057 (0.0209)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8949 (-0.8593)\n",
            "Epoch: [66][100/195]\tTime  0.995 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9809 (0.9841)\tD(fake) 0.0060 (0.0211)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8402 (-0.8599)\n",
            "Epoch: [66][110/195]\tTime  0.980 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0242 (0.9840)\tD(fake) 0.0225 (0.0214)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8802 (-0.8601)\n",
            "Epoch: [66][120/195]\tTime  1.000 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9152 (0.9838)\tD(fake) 0.0698 (0.0215)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8808 (-0.8596)\n",
            "Epoch: [66][130/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0071 (0.9840)\tD(fake) -0.0031 (0.0212)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8462 (-0.8599)\n",
            "Epoch: [66][140/195]\tTime  1.000 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9487 (0.9839)\tD(fake) 0.0458 (0.0212)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8846 (-0.8603)\n",
            "Epoch: [66][150/195]\tTime  0.981 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0216 (0.9840)\tD(fake) 0.0331 (0.0212)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8689 (-0.8600)\n",
            "Epoch: [66][160/195]\tTime  0.977 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9503 (0.9839)\tD(fake) 0.0549 (0.0213)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8618 (-0.8606)\n",
            "Epoch: [66][170/195]\tTime  1.002 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0170 (0.9840)\tD(fake) -0.0157 (0.0212)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8652 (-0.8603)\n",
            "Epoch: [66][180/195]\tTime  0.987 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9147 (0.9840)\tD(fake) -0.0033 (0.0210)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8861 (-0.8609)\n",
            "Epoch: [66][190/195]\tTime  0.985 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0209 (0.9841)\tD(fake) -0.0053 (0.0209)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8582 (-0.8613)\n",
            "Epoch: [67][  0/195]\tTime  1.370 ( 1.370)\tData  0.335 ( 0.335)\tD(real) 0.9578 (0.9808)\tD(fake) 0.0231 (0.0041)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8746 (-0.8746)\n",
            "Epoch: [67][ 10/195]\tTime  0.987 ( 1.018)\tData  0.000 ( 0.031)\tD(real) 0.9302 (0.9839)\tD(fake) 0.0013 (0.0198)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8515 (-0.8572)\n",
            "Epoch: [67][ 20/195]\tTime  0.981 ( 1.002)\tData  0.000 ( 0.016)\tD(real) 0.9870 (0.9845)\tD(fake) 0.0151 (0.0206)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8690 (-0.8502)\n",
            "Epoch: [67][ 30/195]\tTime  0.986 ( 0.996)\tData  0.000 ( 0.011)\tD(real) 0.9575 (0.9847)\tD(fake) 0.0089 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8923 (-0.8565)\n",
            "Epoch: [67][ 40/195]\tTime  0.979 ( 0.993)\tData  0.000 ( 0.008)\tD(real) 0.9458 (0.9847)\tD(fake) 0.0160 (0.0198)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8808 (-0.8578)\n",
            "Epoch: [67][ 50/195]\tTime  0.980 ( 0.991)\tData  0.000 ( 0.007)\tD(real) 0.9942 (0.9846)\tD(fake) 0.0433 (0.0202)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8944 (-0.8613)\n",
            "Epoch: [67][ 60/195]\tTime  0.975 ( 0.991)\tData  0.000 ( 0.006)\tD(real) 1.0504 (0.9845)\tD(fake) 0.0112 (0.0202)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8896 (-0.8623)\n",
            "Epoch: [67][ 70/195]\tTime  0.995 ( 0.990)\tData  0.000 ( 0.005)\tD(real) 1.0202 (0.9846)\tD(fake) 0.0151 (0.0202)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8353 (-0.8630)\n",
            "Epoch: [67][ 80/195]\tTime  0.983 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9668 (0.9845)\tD(fake) 0.0095 (0.0204)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8706 (-0.8640)\n",
            "Epoch: [67][ 90/195]\tTime  0.988 ( 0.989)\tData  0.000 ( 0.004)\tD(real) 1.0326 (0.9846)\tD(fake) -0.0037 (0.0205)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8819 (-0.8641)\n",
            "Epoch: [67][100/195]\tTime  1.002 ( 0.989)\tData  0.000 ( 0.004)\tD(real) 1.0318 (0.9845)\tD(fake) -0.0080 (0.0207)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8820 (-0.8640)\n",
            "Epoch: [67][110/195]\tTime  0.996 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0170 (0.9846)\tD(fake) 0.0059 (0.0203)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8768 (-0.8640)\n",
            "Epoch: [67][120/195]\tTime  0.990 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0481 (0.9847)\tD(fake) -0.0054 (0.0204)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8340 (-0.8645)\n",
            "Epoch: [67][130/195]\tTime  0.990 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0412 (0.9847)\tD(fake) -0.0016 (0.0205)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8544 (-0.8633)\n",
            "Epoch: [67][140/195]\tTime  0.995 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 1.0226 (0.9847)\tD(fake) -0.0001 (0.0206)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7665 (-0.8622)\n",
            "Epoch: [67][150/195]\tTime  0.995 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 0.9374 (0.9846)\tD(fake) 0.0226 (0.0205)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8887 (-0.8626)\n",
            "Epoch: [67][160/195]\tTime  0.988 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9761 (0.9846)\tD(fake) 0.0008 (0.0205)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8666 (-0.8627)\n",
            "Epoch: [67][170/195]\tTime  0.981 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9342 (0.9845)\tD(fake) 0.0777 (0.0206)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8317 (-0.8631)\n",
            "Epoch: [67][180/195]\tTime  0.973 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0570 (0.9846)\tD(fake) -0.0226 (0.0206)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8264 (-0.8630)\n",
            "Epoch: [67][190/195]\tTime  0.983 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0227 (0.9844)\tD(fake) -0.0112 (0.0209)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9043 (-0.8627)\n",
            "Epoch: [68][  0/195]\tTime  1.409 ( 1.409)\tData  0.347 ( 0.347)\tD(real) 0.9444 (0.9814)\tD(fake) 0.0089 (0.0026)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8825 (-0.8825)\n",
            "Epoch: [68][ 10/195]\tTime  0.989 ( 1.025)\tData  0.000 ( 0.032)\tD(real) 1.0273 (0.9856)\tD(fake) -0.0160 (0.0215)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8558 (-0.8641)\n",
            "Epoch: [68][ 20/195]\tTime  0.976 ( 1.007)\tData  0.000 ( 0.017)\tD(real) 1.0326 (0.9849)\tD(fake) 0.0013 (0.0213)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8415 (-0.8637)\n",
            "Epoch: [68][ 30/195]\tTime  0.989 ( 1.002)\tData  0.000 ( 0.012)\tD(real) 1.0143 (0.9854)\tD(fake) -0.0020 (0.0199)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8547 (-0.8645)\n",
            "Epoch: [68][ 40/195]\tTime  0.983 ( 1.000)\tData  0.000 ( 0.009)\tD(real) 0.9576 (0.9851)\tD(fake) 0.0367 (0.0203)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8850 (-0.8610)\n",
            "Epoch: [68][ 50/195]\tTime  0.997 ( 0.998)\tData  0.000 ( 0.007)\tD(real) 1.0271 (0.9852)\tD(fake) -0.0046 (0.0202)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8236 (-0.8613)\n",
            "Epoch: [68][ 60/195]\tTime  0.997 ( 0.997)\tData  0.000 ( 0.006)\tD(real) 0.9735 (0.9850)\tD(fake) 0.0024 (0.0203)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8401 (-0.8615)\n",
            "Epoch: [68][ 70/195]\tTime  0.985 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 1.0381 (0.9849)\tD(fake) 0.0117 (0.0205)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8842 (-0.8621)\n",
            "Epoch: [68][ 80/195]\tTime  0.978 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 0.9408 (0.9848)\tD(fake) 0.0219 (0.0207)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8638 (-0.8632)\n",
            "Epoch: [68][ 90/195]\tTime  0.980 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0225 (0.9848)\tD(fake) -0.0016 (0.0209)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8636 (-0.8636)\n",
            "Epoch: [68][100/195]\tTime  1.004 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.9471 (0.9848)\tD(fake) 0.0502 (0.0209)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8647 (-0.8640)\n",
            "Epoch: [68][110/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9380 (0.9847)\tD(fake) 0.0624 (0.0210)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8804 (-0.8643)\n",
            "Epoch: [68][120/195]\tTime  1.004 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0333 (0.9847)\tD(fake) -0.0043 (0.0211)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8838 (-0.8634)\n",
            "Epoch: [68][130/195]\tTime  0.982 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9383 (0.9846)\tD(fake) 0.0736 (0.0210)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8732 (-0.8645)\n",
            "Epoch: [68][140/195]\tTime  0.979 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9600 (0.9846)\tD(fake) 0.0213 (0.0210)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7655 (-0.8641)\n",
            "Epoch: [68][150/195]\tTime  0.994 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9212 (0.9845)\tD(fake) 0.0642 (0.0212)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8766 (-0.8642)\n",
            "Epoch: [68][160/195]\tTime  0.977 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9994 (0.9845)\tD(fake) 0.0050 (0.0213)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8707 (-0.8645)\n",
            "Epoch: [68][170/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0522 (0.9845)\tD(fake) 0.0012 (0.0212)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8840 (-0.8643)\n",
            "Epoch: [68][180/195]\tTime  0.976 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9560 (0.9845)\tD(fake) 0.0121 (0.0210)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8717 (-0.8650)\n",
            "Epoch: [68][190/195]\tTime  0.976 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9426 (0.9844)\tD(fake) 0.0229 (0.0212)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8716 (-0.8652)\n",
            "Epoch: [69][  0/195]\tTime  1.428 ( 1.428)\tData  0.371 ( 0.371)\tD(real) 0.9798 (0.9853)\tD(fake) 0.0195 (0.0049)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8800 (-0.8800)\n",
            "Epoch: [69][ 10/195]\tTime  0.993 ( 1.027)\tData  0.000 ( 0.034)\tD(real) 1.0218 (0.9864)\tD(fake) -0.0026 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7703 (-0.8569)\n",
            "Epoch: [69][ 20/195]\tTime  0.979 ( 1.007)\tData  0.000 ( 0.018)\tD(real) 1.0299 (0.9862)\tD(fake) 0.0039 (0.0174)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8853 (-0.8509)\n",
            "Epoch: [69][ 30/195]\tTime  0.987 ( 1.002)\tData  0.000 ( 0.012)\tD(real) 0.9461 (0.9861)\tD(fake) 0.0018 (0.0174)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8822 (-0.8555)\n",
            "Epoch: [69][ 40/195]\tTime  0.992 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 1.0329 (0.9861)\tD(fake) -0.0029 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8590 (-0.8553)\n",
            "Epoch: [69][ 50/195]\tTime  1.000 ( 0.997)\tData  0.000 ( 0.008)\tD(real) 0.9677 (0.9858)\tD(fake) 0.0238 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8716 (-0.8580)\n",
            "Epoch: [69][ 60/195]\tTime  0.986 ( 0.996)\tData  0.000 ( 0.006)\tD(real) 1.0575 (0.9859)\tD(fake) 0.0032 (0.0182)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8637 (-0.8599)\n",
            "Epoch: [69][ 70/195]\tTime  0.975 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 0.9444 (0.9857)\tD(fake) 0.0709 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8629 (-0.8610)\n",
            "Epoch: [69][ 80/195]\tTime  1.004 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 1.0307 (0.9858)\tD(fake) -0.0283 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8553 (-0.8633)\n",
            "Epoch: [69][ 90/195]\tTime  0.989 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 1.0254 (0.9857)\tD(fake) 0.0135 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8807 (-0.8637)\n",
            "Epoch: [69][100/195]\tTime  1.007 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.9474 (0.9855)\tD(fake) 0.0726 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8321 (-0.8636)\n",
            "Epoch: [69][110/195]\tTime  0.986 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0763 (0.9857)\tD(fake) -0.0161 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8802 (-0.8644)\n",
            "Epoch: [69][120/195]\tTime  1.007 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9568 (0.9855)\tD(fake) -0.0077 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8270 (-0.8650)\n",
            "Epoch: [69][130/195]\tTime  1.004 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0233 (0.9855)\tD(fake) -0.0208 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8785 (-0.8656)\n",
            "Epoch: [69][140/195]\tTime  0.976 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9737 (0.9854)\tD(fake) 0.0036 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8752 (-0.8657)\n",
            "Epoch: [69][150/195]\tTime  0.988 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0250 (0.9854)\tD(fake) 0.0012 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8729 (-0.8666)\n",
            "Epoch: [69][160/195]\tTime  1.004 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0506 (0.9853)\tD(fake) 0.0103 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8634 (-0.8675)\n",
            "Epoch: [69][170/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0347 (0.9852)\tD(fake) -0.0161 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8570 (-0.8665)\n",
            "Epoch: [69][180/195]\tTime  0.994 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0329 (0.9852)\tD(fake) -0.0094 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8968 (-0.8667)\n",
            "Epoch: [69][190/195]\tTime  0.991 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0043 (0.9852)\tD(fake) 0.0221 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8820 (-0.8674)\n",
            "Epoch: [70][  0/195]\tTime  1.375 ( 1.375)\tData  0.330 ( 0.330)\tD(real) 1.0142 (0.9898)\tD(fake) 0.0057 (0.0225)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8642 (-0.8642)\n",
            "Epoch: [70][ 10/195]\tTime  0.989 ( 1.023)\tData  0.000 ( 0.030)\tD(real) 1.0410 (0.9864)\tD(fake) -0.0050 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8805 (-0.8763)\n",
            "Epoch: [70][ 20/195]\tTime  0.988 ( 1.009)\tData  0.000 ( 0.016)\tD(real) 0.9546 (0.9858)\tD(fake) 0.0429 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8437 (-0.8756)\n",
            "Epoch: [70][ 30/195]\tTime  0.984 ( 1.002)\tData  0.000 ( 0.011)\tD(real) 1.0188 (0.9859)\tD(fake) -0.0257 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8760 (-0.8752)\n",
            "Epoch: [70][ 40/195]\tTime  0.981 ( 0.998)\tData  0.000 ( 0.008)\tD(real) 1.0343 (0.9860)\tD(fake) -0.0084 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8748 (-0.8780)\n",
            "Epoch: [70][ 50/195]\tTime  0.984 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 0.9751 (0.9859)\tD(fake) 0.0218 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8674 (-0.8768)\n",
            "Epoch: [70][ 60/195]\tTime  0.991 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 0.9454 (0.9857)\tD(fake) 0.0463 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8827 (-0.8760)\n",
            "Epoch: [70][ 70/195]\tTime  0.977 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9226 (0.9856)\tD(fake) 0.0278 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8972 (-0.8766)\n",
            "Epoch: [70][ 80/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9966 (0.9856)\tD(fake) 0.0029 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8550 (-0.8762)\n",
            "Epoch: [70][ 90/195]\tTime  0.981 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9737 (0.9855)\tD(fake) 0.0174 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8816 (-0.8735)\n",
            "Epoch: [70][100/195]\tTime  0.995 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0345 (0.9855)\tD(fake) -0.0069 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8674 (-0.8743)\n",
            "Epoch: [70][110/195]\tTime  0.982 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0269 (0.9856)\tD(fake) -0.0116 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9106 (-0.8744)\n",
            "Epoch: [70][120/195]\tTime  0.975 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0215 (0.9855)\tD(fake) 0.0017 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8899 (-0.8745)\n",
            "Epoch: [70][130/195]\tTime  0.983 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9809 (0.9855)\tD(fake) 0.0052 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8499 (-0.8741)\n",
            "Epoch: [70][140/195]\tTime  0.982 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9526 (0.9855)\tD(fake) 0.0127 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8907 (-0.8742)\n",
            "Epoch: [70][150/195]\tTime  0.981 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 0.9735 (0.9855)\tD(fake) -0.0027 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8680 (-0.8743)\n",
            "Epoch: [70][160/195]\tTime  0.975 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9498 (0.9854)\tD(fake) 0.0230 (0.0197)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8690 (-0.8739)\n",
            "Epoch: [70][170/195]\tTime  0.981 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0421 (0.9854)\tD(fake) -0.0035 (0.0198)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8567 (-0.8739)\n",
            "Epoch: [70][180/195]\tTime  0.975 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9863 (0.9854)\tD(fake) 0.0149 (0.0199)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8803 (-0.8736)\n",
            "Epoch: [70][190/195]\tTime  0.991 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0368 (0.9854)\tD(fake) -0.0253 (0.0200)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8752 (-0.8729)\n",
            "Epoch: [71][  0/195]\tTime  1.392 ( 1.392)\tData  0.354 ( 0.354)\tD(real) 0.9805 (0.9845)\tD(fake) 0.0406 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8908 (-0.8908)\n",
            "Epoch: [71][ 10/195]\tTime  0.992 ( 1.024)\tData  0.000 ( 0.032)\tD(real) 0.9701 (0.9857)\tD(fake) 0.0107 (0.0181)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8749 (-0.8827)\n",
            "Epoch: [71][ 20/195]\tTime  0.984 ( 1.004)\tData  0.000 ( 0.017)\tD(real) 0.9685 (0.9854)\tD(fake) 0.0202 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8584 (-0.8743)\n",
            "Epoch: [71][ 30/195]\tTime  0.984 ( 0.998)\tData  0.000 ( 0.012)\tD(real) 1.0530 (0.9857)\tD(fake) -0.0086 (0.0198)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8730 (-0.8755)\n",
            "Epoch: [71][ 40/195]\tTime  0.991 ( 0.994)\tData  0.000 ( 0.009)\tD(real) 0.9442 (0.9856)\tD(fake) 0.0406 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8824 (-0.8721)\n",
            "Epoch: [71][ 50/195]\tTime  0.993 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 0.9555 (0.9855)\tD(fake) 0.0049 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8740 (-0.8727)\n",
            "Epoch: [71][ 60/195]\tTime  0.981 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 0.9736 (0.9855)\tD(fake) 0.0146 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8868 (-0.8733)\n",
            "Epoch: [71][ 70/195]\tTime  0.990 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0401 (0.9858)\tD(fake) -0.0156 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8808 (-0.8736)\n",
            "Epoch: [71][ 80/195]\tTime  0.986 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9411 (0.9858)\tD(fake) 0.0405 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8718 (-0.8719)\n",
            "Epoch: [71][ 90/195]\tTime  0.978 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9645 (0.9859)\tD(fake) 0.0178 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8831 (-0.8716)\n",
            "Epoch: [71][100/195]\tTime  0.997 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9829 (0.9860)\tD(fake) -0.0164 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8891 (-0.8720)\n",
            "Epoch: [71][110/195]\tTime  0.980 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0736 (0.9861)\tD(fake) -0.0042 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8536 (-0.8719)\n",
            "Epoch: [71][120/195]\tTime  0.998 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9604 (0.9861)\tD(fake) 0.0199 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9109 (-0.8730)\n",
            "Epoch: [71][130/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9459 (0.9860)\tD(fake) 0.0361 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8961 (-0.8725)\n",
            "Epoch: [71][140/195]\tTime  1.007 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9720 (0.9860)\tD(fake) -0.0034 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8908 (-0.8726)\n",
            "Epoch: [71][150/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0239 (0.9860)\tD(fake) 0.0047 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8709 (-0.8728)\n",
            "Epoch: [71][160/195]\tTime  0.976 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9512 (0.9858)\tD(fake) 0.0586 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8536 (-0.8733)\n",
            "Epoch: [71][170/195]\tTime  0.992 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0144 (0.9858)\tD(fake) -0.0078 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9048 (-0.8743)\n",
            "Epoch: [71][180/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0122 (0.9858)\tD(fake) 0.0083 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8765 (-0.8744)\n",
            "Epoch: [71][190/195]\tTime  0.983 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9549 (0.9857)\tD(fake) 0.0001 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8718 (-0.8746)\n",
            "Epoch: [72][  0/195]\tTime  1.400 ( 1.400)\tData  0.343 ( 0.343)\tD(real) 0.9716 (0.9857)\tD(fake) 0.0438 (0.0142)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8628 (-0.8628)\n",
            "Epoch: [72][ 10/195]\tTime  0.985 ( 1.025)\tData  0.000 ( 0.032)\tD(real) 0.9531 (0.9858)\tD(fake) 0.0097 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8623 (-0.8795)\n",
            "Epoch: [72][ 20/195]\tTime  0.987 ( 1.008)\tData  0.000 ( 0.017)\tD(real) 0.9435 (0.9854)\tD(fake) 0.0532 (0.0198)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7901 (-0.8719)\n",
            "Epoch: [72][ 30/195]\tTime  0.988 ( 1.001)\tData  0.000 ( 0.011)\tD(real) 1.0214 (0.9859)\tD(fake) 0.0024 (0.0205)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8844 (-0.8700)\n",
            "Epoch: [72][ 40/195]\tTime  0.991 ( 0.998)\tData  0.000 ( 0.009)\tD(real) 1.0614 (0.9859)\tD(fake) -0.0004 (0.0202)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8680 (-0.8714)\n",
            "Epoch: [72][ 50/195]\tTime  0.990 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 0.9298 (0.9856)\tD(fake) 0.0446 (0.0199)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8793 (-0.8721)\n",
            "Epoch: [72][ 60/195]\tTime  0.987 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 1.0395 (0.9859)\tD(fake) -0.0107 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8013 (-0.8697)\n",
            "Epoch: [72][ 70/195]\tTime  0.988 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9879 (0.9858)\tD(fake) 0.0320 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8999 (-0.8709)\n",
            "Epoch: [72][ 80/195]\tTime  0.974 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0333 (0.9858)\tD(fake) 0.0004 (0.0196)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8895 (-0.8720)\n",
            "Epoch: [72][ 90/195]\tTime  1.002 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0292 (0.9857)\tD(fake) 0.0015 (0.0197)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9092 (-0.8717)\n",
            "Epoch: [72][100/195]\tTime  1.003 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0534 (0.9858)\tD(fake) -0.0020 (0.0196)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9018 (-0.8714)\n",
            "Epoch: [72][110/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0416 (0.9858)\tD(fake) -0.0132 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8794 (-0.8713)\n",
            "Epoch: [72][120/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0513 (0.9858)\tD(fake) 0.0026 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9071 (-0.8725)\n",
            "Epoch: [72][130/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0424 (0.9859)\tD(fake) -0.0037 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8847 (-0.8730)\n",
            "Epoch: [72][140/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9487 (0.9858)\tD(fake) -0.0018 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8691 (-0.8738)\n",
            "Epoch: [72][150/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0670 (0.9859)\tD(fake) -0.0187 (0.0196)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8072 (-0.8735)\n",
            "Epoch: [72][160/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0324 (0.9858)\tD(fake) 0.0259 (0.0197)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8586 (-0.8736)\n",
            "Epoch: [72][170/195]\tTime  1.001 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0205 (0.9858)\tD(fake) -0.0184 (0.0196)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8693 (-0.8742)\n",
            "Epoch: [72][180/195]\tTime  0.985 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0552 (0.9858)\tD(fake) -0.0064 (0.0198)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8915 (-0.8746)\n",
            "Epoch: [72][190/195]\tTime  0.996 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9963 (0.9857)\tD(fake) 0.0101 (0.0198)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8726 (-0.8746)\n",
            "Epoch: [73][  0/195]\tTime  1.361 ( 1.361)\tData  0.331 ( 0.331)\tD(real) 1.0113 (0.9907)\tD(fake) -0.0009 (0.0283)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8874 (-0.8874)\n",
            "Epoch: [73][ 10/195]\tTime  0.993 ( 1.023)\tData  0.000 ( 0.031)\tD(real) 1.0169 (0.9869)\tD(fake) 0.0114 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8723 (-0.8697)\n",
            "Epoch: [73][ 20/195]\tTime  0.976 ( 1.005)\tData  0.000 ( 0.016)\tD(real) 1.0363 (0.9875)\tD(fake) -0.0061 (0.0177)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8678 (-0.8750)\n",
            "Epoch: [73][ 30/195]\tTime  0.997 ( 1.000)\tData  0.000 ( 0.011)\tD(real) 1.0201 (0.9873)\tD(fake) -0.0053 (0.0173)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9011 (-0.8782)\n",
            "Epoch: [73][ 40/195]\tTime  0.986 ( 0.998)\tData  0.000 ( 0.008)\tD(real) 1.0315 (0.9873)\tD(fake) 0.0205 (0.0172)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8492 (-0.8766)\n",
            "Epoch: [73][ 50/195]\tTime  0.984 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 1.0334 (0.9871)\tD(fake) -0.0204 (0.0176)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8551 (-0.8770)\n",
            "Epoch: [73][ 60/195]\tTime  0.986 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0434 (0.9872)\tD(fake) -0.0077 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8846 (-0.8759)\n",
            "Epoch: [73][ 70/195]\tTime  0.994 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 1.0340 (0.9870)\tD(fake) 0.0028 (0.0180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8860 (-0.8742)\n",
            "Epoch: [73][ 80/195]\tTime  0.998 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0480 (0.9870)\tD(fake) -0.0156 (0.0180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8808 (-0.8738)\n",
            "Epoch: [73][ 90/195]\tTime  0.988 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9297 (0.9870)\tD(fake) 0.0099 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8707 (-0.8746)\n",
            "Epoch: [73][100/195]\tTime  0.997 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0295 (0.9870)\tD(fake) -0.0016 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8742 (-0.8747)\n",
            "Epoch: [73][110/195]\tTime  0.978 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0197 (0.9870)\tD(fake) -0.0032 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8988 (-0.8755)\n",
            "Epoch: [73][120/195]\tTime  0.985 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0158 (0.9869)\tD(fake) 0.0203 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8985 (-0.8753)\n",
            "Epoch: [73][130/195]\tTime  0.984 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0302 (0.9870)\tD(fake) -0.0091 (0.0174)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8713 (-0.8746)\n",
            "Epoch: [73][140/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9880 (0.9868)\tD(fake) 0.0126 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9160 (-0.8752)\n",
            "Epoch: [73][150/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0347 (0.9869)\tD(fake) 0.0028 (0.0177)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8910 (-0.8758)\n",
            "Epoch: [73][160/195]\tTime  0.986 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0451 (0.9869)\tD(fake) -0.0030 (0.0177)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8731 (-0.8746)\n",
            "Epoch: [73][170/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9294 (0.9868)\tD(fake) 0.0239 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8991 (-0.8745)\n",
            "Epoch: [73][180/195]\tTime  0.975 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9395 (0.9868)\tD(fake) 0.0007 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8611 (-0.8750)\n",
            "Epoch: [73][190/195]\tTime  1.005 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9498 (0.9868)\tD(fake) 0.0531 (0.0176)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8901 (-0.8752)\n",
            "Epoch: [74][  0/195]\tTime  1.398 ( 1.398)\tData  0.341 ( 0.341)\tD(real) 1.0431 (0.9937)\tD(fake) 0.0010 (0.0243)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8911 (-0.8911)\n",
            "Epoch: [74][ 10/195]\tTime  0.997 ( 1.032)\tData  0.000 ( 0.031)\tD(real) 1.0352 (0.9892)\tD(fake) -0.0054 (0.0146)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8389 (-0.8773)\n",
            "Epoch: [74][ 20/195]\tTime  0.997 ( 1.011)\tData  0.000 ( 0.017)\tD(real) 0.9514 (0.9878)\tD(fake) 0.0268 (0.0168)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8884 (-0.8697)\n",
            "Epoch: [74][ 30/195]\tTime  0.979 ( 1.003)\tData  0.000 ( 0.011)\tD(real) 0.9565 (0.9876)\tD(fake) 0.0440 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8750 (-0.8688)\n",
            "Epoch: [74][ 40/195]\tTime  0.980 ( 1.000)\tData  0.000 ( 0.009)\tD(real) 1.0042 (0.9875)\tD(fake) 0.0037 (0.0173)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8772 (-0.8725)\n",
            "Epoch: [74][ 50/195]\tTime  0.993 ( 0.999)\tData  0.000 ( 0.007)\tD(real) 0.9430 (0.9874)\tD(fake) 0.0098 (0.0174)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8284 (-0.8736)\n",
            "Epoch: [74][ 60/195]\tTime  0.976 ( 0.997)\tData  0.000 ( 0.006)\tD(real) 1.0522 (0.9873)\tD(fake) 0.0023 (0.0178)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8976 (-0.8760)\n",
            "Epoch: [74][ 70/195]\tTime  1.002 ( 0.996)\tData  0.000 ( 0.005)\tD(real) 0.9890 (0.9872)\tD(fake) 0.0100 (0.0177)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8052 (-0.8745)\n",
            "Epoch: [74][ 80/195]\tTime  0.980 ( 0.996)\tData  0.000 ( 0.005)\tD(real) 1.0262 (0.9872)\tD(fake) 0.0043 (0.0180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8833 (-0.8745)\n",
            "Epoch: [74][ 90/195]\tTime  0.999 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 0.9669 (0.9871)\tD(fake) 0.0496 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8938 (-0.8752)\n",
            "Epoch: [74][100/195]\tTime  1.004 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 1.0402 (0.9872)\tD(fake) -0.0025 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8929 (-0.8755)\n",
            "Epoch: [74][110/195]\tTime  0.988 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 0.9641 (0.9872)\tD(fake) -0.0048 (0.0178)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8932 (-0.8757)\n",
            "Epoch: [74][120/195]\tTime  0.995 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 1.0422 (0.9872)\tD(fake) -0.0078 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8809 (-0.8753)\n",
            "Epoch: [74][130/195]\tTime  0.990 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 1.0158 (0.9871)\tD(fake) 0.0181 (0.0180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8823 (-0.8760)\n",
            "Epoch: [74][140/195]\tTime  0.980 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0198 (0.9872)\tD(fake) -0.0025 (0.0181)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8887 (-0.8766)\n",
            "Epoch: [74][150/195]\tTime  1.001 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 1.0415 (0.9871)\tD(fake) -0.0173 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8790 (-0.8769)\n",
            "Epoch: [74][160/195]\tTime  0.993 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 0.9506 (0.9870)\tD(fake) 0.0175 (0.0182)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8672 (-0.8771)\n",
            "Epoch: [74][170/195]\tTime  0.990 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 1.0231 (0.9870)\tD(fake) 0.0079 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8916 (-0.8776)\n",
            "Epoch: [74][180/195]\tTime  0.979 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 1.0151 (0.9871)\tD(fake) -0.0133 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8940 (-0.8775)\n",
            "Epoch: [74][190/195]\tTime  0.979 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 0.9650 (0.9870)\tD(fake) 0.0080 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8631 (-0.8775)\n",
            "Epoch: [75][  0/195]\tTime  1.413 ( 1.413)\tData  0.348 ( 0.348)\tD(real) 1.0385 (0.9962)\tD(fake) -0.0193 (0.0135)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9019 (-0.9019)\n",
            "Epoch: [75][ 10/195]\tTime  0.982 ( 1.026)\tData  0.000 ( 0.032)\tD(real) 0.9639 (0.9877)\tD(fake) 0.0515 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8750 (-0.8825)\n",
            "Epoch: [75][ 20/195]\tTime  0.987 ( 1.010)\tData  0.000 ( 0.017)\tD(real) 0.9470 (0.9880)\tD(fake) 0.0161 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8856 (-0.8833)\n",
            "Epoch: [75][ 30/195]\tTime  0.983 ( 1.003)\tData  0.000 ( 0.012)\tD(real) 0.9326 (0.9879)\tD(fake) 0.0024 (0.0171)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8724 (-0.8826)\n",
            "Epoch: [75][ 40/195]\tTime  0.997 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 0.9514 (0.9877)\tD(fake) 0.0055 (0.0173)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9084 (-0.8808)\n",
            "Epoch: [75][ 50/195]\tTime  1.001 ( 0.999)\tData  0.000 ( 0.007)\tD(real) 1.0219 (0.9878)\tD(fake) 0.0002 (0.0177)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8776 (-0.8814)\n",
            "Epoch: [75][ 60/195]\tTime  0.980 ( 0.997)\tData  0.000 ( 0.006)\tD(real) 0.9873 (0.9875)\tD(fake) 0.0219 (0.0181)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8873 (-0.8812)\n",
            "Epoch: [75][ 70/195]\tTime  1.005 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 1.0414 (0.9874)\tD(fake) 0.0051 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8881 (-0.8824)\n",
            "Epoch: [75][ 80/195]\tTime  1.001 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 1.0119 (0.9873)\tD(fake) 0.0068 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8723 (-0.8800)\n",
            "Epoch: [75][ 90/195]\tTime  0.993 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 0.9499 (0.9872)\tD(fake) 0.0522 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8626 (-0.8797)\n",
            "Epoch: [75][100/195]\tTime  1.010 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 0.9280 (0.9872)\tD(fake) 0.0120 (0.0182)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8685 (-0.8795)\n",
            "Epoch: [75][110/195]\tTime  1.000 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 1.0307 (0.9873)\tD(fake) -0.0063 (0.0182)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8935 (-0.8785)\n",
            "Epoch: [75][120/195]\tTime  0.993 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 0.9626 (0.9872)\tD(fake) 0.0335 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8767 (-0.8784)\n",
            "Epoch: [75][130/195]\tTime  0.985 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9414 (0.9871)\tD(fake) 0.0080 (0.0181)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8679 (-0.8785)\n",
            "Epoch: [75][140/195]\tTime  0.985 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0151 (0.9871)\tD(fake) -0.0082 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8925 (-0.8791)\n",
            "Epoch: [75][150/195]\tTime  0.995 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0277 (0.9870)\tD(fake) -0.0065 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8312 (-0.8789)\n",
            "Epoch: [75][160/195]\tTime  0.978 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 1.0360 (0.9870)\tD(fake) 0.0122 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8665 (-0.8785)\n",
            "Epoch: [75][170/195]\tTime  0.980 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 1.0469 (0.9870)\tD(fake) 0.0031 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8797 (-0.8773)\n",
            "Epoch: [75][180/195]\tTime  0.982 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 1.0295 (0.9870)\tD(fake) -0.0269 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8680 (-0.8774)\n",
            "Epoch: [75][190/195]\tTime  0.988 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9795 (0.9869)\tD(fake) 0.0012 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8945 (-0.8762)\n",
            "Epoch: [76][  0/195]\tTime  1.405 ( 1.405)\tData  0.354 ( 0.354)\tD(real) 1.0198 (0.9993)\tD(fake) -0.0119 (0.0098)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8385 (-0.8385)\n",
            "Epoch: [76][ 10/195]\tTime  1.000 ( 1.029)\tData  0.000 ( 0.033)\tD(real) 1.0164 (0.9886)\tD(fake) 0.0051 (0.0180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8829 (-0.8717)\n",
            "Epoch: [76][ 20/195]\tTime  0.986 ( 1.014)\tData  0.000 ( 0.017)\tD(real) 0.9570 (0.9874)\tD(fake) 0.0494 (0.0199)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8688 (-0.8732)\n",
            "Epoch: [76][ 30/195]\tTime  0.989 ( 1.006)\tData  0.000 ( 0.012)\tD(real) 1.0325 (0.9876)\tD(fake) -0.0111 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8849 (-0.8738)\n",
            "Epoch: [76][ 40/195]\tTime  0.982 ( 1.002)\tData  0.000 ( 0.009)\tD(real) 1.0453 (0.9876)\tD(fake) 0.0143 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8307 (-0.8734)\n",
            "Epoch: [76][ 50/195]\tTime  0.987 ( 0.999)\tData  0.000 ( 0.007)\tD(real) 0.9969 (0.9876)\tD(fake) 0.0021 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8353 (-0.8751)\n",
            "Epoch: [76][ 60/195]\tTime  0.997 ( 0.998)\tData  0.000 ( 0.006)\tD(real) 0.9867 (0.9874)\tD(fake) -0.0132 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8871 (-0.8762)\n",
            "Epoch: [76][ 70/195]\tTime  0.999 ( 0.996)\tData  0.000 ( 0.005)\tD(real) 0.9640 (0.9873)\tD(fake) 0.0222 (0.0182)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8957 (-0.8765)\n",
            "Epoch: [76][ 80/195]\tTime  0.983 ( 0.996)\tData  0.000 ( 0.005)\tD(real) 1.0077 (0.9873)\tD(fake) -0.0010 (0.0182)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8972 (-0.8779)\n",
            "Epoch: [76][ 90/195]\tTime  0.979 ( 0.996)\tData  0.000 ( 0.004)\tD(real) 1.0229 (0.9871)\tD(fake) 0.0006 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8858 (-0.8771)\n",
            "Epoch: [76][100/195]\tTime  0.992 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 1.0426 (0.9871)\tD(fake) 0.0087 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8682 (-0.8773)\n",
            "Epoch: [76][110/195]\tTime  0.982 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 0.9932 (0.9870)\tD(fake) 0.0236 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8862 (-0.8772)\n",
            "Epoch: [76][120/195]\tTime  0.983 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 0.9979 (0.9870)\tD(fake) 0.0116 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8565 (-0.8774)\n",
            "Epoch: [76][130/195]\tTime  0.976 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9672 (0.9869)\tD(fake) 0.0403 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8734 (-0.8779)\n",
            "Epoch: [76][140/195]\tTime  1.001 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9961 (0.9869)\tD(fake) 0.0074 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8905 (-0.8779)\n",
            "Epoch: [76][150/195]\tTime  1.017 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9528 (0.9869)\tD(fake) 0.0384 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9014 (-0.8782)\n",
            "Epoch: [76][160/195]\tTime  0.986 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0134 (0.9870)\tD(fake) 0.0004 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9020 (-0.8787)\n",
            "Epoch: [76][170/195]\tTime  0.985 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 1.0178 (0.9870)\tD(fake) -0.0082 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9054 (-0.8785)\n",
            "Epoch: [76][180/195]\tTime  0.986 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 1.0441 (0.9869)\tD(fake) 0.0039 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8957 (-0.8789)\n",
            "Epoch: [76][190/195]\tTime  0.983 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 0.9917 (0.9869)\tD(fake) 0.0090 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8837 (-0.8779)\n",
            "Epoch: [77][  0/195]\tTime  1.380 ( 1.380)\tData  0.358 ( 0.358)\tD(real) 1.0386 (0.9946)\tD(fake) -0.0153 (0.0242)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8896 (-0.8896)\n",
            "Epoch: [77][ 10/195]\tTime  0.983 ( 1.024)\tData  0.000 ( 0.033)\tD(real) 1.0260 (0.9870)\tD(fake) 0.0057 (0.0207)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8616 (-0.8856)\n",
            "Epoch: [77][ 20/195]\tTime  0.980 ( 1.007)\tData  0.000 ( 0.017)\tD(real) 1.0398 (0.9870)\tD(fake) -0.0034 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8904 (-0.8794)\n",
            "Epoch: [77][ 30/195]\tTime  0.985 ( 1.002)\tData  0.000 ( 0.012)\tD(real) 0.9560 (0.9870)\tD(fake) 0.0325 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8603 (-0.8756)\n",
            "Epoch: [77][ 40/195]\tTime  1.000 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 0.9750 (0.9870)\tD(fake) 0.0178 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8702 (-0.8775)\n",
            "Epoch: [77][ 50/195]\tTime  0.984 ( 0.998)\tData  0.000 ( 0.007)\tD(real) 0.9672 (0.9872)\tD(fake) 0.0041 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8838 (-0.8786)\n",
            "Epoch: [77][ 60/195]\tTime  0.989 ( 0.996)\tData  0.000 ( 0.006)\tD(real) 0.9188 (0.9870)\tD(fake) 0.0281 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8763 (-0.8793)\n",
            "Epoch: [77][ 70/195]\tTime  0.971 ( 0.996)\tData  0.000 ( 0.005)\tD(real) 1.0206 (0.9872)\tD(fake) 0.0123 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8907 (-0.8780)\n",
            "Epoch: [77][ 80/195]\tTime  0.999 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 1.0375 (0.9872)\tD(fake) 0.0113 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8717 (-0.8756)\n",
            "Epoch: [77][ 90/195]\tTime  0.983 ( 0.994)\tData  0.000 ( 0.004)\tD(real) 0.9737 (0.9870)\tD(fake) 0.0318 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8885 (-0.8770)\n",
            "Epoch: [77][100/195]\tTime  1.004 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0304 (0.9871)\tD(fake) -0.0086 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8721 (-0.8776)\n",
            "Epoch: [77][110/195]\tTime  0.991 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0300 (0.9872)\tD(fake) -0.0100 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8793 (-0.8776)\n",
            "Epoch: [77][120/195]\tTime  0.974 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0483 (0.9873)\tD(fake) -0.0059 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8809 (-0.8783)\n",
            "Epoch: [77][130/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9664 (0.9873)\tD(fake) 0.0180 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8200 (-0.8777)\n",
            "Epoch: [77][140/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0323 (0.9873)\tD(fake) 0.0180 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9054 (-0.8775)\n",
            "Epoch: [77][150/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0178 (0.9873)\tD(fake) 0.0132 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8996 (-0.8779)\n",
            "Epoch: [77][160/195]\tTime  0.993 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9608 (0.9873)\tD(fake) 0.0165 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8875 (-0.8784)\n",
            "Epoch: [77][170/195]\tTime  0.975 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0201 (0.9873)\tD(fake) -0.0100 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8971 (-0.8782)\n",
            "Epoch: [77][180/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0317 (0.9873)\tD(fake) 0.0019 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8755 (-0.8782)\n",
            "Epoch: [77][190/195]\tTime  0.985 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9993 (0.9873)\tD(fake) 0.0036 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8890 (-0.8778)\n",
            "Epoch: [78][  0/195]\tTime  1.373 ( 1.373)\tData  0.330 ( 0.330)\tD(real) 1.0296 (0.9883)\tD(fake) 0.0062 (0.0356)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8599 (-0.8599)\n",
            "Epoch: [78][ 10/195]\tTime  0.984 ( 1.021)\tData  0.000 ( 0.030)\tD(real) 1.0488 (0.9885)\tD(fake) -0.0209 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7738 (-0.8749)\n",
            "Epoch: [78][ 20/195]\tTime  0.995 ( 1.007)\tData  0.000 ( 0.016)\tD(real) 1.0309 (0.9880)\tD(fake) -0.0090 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9086 (-0.8789)\n",
            "Epoch: [78][ 30/195]\tTime  0.973 ( 1.000)\tData  0.000 ( 0.011)\tD(real) 1.0169 (0.9880)\tD(fake) -0.0011 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8690 (-0.8774)\n",
            "Epoch: [78][ 40/195]\tTime  0.980 ( 0.996)\tData  0.000 ( 0.008)\tD(real) 0.9783 (0.9876)\tD(fake) 0.0106 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8788 (-0.8789)\n",
            "Epoch: [78][ 50/195]\tTime  0.985 ( 0.993)\tData  0.000 ( 0.007)\tD(real) 0.9534 (0.9877)\tD(fake) 0.0106 (0.0181)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8794 (-0.8783)\n",
            "Epoch: [78][ 60/195]\tTime  0.991 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 1.0544 (0.9878)\tD(fake) -0.0099 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8947 (-0.8787)\n",
            "Epoch: [78][ 70/195]\tTime  0.984 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9856 (0.9877)\tD(fake) 0.0537 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8990 (-0.8777)\n",
            "Epoch: [78][ 80/195]\tTime  1.002 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9737 (0.9877)\tD(fake) 0.0043 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8697 (-0.8775)\n",
            "Epoch: [78][ 90/195]\tTime  0.993 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9536 (0.9877)\tD(fake) 0.0326 (0.0178)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8723 (-0.8773)\n",
            "Epoch: [78][100/195]\tTime  1.003 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0436 (0.9877)\tD(fake) -0.0048 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8811 (-0.8781)\n",
            "Epoch: [78][110/195]\tTime  0.987 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9964 (0.9876)\tD(fake) 0.0133 (0.0180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8717 (-0.8788)\n",
            "Epoch: [78][120/195]\tTime  0.976 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9681 (0.9875)\tD(fake) 0.0478 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8854 (-0.8794)\n",
            "Epoch: [78][130/195]\tTime  0.993 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9670 (0.9875)\tD(fake) 0.0145 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8745 (-0.8782)\n",
            "Epoch: [78][140/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0490 (0.9875)\tD(fake) -0.0050 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8912 (-0.8784)\n",
            "Epoch: [78][150/195]\tTime  1.000 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9217 (0.9874)\tD(fake) 0.0196 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8935 (-0.8779)\n",
            "Epoch: [78][160/195]\tTime  0.997 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9721 (0.9875)\tD(fake) 0.0208 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8801 (-0.8781)\n",
            "Epoch: [78][170/195]\tTime  0.990 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9285 (0.9875)\tD(fake) 0.0361 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8926 (-0.8761)\n",
            "Epoch: [78][180/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0357 (0.9875)\tD(fake) -0.0149 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8054 (-0.8763)\n",
            "Epoch: [78][190/195]\tTime  0.989 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9741 (0.9874)\tD(fake) 0.0105 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9026 (-0.8766)\n",
            "Epoch: [79][  0/195]\tTime  1.420 ( 1.420)\tData  0.364 ( 0.364)\tD(real) 1.0376 (0.9933)\tD(fake) -0.0020 (0.0283)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8985 (-0.8985)\n",
            "Epoch: [79][ 10/195]\tTime  0.986 ( 1.026)\tData  0.000 ( 0.033)\tD(real) 0.9576 (0.9866)\tD(fake) 0.0552 (0.0208)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8530 (-0.8833)\n",
            "Epoch: [79][ 20/195]\tTime  0.981 ( 1.006)\tData  0.000 ( 0.018)\tD(real) 0.9706 (0.9870)\tD(fake) 0.0336 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8853 (-0.8849)\n",
            "Epoch: [79][ 30/195]\tTime  0.980 ( 0.998)\tData  0.000 ( 0.012)\tD(real) 1.0191 (0.9873)\tD(fake) -0.0050 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8678 (-0.8840)\n",
            "Epoch: [79][ 40/195]\tTime  0.984 ( 0.995)\tData  0.000 ( 0.009)\tD(real) 0.9877 (0.9872)\tD(fake) 0.0052 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8509 (-0.8827)\n",
            "Epoch: [79][ 50/195]\tTime  0.992 ( 0.994)\tData  0.000 ( 0.007)\tD(real) 0.9552 (0.9871)\tD(fake) 0.0118 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8989 (-0.8838)\n",
            "Epoch: [79][ 60/195]\tTime  0.991 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 0.9690 (0.9872)\tD(fake) 0.0186 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8885 (-0.8840)\n",
            "Epoch: [79][ 70/195]\tTime  0.979 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9712 (0.9872)\tD(fake) 0.0358 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9037 (-0.8836)\n",
            "Epoch: [79][ 80/195]\tTime  0.974 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0374 (0.9872)\tD(fake) -0.0112 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8883 (-0.8833)\n",
            "Epoch: [79][ 90/195]\tTime  0.979 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9783 (0.9871)\tD(fake) 0.0146 (0.0197)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9015 (-0.8836)\n",
            "Epoch: [79][100/195]\tTime  1.003 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9410 (0.9869)\tD(fake) 0.0303 (0.0200)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8762 (-0.8836)\n",
            "Epoch: [79][110/195]\tTime  0.979 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0166 (0.9870)\tD(fake) -0.0020 (0.0199)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8805 (-0.8842)\n",
            "Epoch: [79][120/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0399 (0.9870)\tD(fake) -0.0143 (0.0198)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8964 (-0.8826)\n",
            "Epoch: [79][130/195]\tTime  1.003 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9620 (0.9870)\tD(fake) 0.0188 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8860 (-0.8826)\n",
            "Epoch: [79][140/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9323 (0.9869)\tD(fake) 0.0211 (0.0197)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8779 (-0.8821)\n",
            "Epoch: [79][150/195]\tTime  1.003 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0770 (0.9871)\tD(fake) 0.0063 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8854 (-0.8812)\n",
            "Epoch: [79][160/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0314 (0.9871)\tD(fake) 0.0151 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7972 (-0.8808)\n",
            "Epoch: [79][170/195]\tTime  0.985 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0214 (0.9871)\tD(fake) -0.0045 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8924 (-0.8810)\n",
            "Epoch: [79][180/195]\tTime  1.002 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0191 (0.9872)\tD(fake) -0.0204 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8790 (-0.8805)\n",
            "Epoch: [79][190/195]\tTime  0.980 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0290 (0.9872)\tD(fake) -0.0055 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8797 (-0.8799)\n",
            "Epoch: [80][  0/195]\tTime  1.396 ( 1.396)\tData  0.335 ( 0.335)\tD(real) 1.0301 (0.9909)\tD(fake) 0.0002 (0.0127)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8759 (-0.8759)\n",
            "Epoch: [80][ 10/195]\tTime  0.974 ( 1.021)\tData  0.000 ( 0.031)\tD(real) 1.0444 (0.9893)\tD(fake) -0.0167 (0.0150)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8789 (-0.8813)\n",
            "Epoch: [80][ 20/195]\tTime  0.980 ( 1.004)\tData  0.000 ( 0.016)\tD(real) 1.0535 (0.9880)\tD(fake) 0.0178 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8868 (-0.8830)\n",
            "Epoch: [80][ 30/195]\tTime  0.999 ( 1.000)\tData  0.000 ( 0.011)\tD(real) 0.9358 (0.9873)\tD(fake) 0.0448 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8345 (-0.8803)\n",
            "Epoch: [80][ 40/195]\tTime  0.988 ( 0.997)\tData  0.000 ( 0.009)\tD(real) 1.0317 (0.9873)\tD(fake) 0.0057 (0.0198)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8982 (-0.8811)\n",
            "Epoch: [80][ 50/195]\tTime  1.002 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 1.0388 (0.9875)\tD(fake) -0.0142 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8845 (-0.8821)\n",
            "Epoch: [80][ 60/195]\tTime  0.977 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 0.9662 (0.9875)\tD(fake) 0.0210 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8767 (-0.8828)\n",
            "Epoch: [80][ 70/195]\tTime  0.981 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9873 (0.9875)\tD(fake) 0.0229 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8851 (-0.8801)\n",
            "Epoch: [80][ 80/195]\tTime  0.975 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9537 (0.9874)\tD(fake) 0.0479 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8903 (-0.8798)\n",
            "Epoch: [80][ 90/195]\tTime  0.981 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0037 (0.9875)\tD(fake) -0.0007 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8677 (-0.8809)\n",
            "Epoch: [80][100/195]\tTime  0.994 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0171 (0.9875)\tD(fake) 0.0062 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8743 (-0.8800)\n",
            "Epoch: [80][110/195]\tTime  0.993 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0239 (0.9874)\tD(fake) 0.0005 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8701 (-0.8801)\n",
            "Epoch: [80][120/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9607 (0.9874)\tD(fake) 0.0153 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8978 (-0.8793)\n",
            "Epoch: [80][130/195]\tTime  0.993 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9671 (0.9875)\tD(fake) 0.0376 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8829 (-0.8795)\n",
            "Epoch: [80][140/195]\tTime  0.983 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9566 (0.9873)\tD(fake) 0.0316 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8569 (-0.8789)\n",
            "Epoch: [80][150/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9752 (0.9874)\tD(fake) 0.0166 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8990 (-0.8797)\n",
            "Epoch: [80][160/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9752 (0.9874)\tD(fake) 0.0291 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8614 (-0.8794)\n",
            "Epoch: [80][170/195]\tTime  0.988 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0127 (0.9875)\tD(fake) -0.0257 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8784 (-0.8793)\n",
            "Epoch: [80][180/195]\tTime  0.995 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0544 (0.9875)\tD(fake) 0.0089 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8923 (-0.8803)\n",
            "Epoch: [80][190/195]\tTime  0.998 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0765 (0.9875)\tD(fake) 0.0001 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8890 (-0.8807)\n",
            "Epoch: [81][  0/195]\tTime  1.419 ( 1.419)\tData  0.358 ( 0.358)\tD(real) 0.9233 (0.9719)\tD(fake) 0.0630 (0.0280)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8644 (-0.8644)\n",
            "Epoch: [81][ 10/195]\tTime  0.993 ( 1.028)\tData  0.000 ( 0.033)\tD(real) 1.0222 (0.9873)\tD(fake) -0.0080 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9007 (-0.8853)\n",
            "Epoch: [81][ 20/195]\tTime  0.983 ( 1.007)\tData  0.000 ( 0.017)\tD(real) 1.0172 (0.9876)\tD(fake) 0.0017 (0.0181)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8493 (-0.8823)\n",
            "Epoch: [81][ 30/195]\tTime  0.980 ( 1.001)\tData  0.000 ( 0.012)\tD(real) 1.0159 (0.9878)\tD(fake) 0.0012 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8591 (-0.8831)\n",
            "Epoch: [81][ 40/195]\tTime  1.007 ( 0.998)\tData  0.000 ( 0.009)\tD(real) 1.0020 (0.9878)\tD(fake) -0.0148 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8840 (-0.8836)\n",
            "Epoch: [81][ 50/195]\tTime  0.977 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 0.9399 (0.9876)\tD(fake) 0.0245 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8856 (-0.8849)\n",
            "Epoch: [81][ 60/195]\tTime  0.996 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0256 (0.9878)\tD(fake) -0.0048 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8776 (-0.8843)\n",
            "Epoch: [81][ 70/195]\tTime  0.979 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 1.0438 (0.9878)\tD(fake) -0.0068 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8617 (-0.8840)\n",
            "Epoch: [81][ 80/195]\tTime  0.993 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0024 (0.9876)\tD(fake) 0.0037 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9099 (-0.8829)\n",
            "Epoch: [81][ 90/195]\tTime  0.995 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9491 (0.9876)\tD(fake) 0.0149 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8816 (-0.8817)\n",
            "Epoch: [81][100/195]\tTime  1.004 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9315 (0.9876)\tD(fake) 0.0574 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8103 (-0.8822)\n",
            "Epoch: [81][110/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9309 (0.9877)\tD(fake) 0.0513 (0.0182)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8706 (-0.8818)\n",
            "Epoch: [81][120/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0294 (0.9878)\tD(fake) -0.0182 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7911 (-0.8815)\n",
            "Epoch: [81][130/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9802 (0.9878)\tD(fake) 0.0071 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8843 (-0.8819)\n",
            "Epoch: [81][140/195]\tTime  0.986 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9556 (0.9877)\tD(fake) 0.0037 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8935 (-0.8823)\n",
            "Epoch: [81][150/195]\tTime  0.999 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0345 (0.9878)\tD(fake) -0.0120 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8861 (-0.8821)\n",
            "Epoch: [81][160/195]\tTime  0.979 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0152 (0.9877)\tD(fake) 0.0122 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8463 (-0.8822)\n",
            "Epoch: [81][170/195]\tTime  0.976 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0240 (0.9877)\tD(fake) 0.0080 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8720 (-0.8808)\n",
            "Epoch: [81][180/195]\tTime  0.972 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9640 (0.9876)\tD(fake) 0.0074 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8925 (-0.8809)\n",
            "Epoch: [81][190/195]\tTime  0.978 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0258 (0.9876)\tD(fake) -0.0135 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8666 (-0.8810)\n",
            "Epoch: [82][  0/195]\tTime  1.379 ( 1.379)\tData  0.337 ( 0.337)\tD(real) 0.9615 (0.9822)\tD(fake) 0.0163 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8750 (-0.8750)\n",
            "Epoch: [82][ 10/195]\tTime  0.977 ( 1.024)\tData  0.000 ( 0.031)\tD(real) 0.9387 (0.9877)\tD(fake) 0.0708 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8696 (-0.8864)\n",
            "Epoch: [82][ 20/195]\tTime  0.998 ( 1.007)\tData  0.000 ( 0.016)\tD(real) 0.9484 (0.9879)\tD(fake) 0.0292 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8993 (-0.8835)\n",
            "Epoch: [82][ 30/195]\tTime  0.976 ( 1.001)\tData  0.000 ( 0.011)\tD(real) 0.9593 (0.9880)\tD(fake) 0.0159 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8972 (-0.8824)\n",
            "Epoch: [82][ 40/195]\tTime  0.975 ( 0.997)\tData  0.000 ( 0.009)\tD(real) 0.9393 (0.9878)\tD(fake) 0.0238 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8859 (-0.8846)\n",
            "Epoch: [82][ 50/195]\tTime  0.991 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 0.9174 (0.9878)\tD(fake) 0.0104 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8812 (-0.8831)\n",
            "Epoch: [82][ 60/195]\tTime  1.003 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 0.9316 (0.9880)\tD(fake) 0.0541 (0.0181)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8367 (-0.8820)\n",
            "Epoch: [82][ 70/195]\tTime  0.985 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0071 (0.9882)\tD(fake) 0.0043 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9141 (-0.8823)\n",
            "Epoch: [82][ 80/195]\tTime  0.984 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 1.0353 (0.9882)\tD(fake) -0.0105 (0.0178)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8812 (-0.8826)\n",
            "Epoch: [82][ 90/195]\tTime  0.984 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9699 (0.9883)\tD(fake) 0.0300 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8928 (-0.8833)\n",
            "Epoch: [82][100/195]\tTime  1.003 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9138 (0.9881)\tD(fake) 0.0713 (0.0180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8407 (-0.8810)\n",
            "Epoch: [82][110/195]\tTime  0.995 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9993 (0.9882)\tD(fake) 0.0251 (0.0180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8443 (-0.8813)\n",
            "Epoch: [82][120/195]\tTime  0.978 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0366 (0.9882)\tD(fake) -0.0107 (0.0181)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8663 (-0.8818)\n",
            "Epoch: [82][130/195]\tTime  0.987 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0193 (0.9882)\tD(fake) -0.0036 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8786 (-0.8811)\n",
            "Epoch: [82][140/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0403 (0.9882)\tD(fake) 0.0016 (0.0178)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8777 (-0.8815)\n",
            "Epoch: [82][150/195]\tTime  0.982 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0174 (0.9882)\tD(fake) -0.0112 (0.0178)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8844 (-0.8815)\n",
            "Epoch: [82][160/195]\tTime  0.979 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0468 (0.9882)\tD(fake) -0.0177 (0.0178)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7961 (-0.8809)\n",
            "Epoch: [82][170/195]\tTime  0.997 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0242 (0.9881)\tD(fake) -0.0047 (0.0178)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9075 (-0.8816)\n",
            "Epoch: [82][180/195]\tTime  0.977 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9313 (0.9881)\tD(fake) 0.0231 (0.0177)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8787 (-0.8811)\n",
            "Epoch: [82][190/195]\tTime  0.980 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0084 (0.9881)\tD(fake) 0.0020 (0.0177)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9130 (-0.8812)\n",
            "Epoch: [83][  0/195]\tTime  1.408 ( 1.408)\tData  0.369 ( 0.369)\tD(real) 1.0192 (0.9924)\tD(fake) 0.0164 (0.0272)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8720 (-0.8720)\n",
            "Epoch: [83][ 10/195]\tTime  0.977 ( 1.021)\tData  0.000 ( 0.034)\tD(real) 1.0172 (0.9889)\tD(fake) -0.0136 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9047 (-0.8825)\n",
            "Epoch: [83][ 20/195]\tTime  0.977 ( 1.003)\tData  0.000 ( 0.018)\tD(real) 1.0208 (0.9883)\tD(fake) 0.0043 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8903 (-0.8790)\n",
            "Epoch: [83][ 30/195]\tTime  0.980 ( 0.997)\tData  0.000 ( 0.012)\tD(real) 1.0132 (0.9882)\tD(fake) 0.0095 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8966 (-0.8824)\n",
            "Epoch: [83][ 40/195]\tTime  0.976 ( 0.994)\tData  0.000 ( 0.009)\tD(real) 1.0302 (0.9883)\tD(fake) -0.0045 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8700 (-0.8806)\n",
            "Epoch: [83][ 50/195]\tTime  0.973 ( 0.992)\tData  0.000 ( 0.008)\tD(real) 0.9592 (0.9882)\tD(fake) 0.0093 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9058 (-0.8824)\n",
            "Epoch: [83][ 60/195]\tTime  0.991 ( 0.991)\tData  0.000 ( 0.006)\tD(real) 0.9677 (0.9883)\tD(fake) 0.0180 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8605 (-0.8833)\n",
            "Epoch: [83][ 70/195]\tTime  1.001 ( 0.991)\tData  0.000 ( 0.006)\tD(real) 1.0285 (0.9883)\tD(fake) -0.0056 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8576 (-0.8844)\n",
            "Epoch: [83][ 80/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.005)\tD(real) 1.0112 (0.9883)\tD(fake) -0.0033 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8666 (-0.8835)\n",
            "Epoch: [83][ 90/195]\tTime  0.987 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0304 (0.9883)\tD(fake) 0.0047 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8869 (-0.8844)\n",
            "Epoch: [83][100/195]\tTime  0.999 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0589 (0.9882)\tD(fake) -0.0014 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8722 (-0.8844)\n",
            "Epoch: [83][110/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9574 (0.9880)\tD(fake) 0.0353 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9009 (-0.8849)\n",
            "Epoch: [83][120/195]\tTime  0.975 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0401 (0.9881)\tD(fake) -0.0023 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8651 (-0.8840)\n",
            "Epoch: [83][130/195]\tTime  0.993 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9636 (0.9880)\tD(fake) 0.0209 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7860 (-0.8836)\n",
            "Epoch: [83][140/195]\tTime  0.985 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0390 (0.9880)\tD(fake) 0.0031 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8560 (-0.8837)\n",
            "Epoch: [83][150/195]\tTime  0.988 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9953 (0.9880)\tD(fake) 0.0060 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9078 (-0.8845)\n",
            "Epoch: [83][160/195]\tTime  0.991 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0268 (0.9880)\tD(fake) -0.0120 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8008 (-0.8842)\n",
            "Epoch: [83][170/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0498 (0.9880)\tD(fake) 0.0004 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8639 (-0.8840)\n",
            "Epoch: [83][180/195]\tTime  0.996 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9339 (0.9879)\tD(fake) 0.0579 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8796 (-0.8844)\n",
            "Epoch: [83][190/195]\tTime  0.982 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0225 (0.9879)\tD(fake) 0.0022 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9028 (-0.8843)\n",
            "Epoch: [84][  0/195]\tTime  1.385 ( 1.385)\tData  0.344 ( 0.344)\tD(real) 0.9923 (0.9864)\tD(fake) 0.0062 (0.0065)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9138 (-0.9138)\n",
            "Epoch: [84][ 10/195]\tTime  0.999 ( 1.026)\tData  0.000 ( 0.032)\tD(real) 0.9442 (0.9883)\tD(fake) 0.0065 (0.0178)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8965 (-0.8928)\n",
            "Epoch: [84][ 20/195]\tTime  0.993 ( 1.007)\tData  0.000 ( 0.017)\tD(real) 1.0319 (0.9888)\tD(fake) -0.0023 (0.0178)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8963 (-0.8853)\n",
            "Epoch: [84][ 30/195]\tTime  0.985 ( 1.002)\tData  0.000 ( 0.011)\tD(real) 1.0322 (0.9885)\tD(fake) -0.0054 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9026 (-0.8851)\n",
            "Epoch: [84][ 40/195]\tTime  0.977 ( 0.999)\tData  0.000 ( 0.009)\tD(real) 0.9867 (0.9883)\tD(fake) 0.0153 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8791 (-0.8842)\n",
            "Epoch: [84][ 50/195]\tTime  0.975 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 1.0214 (0.9881)\tD(fake) 0.0005 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8919 (-0.8848)\n",
            "Epoch: [84][ 60/195]\tTime  0.979 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0567 (0.9882)\tD(fake) -0.0176 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8644 (-0.8839)\n",
            "Epoch: [84][ 70/195]\tTime  0.996 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9512 (0.9881)\tD(fake) 0.0201 (0.0185)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8786 (-0.8832)\n",
            "Epoch: [84][ 80/195]\tTime  0.987 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9890 (0.9881)\tD(fake) -0.0025 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8706 (-0.8834)\n",
            "Epoch: [84][ 90/195]\tTime  0.985 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 1.0319 (0.9882)\tD(fake) -0.0050 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8878 (-0.8844)\n",
            "Epoch: [84][100/195]\tTime  1.001 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9619 (0.9879)\tD(fake) 0.0566 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9075 (-0.8853)\n",
            "Epoch: [84][110/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0128 (0.9879)\tD(fake) 0.0077 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8873 (-0.8839)\n",
            "Epoch: [84][120/195]\tTime  1.000 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9611 (0.9879)\tD(fake) 0.0216 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8767 (-0.8842)\n",
            "Epoch: [84][130/195]\tTime  0.978 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9480 (0.9880)\tD(fake) 0.0368 (0.0193)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9250 (-0.8837)\n",
            "Epoch: [84][140/195]\tTime  0.983 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0315 (0.9881)\tD(fake) -0.0134 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8864 (-0.8840)\n",
            "Epoch: [84][150/195]\tTime  0.980 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9548 (0.9881)\tD(fake) 0.0241 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9049 (-0.8844)\n",
            "Epoch: [84][160/195]\tTime  0.973 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9635 (0.9880)\tD(fake) 0.0341 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8991 (-0.8847)\n",
            "Epoch: [84][170/195]\tTime  0.978 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9150 (0.9880)\tD(fake) 0.0565 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8587 (-0.8848)\n",
            "Epoch: [84][180/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9336 (0.9880)\tD(fake) 0.0374 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8876 (-0.8847)\n",
            "Epoch: [84][190/195]\tTime  0.978 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0085 (0.9881)\tD(fake) 0.0026 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8789 (-0.8838)\n",
            "Epoch: [85][  0/195]\tTime  1.341 ( 1.341)\tData  0.316 ( 0.316)\tD(real) 0.9923 (0.9859)\tD(fake) 0.0156 (0.0170)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8842 (-0.8842)\n",
            "Epoch: [85][ 10/195]\tTime  0.990 ( 1.023)\tData  0.000 ( 0.029)\tD(real) 1.0412 (0.9891)\tD(fake) 0.0006 (0.0177)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8836 (-0.8832)\n",
            "Epoch: [85][ 20/195]\tTime  0.996 ( 1.008)\tData  0.000 ( 0.015)\tD(real) 1.0003 (0.9885)\tD(fake) 0.0120 (0.0178)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8837 (-0.8843)\n",
            "Epoch: [85][ 30/195]\tTime  0.996 ( 1.000)\tData  0.000 ( 0.011)\tD(real) 0.9644 (0.9886)\tD(fake) -0.0006 (0.0166)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8639 (-0.8768)\n",
            "Epoch: [85][ 40/195]\tTime  0.999 ( 0.997)\tData  0.000 ( 0.008)\tD(real) 0.9597 (0.9885)\tD(fake) 0.0184 (0.0167)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9006 (-0.8783)\n",
            "Epoch: [85][ 50/195]\tTime  0.997 ( 0.996)\tData  0.000 ( 0.007)\tD(real) 0.9959 (0.9887)\tD(fake) 0.0063 (0.0169)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8937 (-0.8773)\n",
            "Epoch: [85][ 60/195]\tTime  1.004 ( 0.996)\tData  0.000 ( 0.006)\tD(real) 0.9498 (0.9885)\tD(fake) 0.0031 (0.0172)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8837 (-0.8786)\n",
            "Epoch: [85][ 70/195]\tTime  0.982 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 0.9862 (0.9885)\tD(fake) 0.0167 (0.0177)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8804 (-0.8808)\n",
            "Epoch: [85][ 80/195]\tTime  0.987 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 1.0560 (0.9887)\tD(fake) -0.0212 (0.0172)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8795 (-0.8804)\n",
            "Epoch: [85][ 90/195]\tTime  0.992 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0217 (0.9886)\tD(fake) -0.0077 (0.0173)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8742 (-0.8809)\n",
            "Epoch: [85][100/195]\tTime  1.009 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 0.9775 (0.9885)\tD(fake) 0.0067 (0.0172)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8684 (-0.8807)\n",
            "Epoch: [85][110/195]\tTime  0.982 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9428 (0.9885)\tD(fake) 0.0522 (0.0173)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8361 (-0.8808)\n",
            "Epoch: [85][120/195]\tTime  0.975 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0381 (0.9886)\tD(fake) -0.0001 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8970 (-0.8813)\n",
            "Epoch: [85][130/195]\tTime  0.992 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9905 (0.9885)\tD(fake) 0.0117 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8923 (-0.8817)\n",
            "Epoch: [85][140/195]\tTime  0.998 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9814 (0.9885)\tD(fake) -0.0068 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9010 (-0.8822)\n",
            "Epoch: [85][150/195]\tTime  0.982 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 0.9733 (0.9884)\tD(fake) 0.0081 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8805 (-0.8827)\n",
            "Epoch: [85][160/195]\tTime  0.996 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 1.0347 (0.9884)\tD(fake) -0.0148 (0.0178)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8964 (-0.8830)\n",
            "Epoch: [85][170/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0407 (0.9883)\tD(fake) -0.0036 (0.0180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8979 (-0.8835)\n",
            "Epoch: [85][180/195]\tTime  0.977 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0602 (0.9883)\tD(fake) -0.0080 (0.0181)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8948 (-0.8837)\n",
            "Epoch: [85][190/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0228 (0.9882)\tD(fake) 0.0018 (0.0182)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8878 (-0.8840)\n",
            "Epoch: [86][  0/195]\tTime  1.396 ( 1.396)\tData  0.343 ( 0.343)\tD(real) 1.0023 (0.9889)\tD(fake) 0.0185 (0.0215)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8858 (-0.8858)\n",
            "Epoch: [86][ 10/195]\tTime  0.997 ( 1.028)\tData  0.000 ( 0.032)\tD(real) 0.9761 (0.9884)\tD(fake) 0.0104 (0.0181)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8722 (-0.8839)\n",
            "Epoch: [86][ 20/195]\tTime  0.995 ( 1.007)\tData  0.000 ( 0.017)\tD(real) 1.0514 (0.9886)\tD(fake) -0.0173 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8839 (-0.8818)\n",
            "Epoch: [86][ 30/195]\tTime  0.980 ( 1.001)\tData  0.000 ( 0.011)\tD(real) 1.0441 (0.9883)\tD(fake) -0.0164 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8982 (-0.8829)\n",
            "Epoch: [86][ 40/195]\tTime  0.983 ( 0.996)\tData  0.000 ( 0.009)\tD(real) 1.0460 (0.9881)\tD(fake) 0.0073 (0.0196)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8880 (-0.8839)\n",
            "Epoch: [86][ 50/195]\tTime  0.999 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 1.0329 (0.9883)\tD(fake) -0.0032 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8852 (-0.8851)\n",
            "Epoch: [86][ 60/195]\tTime  0.980 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 1.0081 (0.9881)\tD(fake) -0.0093 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8981 (-0.8856)\n",
            "Epoch: [86][ 70/195]\tTime  0.988 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0619 (0.9881)\tD(fake) 0.0302 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8794 (-0.8852)\n",
            "Epoch: [86][ 80/195]\tTime  0.981 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 0.9828 (0.9880)\tD(fake) 0.0099 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8667 (-0.8843)\n",
            "Epoch: [86][ 90/195]\tTime  0.990 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9408 (0.9879)\tD(fake) 0.0357 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8924 (-0.8837)\n",
            "Epoch: [86][100/195]\tTime  1.011 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9469 (0.9879)\tD(fake) 0.0102 (0.0194)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9015 (-0.8826)\n",
            "Epoch: [86][110/195]\tTime  0.978 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9612 (0.9880)\tD(fake) 0.0325 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8973 (-0.8828)\n",
            "Epoch: [86][120/195]\tTime  0.977 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9748 (0.9880)\tD(fake) 0.0108 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8826 (-0.8836)\n",
            "Epoch: [86][130/195]\tTime  0.983 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9083 (0.9879)\tD(fake) 0.0519 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8740 (-0.8843)\n",
            "Epoch: [86][140/195]\tTime  0.980 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9797 (0.9880)\tD(fake) 0.0093 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8963 (-0.8843)\n",
            "Epoch: [86][150/195]\tTime  0.984 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 0.9776 (0.9880)\tD(fake) -0.0020 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8983 (-0.8841)\n",
            "Epoch: [86][160/195]\tTime  0.994 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9706 (0.9879)\tD(fake) 0.0091 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8869 (-0.8845)\n",
            "Epoch: [86][170/195]\tTime  0.979 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9690 (0.9879)\tD(fake) 0.0214 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8641 (-0.8843)\n",
            "Epoch: [86][180/195]\tTime  0.972 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9696 (0.9879)\tD(fake) 0.0072 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8795 (-0.8839)\n",
            "Epoch: [86][190/195]\tTime  0.994 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 0.9832 (0.9878)\tD(fake) 0.0210 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8962 (-0.8842)\n",
            "Epoch: [87][  0/195]\tTime  1.391 ( 1.391)\tData  0.347 ( 0.347)\tD(real) 1.0078 (0.9971)\tD(fake) -0.0027 (0.0214)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8869 (-0.8869)\n",
            "Epoch: [87][ 10/195]\tTime  0.984 ( 1.024)\tData  0.000 ( 0.032)\tD(real) 1.0299 (0.9902)\tD(fake) -0.0132 (0.0168)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8869 (-0.8780)\n",
            "Epoch: [87][ 20/195]\tTime  0.982 ( 1.005)\tData  0.000 ( 0.017)\tD(real) 1.0135 (0.9896)\tD(fake) 0.0029 (0.0170)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8726 (-0.8819)\n",
            "Epoch: [87][ 30/195]\tTime  0.979 ( 0.999)\tData  0.000 ( 0.012)\tD(real) 1.0346 (0.9894)\tD(fake) 0.0053 (0.0176)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8988 (-0.8855)\n",
            "Epoch: [87][ 40/195]\tTime  0.979 ( 0.996)\tData  0.000 ( 0.009)\tD(real) 1.0257 (0.9894)\tD(fake) -0.0108 (0.0172)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8637 (-0.8835)\n",
            "Epoch: [87][ 50/195]\tTime  0.995 ( 0.993)\tData  0.000 ( 0.007)\tD(real) 1.0195 (0.9893)\tD(fake) 0.0111 (0.0171)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8480 (-0.8810)\n",
            "Epoch: [87][ 60/195]\tTime  1.000 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 1.0457 (0.9893)\tD(fake) 0.0074 (0.0172)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8919 (-0.8810)\n",
            "Epoch: [87][ 70/195]\tTime  0.996 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 0.9759 (0.9892)\tD(fake) 0.0123 (0.0171)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9077 (-0.8822)\n",
            "Epoch: [87][ 80/195]\tTime  0.994 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 0.9457 (0.9892)\tD(fake) 0.0089 (0.0169)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8729 (-0.8820)\n",
            "Epoch: [87][ 90/195]\tTime  1.000 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9607 (0.9892)\tD(fake) 0.0083 (0.0173)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8963 (-0.8824)\n",
            "Epoch: [87][100/195]\tTime  1.004 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 0.9345 (0.9891)\tD(fake) 0.0240 (0.0174)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8865 (-0.8834)\n",
            "Epoch: [87][110/195]\tTime  0.977 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0195 (0.9891)\tD(fake) -0.0165 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8787 (-0.8840)\n",
            "Epoch: [87][120/195]\tTime  0.974 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9801 (0.9890)\tD(fake) 0.0185 (0.0176)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8833 (-0.8833)\n",
            "Epoch: [87][130/195]\tTime  0.976 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9505 (0.9889)\tD(fake) 0.0342 (0.0176)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8931 (-0.8833)\n",
            "Epoch: [87][140/195]\tTime  0.983 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0139 (0.9889)\tD(fake) -0.0019 (0.0176)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8595 (-0.8838)\n",
            "Epoch: [87][150/195]\tTime  0.987 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9748 (0.9888)\tD(fake) 0.0147 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8973 (-0.8836)\n",
            "Epoch: [87][160/195]\tTime  0.981 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0553 (0.9888)\tD(fake) 0.0020 (0.0178)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8964 (-0.8842)\n",
            "Epoch: [87][170/195]\tTime  0.977 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0305 (0.9887)\tD(fake) 0.0121 (0.0180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9007 (-0.8843)\n",
            "Epoch: [87][180/195]\tTime  0.989 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0525 (0.9886)\tD(fake) -0.0094 (0.0180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8171 (-0.8841)\n",
            "Epoch: [87][190/195]\tTime  0.995 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 0.9730 (0.9886)\tD(fake) 0.0046 (0.0180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8861 (-0.8839)\n",
            "Epoch: [88][  0/195]\tTime  1.360 ( 1.360)\tData  0.329 ( 0.329)\tD(real) 1.0083 (0.9921)\tD(fake) 0.0117 (0.0253)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8998 (-0.8998)\n",
            "Epoch: [88][ 10/195]\tTime  0.987 ( 1.023)\tData  0.000 ( 0.030)\tD(real) 1.0191 (0.9896)\tD(fake) -0.0166 (0.0173)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8763 (-0.8700)\n",
            "Epoch: [88][ 20/195]\tTime  0.979 ( 1.004)\tData  0.000 ( 0.016)\tD(real) 0.9297 (0.9885)\tD(fake) 0.0683 (0.0182)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8644 (-0.8760)\n",
            "Epoch: [88][ 30/195]\tTime  0.982 ( 0.999)\tData  0.000 ( 0.011)\tD(real) 1.0147 (0.9892)\tD(fake) 0.0093 (0.0175)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8688 (-0.8793)\n",
            "Epoch: [88][ 40/195]\tTime  0.994 ( 0.996)\tData  0.000 ( 0.008)\tD(real) 1.0294 (0.9890)\tD(fake) 0.0095 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9051 (-0.8801)\n",
            "Epoch: [88][ 50/195]\tTime  0.987 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 0.9469 (0.9888)\tD(fake) 0.0136 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8682 (-0.8808)\n",
            "Epoch: [88][ 60/195]\tTime  1.004 ( 0.994)\tData  0.000 ( 0.006)\tD(real) 0.9536 (0.9888)\tD(fake) 0.0308 (0.0179)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8173 (-0.8812)\n",
            "Epoch: [88][ 70/195]\tTime  0.982 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0358 (0.9888)\tD(fake) -0.0015 (0.0181)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8956 (-0.8826)\n",
            "Epoch: [88][ 80/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.004)\tD(real) 0.9592 (0.9886)\tD(fake) 0.0510 (0.0184)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8826 (-0.8829)\n",
            "Epoch: [88][ 90/195]\tTime  0.981 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9501 (0.9887)\tD(fake) 0.0193 (0.0180)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8617 (-0.8833)\n",
            "Epoch: [88][100/195]\tTime  1.001 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 0.9501 (0.9886)\tD(fake) 0.0556 (0.0182)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9002 (-0.8841)\n",
            "Epoch: [88][110/195]\tTime  0.991 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0459 (0.9886)\tD(fake) 0.0211 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8987 (-0.8837)\n",
            "Epoch: [88][120/195]\tTime  0.983 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0419 (0.9886)\tD(fake) -0.0070 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9010 (-0.8843)\n",
            "Epoch: [88][130/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9446 (0.9884)\tD(fake) 0.0055 (0.0183)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8907 (-0.8845)\n",
            "Epoch: [88][140/195]\tTime  0.979 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9946 (0.9884)\tD(fake) 0.0004 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8986 (-0.8844)\n",
            "Epoch: [88][150/195]\tTime  0.976 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 0.9794 (0.9884)\tD(fake) 0.0421 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8783 (-0.8842)\n",
            "Epoch: [88][160/195]\tTime  0.980 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9843 (0.9883)\tD(fake) 0.0182 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8841 (-0.8846)\n",
            "Epoch: [88][170/195]\tTime  1.002 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0167 (0.9884)\tD(fake) 0.0100 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8803 (-0.8848)\n",
            "Epoch: [88][180/195]\tTime  0.994 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0120 (0.9884)\tD(fake) -0.0265 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8820 (-0.8845)\n",
            "Epoch: [88][190/195]\tTime  1.001 ( 0.989)\tData  0.000 ( 0.002)\tD(real) 1.0309 (0.9884)\tD(fake) 0.0055 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8853 (-0.8849)\n",
            "Epoch: [89][  0/195]\tTime  1.381 ( 1.381)\tData  0.329 ( 0.329)\tD(real) 1.0151 (0.9984)\tD(fake) 0.0026 (0.0152)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9014 (-0.9014)\n",
            "Epoch: [89][ 10/195]\tTime  0.982 ( 1.023)\tData  0.000 ( 0.030)\tD(real) 1.0309 (0.9896)\tD(fake) -0.0159 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9075 (-0.8812)\n",
            "Epoch: [89][ 20/195]\tTime  0.983 ( 1.006)\tData  0.000 ( 0.016)\tD(real) 1.0115 (0.9889)\tD(fake) 0.0085 (0.0186)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8825 (-0.8836)\n",
            "Epoch: [89][ 30/195]\tTime  0.969 ( 1.000)\tData  0.000 ( 0.011)\tD(real) 1.0121 (0.9886)\tD(fake) -0.0054 (0.0195)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8935 (-0.8853)\n",
            "Epoch: [89][ 40/195]\tTime  0.995 ( 0.997)\tData  0.000 ( 0.008)\tD(real) 1.0152 (0.9886)\tD(fake) 0.0009 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8934 (-0.8866)\n",
            "Epoch: [89][ 50/195]\tTime  0.990 ( 0.995)\tData  0.000 ( 0.007)\tD(real) 0.9518 (0.9885)\tD(fake) 0.0227 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9155 (-0.8864)\n",
            "Epoch: [89][ 60/195]\tTime  0.983 ( 0.993)\tData  0.000 ( 0.006)\tD(real) 1.0359 (0.9887)\tD(fake) 0.0107 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8966 (-0.8882)\n",
            "Epoch: [89][ 70/195]\tTime  1.001 ( 0.992)\tData  0.000 ( 0.005)\tD(real) 1.0248 (0.9886)\tD(fake) -0.0083 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8725 (-0.8874)\n",
            "Epoch: [89][ 80/195]\tTime  0.985 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0420 (0.9886)\tD(fake) 0.0019 (0.0192)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8770 (-0.8873)\n",
            "Epoch: [89][ 90/195]\tTime  1.000 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0115 (0.9887)\tD(fake) 0.0059 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8245 (-0.8864)\n",
            "Epoch: [89][100/195]\tTime  1.008 ( 0.992)\tData  0.000 ( 0.004)\tD(real) 1.0264 (0.9886)\tD(fake) -0.0132 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9128 (-0.8860)\n",
            "Epoch: [89][110/195]\tTime  0.977 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9385 (0.9886)\tD(fake) 0.0043 (0.0187)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8973 (-0.8864)\n",
            "Epoch: [89][120/195]\tTime  0.987 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9923 (0.9886)\tD(fake) 0.0018 (0.0188)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8591 (-0.8866)\n",
            "Epoch: [89][130/195]\tTime  1.002 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9463 (0.9885)\tD(fake) 0.0578 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9055 (-0.8859)\n",
            "Epoch: [89][140/195]\tTime  0.993 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9647 (0.9884)\tD(fake) 0.0135 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8786 (-0.8865)\n",
            "Epoch: [89][150/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 1.0247 (0.9884)\tD(fake) 0.0053 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7733 (-0.8853)\n",
            "Epoch: [89][160/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 1.0171 (0.9885)\tD(fake) -0.0037 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8606 (-0.8846)\n",
            "Epoch: [89][170/195]\tTime  0.983 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9336 (0.9884)\tD(fake) 0.0759 (0.0190)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9007 (-0.8852)\n",
            "Epoch: [89][180/195]\tTime  0.983 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0124 (0.9884)\tD(fake) -0.0061 (0.0191)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8751 (-0.8854)\n",
            "Epoch: [89][190/195]\tTime  1.007 ( 0.991)\tData  0.000 ( 0.002)\tD(real) 0.9611 (0.9884)\tD(fake) 0.0243 (0.0189)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8916 (-0.8855)\n",
            "Epoch: [90][  0/195]\tTime  1.425 ( 1.425)\tData  0.364 ( 0.364)\tD(real) 0.9383 (0.9831)\tD(fake) 0.0430 (0.0182)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8883 (-0.8883)\n",
            "Epoch: [90][ 10/195]\tTime  0.988 ( 1.028)\tData  0.000 ( 0.033)\tD(real) 1.0265 (0.9910)\tD(fake) -0.0043 (0.0135)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8770 (-0.8897)\n",
            "Epoch: [90][ 20/195]\tTime  0.983 ( 1.010)\tData  0.000 ( 0.018)\tD(real) 1.0248 (0.9911)\tD(fake) -0.0265 (0.0123)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8880 (-0.8878)\n",
            "Epoch: [90][ 30/195]\tTime  0.992 ( 1.004)\tData  0.000 ( 0.012)\tD(real) 0.9777 (0.9912)\tD(fake) 0.0023 (0.0118)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8908 (-0.8880)\n",
            "Epoch: [90][ 40/195]\tTime  0.976 ( 1.000)\tData  0.000 ( 0.009)\tD(real) 0.9491 (0.9914)\tD(fake) -0.0041 (0.0108)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9205 (-0.8904)\n",
            "Epoch: [90][ 50/195]\tTime  0.978 ( 0.997)\tData  0.000 ( 0.007)\tD(real) 1.0388 (0.9917)\tD(fake) -0.0128 (0.0103)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9041 (-0.8921)\n",
            "Epoch: [90][ 60/195]\tTime  0.986 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 1.0024 (0.9918)\tD(fake) -0.0450 (0.0100)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8643 (-0.8919)\n",
            "Epoch: [90][ 70/195]\tTime  0.979 ( 0.994)\tData  0.000 ( 0.005)\tD(real) 1.0369 (0.9919)\tD(fake) -0.0085 (0.0097)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8944 (-0.8898)\n",
            "Epoch: [90][ 80/195]\tTime  0.976 ( 0.993)\tData  0.000 ( 0.005)\tD(real) 1.0297 (0.9919)\tD(fake) -0.0170 (0.0096)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8538 (-0.8890)\n",
            "Epoch: [90][ 90/195]\tTime  0.991 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 0.9721 (0.9919)\tD(fake) 0.0030 (0.0093)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8912 (-0.8892)\n",
            "Epoch: [90][100/195]\tTime  1.033 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0538 (0.9921)\tD(fake) -0.0165 (0.0091)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9069 (-0.8900)\n",
            "Epoch: [90][110/195]\tTime  1.002 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0313 (0.9921)\tD(fake) -0.0075 (0.0090)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9042 (-0.8908)\n",
            "Epoch: [90][120/195]\tTime  0.978 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0542 (0.9922)\tD(fake) -0.0089 (0.0089)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8794 (-0.8915)\n",
            "Epoch: [90][130/195]\tTime  0.988 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9477 (0.9921)\tD(fake) 0.0469 (0.0088)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8998 (-0.8923)\n",
            "Epoch: [90][140/195]\tTime  0.978 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9637 (0.9922)\tD(fake) 0.0306 (0.0088)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8963 (-0.8933)\n",
            "Epoch: [90][150/195]\tTime  0.983 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9380 (0.9922)\tD(fake) 0.0119 (0.0087)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8861 (-0.8934)\n",
            "Epoch: [90][160/195]\tTime  0.979 ( 0.991)\tData  0.000 ( 0.003)\tD(real) 0.9502 (0.9922)\tD(fake) 0.0285 (0.0085)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7678 (-0.8926)\n",
            "Epoch: [90][170/195]\tTime  0.984 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 1.0084 (0.9923)\tD(fake) 0.0048 (0.0084)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9047 (-0.8932)\n",
            "Epoch: [90][180/195]\tTime  0.977 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9970 (0.9923)\tD(fake) 0.0188 (0.0083)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9136 (-0.8939)\n",
            "Epoch: [90][190/195]\tTime  0.987 ( 0.990)\tData  0.000 ( 0.002)\tD(real) 0.9772 (0.9923)\tD(fake) 0.0215 (0.0082)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8904 (-0.8946)\n",
            "Epoch: [91][  0/195]\tTime  1.376 ( 1.376)\tData  0.349 ( 0.349)\tD(real) 1.0526 (1.0017)\tD(fake) -0.0092 (0.0104)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9112 (-0.9112)\n",
            "Epoch: [91][ 10/195]\tTime  0.981 ( 1.025)\tData  0.000 ( 0.032)\tD(real) 0.9651 (0.9935)\tD(fake) -0.0089 (0.0055)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9093 (-0.9038)\n",
            "Epoch: [91][ 20/195]\tTime  0.993 ( 1.004)\tData  0.000 ( 0.017)\tD(real) 0.9822 (0.9935)\tD(fake) 0.0127 (0.0059)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9173 (-0.9039)\n",
            "Epoch: [91][ 30/195]\tTime  0.999 ( 0.998)\tData  0.000 ( 0.012)\tD(real) 0.9671 (0.9934)\tD(fake) 0.0261 (0.0058)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8143 (-0.9026)\n",
            "Epoch: [91][ 40/195]\tTime  0.981 ( 0.996)\tData  0.000 ( 0.009)\tD(real) 0.9495 (0.9934)\tD(fake) 0.0304 (0.0060)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8971 (-0.9035)\n",
            "Epoch: [91][ 50/195]\tTime  0.980 ( 0.993)\tData  0.000 ( 0.007)\tD(real) 0.9717 (0.9936)\tD(fake) 0.0203 (0.0062)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8691 (-0.9029)\n",
            "Epoch: [91][ 60/195]\tTime  0.976 ( 0.992)\tData  0.000 ( 0.006)\tD(real) 1.0400 (0.9937)\tD(fake) -0.0062 (0.0063)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8750 (-0.9013)\n",
            "Epoch: [91][ 70/195]\tTime  0.986 ( 0.991)\tData  0.000 ( 0.005)\tD(real) 1.0280 (0.9937)\tD(fake) -0.0039 (0.0063)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9210 (-0.9018)\n",
            "Epoch: [91][ 80/195]\tTime  0.986 ( 0.990)\tData  0.001 ( 0.005)\tD(real) 1.0036 (0.9936)\tD(fake) 0.0005 (0.0064)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9004 (-0.9023)\n",
            "Epoch: [91][ 90/195]\tTime  0.983 ( 0.989)\tData  0.000 ( 0.004)\tD(real) 1.0340 (0.9936)\tD(fake) 0.0027 (0.0064)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9156 (-0.9034)\n",
            "Epoch: [91][100/195]\tTime  1.005 ( 0.990)\tData  0.000 ( 0.004)\tD(real) 1.0083 (0.9936)\tD(fake) -0.0193 (0.0065)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9022 (-0.9026)\n",
            "Epoch: [91][110/195]\tTime  0.985 ( 0.990)\tData  0.000 ( 0.003)\tD(real) 1.0276 (0.9936)\tD(fake) -0.0025 (0.0064)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9139 (-0.9028)\n",
            "Epoch: [91][120/195]\tTime  0.984 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0290 (0.9936)\tD(fake) -0.0259 (0.0063)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8951 (-0.9035)\n",
            "Epoch: [91][130/195]\tTime  0.995 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 1.0125 (0.9935)\tD(fake) -0.0273 (0.0064)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.7959 (-0.9030)\n",
            "Epoch: [91][140/195]\tTime  0.998 ( 0.989)\tData  0.000 ( 0.003)\tD(real) 0.9531 (0.9935)\tD(fake) 0.0307 (0.0064)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9222 (-0.9037)\n",
            "Epoch: [91][150/195]\tTime  0.995 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 0.9674 (0.9935)\tD(fake) 0.0107 (0.0064)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9254 (-0.9036)\n",
            "Epoch: [91][160/195]\tTime  0.997 ( 0.988)\tData  0.000 ( 0.003)\tD(real) 0.9664 (0.9935)\tD(fake) 0.0106 (0.0064)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8799 (-0.9031)\n",
            "Epoch: [91][170/195]\tTime  0.987 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0319 (0.9936)\tD(fake) -0.0206 (0.0064)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9022 (-0.9032)\n",
            "Epoch: [91][180/195]\tTime  0.986 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0016 (0.9935)\tD(fake) -0.0104 (0.0064)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9250 (-0.9032)\n",
            "Epoch: [91][190/195]\tTime  0.989 ( 0.988)\tData  0.000 ( 0.002)\tD(real) 1.0347 (0.9935)\tD(fake) -0.0100 (0.0063)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9098 (-0.9032)\n",
            "Epoch: [92][  0/195]\tTime  1.394 ( 1.394)\tData  0.373 ( 0.373)\tD(real) 0.9570 (0.9860)\tD(fake) 0.0464 (0.0135)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9023 (-0.9023)\n",
            "Epoch: [92][ 10/195]\tTime  0.982 ( 1.025)\tData  0.000 ( 0.034)\tD(real) 0.9589 (0.9935)\tD(fake) 0.0073 (0.0067)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9164 (-0.9007)\n",
            "Epoch: [92][ 20/195]\tTime  1.015 ( 1.006)\tData  0.000 ( 0.018)\tD(real) 1.0141 (0.9937)\tD(fake) 0.0004 (0.0064)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9215 (-0.9079)\n",
            "Epoch: [92][ 30/195]\tTime  0.980 ( 1.002)\tData  0.000 ( 0.012)\tD(real) 0.9814 (0.9938)\tD(fake) -0.0115 (0.0065)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9211 (-0.9071)\n",
            "Epoch: [92][ 40/195]\tTime  0.985 ( 0.998)\tData  0.000 ( 0.009)\tD(real) 0.9586 (0.9938)\tD(fake) 0.0268 (0.0062)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8877 (-0.9070)\n",
            "Epoch: [92][ 50/195]\tTime  0.988 ( 0.996)\tData  0.000 ( 0.008)\tD(real) 0.9967 (0.9940)\tD(fake) -0.0102 (0.0062)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8942 (-0.9043)\n",
            "Epoch: [92][ 60/195]\tTime  0.995 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 0.9477 (0.9939)\tD(fake) -0.0061 (0.0061)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8906 (-0.9045)\n",
            "Epoch: [92][ 70/195]\tTime  0.999 ( 0.995)\tData  0.000 ( 0.006)\tD(real) 0.9746 (0.9940)\tD(fake) 0.0168 (0.0061)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9204 (-0.9044)\n",
            "Epoch: [92][ 80/195]\tTime  0.995 ( 0.995)\tData  0.000 ( 0.005)\tD(real) 0.9530 (0.9939)\tD(fake) 0.0085 (0.0061)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8991 (-0.9048)\n",
            "Epoch: [92][ 90/195]\tTime  0.977 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0378 (0.9940)\tD(fake) -0.0029 (0.0060)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9019 (-0.9050)\n",
            "Epoch: [92][100/195]\tTime  1.019 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0227 (0.9940)\tD(fake) -0.0057 (0.0059)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9014 (-0.9046)\n",
            "Epoch: [92][110/195]\tTime  0.981 ( 0.993)\tData  0.000 ( 0.004)\tD(real) 1.0210 (0.9940)\tD(fake) -0.0094 (0.0058)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9024 (-0.9044)\n",
            "Epoch: [92][120/195]\tTime  0.993 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0266 (0.9940)\tD(fake) 0.0033 (0.0058)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9003 (-0.9044)\n",
            "Epoch: [92][130/195]\tTime  0.987 ( 0.993)\tData  0.000 ( 0.003)\tD(real) 1.0399 (0.9940)\tD(fake) -0.0084 (0.0058)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9194 (-0.9046)\n",
            "Epoch: [92][140/195]\tTime  0.994 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 0.9947 (0.9940)\tD(fake) -0.0139 (0.0058)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9037 (-0.9045)\n",
            "Epoch: [92][150/195]\tTime  0.984 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0209 (0.9941)\tD(fake) -0.0188 (0.0059)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9274 (-0.9048)\n",
            "Epoch: [92][160/195]\tTime  1.004 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0102 (0.9940)\tD(fake) -0.0072 (0.0059)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9111 (-0.9053)\n",
            "Epoch: [92][170/195]\tTime  0.991 ( 0.992)\tData  0.000 ( 0.003)\tD(real) 1.0379 (0.9941)\tD(fake) -0.0074 (0.0059)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9238 (-0.9056)\n",
            "Epoch: [92][180/195]\tTime  1.001 ( 0.992)\tData  0.000 ( 0.002)\tD(real) 1.0152 (0.9941)\tD(fake) 0.0110 (0.0059)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9167 (-0.9062)\n",
            "Epoch: [92][190/195]\tTime  0.987 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 1.0067 (0.9941)\tD(fake) -0.0071 (0.0058)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9239 (-0.9064)\n",
            "Epoch: [93][  0/195]\tTime  1.390 ( 1.390)\tData  0.333 ( 0.333)\tD(real) 0.9771 (0.9908)\tD(fake) 0.0213 (0.0044)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9316 (-0.9316)\n",
            "Epoch: [93][ 10/195]\tTime  0.982 ( 1.027)\tData  0.000 ( 0.031)\tD(real) 0.9339 (0.9938)\tD(fake) -0.0052 (0.0046)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9240 (-0.9178)\n",
            "Epoch: [93][ 20/195]\tTime  0.988 ( 1.013)\tData  0.000 ( 0.016)\tD(real) 1.0354 (0.9945)\tD(fake) -0.0119 (0.0043)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8899 (-0.9132)\n",
            "Epoch: [93][ 30/195]\tTime  0.988 ( 1.006)\tData  0.000 ( 0.011)\tD(real) 1.0043 (0.9944)\tD(fake) -0.0169 (0.0046)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9116 (-0.9049)\n",
            "Epoch: [93][ 40/195]\tTime  0.997 ( 1.003)\tData  0.000 ( 0.008)\tD(real) 0.9725 (0.9944)\tD(fake) 0.0068 (0.0050)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8714 (-0.9062)\n",
            "Epoch: [93][ 50/195]\tTime  0.993 ( 1.001)\tData  0.000 ( 0.007)\tD(real) 1.0307 (0.9946)\tD(fake) -0.0094 (0.0050)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9202 (-0.9066)\n",
            "Epoch: [93][ 60/195]\tTime  0.985 ( 1.000)\tData  0.000 ( 0.006)\tD(real) 1.0147 (0.9945)\tD(fake) -0.0212 (0.0051)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9060 (-0.9075)\n",
            "Epoch: [93][ 70/195]\tTime  1.001 ( 0.998)\tData  0.000 ( 0.005)\tD(real) 1.0337 (0.9945)\tD(fake) 0.0024 (0.0051)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8854 (-0.9084)\n",
            "Epoch: [93][ 80/195]\tTime  1.001 ( 0.997)\tData  0.000 ( 0.004)\tD(real) 0.9547 (0.9944)\tD(fake) 0.0224 (0.0050)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9038 (-0.9083)\n",
            "Epoch: [93][ 90/195]\tTime  1.008 ( 0.997)\tData  0.000 ( 0.004)\tD(real) 0.9685 (0.9945)\tD(fake) 0.0118 (0.0050)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9429 (-0.9094)\n",
            "Epoch: [93][100/195]\tTime  0.999 ( 0.997)\tData  0.000 ( 0.004)\tD(real) 0.9745 (0.9945)\tD(fake) -0.0073 (0.0050)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9282 (-0.9091)\n",
            "Epoch: [93][110/195]\tTime  0.985 ( 0.996)\tData  0.000 ( 0.003)\tD(real) 0.9399 (0.9945)\tD(fake) 0.0009 (0.0050)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9280 (-0.9087)\n",
            "Epoch: [93][120/195]\tTime  0.973 ( 0.995)\tData  0.000 ( 0.003)\tD(real) 1.0360 (0.9945)\tD(fake) -0.0124 (0.0051)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9180 (-0.9093)\n",
            "Epoch: [93][130/195]\tTime  0.998 ( 0.995)\tData  0.000 ( 0.003)\tD(real) 1.0337 (0.9945)\tD(fake) -0.0084 (0.0051)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8992 (-0.9098)\n",
            "Epoch: [93][140/195]\tTime  0.991 ( 0.995)\tData  0.000 ( 0.003)\tD(real) 1.0291 (0.9945)\tD(fake) -0.0253 (0.0050)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8990 (-0.9081)\n",
            "Epoch: [93][150/195]\tTime  0.992 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 1.0381 (0.9945)\tD(fake) -0.0122 (0.0051)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9193 (-0.9078)\n",
            "Epoch: [93][160/195]\tTime  1.001 ( 0.994)\tData  0.000 ( 0.002)\tD(real) 1.0171 (0.9945)\tD(fake) -0.0056 (0.0050)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9155 (-0.9075)\n",
            "Epoch: [93][170/195]\tTime  0.995 ( 0.994)\tData  0.000 ( 0.002)\tD(real) 1.0299 (0.9945)\tD(fake) -0.0089 (0.0050)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9224 (-0.9080)\n",
            "Epoch: [93][180/195]\tTime  0.985 ( 0.994)\tData  0.000 ( 0.002)\tD(real) 1.0260 (0.9945)\tD(fake) -0.0205 (0.0050)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9229 (-0.9080)\n",
            "Epoch: [93][190/195]\tTime  0.996 ( 0.993)\tData  0.000 ( 0.002)\tD(real) 1.0233 (0.9945)\tD(fake) -0.0001 (0.0050)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9314 (-0.9085)\n",
            "Epoch: [94][  0/195]\tTime  1.392 ( 1.392)\tData  0.336 ( 0.336)\tD(real) 0.9422 (0.9882)\tD(fake) 0.0289 (0.0066)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9092 (-0.9092)\n",
            "Epoch: [94][ 10/195]\tTime  1.007 ( 1.027)\tData  0.000 ( 0.031)\tD(real) 0.9643 (0.9947)\tD(fake) 0.0307 (0.0048)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8785 (-0.9076)\n",
            "Epoch: [94][ 20/195]\tTime  0.998 ( 1.010)\tData  0.000 ( 0.016)\tD(real) 0.9631 (0.9948)\tD(fake) 0.0259 (0.0049)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9218 (-0.9113)\n",
            "Epoch: [94][ 30/195]\tTime  0.994 ( 1.004)\tData  0.000 ( 0.011)\tD(real) 1.0519 (0.9950)\tD(fake) -0.0108 (0.0049)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9124 (-0.9129)\n",
            "Epoch: [94][ 40/195]\tTime  0.997 ( 1.001)\tData  0.000 ( 0.009)\tD(real) 1.0363 (0.9950)\tD(fake) -0.0016 (0.0045)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9137 (-0.9101)\n",
            "Epoch: [94][ 50/195]\tTime  0.978 ( 0.998)\tData  0.000 ( 0.007)\tD(real) 1.0332 (0.9950)\tD(fake) -0.0082 (0.0048)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9058 (-0.9099)\n",
            "Epoch: [94][ 60/195]\tTime  0.978 ( 0.998)\tData  0.000 ( 0.006)\tD(real) 1.0409 (0.9949)\tD(fake) -0.0016 (0.0048)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.8996 (-0.9108)\n",
            "Epoch: [94][ 70/195]\tTime  0.993 ( 0.997)\tData  0.000 ( 0.005)\tD(real) 0.9764 (0.9948)\tD(fake) 0.0122 (0.0049)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9289 (-0.9123)\n",
            "Epoch: [94][ 80/195]\tTime  0.994 ( 0.996)\tData  0.000 ( 0.005)\tD(real) 1.0288 (0.9949)\tD(fake) -0.0088 (0.0048)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9145 (-0.9130)\n",
            "Epoch: [94][ 90/195]\tTime  0.982 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 1.0383 (0.9949)\tD(fake) 0.0083 (0.0049)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9123 (-0.9139)\n",
            "Epoch: [94][100/195]\tTime  1.020 ( 0.995)\tData  0.000 ( 0.004)\tD(real) 1.0318 (0.9949)\tD(fake) 0.0175 (0.0051)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9057 (-0.9145)\n",
            "Epoch: [94][110/195]\tTime  0.997 ( 0.995)\tData  0.000 ( 0.003)\tD(real) 0.9753 (0.9949)\tD(fake) 0.0066 (0.0051)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9290 (-0.9143)\n",
            "Epoch: [94][120/195]\tTime  0.999 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 0.9936 (0.9949)\tD(fake) -0.0182 (0.0051)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9185 (-0.9141)\n",
            "Epoch: [94][130/195]\tTime  0.994 ( 0.994)\tData  0.000 ( 0.003)\tD(real) 0.9731 (0.9949)\tD(fake) 0.0219 (0.0050)\tgrad(D) 0.0000 (0.0000)\trepr loss -0.9146 (-0.9143)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMSlrEvie7Pa"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvYdbtKQe_2-"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "def show_sample(x, num_samples=16, show_x=False):\n",
        "    x = x.cuda(args.gpu)[:num_samples]\n",
        "    if show_x:\n",
        "        x_grid = vutils.make_grid(inv_normalize(x).cpu(), padding=2, nrow=4)\n",
        "        plt.figure(figsize=(8,8))\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(x_grid.permute(1,2,0))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        latent_noise = sample_latent(num_samples).cuda(args.gpu)\n",
        "        repr = simsiam.encoder(x)\n",
        "        repr = F.normalize(repr)\n",
        "        z = torch.cat([repr, latent_noise], dim=1)\n",
        "        x_fake = model.G(latent_transform(z))\n",
        "    im_grid = vutils.make_grid(x_fake.cpu(), padding=2, nrow=4, normalize=True, range=(-1,1))\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(im_grid.permute(1,2,0))\n",
        "\n",
        "x, _ = next(iter(train_loader))\n",
        "show_sample(x, show_x=True)\n",
        "show_sample(x)\n",
        "show_sample(x)\n",
        "show_sample(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o31-WfwAApY3"
      },
      "source": [
        "vidname = f\"results/progress/grids_per_{args.generate_grid_interval}_iter.mp4\"\n",
        "vidname = os.path.join(GANSIAM_DIR, vidname)\n",
        "create_progress_animation(GENERATED_GRIDS, vidname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gfx3T5m2wah"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}