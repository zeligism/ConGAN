{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConsistentGAN",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeligism/ConGAN/blob/main/ConsistentGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxx3Jy_8qsPE"
      },
      "source": [
        "### Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MFx20xTNkpQ",
        "outputId": "9386a9f7-e12f-4e6e-e97b-a6dae4ca63b4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-QNzdq01hSb"
      },
      "source": [
        "# Header"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSlF68ff2K8L"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf_Qrpq7z3iJ"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import glob\n",
        "import random\n",
        "import datetime\n",
        "import yaml\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "import torchvision\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.tensorboard as tensorboard\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "from math import log2\n",
        "from pprint import pformat\n",
        "from collections import defaultdict"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USDduLe1Qkd9"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiRxrufxw1cm"
      },
      "source": [
        "### Report Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmEyNG58w2kJ"
      },
      "source": [
        "def plot_lines(losses_dict, filename=None, title=\"\"):\n",
        "    \"\"\"\n",
        "    Plots the losses of the discriminator and the generator.\n",
        "\n",
        "    Args:\n",
        "        filename: The plot's filename. If None, plot won't be saved.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(title)\n",
        "    for label, losses in losses_dict.items():\n",
        "        plt.plot(losses, label=label)\n",
        "    plt.xlabel(\"t\")\n",
        "    plt.legend()\n",
        "    \n",
        "    if filename is not None:\n",
        "        plt.savefig(filename)\n",
        "    \n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def create_progress_animation(frames, filename):\n",
        "    \"\"\"\n",
        "    Creates a video of the progress of the generator on a fixed latent vector.\n",
        "\n",
        "    Args:\n",
        "        filename: The animation's filename.\n",
        "    \"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    plt.axis(\"off\")\n",
        "    ims = [[plt.imshow(img.permute(1,2,0), animated=True)]\n",
        "           for img in frames]\n",
        "    ani = animation.ArtistAnimation(fig, ims, blit=True)\n",
        "    \n",
        "    ani.save(filename)\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def generate_grid(generator, latent):\n",
        "    \"\"\"\n",
        "    Check generator's output on latent vectors and return it.\n",
        "\n",
        "    Args:\n",
        "        generator: The generator.\n",
        "        latent: Latent vector from which an image grid will be generated.\n",
        "\n",
        "    Returns:\n",
        "        A grid of images generated by `generator` from `latent`.\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake = generator(latent).detach()\n",
        "\n",
        "    image_grid = vutils.make_grid(fake.cpu(), padding=2, normalize=True, range=(-1,1))\n",
        "\n",
        "    return image_grid\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUzwGurc1qOx"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPhc2oS53G4e"
      },
      "source": [
        "## PyTorch Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU7HFc6t5N8w"
      },
      "source": [
        "### DCGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJHPo8w13JmH"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Default stride and padding half the size of features,\n",
        "    e.g. if input is [in_channels, 64, 64], output will be [out_channels, 32, 32].\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1,\n",
        "                 use_batchnorm=True, use_spectralnorm=False, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                              stride=stride, padding=padding, bias=False)\n",
        "        if use_spectralnorm:\n",
        "            self.conv = nn.utils.parametrizations.spectral_norm(self.conv)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels) if use_batchnorm else None\n",
        "        self.activation = nn.LeakyReLU(0.2, inplace=True) if activation is None else activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.batchnorm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Default stride and padding double the size of features,\n",
        "    e.g. if input is [in_channels, 32, 32], output will be [out_channels, 64, 64].\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1,\n",
        "                 use_batchnorm=True, use_spectralnorm=False, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convT = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                                        stride=stride, padding=padding, bias=False)\n",
        "        if use_spectralnorm:\n",
        "            self.convT = nn.utils.parametrizations.spectral_norm(self.convT)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels) if use_batchnorm else None\n",
        "        self.activation = nn.ReLU(inplace=True) if activation is None else activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convT(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.batchnorm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCGAN_Discriminator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_latents=16,\n",
        "                 num_features=64,\n",
        "                 image_channels=3,\n",
        "                 image_size=64,\n",
        "                 feature_multiplier=2,\n",
        "                 max_features=512,\n",
        "                 gan_type=\"gan\",\n",
        "                 fully_convolutional=True,\n",
        "                 activation=None,\n",
        "                 use_batchnorm=True,\n",
        "                 use_spectralnorm=False,\n",
        "                 output_sigmoid=True,\n",
        "                 D_block=ConvBlock):\n",
        "        super().__init__()\n",
        "\n",
        "        using_grad_penalty = gan_type in (\"gan-gp\", \"wgan-gp\")\n",
        "        output_sigmoid = output_sigmoid and gan_type in (\"gan\", \"gan-gp\")\n",
        "\n",
        "        block_config = {\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": use_batchnorm and not using_grad_penalty,\n",
        "            \"use_spectralnorm\": use_spectralnorm,\n",
        "        }\n",
        "\n",
        "        # Calculate intermediate image sizes\n",
        "        image_sizes = [image_size]\n",
        "        while image_sizes[-1] > 5:\n",
        "            image_sizes.append(image_sizes[-1] // 2)\n",
        "        latent_kernel = image_sizes[-1]  # should be either 3, 4, or 5\n",
        "        num_layers = len(image_sizes) - 1\n",
        "\n",
        "        # Calculate feature sizes\n",
        "        features = [min(max_features, round(num_features * feature_multiplier**i))\n",
        "                    for i in range(num_layers)]\n",
        "\n",
        "        # Input layer\n",
        "        self.input_layer = D_block(image_channels, features[0], **block_config)\n",
        "\n",
        "        # Intermediate layers\n",
        "        self.main_layers = nn.Sequential(*[\n",
        "            D_block(in_features, out_features, **block_config)\n",
        "            for in_features, out_features in zip(features, features[1:])\n",
        "        ])\n",
        "\n",
        "        # Output layer (feature_size = 3, 4, or 5 -> 1)\n",
        "        if fully_convolutional:\n",
        "            conv = nn.Conv2d(features[-1], num_latents, latent_kernel, bias=False)\n",
        "            if use_spectralnorm:\n",
        "                conv = nn.utils.parametrizations.spectral_norm(conv)\n",
        "            self.output_layer = nn.Sequential(conv, nn.Flatten())\n",
        "        else:\n",
        "            linear = nn.Linear(features[-1] * latent_kernel**2, num_latents, bias=False)\n",
        "            if use_spectralnorm:\n",
        "                linear = nn.utils.parametrizations.spectral_norm(linear)\n",
        "            self.output_layer = nn.Sequential(nn.Flatten(), linear)\n",
        "        \n",
        "        self.hidden_dim = features[-1] * latent_kernel**2\n",
        "\n",
        "        # Add sigmoid activation if using regular GAN loss\n",
        "        self.output_activation = nn.Sigmoid() if output_sigmoid else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.main_layers(x)\n",
        "        x = self.output_layer(x)\n",
        "        if self.output_activation:\n",
        "            x = self.output_activation(x)\n",
        "        # Remove H and W dimensions, infer channels dim (remove if 1)\n",
        "        x = x.view(x.size(0), -1).squeeze(1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCGAN_Generator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_latents=100,\n",
        "                 num_features=64,\n",
        "                 image_channels=3,\n",
        "                 image_size=64,\n",
        "                 feature_multiplier=2,\n",
        "                 max_features=512,\n",
        "                 gan_type=\"gan\",\n",
        "                 fully_convolutional=True,\n",
        "                 activation=None,\n",
        "                 use_batchnorm=True,\n",
        "                 use_spectralnorm=False,\n",
        "                 G_block=ConvTBlock):\n",
        "        super().__init__()\n",
        "\n",
        "        block_config = {\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": use_batchnorm,\n",
        "            \"use_spectralnorm\": use_spectralnorm\n",
        "        }\n",
        "\n",
        "        # Calculate intermediate image sizes\n",
        "        image_sizes = [image_size]\n",
        "        while image_sizes[-1] > 5:\n",
        "            image_sizes.append(image_sizes[-1] // 2)\n",
        "        latent_kernel = image_sizes[-1]  # should be either 3, 4, or 5\n",
        "        num_layers = len(image_sizes) - 1\n",
        "\n",
        "        # Calculate feature sizes\n",
        "        features = [min(max_features, round(num_features * feature_multiplier**i))\n",
        "                    for i in range(num_layers)]\n",
        "\n",
        "        # Reverse order of image sizes and features for generator\n",
        "        image_sizes = image_sizes[::-1]\n",
        "        features = features[::-1]\n",
        "\n",
        "        # Input layer\n",
        "        if fully_convolutional:\n",
        "            self.input_layer = G_block(num_latents, features[0], kernel_size=latent_kernel,\n",
        "                                       stride=1, padding=0, **block_config)\n",
        "        else:\n",
        "            self.input_layer = nn.Sequential(\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(num_latents, features[0] * image_sizes[0]**2, bias=False),\n",
        "                View(features[0], image_sizes[0], image_sizes[0])\n",
        "            )\n",
        "\n",
        "        # Intermediate layers\n",
        "        self.main_layers = nn.Sequential(*[\n",
        "            G_block(in_features, out_features, kernel_size=4+(expected_size%2), **block_config)\n",
        "            for in_features, out_features, expected_size in zip(features, features[1:], image_sizes[1:])\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.ConvTranspose2d(features[-1], image_channels, kernel_size=4+(image_size%2),\n",
        "                                               stride=2, padding=1, bias=False)\n",
        "        self.output_activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add H and W dimensions, infer channels dim (add if none)\n",
        "        x = x.view(x.size(0), -1, 1, 1)\n",
        "        x = self.input_layer(x)\n",
        "        x = self.main_layers(x)\n",
        "        x = self.output_layer(x)\n",
        "        x = self.output_activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCGAN(nn.Module):\n",
        "    \"\"\"Deep Convolutional Generative Adversarial Network\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_latents=100,\n",
        "                 D_num_features=64,\n",
        "                 G_num_features=64,\n",
        "                 image_channels=3,\n",
        "                 image_size=64,\n",
        "                 feature_multiplier=2,\n",
        "                 gan_type=\"gan\",\n",
        "                 fully_convolutional=True,\n",
        "                 activation=None,\n",
        "                 use_batchnorm=True,\n",
        "                 use_spectralnorm=False,\n",
        "                 output_sigmoid=True,):\n",
        "        \"\"\"\n",
        "        Initializes DCGAN.\n",
        "\n",
        "        Args:\n",
        "            num_latents: Number of latent factors.\n",
        "            num_features: Number of features in the convolutions.\n",
        "            image_channels: Number of channels in the input image.\n",
        "            image_size: Size (i.e. height or width) of image.\n",
        "            gan_type: Type of GAN (e.g. \"gan\" or \"wgan-gp\").\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_latents = num_latents\n",
        "        self.D_num_features = D_num_features\n",
        "        self.G_num_features = G_num_features\n",
        "        self.image_channels = image_channels\n",
        "        self.image_size = image_size\n",
        "        self.feature_multiplier = feature_multiplier\n",
        "        self.gan_type = gan_type\n",
        "        self.fully_convolutional = fully_convolutional\n",
        "        self.activation = activation\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        self.use_spectralnorm = use_spectralnorm\n",
        "\n",
        "        D_params = {\n",
        "            \"num_latents\": 1,  # XXX\n",
        "            \"num_features\": D_num_features,\n",
        "            \"image_channels\": image_channels,\n",
        "            \"image_size\": image_size,\n",
        "            \"feature_multiplier\": feature_multiplier,\n",
        "            \"gan_type\": gan_type,\n",
        "            \"fully_convolutional\": fully_convolutional,\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": use_batchnorm,\n",
        "            \"use_spectralnorm\": use_spectralnorm,\n",
        "            \"output_sigmoid\": output_sigmoid,\n",
        "        }\n",
        "        G_params = {\n",
        "            \"num_latents\": num_latents,\n",
        "            \"num_features\": G_num_features,\n",
        "            \"image_channels\": image_channels,\n",
        "            \"image_size\": image_size,\n",
        "            \"feature_multiplier\": feature_multiplier,\n",
        "            \"gan_type\": gan_type,\n",
        "            \"fully_convolutional\": fully_convolutional,\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": True,\n",
        "            \"use_spectralnorm\": False,  # XXX\n",
        "        }\n",
        "\n",
        "        self.D = DCGAN_Discriminator(**D_params)\n",
        "        self.G = DCGAN_Generator(**G_params)\n",
        "\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self, *shape, including_batch=False):\n",
        "        super().__init__()\n",
        "        self.shape = shape\n",
        "        self.including_batch = including_batch\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.including_batch:\n",
        "            return x.view(*self.shape)\n",
        "        else:\n",
        "            return x.view(x.size(0), *self.shape)\n",
        "\n",
        "class ChannelNoise(nn.Module):\n",
        "    \"\"\"\n",
        "    Channel noise injection module.\n",
        "    Adds a linearly transformed noise to a convolution layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, std=0.02):\n",
        "        super().__init__()\n",
        "        self.std = std\n",
        "        self.scale = nn.Parameter(torch.ones(1, num_channels, 1, 1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        noise_size = [x.size()[0], 1, *x.size()[2:]]  # single channel\n",
        "        noise = self.std * torch.randn(noise_size).to(x)\n",
        "\n",
        "        return x + self.scale * noise"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSPvaklIYvwT"
      },
      "source": [
        "### Third-party modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS9B8z4ZY4oX"
      },
      "source": [
        "#### DCGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLNQ90KUY_Is"
      },
      "source": [
        "class ConditionalBatchNorm2d(nn.Module):\n",
        "  def __init__(self, num_features, repr_dim):\n",
        "    super().__init__()\n",
        "    self.num_features = num_features\n",
        "    self.bn = nn.BatchNorm2d(num_features, affine=False)\n",
        "    self.embed = nn.Linear(repr_dim, num_features * 2)\n",
        "    self.embed.weight.data[:, :num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n",
        "    self.embed.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    out = self.bn(x)\n",
        "    gamma, beta = self.embed(y).chunk(2, 1)\n",
        "    out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n",
        "    return out\n",
        "\n",
        "\n",
        "#https://github.com/christiancosgrove/pytorch-spectral-normalization-gan/blob/master/model.py\n",
        "\n",
        "from torch.nn.utils.parametrizations import spectral_norm\n",
        "\n",
        "class SNDCGAN_Generator(nn.Module):\n",
        "    def __init__(self, z_dim, num_features=64, channels=3, repr_dim=None):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.conv1 = nn.ConvTranspose2d(z_dim, 8*num_features, 4, stride=1)\n",
        "        self.bn1 = ConditionalBatchNorm2d(8*num_features, repr_dim)\n",
        "        self.conv2 = nn.ConvTranspose2d(8*num_features, 4*num_features, 4, stride=2, padding=(1,1))\n",
        "        self.bn2 = ConditionalBatchNorm2d(4*num_features, repr_dim)\n",
        "        self.conv3 = nn.ConvTranspose2d(4*num_features, 2*num_features, 4, stride=2, padding=(1,1))\n",
        "        self.bn3 = ConditionalBatchNorm2d(2*num_features, repr_dim)\n",
        "        self.conv4 = nn.ConvTranspose2d(2*num_features, num_features, 4, stride=2, padding=(1,1))\n",
        "        self.bn4 = ConditionalBatchNorm2d(num_features, repr_dim)\n",
        "        self.conv5 = nn.ConvTranspose2d(num_features, channels, 3, stride=1, padding=(1,1))\n",
        "        # use this instead of last line for 64:\n",
        "        # nn.ConvTranspose2d(64, 32, 4, stride=2, padding=(1,1))\n",
        "        self.block_activation = nn.ReLU()\n",
        "        self.output_activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, z, repr=None):\n",
        "        h = z.view(-1, self.z_dim, 1, 1)\n",
        "        h = self.block_activation(self.bn1(self.conv1(h), repr))\n",
        "        h = self.block_activation(self.bn2(self.conv2(h), repr))\n",
        "        h = self.block_activation(self.bn3(self.conv3(h), repr))\n",
        "        h = self.block_activation(self.bn4(self.conv4(h), repr))\n",
        "        return self.output_activation(self.conv5(h))\n",
        "\n",
        "class SNDCGAN_Discriminator(nn.Module):\n",
        "    def __init__(self, num_features=64, channels=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            spectral_norm(nn.Conv2d(channels, num_features, 3, stride=1, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(num_features, num_features, 4, stride=2, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(num_features, 2*num_features, 3, stride=1, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(2*num_features, 2*num_features, 4, stride=2, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(2*num_features, 4*num_features, 3, stride=1, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(4*num_features, 4*num_features, 4, stride=2, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(4*num_features, 8*num_features, 3, stride=1, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # use this instead of last 2 lines for 64:\n",
        "            # spectral_norm(nn.Conv2d(256, 256, 3, stride=1, padding=(1,1))),\n",
        "            # nn.LeakyReLU(0.1, inplace=True),\n",
        "            # spectral_norm(nn.Conv2d(256, 512, 3, stride=1, padding=(1,1))),\n",
        "            # nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        self.hidden_dim = 4*4 * 8*num_features\n",
        "        self.fc = spectral_norm(nn.Linear(self.hidden_dim, 1))\n",
        "\n",
        "    def forward(self, x, return_h=False):\n",
        "        h = self.main(x)\n",
        "        out = self.fc(h).squeeze(1)\n",
        "        if return_h:\n",
        "            return out, h\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class SNDCGAN(nn.Module):\n",
        "    def __init__(self, num_latents, num_features=64, channels=3, repr_dim=None):\n",
        "        super().__init__()\n",
        "        self.num_latents = num_latents\n",
        "        self.channels = channels\n",
        "        self.D = SNDCGAN_Discriminator(channels=channels, num_features=num_features)\n",
        "        self.G = SNDCGAN_Generator(num_latents, channels=channels,\n",
        "                                   num_features=num_features, repr_dim=repr_dim)\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eiu_Ri-XY6yy"
      },
      "source": [
        "#### ResNet GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFjWBbXzdlSF"
      },
      "source": [
        "# https://github.com/christiancosgrove/pytorch-spectral-normalization-gan/blob/master/model_resnet.py\n",
        "\n",
        "from torch.nn.utils.parametrizations import spectral_norm\n",
        "\n",
        "class ResBlockGenerator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResBlockGenerator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n",
        "        nn.init.xavier_uniform_(self.conv1.weight.data, 1.)\n",
        "        nn.init.xavier_uniform_(self.conv2.weight.data, 1.)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            self.conv1,\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            self.conv2\n",
        "            )\n",
        "        self.bypass = nn.Sequential()\n",
        "        if stride != 1:\n",
        "            self.bypass = nn.Upsample(scale_factor=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x) + self.bypass(x)\n",
        "\n",
        "\n",
        "class ResBlockDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResBlockDiscriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n",
        "        nn.init.xavier_uniform_(self.conv1.weight.data, 1.)\n",
        "        nn.init.xavier_uniform_(self.conv2.weight.data, 1.)\n",
        "\n",
        "        if stride == 1:\n",
        "            self.model = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                spectral_norm(self.conv1),\n",
        "                nn.ReLU(),\n",
        "                spectral_norm(self.conv2)\n",
        "                )\n",
        "        else:\n",
        "            self.model = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                spectral_norm(self.conv1),\n",
        "                nn.ReLU(),\n",
        "                spectral_norm(self.conv2),\n",
        "                nn.AvgPool2d(2, stride=stride, padding=0)\n",
        "                )\n",
        "        self.bypass = nn.Sequential()\n",
        "        if stride != 1:\n",
        "\n",
        "            self.bypass_conv = nn.Conv2d(in_channels,out_channels, 1, 1, padding=0)\n",
        "            nn.init.xavier_uniform_(self.bypass_conv.weight.data, np.sqrt(2))\n",
        "\n",
        "            self.bypass = nn.Sequential(\n",
        "                spectral_norm(self.bypass_conv),\n",
        "                nn.AvgPool2d(2, stride=stride, padding=0)\n",
        "            )\n",
        "            # if in_channels == out_channels:\n",
        "            #     self.bypass = nn.AvgPool2d(2, stride=stride, padding=0)\n",
        "            # else:\n",
        "            #     self.bypass = nn.Sequential(\n",
        "            #         spectral_norm(nn.Conv2d(in_channels,out_channels, 1, 1, padding=0)),\n",
        "            #         nn.AvgPool2d(2, stride=stride, padding=0)\n",
        "            #     )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x) + self.bypass(x)\n",
        "\n",
        "# special ResBlock just for the first layer of the discriminator\n",
        "class FirstResBlockDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(FirstResBlockDiscriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n",
        "        self.bypass_conv = nn.Conv2d(in_channels, out_channels, 1, 1, padding=0)\n",
        "        nn.init.xavier_uniform_(self.conv1.weight.data, 1.)\n",
        "        nn.init.xavier_uniform_(self.conv2.weight.data, 1.)\n",
        "        nn.init.xavier_uniform_(self.bypass_conv.weight.data, np.sqrt(2))\n",
        "\n",
        "        # we don't want to apply ReLU activation to raw image before convolution transformation.\n",
        "        self.model = nn.Sequential(\n",
        "            spectral_norm(self.conv1),\n",
        "            nn.ReLU(),\n",
        "            spectral_norm(self.conv2),\n",
        "            nn.AvgPool2d(2)\n",
        "            )\n",
        "        self.bypass = nn.Sequential(\n",
        "            nn.AvgPool2d(2),\n",
        "            spectral_norm(self.bypass_conv),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x) + self.bypass(x)\n",
        "\n",
        "class SNResNet_Generator(nn.Module):\n",
        "    def __init__(self, z_dim, image_size=64, channels=3):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.dense = nn.Linear(self.z_dim, 4 * 4 * image_size)\n",
        "        self.final = nn.Conv2d(image_size, channels, 3, stride=1, padding=1)\n",
        "        nn.init.xavier_uniform_(self.dense.weight.data, 1.)\n",
        "        nn.init.xavier_uniform_(self.final.weight.data, 1.)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            ResBlockGenerator(image_size, image_size, stride=2),\n",
        "            ResBlockGenerator(image_size, image_size, stride=2),\n",
        "            ResBlockGenerator(image_size, image_size, stride=2),\n",
        "            nn.BatchNorm2d(image_size),\n",
        "            nn.ReLU(),\n",
        "            self.final,\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(self.dense(z).view(-1, self.image_size, 4, 4))\n",
        "\n",
        "class SNResNet_Discriminator(nn.Module):\n",
        "    def __init__(self, image_size=64, channels=3):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "                FirstResBlockDiscriminator(channels, image_size, stride=2),\n",
        "                ResBlockDiscriminator(image_size, image_size, stride=2),\n",
        "                ResBlockDiscriminator(image_size, image_size),\n",
        "                ResBlockDiscriminator(image_size, image_size),\n",
        "                nn.ReLU(),\n",
        "                nn.AvgPool2d(8),\n",
        "            )\n",
        "        self.fc = nn.Linear(image_size, 1)\n",
        "        nn.init.xavier_uniform_(self.fc.weight.data, 1.)\n",
        "        self.fc = spectral_norm(self.fc)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.model(x).view(-1, self.image_size))\n",
        "\n",
        "\n",
        "class SNResNetGAN(nn.Module):\n",
        "    def __init__(self, num_latents, image_size=64, channels=3):\n",
        "        super().__init__()\n",
        "        self.num_latents = num_latents\n",
        "        self.channels = channels\n",
        "        self.D = SNResNet_Discriminator(image_size=image_size, channels=channels)\n",
        "        self.G = SNResNet_Generator(num_latents, image_size=image_size, channels=channels)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYujBEzC7EOO"
      },
      "source": [
        "#### SimSiam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-YcNut27F-v"
      },
      "source": [
        "class SimSiam(nn.Module):\n",
        "    \"\"\"\n",
        "    Build a SimSiam model.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_encoder, dim=2048, pred_dim=512):\n",
        "        \"\"\"\n",
        "        dim: feature dimension (default: 2048)\n",
        "        pred_dim: hidden dimension of the predictor (default: 512)\n",
        "        \"\"\"\n",
        "        super(SimSiam, self).__init__()\n",
        "\n",
        "        # create the encoder\n",
        "        # num_classes is the output fc dimension, zero-initialize last BNs\n",
        "        self.encoder = base_encoder(num_classes=dim, zero_init_residual=True)\n",
        "\n",
        "        # build a 3-layer projector\n",
        "        prev_dim = self.encoder.fc.weight.shape[1]\n",
        "        self.encoder.fc = nn.Sequential(nn.Linear(prev_dim, prev_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(prev_dim),\n",
        "                                        nn.ReLU(inplace=True), # first layer\n",
        "                                        nn.Linear(prev_dim, prev_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(prev_dim),\n",
        "                                        nn.ReLU(inplace=True), # second layer\n",
        "                                        self.encoder.fc,\n",
        "                                        nn.BatchNorm1d(dim, affine=False)) # output layer\n",
        "        self.encoder.fc[6].bias.requires_grad = False # hack: not use bias as it is followed by BN\n",
        "\n",
        "        # build a 2-layer predictor\n",
        "        self.predictor = nn.Sequential(nn.Linear(dim, pred_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(pred_dim),\n",
        "                                        nn.ReLU(inplace=True), # hidden layer\n",
        "                                        nn.Linear(pred_dim, dim)) # output layer\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            x1: first views of images\n",
        "            x2: second views of images\n",
        "        Output:\n",
        "            p1, p2, z1, z2: predictors and targets of the network\n",
        "            See Sec. 3 of https://arxiv.org/abs/2011.10566 for detailed notations\n",
        "        \"\"\"\n",
        "\n",
        "        # compute features for one view\n",
        "        z1 = self.encoder(x1) # NxC\n",
        "        z2 = self.encoder(x2) # NxC\n",
        "\n",
        "        p1 = self.predictor(z1) # NxC\n",
        "        p2 = self.predictor(z2) # NxC\n",
        "\n",
        "        return p1, p2, z1.detach(), z2.detach()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEuurkcmLLd3"
      },
      "source": [
        "### Latent Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eTlMYVgLN5E"
      },
      "source": [
        "class LatentTransform(nn.Module):\n",
        "    def __init__(self, repr_dim, latent_dim, hidden_dim, full_transform=True, noop=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.repr_dim = repr_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.full_transform = full_transform\n",
        "        self.noop = noop\n",
        "\n",
        "        if self.noop:\n",
        "            self.output_dim = self.latent_dim\n",
        "            return\n",
        "        elif self.full_transform:\n",
        "            self.input_dim = self.repr_dim + self.latent_dim\n",
        "            self.output_dim = self.hidden_dim\n",
        "        else:\n",
        "            self.input_dim = self.repr_dim\n",
        "            self.output_dim = self.hidden_dim + self.latent_dim\n",
        "\n",
        "        self.transform = nn.Linear(self.input_dim, self.hidden_dim)\n",
        "    \n",
        "    def forward(self, repr, noise):\n",
        "        if self.noop:\n",
        "            return noise\n",
        "\n",
        "        # assuming latent is concat as [repr,noise] XXX\n",
        "        if self.full_transform:\n",
        "            latent = torch.cat([repr, noise], dim=1)\n",
        "            latent = self.transform(latent)\n",
        "        else:\n",
        "            repr = self.transform(repr)\n",
        "            latent = torch.cat([repr, noise], dim=1)\n",
        "\n",
        "        return latent\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yRivPV9BwFk"
      },
      "source": [
        "# Training v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5rpQp2E9rE5"
      },
      "source": [
        "### Imports and globals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51zFNn509xLz"
      },
      "source": [
        "import argparse\n",
        "import builtins\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.multiprocessing as mp\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "GANSIAM_DIR = \"/content/drive/My Drive/gansiam/\"\n",
        "SIMSIAM_PATH = os.path.join(GANSIAM_DIR, \"pretrained_batch256.tar\")\n",
        "TINYIMAGENET_DIR = \"tiny-imagenet-200\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9X_JYE2Vwxd"
      },
      "source": [
        "### Download Tiny Imagenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "559H2an_V03M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f50354f-ee15-47dc-91ea-c784ec710481"
      },
      "source": [
        "%%bash\n",
        "if [[ -d  \"tiny-imagenet-200\" ]]; then\n",
        "    echo \"Tiny Imagenet exists.\"\n",
        "else\n",
        "    wget -q \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "    unzip -qq \"tiny-imagenet-200.zip\" && rm \"tiny-imagenet-200.zip\"\n",
        "    echo \"Downloaded Tiny Imagenet.\"\n",
        "fi"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded Tiny Imagenet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbo7T6blPVTc"
      },
      "source": [
        "### Load pre-trained SimSiam model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyzHmINsryyh"
      },
      "source": [
        "#### SimSiam Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8KJUUeWr1dI"
      },
      "source": [
        "from PIL import ImageFilter\n",
        "import random\n",
        "\n",
        "\n",
        "class TwoCropsTransform:\n",
        "    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n",
        "\n",
        "    def __init__(self, base_transform):\n",
        "        self.base_transform = base_transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        q = self.base_transform(x)\n",
        "        k = self.base_transform(x)\n",
        "        return [q, k]\n",
        "\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
        "\n",
        "    def __init__(self, sigma=[.1, 2.]):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, x):\n",
        "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
        "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
        "        return x\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nr8clgi_AzY",
        "outputId": "523daabc-e72e-4ebb-f6f3-ff23f6519e35"
      },
      "source": [
        "checkpoint = torch.load(SIMSIAM_PATH, map_location=\"cuda:0\")\n",
        "# remove 'module.' from dict keys\n",
        "model_dict = OrderedDict((k[7:], v) for k, v in checkpoint[\"state_dict\"].items())\n",
        "\n",
        "# Load model\n",
        "simsiam = SimSiam(models.__dict__[\"resnet50\"])\n",
        "simsiam.load_state_dict(model_dict)\n",
        "#print(simsiam)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kckB_xSVX8kB"
      },
      "source": [
        "# Training v3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nouxldrYb0r"
      },
      "source": [
        "## Args"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUP5vn8OX--p"
      },
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.load = False\n",
        "        self.print_freq = 10\n",
        "        self.seed = None\n",
        "        self.gpu = 0\n",
        "        self.workers = 2\n",
        "        self.epochs = 100\n",
        "\n",
        "        ### lr is about 2e-4 for batch size of 64\n",
        "        # we scale according to our choice of batch size\n",
        "        self.batch_size = 64\n",
        "        self.D_lr = 2e-4 * (64 / self.batch_size)\n",
        "        self.G_lr = 2e-4 * (64 / self.batch_size)\n",
        "        self.Q_lr = self.D_lr\n",
        "        self.latent_transform_lr = self.G_lr\n",
        "        self.lr_decay = 0.02\n",
        "        #self.betas = (0.5, 0.999)\n",
        "        self.betas = (0., 0.9)\n",
        "\n",
        "        # SimSiam (_don't change_ if loading pre-trained)\n",
        "        self.dim = 2048\n",
        "        self.pred_dim = 512\n",
        "\n",
        "        # GAN\n",
        "        self.repr_dim = self.dim  # don't change\n",
        "        self.latent_full_transform = False\n",
        "        self.latent_noise_dim = 128\n",
        "        self.latent_hidden_dim = 128  # dim of transform output\n",
        "        self.Q_hidden_dim = 128\n",
        "        self.num_features = 64\n",
        "        self.D_iters = 5\n",
        "\n",
        "        self.gan_type = \"gan\"  # ignore this\n",
        "        self.wgan = False  # if False, use spectral norm\n",
        "        self.grad_penalty = 0.  # 0 if wgan is False\n",
        "        self.grad_center = 1.  # not important\n",
        "\n",
        "        self.generate_grid_interval = 100\n",
        "\n",
        "        # make noise proportional to sd(data)\n",
        "        self.im_noise = 1e-2  # image sd is about 1.0\n",
        "        self.repr_noise = 0.  # repr sd is about 0.001\n",
        "\n",
        "        # Start small, increase later\n",
        "        self.G_consistency = 0.1\n",
        "        self.D_consistency = 0.1\n",
        "\n",
        "\n",
        "GENERATED_GRIDS = []\n",
        "IMAGE_SIZE = 32\n",
        "DATASET = \"CIFAR10\"\n",
        "args = Args()"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xihRlU0PYhiJ"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW_P-JfeYiOe",
        "outputId": "c1de240f-017f-4830-87ce-d50e64ee45d0"
      },
      "source": [
        "# image normalization\n",
        "#mean = [0.485, 0.456, 0.406]\n",
        "#std = [0.229, 0.224, 0.225]\n",
        "mean = [0.5]\n",
        "std = [0.5]\n",
        "normalize = transforms.Normalize(mean=mean, std=std)\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean= [-m/s for m, s in zip(mean, std)],\n",
        "   std= [1/s for s in std]\n",
        ")\n",
        "\n",
        "augmentation = [\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "]\n",
        "\n",
        "# MoCo v2's aug: similar to SimCLR https://arxiv.org/abs/2002.05709\n",
        "_augmentation = [\n",
        "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.2, 1.)),\n",
        "    transforms.RandomApply([\n",
        "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
        "    ], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "]\n",
        "\n",
        "if DATASET == \"MNIST\":\n",
        "    augmentation = [transforms.Grayscale(3)] + augmentation\n",
        "    train_dataset = datasets.MNIST(\n",
        "        root=os.path.join(GANSIAM_DIR, \"mnist/train\"), train=True, download=True,\n",
        "        transform=transforms.Compose(augmentation))\n",
        "elif DATASET == \"CelebA\":\n",
        "    train_dataset = datasets.CelebA(\n",
        "        root=os.path.join(GANSIAM_DIR, \"celeba\"), download=True,\n",
        "        transform=transforms.Compose(augmentation))\n",
        "elif DATASET == \"CIFAR10\":\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=os.path.join(GANSIAM_DIR, \"cifar10/train\"), train=True, download=True,\n",
        "        transform=transforms.Compose(augmentation))\n",
        "        #transform=TwoCropsTransform(transforms.Compose(augmentation)))\n",
        "elif DATASET == \"Tiny Imagenet\":\n",
        "    train_dataset = datasets.ImageFolder(\n",
        "        root=os.path.join(TINYIMAGENET_DIR, 'train'),\n",
        "        transform=transforms.Compose(augmentation))\n",
        "else:\n",
        "    raise Exception(f\"Dataset '{DATASET}' not found\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "    num_workers=args.workers, pin_memory=True, sampler=None, drop_last=True)"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oQ0_BfjmAHf"
      },
      "source": [
        "### Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmLiHFvBimb3"
      },
      "source": [
        "def D_criterion_NS(D_real, D_fake):\n",
        "    d_loss = F.softplus(-D_real) + F.softplus(D_fake)\n",
        "    return d_loss.mean()\n",
        "\n",
        "def G_criterion_NS(D_fake):\n",
        "    return F.softplus(-D_fake).mean()\n",
        "\n",
        "def D_criterion_LS(D_real, D_fake):\n",
        "    d_loss = 0.5 * (D_real - torch.ones_like(D_real))**2 + 0.5 * (D_fake)**2\n",
        "    return d_loss.mean()\n",
        "\n",
        "def G_criterion_LS(D_fake):\n",
        "    gen_loss = 0.5 * (D_fake - torch.ones_like(D_fake))**2\n",
        "    return gen_loss.mean()\n",
        "\n",
        "def D_criterion_hinge(D_real, D_fake):\n",
        "    return torch.mean(F.relu(1. - D_real)) + torch.mean(F.relu(1. + D_fake))\n",
        "\n",
        "def G_criterion_hinge(D_fake):\n",
        "    return -torch.mean(D_fake)\n",
        "\n",
        "def D_criterion_wasserstein(D_real, D_fake):\n",
        "    return torch.mean(D_fake - D_real)\n",
        "\n",
        "def G_criterion_wasserstein(D_fake):\n",
        "    return -torch.mean(D_fake)\n",
        "\n",
        "def interpolate(real, fake, eps=None):\n",
        "    if eps is None:\n",
        "        eps_size = [1] * len(real.size())\n",
        "        eps_size[0] = real.size(0)\n",
        "        eps = torch.rand(eps_size).to(real)\n",
        "    return eps * real + (1 - eps) * fake\n",
        "    \n",
        "def simple_gradient_penalty(D, x, center=0.):\n",
        "    x.requires_grad_()\n",
        "    D_x = D(x)\n",
        "    D_grad = torch.autograd.grad(D_x, x, torch.ones_like(D_x), create_graph=True)\n",
        "    D_grad_norm = D_grad[0].view(x.size(0), -1).norm(dim=1)\n",
        "    return (D_grad_norm - center).pow(2).mean()\n",
        "\n",
        "def lerp(x1, x2, t=None):\n",
        "    return interpolate(x1, x2, t)\n",
        "\n",
        "def slerp(x1, x2, t=None):\n",
        "    if t is None:\n",
        "        t = torch.rand(x1.size(0)).to(x1)\n",
        "    # assuming normalized x and x2\n",
        "    omega = torch.acos((x1*x2).sum(1))\n",
        "    so = torch.sin(omega)\n",
        "    res = (torch.sin((1.-t)*omega) / so).unsqueeze(1) * x1 \\\n",
        "        + (torch.sin(t*omega) / so).unsqueeze(1) * x2\n",
        "    return res\n"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0Z4Ke6DYkDg"
      },
      "source": [
        "## Model + Opt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rO_MvGzYldx",
        "outputId": "9c186507-a128-4942-b97e-a3ec6cf4ca9d"
      },
      "source": [
        "if args.seed is not None:\n",
        "    random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    cudnn.deterministic = True\n",
        "\n",
        "torch.cuda.set_device(args.gpu)\n",
        "\n",
        "latent_transform = LatentTransform(repr_dim=args.repr_dim,\n",
        "                                   latent_dim=args.latent_noise_dim,\n",
        "                                   hidden_dim=args.latent_hidden_dim,\n",
        "                                   full_transform=args.latent_full_transform,\n",
        "                                   )\n",
        "\n",
        "model = DCGAN(num_latents=latent_transform.output_dim,\n",
        "              image_size=IMAGE_SIZE,\n",
        "              gan_type=args.gan_type,  # doesn't make a difference\n",
        "              D_num_features=args.num_features,\n",
        "              G_num_features=args.num_features,\n",
        "              use_batchnorm=False,  # for D only\n",
        "              output_sigmoid=False,  # for D only\n",
        "              use_spectralnorm=not args.wgan,  # for spectral norm, use the below model\n",
        "              )\n",
        "\n",
        "\n",
        "if not args.wgan:\n",
        "    model = SNDCGAN(num_latents=latent_transform.output_dim,\n",
        "                    num_features=args.num_features,\n",
        "                    repr_dim=args.repr_dim)\n",
        "\n",
        "if args.D_consistency == 0.:\n",
        "    Q = nn.Module()\n",
        "else:\n",
        "    Q_hidden_dim = args.Q_hidden_dim\n",
        "    Q = nn.Sequential(nn.Linear(model.D.hidden_dim, Q_hidden_dim, bias=False),\n",
        "                      nn.Linear(Q_hidden_dim, args.repr_dim))\n",
        "\n",
        "model = model.cuda(args.gpu)\n",
        "Q = Q.cuda(args.gpu)\n",
        "latent_transform = latent_transform.cuda(args.gpu)\n",
        "simsiam = simsiam.cuda(args.gpu)\n",
        "\n",
        "print(\"Num of params in D:\", sum(map(torch.numel, model.D.parameters())))\n",
        "print(\"Num of params in G:\", sum(map(torch.numel, model.G.parameters())))\n",
        "print(\"Num of params in Q:\", sum(map(torch.numel, Q.parameters())))\n",
        "print(\"Num of params in L:\", sum(map(torch.numel, latent_transform.parameters())))\n",
        "\n",
        "# Define D and G loss functions\n",
        "if args.wgan:\n",
        "    args.grad_penalty = 10.\n",
        "    D_criterion = D_criterion_wasserstein\n",
        "    G_criterion = G_criterion_wasserstein\n",
        "else:\n",
        "    args.grad_penalty = 0.\n",
        "    D_criterion = D_criterion_hinge\n",
        "    G_criterion = G_criterion_hinge\n",
        "\n",
        "# Optimizers\n",
        "G_optimizer = torch.optim.Adam(\n",
        "    [{\"params\": model.G.parameters()},\n",
        "     {\"params\": latent_transform.parameters(), \"lr\": args.latent_transform_lr}],\n",
        "     args.G_lr, betas=args.betas)\n",
        "D_optimizer = torch.optim.Adam(\n",
        "    [{\"params\": model.D.parameters(), \"lr\": args.D_lr},\n",
        "     {\"params\": Q.parameters(), \"lr\": args.Q_lr}],\n",
        "     args.Q_lr, betas=args.betas)\n",
        "\n",
        "D_sched = torch.optim.lr_scheduler.ExponentialLR(D_optimizer, 1. - args.lr_decay)\n",
        "G_sched = torch.optim.lr_scheduler.ExponentialLR(G_optimizer, 1. - args.lr_decay)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "if args.load:\n",
        "    model.load_state_dict(torch.load(f\"{GANSIAM_DIR}/results/model.pth.tar\"))\n",
        "    latent_transform.load_state_dict(torch.load(f\"{GANSIAM_DIR}/results/latent_transform.pth.tar\"))\n",
        "    Q.load_state_dict(torch.load(f\"{GANSIAM_DIR}/results/Q.pth.tar\"))"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of params in D: 2935873\n",
            "Num of params in G: 8786435\n",
            "Num of params in Q: 1312768\n",
            "Num of params in L: 262272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUTqwby4cX0i",
        "outputId": "446a4438-e2bd-4be3-d597-bf0d8f890d1f"
      },
      "source": [
        "print(model)\n",
        "print(Q)\n",
        "print(latent_transform)"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SNDCGAN(\n",
            "  (D): SNDCGAN_Discriminator(\n",
            "    (main): Sequential(\n",
            "      (0): ParametrizedConv2d(\n",
            "        3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (2): ParametrizedConv2d(\n",
            "        64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (4): ParametrizedConv2d(\n",
            "        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (6): ParametrizedConv2d(\n",
            "        128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (7): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (8): ParametrizedConv2d(\n",
            "        128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (9): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (10): ParametrizedConv2d(\n",
            "        256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (11): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (12): ParametrizedConv2d(\n",
            "        256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (13): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (14): Flatten(start_dim=1, end_dim=-1)\n",
            "    )\n",
            "    (fc): ParametrizedLinear(\n",
            "      in_features=8192, out_features=1, bias=True\n",
            "      (parametrizations): ModuleDict(\n",
            "        (weight): ParametrizationList(\n",
            "          (0): _SpectralNorm()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (G): SNDCGAN_Generator(\n",
            "    (conv1): ConvTranspose2d(256, 512, kernel_size=(4, 4), stride=(1, 1))\n",
            "    (bn1): ConditionalBatchNorm2d(\n",
            "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "      (embed): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "    )\n",
            "    (conv2): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (bn2): ConditionalBatchNorm2d(\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "      (embed): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    )\n",
            "    (conv3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (bn3): ConditionalBatchNorm2d(\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "      (embed): Linear(in_features=2048, out_features=256, bias=True)\n",
            "    )\n",
            "    (conv4): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (bn4): ConditionalBatchNorm2d(\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "      (embed): Linear(in_features=2048, out_features=128, bias=True)\n",
            "    )\n",
            "    (conv5): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (block_activation): ReLU()\n",
            "    (output_activation): Tanh()\n",
            "  )\n",
            ")\n",
            "Sequential(\n",
            "  (0): Linear(in_features=8192, out_features=128, bias=False)\n",
            "  (1): Linear(in_features=128, out_features=2048, bias=True)\n",
            ")\n",
            "LatentTransform(\n",
            "  (transform): Linear(in_features=2048, out_features=128, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeDicP6QZNQ2"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckj9x-2fNtnp"
      },
      "source": [
        "def sample_noise(num_samples):\n",
        "    return torch.randn(num_samples, args.latent_noise_dim)\n",
        "\n",
        "def get_repr(img):\n",
        "    with torch.no_grad():\n",
        "        repr = simsiam.encoder(img)\n",
        "        repr = repr + args.repr_noise * torch.randn_like(repr)\n",
        "    return repr\n",
        "\n",
        "def sample_G(repr):\n",
        "    noise = sample_noise(repr.size(0)).cuda(args.gpu)\n",
        "    z = latent_transform(repr, noise)\n",
        "    fake = model.G(z, repr)\n",
        "    fake = fake + args.im_noise * torch.randn_like(fake)\n",
        "    return fake"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEVERm6alJt6"
      },
      "source": [
        "# Sample a global latent for reuse\n",
        "fixed_x, _ = next(iter(train_loader))\n",
        "fixed_x = fixed_x[:32].cuda(args.gpu)\n",
        "fixed_repr = get_repr(fixed_x)\n",
        "fixed_noise = sample_noise(32).cuda(args.gpu)\n",
        "\n",
        "def check_G_progress(G):\n",
        "    with torch.no_grad():\n",
        "        z = latent_transform(fixed_repr, fixed_noise)\n",
        "        fake_progress = G(z, fixed_repr)\n",
        "    im_grid = torch.cat([fixed_x, fake_progress], dim=0)\n",
        "    grid = vutils.make_grid(im_grid.cpu(), padding=2, normalize=True, range=(-1,1))\n",
        "    return grid"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7gU4I8OZOer"
      },
      "source": [
        "def train(train_loader, model, simsiam,\n",
        "          D_criterion, G_criterion, D_optimizer, G_optimizer, epoch, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    data_time = AverageMeter('Data', ':6.3f')\n",
        "    D_on_reals = AverageMeter('D(real)', ':.4f')\n",
        "    D_on_fakes1 = AverageMeter('D(fake)1', ':.4f')\n",
        "    D_on_fakes2 = AverageMeter('D(fake)2', ':.4f')\n",
        "    D_grads = AverageMeter('grad(D)', ':.4f')\n",
        "    G_repr_losses = AverageMeter('G repr loss', ':.4f')\n",
        "    D_repr_losses = AverageMeter('D repr loss', ':.4f')\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader),\n",
        "        [batch_time, data_time,\n",
        "         D_on_reals, D_on_fakes1, D_on_fakes2, D_grads, G_repr_losses, D_repr_losses],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    \n",
        "    # Create dataset sampler\n",
        "    data_iter = iter(enumerate(train_loader))\n",
        "    batch_idx = [0]  # just an ugly hack\n",
        "    def sample_data():\n",
        "        i, (x, y) = next(data_iter)\n",
        "        batch_idx[0] = i\n",
        "        x = x.cuda(args.gpu, non_blocking=True)\n",
        "        real = x + args.im_noise * torch.randn_like(x)\n",
        "        return real\n",
        "    \n",
        "    def sample_repr():\n",
        "        # (1) Sample a random representation from a prior distribution\n",
        "        #repr = -1+2*torch.rand(args.batch_size, args.repr_dim).cuda(args.gpu)\n",
        "\n",
        "        # (2) Sample a real represenation with noise\n",
        "        repr = get_repr(sample_data())\n",
        "\n",
        "        # (3) Sample an interpolated represenation\n",
        "        \"\"\"\n",
        "        real1, real2 = sample_data(), sample_data()\n",
        "        repr1, repr2 = get_repr(real1), get_repr(real2)\n",
        "        repr = lerp(repr1, repr2)\n",
        "        #repr = slerp(repr1, repr2)\n",
        "        \"\"\"\n",
        "        \n",
        "        return repr\n",
        "\n",
        "    end = time.time()\n",
        "    # Train until data_iter is exhausted\n",
        "    try:\n",
        "        i = -1\n",
        "        while True:\n",
        "            i += 1\n",
        "            # measure data loading time\n",
        "            data_time.update(time.time() - end)\n",
        "\n",
        "            ### Train generator\n",
        "            # Sample data and get representation\n",
        "            repr = sample_repr()\n",
        "            # Sample from generator given repr\n",
        "            fake = sample_G(repr)\n",
        "            # Classify fake data\n",
        "            D_fake, h_fake = model.D(fake, return_h=True)\n",
        "            # Calculate adversarial loss\n",
        "            G_loss = G_criterion(D_fake)\n",
        "            # Calculate consistency loss\n",
        "            if args.G_consistency != 0.:\n",
        "                fake_repr = simsiam.encoder(fake)\n",
        "                G_repr_loss = -F.cosine_similarity(fake_repr, repr).mean()\n",
        "                G_repr_losses.update(G_repr_loss.mean().item(), args.batch_size)\n",
        "                G_loss = G_loss + args.G_consistency * G_repr_loss\n",
        "            if args.D_consistency != 0.:\n",
        "                D_repr_loss = -F.cosine_similarity(Q(h_fake), repr).mean()\n",
        "                D_repr_losses.update(D_repr_loss.mean().item(), args.batch_size)\n",
        "                G_loss = G_loss + args.D_consistency * D_repr_loss\n",
        "            # Calculate gradient and minimize\n",
        "            G_optimizer.zero_grad()\n",
        "            G_loss.backward()\n",
        "            G_optimizer.step()\n",
        "            # Update average\n",
        "            D_on_fakes1.update(D_fake.mean().item(), args.batch_size)\n",
        "\n",
        "            ### Train discriminator\n",
        "            for _ in range(args.D_iters):\n",
        "                # Sample data and get representation\n",
        "                real = sample_data()\n",
        "                repr = sample_repr()\n",
        "                # Sample from generator given repr\n",
        "                with torch.no_grad():\n",
        "                    fake = sample_G(repr)\n",
        "                # Classify real and fake data\n",
        "                D_real, h_real = model.D(real, return_h=True)\n",
        "                D_fake, h_fake = model.D(fake, return_h=True)\n",
        "                # Calculate loss\n",
        "                D_loss = D_criterion(D_real, D_fake)\n",
        "                # Gradient penalty\n",
        "                if args.grad_penalty != 0.:\n",
        "                    D_grad_penalty = simple_gradient_penalty(\n",
        "                        model.D, interpolate(real, fake), center=args.grad_center)\n",
        "                    D_loss = D_loss + args.grad_penalty * D_grad_penalty\n",
        "                    D_grads.update(D_grad_penalty.mean().item(), args.batch_size)\n",
        "                # Calculate consistency loss\n",
        "                if args.D_consistency != 0.:\n",
        "                    real_repr = simsiam.encoder(real)\n",
        "                    D_repr_loss_real = -F.cosine_similarity(Q(h_real), real_repr).mean()\n",
        "                    D_repr_loss_fake = -F.cosine_similarity(Q(h_fake), repr).mean()\n",
        "                    D_repr_loss = 0.5 * (D_repr_loss_real + D_repr_loss_fake)\n",
        "                    D_repr_losses.update(D_repr_loss.mean().item(), args.batch_size)\n",
        "                    D_loss = D_loss + args.D_consistency * D_repr_loss\n",
        "                # Calculate gradient and minimize\n",
        "                D_optimizer.zero_grad()\n",
        "                D_loss.backward()\n",
        "                D_optimizer.step()\n",
        "                # Update average\n",
        "                D_on_reals.update(D_real.mean().item(), args.batch_size)\n",
        "                D_on_fakes2.update(D_fake.mean().item(), args.batch_size)\n",
        "\n",
        "            # Check generator's progress by recording its output on a fixed input\n",
        "            if i % args.generate_grid_interval == 0:\n",
        "                grid = check_G_progress(model.G)\n",
        "                GENERATED_GRIDS.append(grid)\n",
        "\n",
        "            if i % args.print_freq == 0:\n",
        "                progress.display(batch_idx[0])\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "    \n",
        "    except StopIteration:\n",
        "        progress.display(batch_idx[0])\n",
        "        return"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v24ES94Re55W"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDMPwe-ae6q-"
      },
      "source": [
        "def save():\n",
        "    torch.save({'state_dict': model.state_dict()},\n",
        "                f\"{GANSIAM_DIR}/results/model.pth.tar\")\n",
        "    torch.save({'state_dict': latent_transform.state_dict()},\n",
        "                f\"{GANSIAM_DIR}/results/latent_transform.pth.tar\")\n",
        "    torch.save({'state_dict': Q.state_dict()},\n",
        "                f\"{GANSIAM_DIR}/results/Q.pth.tar\")\n",
        "\n",
        "def save_vid():\n",
        "    vidname = f\"grids_per_{args.generate_grid_interval}_iters.mp4\"\n",
        "    vidname = os.path.join(GANSIAM_DIR, \"results\", \"progress\", vidname)\n",
        "    create_progress_animation(GENERATED_GRIDS, vidname)\n",
        "\n",
        "def run(epochs):\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # train for one epoch\n",
        "        train(train_loader, model, simsiam,\n",
        "            D_criterion, G_criterion, D_optimizer, G_optimizer, epoch, args)\n",
        "        D_sched.step()\n",
        "        G_sched.step()\n",
        "\n",
        "        # Check G's progress evey epoch by generating an image\n",
        "        grid = check_G_progress(model.G)\n",
        "        imname = f'{GANSIAM_DIR}/results/progress/grid_{epoch:04d}.png'\n",
        "        plt.imsave(imname, grid.permute(1,2,0).numpy())\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            save()\n"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qjya-BRT_cJ"
      },
      "source": [
        "epochs_per_cell = 10"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmNNZfXWe5Rr",
        "outputId": "baf4fed5-fc05-4155-8529-bf65728af80f"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) -1.3145 (0.0699)\tD(fake)1 0.0000 (0.0000)\tD(fake)2 -2.6707 (-0.5495)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0013 (0.0013)\tD repr loss -0.0084 (-0.0046)\n",
            "Epoch: [0][ 54/781]\tTime  0.650 ( 0.653)\tData  0.000 ( 0.000)\tD(real) 2.0345 (2.7707)\tD(fake)1 -2.9054 (-1.8528)\tD(fake)2 -2.7340 (-2.0494)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0025 (0.0011)\tD repr loss -0.0438 (-0.0319)\n",
            "Epoch: [0][104/781]\tTime  0.645 ( 0.650)\tData  0.000 ( 0.000)\tD(real) 2.2192 (2.7491)\tD(fake)1 -1.0913 (-1.8596)\tD(fake)2 -2.0596 (-1.9510)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0049 (-0.0001)\tD repr loss -0.0745 (-0.0441)\n",
            "Epoch: [0][154/781]\tTime  0.653 ( 0.649)\tData  0.000 ( 0.000)\tD(real) 0.7912 (2.3504)\tD(fake)1 -1.7106 (-1.8473)\tD(fake)2 -1.8604 (-1.8599)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0015 (-0.0000)\tD repr loss -0.0814 (-0.0490)\n",
            "Epoch: [0][204/781]\tTime  0.639 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 1.9753 (2.3392)\tD(fake)1 -3.2804 (-1.9128)\tD(fake)2 -2.5997 (-1.9677)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0007 (-0.0000)\tD repr loss -0.0883 (-0.0524)\n",
            "Epoch: [0][254/781]\tTime  0.636 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.0382 (2.4856)\tD(fake)1 -3.1438 (-1.9957)\tD(fake)2 -1.7240 (-2.0880)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0032 (-0.0000)\tD repr loss -0.0948 (-0.0555)\n",
            "Epoch: [0][304/781]\tTime  0.630 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.0960 (2.6978)\tD(fake)1 -2.4918 (-2.0047)\tD(fake)2 -2.6162 (-2.0881)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0020 (0.0000)\tD repr loss -0.0957 (-0.0588)\n",
            "Epoch: [0][354/781]\tTime  0.639 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.8106 (2.8573)\tD(fake)1 -2.5006 (-2.1120)\tD(fake)2 -2.4123 (-2.1275)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0016 (-0.0000)\tD repr loss -0.1756 (-0.0637)\n",
            "Epoch: [0][404/781]\tTime  0.641 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 2.6667 (3.1430)\tD(fake)1 -2.6225 (-2.1512)\tD(fake)2 -2.2234 (-2.1490)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0019 (0.0001)\tD repr loss -0.0844 (-0.0663)\n",
            "Epoch: [0][454/781]\tTime  0.637 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.3617 (3.3528)\tD(fake)1 -0.3828 (-2.1373)\tD(fake)2 -2.2851 (-2.1394)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0017 (0.0003)\tD repr loss -0.0229 (-0.0669)\n",
            "Epoch: [0][504/781]\tTime  0.653 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 6.8380 (3.5291)\tD(fake)1 -2.5646 (-2.1503)\tD(fake)2 -1.6867 (-2.1561)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0014 (0.0002)\tD repr loss -0.0967 (-0.0684)\n",
            "Epoch: [0][554/781]\tTime  0.630 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 5.3739 (3.6403)\tD(fake)1 -1.3279 (-2.1369)\tD(fake)2 -2.9317 (-2.1621)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0011 (0.0002)\tD repr loss -0.1073 (-0.0696)\n",
            "Epoch: [0][604/781]\tTime  0.637 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.3728 (3.8770)\tD(fake)1 -2.1013 (-2.1567)\tD(fake)2 -2.4210 (-2.1650)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0025 (0.0003)\tD repr loss -0.1258 (-0.0713)\n",
            "Epoch: [0][654/781]\tTime  0.657 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.9289 (4.0429)\tD(fake)1 -2.9439 (-2.1808)\tD(fake)2 -2.5830 (-2.1839)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0030 (0.0003)\tD repr loss -0.1370 (-0.0728)\n",
            "Epoch: [0][704/781]\tTime  0.631 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 7.7520 (4.1784)\tD(fake)1 -2.3362 (-2.2008)\tD(fake)2 -2.1037 (-2.2014)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0040 (0.0004)\tD repr loss -0.1181 (-0.0743)\n",
            "Epoch: [0][754/781]\tTime  0.655 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.3571 (4.4648)\tD(fake)1 -1.4588 (-2.2013)\tD(fake)2 -4.1878 (-2.2135)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0000 (0.0003)\tD repr loss -0.0355 (-0.0747)\n",
            "Epoch: [0][780/781]\tTime  0.634 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 9.6145 (4.5879)\tD(fake)1 -2.0129 (-2.1949)\tD(fake)2 -1.9705 (-2.2093)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0021 (0.0002)\tD repr loss -0.1399 (-0.0750)\n",
            "Epoch: [1][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 9.3622 (10.1593)\tD(fake)1 -2.0605 (-2.0605)\tD(fake)2 -2.0415 (-1.9206)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0015 (0.0015)\tD repr loss -0.1567 (-0.1203)\n",
            "Epoch: [1][ 54/781]\tTime  0.713 ( 0.651)\tData  0.000 ( 0.000)\tD(real) 8.4903 (8.2594)\tD(fake)1 -2.1200 (-1.8582)\tD(fake)2 -2.2400 (-1.8883)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0021 (0.0010)\tD repr loss -0.0798 (-0.0972)\n",
            "Epoch: [1][104/781]\tTime  0.651 ( 0.648)\tData  0.000 ( 0.000)\tD(real) 9.7666 (8.1211)\tD(fake)1 -2.1514 (-1.9759)\tD(fake)2 -1.8389 (-1.9636)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0051 (0.0005)\tD repr loss -0.1288 (-0.1002)\n",
            "Epoch: [1][154/781]\tTime  0.631 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 9.4560 (8.1250)\tD(fake)1 -2.2442 (-2.0145)\tD(fake)2 -1.9653 (-2.0415)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0016 (0.0005)\tD repr loss -0.1045 (-0.1002)\n",
            "Epoch: [1][204/781]\tTime  0.642 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 6.9434 (7.4999)\tD(fake)1 -2.1461 (-1.9457)\tD(fake)2 -2.0142 (-2.0063)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0011 (0.0001)\tD repr loss -0.1153 (-0.1000)\n",
            "Epoch: [1][254/781]\tTime  0.634 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 5.1232 (7.2599)\tD(fake)1 -2.2062 (-2.0359)\tD(fake)2 -2.4589 (-2.0918)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0002 (0.0002)\tD repr loss -0.1411 (-0.1015)\n",
            "Epoch: [1][304/781]\tTime  0.637 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.7866 (6.8102)\tD(fake)1 -1.8321 (-2.0033)\tD(fake)2 -1.3351 (-2.0421)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0003 (0.0001)\tD repr loss -0.0666 (-0.1010)\n",
            "Epoch: [1][354/781]\tTime  0.661 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 2.8789 (6.4670)\tD(fake)1 -2.0627 (-1.9885)\tD(fake)2 -2.2472 (-2.0151)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0022 (0.0000)\tD repr loss -0.1438 (-0.1028)\n",
            "Epoch: [1][404/781]\tTime  0.645 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 5.7310 (6.3563)\tD(fake)1 -2.6563 (-2.0123)\tD(fake)2 -2.2663 (-2.0495)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0055 (-0.0001)\tD repr loss -0.1592 (-0.1042)\n",
            "Epoch: [1][454/781]\tTime  0.649 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 5.6649 (6.2891)\tD(fake)1 -1.9220 (-2.0521)\tD(fake)2 -2.3074 (-2.0778)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0017 (0.0001)\tD repr loss -0.1003 (-0.1042)\n",
            "Epoch: [1][504/781]\tTime  0.640 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 5.5127 (6.2615)\tD(fake)1 -2.7939 (-2.0602)\tD(fake)2 -2.4292 (-2.0818)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0029 (0.0001)\tD repr loss -0.1268 (-0.1050)\n",
            "Epoch: [1][554/781]\tTime  0.634 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.5568 (6.2123)\tD(fake)1 -2.2637 (-2.0781)\tD(fake)2 -2.4945 (-2.1037)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0073 (0.0001)\tD repr loss -0.0836 (-0.1060)\n",
            "Epoch: [1][604/781]\tTime  0.632 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 5.4711 (6.2317)\tD(fake)1 -3.5062 (-2.0989)\tD(fake)2 -2.4996 (-2.1117)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0066 (0.0001)\tD repr loss -0.1264 (-0.1057)\n",
            "Epoch: [1][654/781]\tTime  0.639 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 5.8343 (6.2812)\tD(fake)1 -1.5193 (-2.0944)\tD(fake)2 -2.8245 (-2.1043)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0000 (0.0001)\tD repr loss -0.1680 (-0.1063)\n",
            "Epoch: [1][704/781]\tTime  0.632 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.7876 (6.2662)\tD(fake)1 -2.1539 (-2.1099)\tD(fake)2 -1.8914 (-2.1102)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0003 (0.0001)\tD repr loss -0.1767 (-0.1069)\n",
            "Epoch: [1][754/781]\tTime  0.632 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 5.7406 (6.2358)\tD(fake)1 -2.7348 (-2.1413)\tD(fake)2 -3.4950 (-2.1361)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0013 (0.0003)\tD repr loss -0.0996 (-0.1079)\n",
            "Epoch: [1][780/781]\tTime  0.639 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 5.7355 (6.2020)\tD(fake)1 -1.9629 (-2.1564)\tD(fake)2 -1.9892 (-2.1535)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0022 (0.0004)\tD repr loss -0.0921 (-0.1084)\n",
            "Epoch: [2][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 4.3020 (4.1879)\tD(fake)1 -1.9684 (-1.9684)\tD(fake)2 -2.0225 (-2.0262)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0003 (0.0003)\tD repr loss -0.1304 (-0.1307)\n",
            "Epoch: [2][ 54/781]\tTime  0.646 ( 0.648)\tData  0.000 ( 0.000)\tD(real) 4.6114 (4.3642)\tD(fake)1 -2.5822 (-2.2806)\tD(fake)2 -2.3181 (-2.2291)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0057 (-0.0006)\tD repr loss -0.1567 (-0.1187)\n",
            "Epoch: [2][104/781]\tTime  0.636 ( 0.649)\tData  0.000 ( 0.000)\tD(real) 4.1247 (4.4613)\tD(fake)1 -2.3474 (-2.3540)\tD(fake)2 -2.9918 (-2.4065)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0017 (-0.0007)\tD repr loss -0.1843 (-0.1194)\n",
            "Epoch: [2][154/781]\tTime  0.644 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 4.7532 (4.3582)\tD(fake)1 -1.7395 (-2.3294)\tD(fake)2 -3.2760 (-2.3780)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0024 (-0.0002)\tD repr loss -0.1741 (-0.1210)\n",
            "Epoch: [2][204/781]\tTime  0.651 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 5.4662 (4.5995)\tD(fake)1 -2.3044 (-2.3446)\tD(fake)2 -2.2781 (-2.3707)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0020 (-0.0003)\tD repr loss -0.1066 (-0.1198)\n",
            "Epoch: [2][254/781]\tTime  0.631 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.8425 (4.6149)\tD(fake)1 -2.3643 (-2.3959)\tD(fake)2 -2.1219 (-2.3875)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0025 (-0.0001)\tD repr loss -0.1637 (-0.1218)\n",
            "Epoch: [2][304/781]\tTime  0.643 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.2197 (4.5306)\tD(fake)1 -1.6936 (-2.3635)\tD(fake)2 -2.2352 (-2.3664)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0045 (-0.0000)\tD repr loss -0.1808 (-0.1214)\n",
            "Epoch: [2][354/781]\tTime  0.634 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.1244 (4.3935)\tD(fake)1 -1.9033 (-2.3345)\tD(fake)2 -2.0600 (-2.3339)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0018 (0.0001)\tD repr loss -0.2158 (-0.1228)\n",
            "Epoch: [2][404/781]\tTime  0.650 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.9730 (4.2013)\tD(fake)1 -1.8146 (-2.3119)\tD(fake)2 -1.8448 (-2.3109)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0002 (-0.0000)\tD repr loss -0.0638 (-0.1238)\n",
            "Epoch: [2][454/781]\tTime  0.633 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.2526 (4.0867)\tD(fake)1 -2.6116 (-2.3304)\tD(fake)2 -2.0534 (-2.3212)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0055 (0.0000)\tD repr loss -0.0621 (-0.1243)\n",
            "Epoch: [2][504/781]\tTime  0.630 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.8298 (4.0001)\tD(fake)1 -1.9370 (-2.3176)\tD(fake)2 -2.0228 (-2.3025)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0027 (0.0001)\tD repr loss -0.0836 (-0.1255)\n",
            "Epoch: [2][554/781]\tTime  0.636 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.7976 (3.9356)\tD(fake)1 -2.3457 (-2.3103)\tD(fake)2 -2.1767 (-2.2964)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0018 (0.0001)\tD repr loss -0.1823 (-0.1249)\n",
            "Epoch: [2][604/781]\tTime  0.645 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.9468 (3.9112)\tD(fake)1 -2.6636 (-2.3303)\tD(fake)2 -2.2514 (-2.3153)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0015 (0.0001)\tD repr loss -0.1170 (-0.1245)\n",
            "Epoch: [2][654/781]\tTime  0.641 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.6521 (3.8990)\tD(fake)1 -2.5338 (-2.3221)\tD(fake)2 -2.6455 (-2.3220)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0017 (0.0002)\tD repr loss -0.2338 (-0.1241)\n",
            "Epoch: [2][704/781]\tTime  0.644 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.5979 (3.8586)\tD(fake)1 -2.5023 (-2.3340)\tD(fake)2 -2.3319 (-2.3357)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0032 (0.0002)\tD repr loss -0.2198 (-0.1245)\n",
            "Epoch: [2][754/781]\tTime  0.633 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.0964 (3.8188)\tD(fake)1 -2.8242 (-2.3403)\tD(fake)2 -2.4968 (-2.3380)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0001 (0.0001)\tD repr loss -0.1626 (-0.1251)\n",
            "Epoch: [2][780/781]\tTime  0.640 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.1623 (3.8013)\tD(fake)1 -2.9340 (-2.3501)\tD(fake)2 -2.6756 (-2.3414)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0016 (0.0002)\tD repr loss -0.1038 (-0.1257)\n",
            "Epoch: [3][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 4.3773 (4.1508)\tD(fake)1 -2.4446 (-2.4446)\tD(fake)2 -2.3328 (-2.4336)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0033 (0.0033)\tD repr loss -0.0715 (-0.0960)\n",
            "Epoch: [3][ 54/781]\tTime  0.650 ( 0.648)\tData  0.000 ( 0.000)\tD(real) 5.1356 (3.5757)\tD(fake)1 -2.5767 (-2.3599)\tD(fake)2 -1.8989 (-2.2701)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0000 (0.0003)\tD repr loss -0.1534 (-0.1311)\n",
            "Epoch: [3][104/781]\tTime  0.631 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 2.1980 (3.5684)\tD(fake)1 -1.9221 (-2.2765)\tD(fake)2 -2.0188 (-2.2019)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0011 (0.0006)\tD repr loss -0.1806 (-0.1333)\n",
            "Epoch: [3][154/781]\tTime  0.632 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.6944 (3.4851)\tD(fake)1 -2.4564 (-2.2160)\tD(fake)2 -2.0019 (-2.1780)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0055 (-0.0003)\tD repr loss -0.0943 (-0.1344)\n",
            "Epoch: [3][204/781]\tTime  0.656 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.6228 (3.5030)\tD(fake)1 -2.3000 (-2.2653)\tD(fake)2 -2.0634 (-2.2434)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0033 (-0.0002)\tD repr loss -0.1540 (-0.1339)\n",
            "Epoch: [3][254/781]\tTime  0.635 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.5770 (3.4516)\tD(fake)1 -1.8875 (-2.2411)\tD(fake)2 -2.6957 (-2.2298)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0000 (-0.0001)\tD repr loss -0.1442 (-0.1349)\n",
            "Epoch: [3][304/781]\tTime  0.630 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.4526 (3.5658)\tD(fake)1 -2.4613 (-2.2850)\tD(fake)2 -2.3188 (-2.2739)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0031 (-0.0002)\tD repr loss -0.2100 (-0.1370)\n",
            "Epoch: [3][354/781]\tTime  0.639 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.8063 (3.6637)\tD(fake)1 -2.1918 (-2.2964)\tD(fake)2 -2.5848 (-2.2780)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0033 (-0.0000)\tD repr loss -0.1286 (-0.1372)\n",
            "Epoch: [3][404/781]\tTime  0.639 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.3707 (3.7084)\tD(fake)1 -2.3076 (-2.3270)\tD(fake)2 -2.7916 (-2.3084)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0009 (0.0000)\tD repr loss -0.1250 (-0.1364)\n",
            "Epoch: [3][454/781]\tTime  0.650 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.3144 (3.7307)\tD(fake)1 -1.6338 (-2.3200)\tD(fake)2 -2.4645 (-2.3126)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0005 (-0.0000)\tD repr loss -0.1479 (-0.1372)\n",
            "Epoch: [3][504/781]\tTime  0.634 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.6324 (3.7767)\tD(fake)1 -2.4358 (-2.3173)\tD(fake)2 -2.3053 (-2.3088)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0026 (-0.0001)\tD repr loss -0.2462 (-0.1378)\n",
            "Epoch: [3][554/781]\tTime  0.651 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 4.2384 (3.8206)\tD(fake)1 -2.1075 (-2.2965)\tD(fake)2 -2.2116 (-2.2921)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0005 (0.0000)\tD repr loss -0.1102 (-0.1373)\n",
            "Epoch: [3][604/781]\tTime  0.643 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 4.2777 (3.8444)\tD(fake)1 -1.7571 (-2.2847)\tD(fake)2 -1.9143 (-2.2761)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0022 (-0.0000)\tD repr loss -0.1200 (-0.1373)\n",
            "Epoch: [3][654/781]\tTime  0.640 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.2283 (3.8731)\tD(fake)1 -2.5361 (-2.2845)\tD(fake)2 -2.6016 (-2.2727)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0029 (0.0001)\tD repr loss -0.1230 (-0.1373)\n",
            "Epoch: [3][704/781]\tTime  0.663 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.2125 (3.8796)\tD(fake)1 -2.0173 (-2.3011)\tD(fake)2 -2.6318 (-2.2866)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0032 (0.0001)\tD repr loss -0.2317 (-0.1381)\n",
            "Epoch: [3][754/781]\tTime  0.638 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.1891 (3.8867)\tD(fake)1 -2.2243 (-2.3148)\tD(fake)2 -2.2535 (-2.3009)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0015 (0.0001)\tD repr loss -0.2074 (-0.1382)\n",
            "Epoch: [3][780/781]\tTime  0.637 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.3858 (3.8934)\tD(fake)1 -2.8221 (-2.3294)\tD(fake)2 -2.6589 (-2.3116)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0016 (0.0001)\tD repr loss -0.2725 (-0.1387)\n",
            "Epoch: [4][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 4.5021 (3.8426)\tD(fake)1 -2.7976 (-2.7976)\tD(fake)2 -2.5666 (-2.6922)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0033 (-0.0033)\tD repr loss -0.1646 (-0.1388)\n",
            "Epoch: [4][ 54/781]\tTime  0.631 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.7841 (4.1551)\tD(fake)1 -2.5839 (-2.6018)\tD(fake)2 -2.7238 (-2.5497)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0092 (0.0001)\tD repr loss -0.1059 (-0.1445)\n",
            "Epoch: [4][104/781]\tTime  0.630 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 4.1802 (4.0845)\tD(fake)1 -2.2107 (-2.6451)\tD(fake)2 -2.8240 (-2.5832)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0017 (0.0002)\tD repr loss -0.1716 (-0.1447)\n",
            "Epoch: [4][154/781]\tTime  0.647 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 2.6270 (4.0653)\tD(fake)1 -2.4698 (-2.5943)\tD(fake)2 -2.5916 (-2.5296)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0012 (0.0006)\tD repr loss -0.2098 (-0.1455)\n",
            "Epoch: [4][204/781]\tTime  0.641 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 2.8482 (4.0518)\tD(fake)1 -1.9210 (-2.4958)\tD(fake)2 -2.4744 (-2.4525)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0020 (0.0003)\tD repr loss -0.1415 (-0.1460)\n",
            "Epoch: [4][254/781]\tTime  0.649 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 2.2759 (3.9858)\tD(fake)1 -2.4005 (-2.4548)\tD(fake)2 -2.5906 (-2.4035)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0008 (0.0004)\tD repr loss -0.1123 (-0.1460)\n",
            "Epoch: [4][304/781]\tTime  0.639 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 1.4840 (3.9119)\tD(fake)1 -2.5403 (-2.4694)\tD(fake)2 -3.4151 (-2.4163)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0026 (0.0001)\tD repr loss -0.1807 (-0.1465)\n",
            "Epoch: [4][354/781]\tTime  0.646 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 2.7444 (3.8896)\tD(fake)1 -2.4396 (-2.4376)\tD(fake)2 -2.3835 (-2.3882)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0000 (0.0001)\tD repr loss -0.2028 (-0.1483)\n",
            "Epoch: [4][404/781]\tTime  0.635 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 5.8454 (3.8704)\tD(fake)1 -2.7561 (-2.4611)\tD(fake)2 -1.7023 (-2.4094)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0019 (0.0001)\tD repr loss -0.1961 (-0.1483)\n",
            "Epoch: [4][454/781]\tTime  0.655 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.1293 (3.9231)\tD(fake)1 -2.0302 (-2.4546)\tD(fake)2 -2.1037 (-2.4059)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0011 (0.0000)\tD repr loss -0.1217 (-0.1479)\n",
            "Epoch: [4][504/781]\tTime  0.656 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.9612 (3.9346)\tD(fake)1 -2.3392 (-2.4285)\tD(fake)2 -2.6987 (-2.3900)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0024 (0.0001)\tD repr loss -0.2140 (-0.1490)\n",
            "Epoch: [4][554/781]\tTime  0.657 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 4.0875 (3.9823)\tD(fake)1 -2.2306 (-2.4345)\tD(fake)2 -2.4442 (-2.4025)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0012 (0.0000)\tD repr loss -0.1927 (-0.1478)\n",
            "Epoch: [4][604/781]\tTime  0.632 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.2267 (3.9869)\tD(fake)1 -2.2153 (-2.4321)\tD(fake)2 -2.5720 (-2.4068)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0023 (0.0001)\tD repr loss -0.2123 (-0.1482)\n",
            "Epoch: [4][654/781]\tTime  0.636 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 4.1661 (3.9556)\tD(fake)1 -2.0511 (-2.4388)\tD(fake)2 -2.2261 (-2.4115)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0014 (0.0001)\tD repr loss -0.2500 (-0.1483)\n",
            "Epoch: [4][704/781]\tTime  0.632 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.5307 (3.9738)\tD(fake)1 -2.4688 (-2.4425)\tD(fake)2 -2.7972 (-2.4106)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0041 (0.0001)\tD repr loss -0.2160 (-0.1478)\n",
            "Epoch: [4][754/781]\tTime  0.635 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 2.1489 (3.9581)\tD(fake)1 -2.7535 (-2.4717)\tD(fake)2 -2.6241 (-2.4317)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0057 (0.0000)\tD repr loss -0.1390 (-0.1480)\n",
            "Epoch: [4][780/781]\tTime  0.644 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 3.9649 (3.9363)\tD(fake)1 -3.1840 (-2.4753)\tD(fake)2 -3.0133 (-2.4373)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0035 (0.0000)\tD repr loss -0.2228 (-0.1481)\n",
            "Epoch: [5][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.0677 (3.2434)\tD(fake)1 -3.4397 (-3.4397)\tD(fake)2 -2.9753 (-3.1313)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0002 (0.0002)\tD repr loss -0.1621 (-0.1640)\n",
            "Epoch: [5][ 54/781]\tTime  0.628 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.9488 (3.7144)\tD(fake)1 -2.8667 (-2.7169)\tD(fake)2 -2.4450 (-2.7383)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0029 (0.0003)\tD repr loss -0.1389 (-0.1498)\n",
            "Epoch: [5][104/781]\tTime  0.648 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 2.2538 (3.4289)\tD(fake)1 -2.4105 (-2.6437)\tD(fake)2 -2.5017 (-2.6974)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0015 (-0.0000)\tD repr loss -0.2152 (-0.1501)\n",
            "Epoch: [5][154/781]\tTime  0.653 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 2.3931 (3.3131)\tD(fake)1 -2.7760 (-2.6260)\tD(fake)2 -3.0671 (-2.7108)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0020 (0.0006)\tD repr loss -0.1656 (-0.1521)\n",
            "Epoch: [5][204/781]\tTime  0.648 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.6507 (3.2314)\tD(fake)1 -2.3774 (-2.5176)\tD(fake)2 -2.3809 (-2.6123)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0037 (0.0005)\tD repr loss -0.1762 (-0.1514)\n",
            "Epoch: [5][254/781]\tTime  0.638 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 2.5414 (3.1453)\tD(fake)1 -2.8639 (-2.5453)\tD(fake)2 -3.1416 (-2.6310)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0010 (0.0007)\tD repr loss -0.1083 (-0.1522)\n",
            "Epoch: [5][304/781]\tTime  0.648 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.9151 (3.1567)\tD(fake)1 -3.4202 (-2.6395)\tD(fake)2 -2.8765 (-2.6975)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0051 (0.0004)\tD repr loss -0.2222 (-0.1531)\n",
            "Epoch: [5][354/781]\tTime  0.643 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.5833 (3.1757)\tD(fake)1 -2.9126 (-2.6484)\tD(fake)2 -2.3529 (-2.6799)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0028 (0.0004)\tD repr loss -0.2147 (-0.1539)\n",
            "Epoch: [5][404/781]\tTime  0.651 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 2.8011 (3.2244)\tD(fake)1 -2.8046 (-2.6914)\tD(fake)2 -2.9692 (-2.7177)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0031 (0.0005)\tD repr loss -0.1325 (-0.1527)\n",
            "Epoch: [5][454/781]\tTime  0.636 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 2.3582 (3.2992)\tD(fake)1 -2.5072 (-2.6881)\tD(fake)2 -3.6400 (-2.7136)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0007 (0.0004)\tD repr loss -0.1699 (-0.1533)\n",
            "Epoch: [5][504/781]\tTime  0.639 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.9650 (3.3350)\tD(fake)1 -3.0417 (-2.7162)\tD(fake)2 -3.1261 (-2.7366)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0035 (0.0003)\tD repr loss -0.0703 (-0.1534)\n",
            "Epoch: [5][554/781]\tTime  0.635 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.0692 (3.3683)\tD(fake)1 -3.5153 (-2.7725)\tD(fake)2 -3.6097 (-2.7976)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0032 (0.0003)\tD repr loss -0.1703 (-0.1539)\n",
            "Epoch: [5][604/781]\tTime  0.645 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.0214 (3.3649)\tD(fake)1 -2.6772 (-2.7785)\tD(fake)2 -2.5057 (-2.8014)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0004 (0.0003)\tD repr loss -0.2208 (-0.1549)\n",
            "Epoch: [5][654/781]\tTime  0.638 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 2.8402 (3.3365)\tD(fake)1 -2.1606 (-2.7628)\tD(fake)2 -2.8199 (-2.7955)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0010 (0.0003)\tD repr loss -0.2189 (-0.1544)\n",
            "Epoch: [5][704/781]\tTime  0.640 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 2.8279 (3.3283)\tD(fake)1 -3.1177 (-2.7961)\tD(fake)2 -3.4323 (-2.8187)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0018 (0.0004)\tD repr loss -0.2009 (-0.1552)\n",
            "Epoch: [5][754/781]\tTime  0.642 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.4246 (3.3235)\tD(fake)1 -4.0017 (-2.8230)\tD(fake)2 -3.5519 (-2.8303)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0005 (0.0004)\tD repr loss -0.1657 (-0.1547)\n",
            "Epoch: [5][780/781]\tTime  0.634 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.7755 (3.3146)\tD(fake)1 -2.4242 (-2.8215)\tD(fake)2 -2.5624 (-2.8316)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0025 (0.0003)\tD repr loss -0.1885 (-0.1548)\n",
            "Epoch: [6][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.1864 (3.5200)\tD(fake)1 -2.3520 (-2.3520)\tD(fake)2 -2.8980 (-2.8054)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0049 (0.0049)\tD repr loss -0.1668 (-0.1311)\n",
            "Epoch: [6][ 54/781]\tTime  0.650 ( 0.649)\tData  0.000 ( 0.000)\tD(real) 3.5418 (3.3671)\tD(fake)1 -3.4352 (-3.1596)\tD(fake)2 -3.4545 (-3.1746)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0034 (-0.0001)\tD repr loss -0.2299 (-0.1537)\n",
            "Epoch: [6][104/781]\tTime  0.627 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 4.0598 (3.5290)\tD(fake)1 -3.5576 (-3.3091)\tD(fake)2 -3.4688 (-3.2847)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0004 (0.0001)\tD repr loss -0.1758 (-0.1586)\n",
            "Epoch: [6][154/781]\tTime  0.644 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.0090 (3.4778)\tD(fake)1 -2.5660 (-3.2507)\tD(fake)2 -2.7919 (-3.2301)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0001 (0.0000)\tD repr loss -0.1541 (-0.1588)\n",
            "Epoch: [6][204/781]\tTime  0.652 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.0502 (3.3713)\tD(fake)1 -2.7529 (-3.1434)\tD(fake)2 -2.9144 (-3.1349)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0028 (0.0001)\tD repr loss -0.1815 (-0.1604)\n",
            "Epoch: [6][254/781]\tTime  0.626 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.1210 (3.3065)\tD(fake)1 -2.7724 (-3.1020)\tD(fake)2 -3.5961 (-3.0993)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0025 (0.0004)\tD repr loss -0.2082 (-0.1591)\n",
            "Epoch: [6][304/781]\tTime  0.644 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.2006 (3.2936)\tD(fake)1 -3.2303 (-3.1064)\tD(fake)2 -3.4154 (-3.0808)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0030 (0.0000)\tD repr loss -0.2001 (-0.1585)\n",
            "Epoch: [6][354/781]\tTime  0.643 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.3046 (3.3251)\tD(fake)1 -3.4508 (-3.1231)\tD(fake)2 -3.1738 (-3.0963)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0050 (0.0000)\tD repr loss -0.2489 (-0.1589)\n",
            "Epoch: [6][404/781]\tTime  0.650 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.3684 (3.3863)\tD(fake)1 -2.9243 (-3.1309)\tD(fake)2 -2.9531 (-3.1099)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0005 (-0.0001)\tD repr loss -0.1322 (-0.1584)\n",
            "Epoch: [6][454/781]\tTime  0.648 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.4326 (3.4466)\tD(fake)1 -2.0418 (-3.1074)\tD(fake)2 -2.7953 (-3.0919)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0002 (-0.0000)\tD repr loss -0.1919 (-0.1590)\n",
            "Epoch: [6][504/781]\tTime  0.641 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.6886 (3.5212)\tD(fake)1 -2.6381 (-3.0914)\tD(fake)2 -2.3508 (-3.0740)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0020 (-0.0000)\tD repr loss -0.2131 (-0.1583)\n",
            "Epoch: [6][554/781]\tTime  0.638 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.8095 (3.5509)\tD(fake)1 -2.7633 (-3.0759)\tD(fake)2 -2.6645 (-3.0643)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0022 (-0.0001)\tD repr loss -0.2730 (-0.1585)\n",
            "Epoch: [6][604/781]\tTime  0.637 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.1675 (3.5588)\tD(fake)1 -3.0221 (-3.0741)\tD(fake)2 -3.3441 (-3.0628)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0046 (-0.0001)\tD repr loss -0.1682 (-0.1582)\n",
            "Epoch: [6][654/781]\tTime  0.645 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.3858 (3.5426)\tD(fake)1 -3.1147 (-3.0601)\tD(fake)2 -2.7876 (-3.0492)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0001 (-0.0001)\tD repr loss -0.2463 (-0.1586)\n",
            "Epoch: [6][704/781]\tTime  0.651 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.1416 (3.5478)\tD(fake)1 -2.4256 (-3.0446)\tD(fake)2 -3.3820 (-3.0312)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0005 (-0.0001)\tD repr loss -0.2465 (-0.1585)\n",
            "Epoch: [6][754/781]\tTime  0.631 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 5.9496 (3.5632)\tD(fake)1 -3.2005 (-3.0332)\tD(fake)2 -2.3380 (-3.0227)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0021 (-0.0002)\tD repr loss -0.1987 (-0.1590)\n",
            "Epoch: [6][780/781]\tTime  0.632 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.2979 (3.5735)\tD(fake)1 -2.6455 (-3.0428)\tD(fake)2 -2.7181 (-3.0337)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0001 (-0.0001)\tD repr loss -0.1798 (-0.1592)\n",
            "Epoch: [7][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.1971 (3.5035)\tD(fake)1 -3.1193 (-3.1193)\tD(fake)2 -3.5540 (-3.3627)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0043 (-0.0043)\tD repr loss -0.2573 (-0.1613)\n",
            "Epoch: [7][ 54/781]\tTime  0.645 ( 0.651)\tData  0.000 ( 0.000)\tD(real) 3.8489 (3.7435)\tD(fake)1 -3.0855 (-3.4976)\tD(fake)2 -2.8726 (-3.4234)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0011 (-0.0001)\tD repr loss -0.1509 (-0.1676)\n",
            "Epoch: [7][104/781]\tTime  0.638 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 3.9873 (3.8652)\tD(fake)1 -2.9160 (-3.1301)\tD(fake)2 -2.8027 (-3.1502)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0012 (0.0003)\tD repr loss -0.2122 (-0.1606)\n",
            "Epoch: [7][154/781]\tTime  0.655 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 3.2500 (3.8945)\tD(fake)1 -2.8722 (-3.0397)\tD(fake)2 -3.6873 (-3.0554)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0043 (-0.0001)\tD repr loss -0.2344 (-0.1612)\n",
            "Epoch: [7][204/781]\tTime  0.636 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.4499 (3.8644)\tD(fake)1 -2.8547 (-2.9928)\tD(fake)2 -3.1040 (-3.0206)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0005 (0.0002)\tD repr loss -0.2000 (-0.1607)\n",
            "Epoch: [7][254/781]\tTime  0.641 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.8668 (3.8716)\tD(fake)1 -3.3975 (-3.0077)\tD(fake)2 -3.2940 (-3.0125)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0022 (-0.0002)\tD repr loss -0.1277 (-0.1618)\n",
            "Epoch: [7][304/781]\tTime  0.632 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.0464 (3.8813)\tD(fake)1 -3.0983 (-2.9916)\tD(fake)2 -3.1389 (-3.0134)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0008 (-0.0002)\tD repr loss -0.1574 (-0.1618)\n",
            "Epoch: [7][354/781]\tTime  0.650 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.1022 (3.9163)\tD(fake)1 -3.3837 (-3.0079)\tD(fake)2 -3.1970 (-3.0365)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0010 (0.0002)\tD repr loss -0.1597 (-0.1612)\n",
            "Epoch: [7][404/781]\tTime  0.639 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.4318 (3.9399)\tD(fake)1 -2.9805 (-3.0480)\tD(fake)2 -3.2757 (-3.0600)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0031 (0.0001)\tD repr loss -0.2583 (-0.1607)\n",
            "Epoch: [7][454/781]\tTime  0.637 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 5.6892 (3.9662)\tD(fake)1 -3.7839 (-3.1125)\tD(fake)2 -1.9596 (-3.1100)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0021 (0.0001)\tD repr loss -0.1792 (-0.1595)\n",
            "Epoch: [7][504/781]\tTime  0.639 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.5562 (3.9769)\tD(fake)1 -3.7710 (-3.1359)\tD(fake)2 -4.5012 (-3.1332)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0018 (0.0001)\tD repr loss -0.1989 (-0.1593)\n",
            "Epoch: [7][554/781]\tTime  0.633 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.5989 (3.9399)\tD(fake)1 -3.1004 (-3.1719)\tD(fake)2 -2.3470 (-3.1707)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0008 (-0.0000)\tD repr loss -0.1444 (-0.1596)\n",
            "Epoch: [7][604/781]\tTime  0.634 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.0298 (3.9390)\tD(fake)1 -3.7585 (-3.2100)\tD(fake)2 -3.6688 (-3.2069)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0002 (0.0001)\tD repr loss -0.1895 (-0.1605)\n",
            "Epoch: [7][654/781]\tTime  0.634 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.5211 (3.9033)\tD(fake)1 -3.3096 (-3.2247)\tD(fake)2 -2.7664 (-3.2213)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0067 (-0.0000)\tD repr loss -0.1078 (-0.1598)\n",
            "Epoch: [7][704/781]\tTime  0.643 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.2458 (3.8733)\tD(fake)1 -3.1681 (-3.2532)\tD(fake)2 -4.0814 (-3.2525)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0025 (0.0000)\tD repr loss -0.2227 (-0.1594)\n",
            "Epoch: [7][754/781]\tTime  0.639 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 4.4499 (3.8662)\tD(fake)1 -3.9066 (-3.2856)\tD(fake)2 -3.3866 (-3.2887)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0011 (0.0000)\tD repr loss -0.1481 (-0.1599)\n",
            "Epoch: [7][780/781]\tTime  0.640 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.8089 (3.8845)\tD(fake)1 -3.5853 (-3.3007)\tD(fake)2 -4.0414 (-3.2949)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0015 (0.0001)\tD repr loss -0.2147 (-0.1598)\n",
            "Epoch: [8][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 4.0732 (4.5249)\tD(fake)1 -3.3993 (-3.3993)\tD(fake)2 -3.6477 (-3.2263)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0059 (0.0059)\tD repr loss -0.1576 (-0.1394)\n",
            "Epoch: [8][ 54/781]\tTime  0.642 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 2.8368 (3.9938)\tD(fake)1 -3.1984 (-3.5575)\tD(fake)2 -3.7649 (-3.5079)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0010 (-0.0011)\tD repr loss -0.1960 (-0.1610)\n",
            "Epoch: [8][104/781]\tTime  0.641 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.6283 (3.8641)\tD(fake)1 -4.5832 (-3.4178)\tD(fake)2 -4.8786 (-3.4951)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0021 (-0.0008)\tD repr loss -0.0882 (-0.1640)\n",
            "Epoch: [8][154/781]\tTime  0.646 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 4.3048 (3.7895)\tD(fake)1 -3.3935 (-3.5394)\tD(fake)2 -3.0819 (-3.5289)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0007 (-0.0001)\tD repr loss -0.1761 (-0.1627)\n",
            "Epoch: [8][204/781]\tTime  0.630 ( 0.639)\tData  0.000 ( 0.000)\tD(real) 3.8087 (3.7536)\tD(fake)1 -2.5476 (-3.3973)\tD(fake)2 -3.0492 (-3.4017)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0029 (-0.0000)\tD repr loss -0.1818 (-0.1643)\n",
            "Epoch: [8][254/781]\tTime  0.633 ( 0.639)\tData  0.000 ( 0.000)\tD(real) 3.3194 (3.7932)\tD(fake)1 -4.0480 (-3.3840)\tD(fake)2 -4.0132 (-3.3947)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0025 (0.0003)\tD repr loss -0.1901 (-0.1656)\n",
            "Epoch: [8][304/781]\tTime  0.644 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 3.7450 (3.7561)\tD(fake)1 -3.5268 (-3.4100)\tD(fake)2 -3.6057 (-3.4202)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0006 (0.0003)\tD repr loss -0.2104 (-0.1640)\n",
            "Epoch: [8][354/781]\tTime  0.641 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 3.6661 (3.7594)\tD(fake)1 -3.2815 (-3.4113)\tD(fake)2 -4.0506 (-3.4214)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0011 (0.0001)\tD repr loss -0.2603 (-0.1639)\n",
            "Epoch: [8][404/781]\tTime  0.662 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 4.0851 (3.8121)\tD(fake)1 -3.8305 (-3.4428)\tD(fake)2 -3.6709 (-3.4600)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0023 (0.0002)\tD repr loss -0.1381 (-0.1640)\n",
            "Epoch: [8][454/781]\tTime  0.632 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.4508 (3.8364)\tD(fake)1 -3.8667 (-3.4854)\tD(fake)2 -3.5751 (-3.4823)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0012 (0.0003)\tD repr loss -0.1889 (-0.1635)\n",
            "Epoch: [8][504/781]\tTime  0.637 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.0740 (3.8401)\tD(fake)1 -3.1376 (-3.4839)\tD(fake)2 -3.0212 (-3.4675)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0039 (0.0003)\tD repr loss -0.1367 (-0.1630)\n",
            "Epoch: [8][554/781]\tTime  0.634 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.0115 (3.8915)\tD(fake)1 -3.6035 (-3.4821)\tD(fake)2 -3.7682 (-3.4703)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0020 (0.0004)\tD repr loss -0.1772 (-0.1625)\n",
            "Epoch: [8][604/781]\tTime  0.627 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.7319 (3.9180)\tD(fake)1 -2.7684 (-3.4812)\tD(fake)2 -3.4587 (-3.4641)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0003 (0.0003)\tD repr loss -0.1790 (-0.1623)\n",
            "Epoch: [8][654/781]\tTime  0.651 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.9165 (3.9233)\tD(fake)1 -3.4152 (-3.4927)\tD(fake)2 -3.4375 (-3.4685)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0026 (0.0002)\tD repr loss -0.1236 (-0.1622)\n",
            "Epoch: [8][704/781]\tTime  0.638 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.2517 (3.9182)\tD(fake)1 -4.1967 (-3.4945)\tD(fake)2 -3.3974 (-3.4652)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0063 (0.0002)\tD repr loss -0.2328 (-0.1628)\n",
            "Epoch: [8][754/781]\tTime  0.654 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 4.0764 (3.9059)\tD(fake)1 -4.0580 (-3.4817)\tD(fake)2 -3.1078 (-3.4520)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0010 (0.0002)\tD repr loss -0.1994 (-0.1629)\n",
            "Epoch: [8][780/781]\tTime  0.629 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.8367 (3.9064)\tD(fake)1 -3.2304 (-3.4715)\tD(fake)2 -3.1858 (-3.4478)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0019 (0.0002)\tD repr loss -0.1586 (-0.1630)\n",
            "Epoch: [9][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.9002 (3.9878)\tD(fake)1 -3.2330 (-3.2330)\tD(fake)2 -3.5867 (-3.3432)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0042 (-0.0042)\tD repr loss -0.1592 (-0.1538)\n",
            "Epoch: [9][ 54/781]\tTime  0.650 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.8626 (3.8591)\tD(fake)1 -4.4855 (-3.4807)\tD(fake)2 -3.0567 (-3.5120)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0014 (0.0000)\tD repr loss -0.2076 (-0.1643)\n",
            "Epoch: [9][104/781]\tTime  0.631 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.7746 (3.7376)\tD(fake)1 -3.0339 (-3.4673)\tD(fake)2 -3.7322 (-3.4868)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0000 (-0.0005)\tD repr loss -0.2017 (-0.1656)\n",
            "Epoch: [9][154/781]\tTime  0.642 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.9983 (3.6723)\tD(fake)1 -3.1650 (-3.4702)\tD(fake)2 -3.0909 (-3.4498)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0001 (-0.0005)\tD repr loss -0.2142 (-0.1670)\n",
            "Epoch: [9][204/781]\tTime  0.640 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 4.2525 (3.6690)\tD(fake)1 -3.0679 (-3.3925)\tD(fake)2 -3.0109 (-3.3743)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0012 (-0.0004)\tD repr loss -0.2011 (-0.1655)\n",
            "Epoch: [9][254/781]\tTime  0.632 ( 0.639)\tData  0.000 ( 0.000)\tD(real) 3.6595 (3.7125)\tD(fake)1 -3.0650 (-3.3741)\tD(fake)2 -4.3485 (-3.3821)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0020 (-0.0002)\tD repr loss -0.1782 (-0.1639)\n",
            "Epoch: [9][304/781]\tTime  0.637 ( 0.639)\tData  0.000 ( 0.000)\tD(real) 3.8026 (3.7463)\tD(fake)1 -3.1948 (-3.4261)\tD(fake)2 -3.3392 (-3.4266)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0004 (-0.0000)\tD repr loss -0.0969 (-0.1630)\n",
            "Epoch: [9][354/781]\tTime  0.639 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 4.4183 (3.7546)\tD(fake)1 -3.1456 (-3.4049)\tD(fake)2 -2.6436 (-3.3964)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0028 (0.0000)\tD repr loss -0.2590 (-0.1632)\n",
            "Epoch: [9][404/781]\tTime  0.652 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.3201 (3.7742)\tD(fake)1 -2.9493 (-3.3987)\tD(fake)2 -4.2451 (-3.3901)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0034 (-0.0000)\tD repr loss -0.1662 (-0.1632)\n",
            "Epoch: [9][454/781]\tTime  0.637 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.9891 (3.7860)\tD(fake)1 -4.1628 (-3.4372)\tD(fake)2 -3.8605 (-3.4273)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0046 (-0.0002)\tD repr loss -0.0642 (-0.1633)\n",
            "Epoch: [9][504/781]\tTime  0.644 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.8070 (3.7963)\tD(fake)1 -4.1296 (-3.4738)\tD(fake)2 -3.6593 (-3.4403)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0001 (-0.0003)\tD repr loss -0.1706 (-0.1638)\n",
            "Epoch: [9][554/781]\tTime  0.629 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 3.4202 (3.7793)\tD(fake)1 -3.4588 (-3.4734)\tD(fake)2 -3.7395 (-3.4479)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0033 (-0.0002)\tD repr loss -0.2724 (-0.1643)\n",
            "Epoch: [9][604/781]\tTime  0.651 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 3.3352 (3.7680)\tD(fake)1 -3.2948 (-3.4689)\tD(fake)2 -3.5619 (-3.4531)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0007 (-0.0002)\tD repr loss -0.0669 (-0.1645)\n",
            "Epoch: [9][654/781]\tTime  0.641 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 3.5794 (3.7555)\tD(fake)1 -3.4766 (-3.4445)\tD(fake)2 -3.0404 (-3.4254)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0037 (-0.0001)\tD repr loss -0.1095 (-0.1648)\n",
            "Epoch: [9][704/781]\tTime  0.630 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 3.4244 (3.7483)\tD(fake)1 -3.1802 (-3.4408)\tD(fake)2 -3.3952 (-3.4229)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0027 (-0.0000)\tD repr loss -0.2063 (-0.1655)\n",
            "Epoch: [9][754/781]\tTime  0.641 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 4.2138 (3.7462)\tD(fake)1 -3.3576 (-3.4454)\tD(fake)2 -3.1404 (-3.4259)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0039 (0.0001)\tD repr loss -0.2290 (-0.1654)\n",
            "Epoch: [9][780/781]\tTime  0.628 ( 0.640)\tData  0.000 ( 0.000)\tD(real) 3.6947 (3.7450)\tD(fake)1 -3.4515 (-3.4367)\tD(fake)2 -3.5988 (-3.4231)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0032 (0.0001)\tD repr loss -0.2203 (-0.1652)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-p4wAlbftUb",
        "outputId": "00e5da63-bd70-468f-aad0-d869ea9d2ce9"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.2135 (3.4228)\tD(fake)1 -3.6163 (-3.6163)\tD(fake)2 -3.6939 (-3.5749)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0028 (-0.0028)\tD repr loss -0.1844 (-0.1623)\n",
            "Epoch: [0][ 54/781]\tTime  0.627 ( 0.662)\tData  0.000 ( 0.000)\tD(real) 2.4893 (3.6475)\tD(fake)1 -3.4799 (-3.5290)\tD(fake)2 -3.5338 (-3.4288)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0034 (0.0005)\tD repr loss -0.1916 (-0.1662)\n",
            "Epoch: [0][104/781]\tTime  0.643 ( 0.653)\tData  0.000 ( 0.000)\tD(real) 2.8864 (3.5060)\tD(fake)1 -3.6918 (-3.5818)\tD(fake)2 -3.6222 (-3.5166)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0004 (0.0001)\tD repr loss -0.2403 (-0.1637)\n",
            "Epoch: [0][154/781]\tTime  0.637 ( 0.650)\tData  0.000 ( 0.000)\tD(real) 3.4255 (3.4027)\tD(fake)1 -4.0531 (-3.5858)\tD(fake)2 -3.8162 (-3.5174)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0032 (0.0001)\tD repr loss -0.1163 (-0.1667)\n",
            "Epoch: [0][204/781]\tTime  0.657 ( 0.648)\tData  0.000 ( 0.000)\tD(real) 3.6215 (3.4470)\tD(fake)1 -3.9553 (-3.5870)\tD(fake)2 -3.6020 (-3.5301)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0060 (0.0001)\tD repr loss -0.1216 (-0.1663)\n",
            "Epoch: [0][254/781]\tTime  0.644 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 3.6846 (3.4878)\tD(fake)1 -4.1774 (-3.6366)\tD(fake)2 -4.2117 (-3.6014)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0018 (-0.0001)\tD repr loss -0.1856 (-0.1680)\n",
            "Epoch: [0][304/781]\tTime  0.635 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 4.0210 (3.5302)\tD(fake)1 -3.0600 (-3.6365)\tD(fake)2 -2.9742 (-3.5964)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0020 (-0.0001)\tD repr loss -0.1581 (-0.1679)\n",
            "Epoch: [0][354/781]\tTime  0.646 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.1742 (3.5646)\tD(fake)1 -3.4701 (-3.6466)\tD(fake)2 -2.9057 (-3.6021)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0000 (-0.0002)\tD repr loss -0.1598 (-0.1676)\n",
            "Epoch: [0][404/781]\tTime  0.635 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.8134 (3.5591)\tD(fake)1 -3.0146 (-3.6310)\tD(fake)2 -2.9351 (-3.5876)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0027 (-0.0002)\tD repr loss -0.1379 (-0.1676)\n",
            "Epoch: [0][454/781]\tTime  0.642 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.4284 (3.6045)\tD(fake)1 -3.1801 (-3.6056)\tD(fake)2 -3.6582 (-3.5643)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0006 (-0.0002)\tD repr loss -0.2493 (-0.1682)\n",
            "Epoch: [0][504/781]\tTime  0.657 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.3781 (3.6643)\tD(fake)1 -4.4386 (-3.6597)\tD(fake)2 -3.8912 (-3.6152)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0048 (-0.0003)\tD repr loss -0.3160 (-0.1681)\n",
            "Epoch: [0][554/781]\tTime  0.644 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.2278 (3.6822)\tD(fake)1 -4.1338 (-3.6656)\tD(fake)2 -4.3343 (-3.6313)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0005 (-0.0002)\tD repr loss -0.2159 (-0.1686)\n",
            "Epoch: [0][604/781]\tTime  0.648 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.5325 (3.7357)\tD(fake)1 -3.3417 (-3.6578)\tD(fake)2 -3.3267 (-3.6446)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0004 (-0.0002)\tD repr loss -0.2356 (-0.1684)\n",
            "Epoch: [0][654/781]\tTime  0.631 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.3752 (3.7595)\tD(fake)1 -3.6088 (-3.6483)\tD(fake)2 -3.9274 (-3.6433)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0005 (-0.0002)\tD repr loss -0.1639 (-0.1684)\n",
            "Epoch: [0][704/781]\tTime  0.642 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.2355 (3.7869)\tD(fake)1 -2.6111 (-3.6486)\tD(fake)2 -3.0915 (-3.6322)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0022 (-0.0002)\tD repr loss -0.1727 (-0.1681)\n",
            "Epoch: [0][754/781]\tTime  0.636 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.9570 (3.7938)\tD(fake)1 -3.0361 (-3.6013)\tD(fake)2 -2.9878 (-3.5924)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0035 (-0.0003)\tD repr loss -0.1840 (-0.1684)\n",
            "Epoch: [0][780/781]\tTime  0.630 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.8847 (3.7963)\tD(fake)1 -3.0040 (-3.5647)\tD(fake)2 -2.8855 (-3.5594)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0005 (-0.0002)\tD repr loss -0.1756 (-0.1686)\n",
            "Epoch: [1][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.9536 (3.7384)\tD(fake)1 -2.9892 (-2.9892)\tD(fake)2 -2.4132 (-2.8336)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0013 (0.0013)\tD repr loss -0.1812 (-0.1873)\n",
            "Epoch: [1][ 54/781]\tTime  0.696 ( 0.656)\tData  0.000 ( 0.000)\tD(real) 3.2056 (3.8804)\tD(fake)1 -3.1350 (-2.9090)\tD(fake)2 -3.9215 (-2.9204)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0038 (0.0002)\tD repr loss -0.1848 (-0.1653)\n",
            "Epoch: [1][104/781]\tTime  0.651 ( 0.651)\tData  0.000 ( 0.000)\tD(real) 3.9368 (3.9251)\tD(fake)1 -3.1691 (-3.0399)\tD(fake)2 -3.1630 (-3.0634)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0035 (0.0005)\tD repr loss -0.1495 (-0.1641)\n",
            "Epoch: [1][154/781]\tTime  0.652 ( 0.650)\tData  0.000 ( 0.000)\tD(real) 3.7375 (3.8892)\tD(fake)1 -3.3507 (-3.0917)\tD(fake)2 -3.3545 (-3.1206)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0025 (0.0002)\tD repr loss -0.2589 (-0.1657)\n",
            "Epoch: [1][204/781]\tTime  0.637 ( 0.648)\tData  0.000 ( 0.000)\tD(real) 3.7997 (3.7973)\tD(fake)1 -3.5334 (-3.1308)\tD(fake)2 -3.4133 (-3.1607)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0010 (0.0002)\tD repr loss -0.2021 (-0.1664)\n",
            "Epoch: [1][254/781]\tTime  0.642 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 3.8506 (3.8237)\tD(fake)1 -3.7979 (-3.1931)\tD(fake)2 -3.9354 (-3.2315)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0003 (0.0003)\tD repr loss -0.2170 (-0.1675)\n",
            "Epoch: [1][304/781]\tTime  0.638 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 4.3005 (3.8155)\tD(fake)1 -3.2542 (-3.2127)\tD(fake)2 -3.1180 (-3.2275)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0022 (0.0002)\tD repr loss -0.2421 (-0.1694)\n",
            "Epoch: [1][354/781]\tTime  0.647 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 4.0294 (3.8332)\tD(fake)1 -3.0413 (-3.1559)\tD(fake)2 -2.8887 (-3.1778)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0006 (0.0005)\tD repr loss -0.3027 (-0.1703)\n",
            "Epoch: [1][404/781]\tTime  0.641 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 4.0881 (3.8558)\tD(fake)1 -3.5494 (-3.2072)\tD(fake)2 -3.4707 (-3.2253)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0003 (0.0006)\tD repr loss -0.1056 (-0.1714)\n",
            "Epoch: [1][454/781]\tTime  0.633 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.1497 (3.8512)\tD(fake)1 -3.4063 (-3.2365)\tD(fake)2 -3.7657 (-3.2463)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0004 (0.0007)\tD repr loss -0.1513 (-0.1715)\n",
            "Epoch: [1][504/781]\tTime  0.639 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.3898 (3.8427)\tD(fake)1 -3.3788 (-3.2644)\tD(fake)2 -3.6073 (-3.2704)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0055 (0.0006)\tD repr loss -0.2152 (-0.1712)\n",
            "Epoch: [1][554/781]\tTime  0.632 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.3526 (3.8414)\tD(fake)1 -3.6174 (-3.3097)\tD(fake)2 -3.6207 (-3.3097)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0014 (0.0006)\tD repr loss -0.0914 (-0.1720)\n",
            "Epoch: [1][604/781]\tTime  0.643 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.7337 (3.8298)\tD(fake)1 -4.1582 (-3.3515)\tD(fake)2 -3.7981 (-3.3497)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0017 (0.0006)\tD repr loss -0.2451 (-0.1716)\n",
            "Epoch: [1][654/781]\tTime  0.662 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.7049 (3.8004)\tD(fake)1 -3.5986 (-3.3858)\tD(fake)2 -3.8854 (-3.3816)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0026 (0.0007)\tD repr loss -0.2466 (-0.1710)\n",
            "Epoch: [1][704/781]\tTime  0.643 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.8840 (3.7886)\tD(fake)1 -3.4556 (-3.3991)\tD(fake)2 -4.0147 (-3.3918)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0031 (0.0006)\tD repr loss -0.2555 (-0.1707)\n",
            "Epoch: [1][754/781]\tTime  0.649 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.0421 (3.7940)\tD(fake)1 -3.6997 (-3.4444)\tD(fake)2 -3.3142 (-3.4242)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0002 (0.0006)\tD repr loss -0.1307 (-0.1701)\n",
            "Epoch: [1][780/781]\tTime  0.630 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.5716 (3.7807)\tD(fake)1 -3.6510 (-3.4522)\tD(fake)2 -3.6338 (-3.4307)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0030 (0.0005)\tD repr loss -0.2755 (-0.1702)\n",
            "Epoch: [2][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.5392 (3.5578)\tD(fake)1 -3.6398 (-3.6398)\tD(fake)2 -3.5179 (-3.4855)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0005 (-0.0005)\tD repr loss -0.2473 (-0.1655)\n",
            "Epoch: [2][ 54/781]\tTime  0.636 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.2454 (3.5179)\tD(fake)1 -3.2840 (-3.4550)\tD(fake)2 -2.8417 (-3.4300)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0111 (0.0011)\tD repr loss -0.1497 (-0.1674)\n",
            "Epoch: [2][104/781]\tTime  0.639 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.5206 (3.4886)\tD(fake)1 -2.8593 (-3.2831)\tD(fake)2 -2.6910 (-3.2406)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0032 (0.0006)\tD repr loss -0.2476 (-0.1691)\n",
            "Epoch: [2][154/781]\tTime  0.649 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.0133 (3.4645)\tD(fake)1 -3.2494 (-3.2887)\tD(fake)2 -3.4449 (-3.2556)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0035 (0.0008)\tD repr loss -0.1210 (-0.1688)\n",
            "Epoch: [2][204/781]\tTime  0.636 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.6935 (3.4518)\tD(fake)1 -3.5445 (-3.2836)\tD(fake)2 -3.1034 (-3.2406)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0040 (0.0011)\tD repr loss -0.2468 (-0.1692)\n",
            "Epoch: [2][254/781]\tTime  0.639 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 1.9165 (3.4564)\tD(fake)1 -3.6494 (-3.2923)\tD(fake)2 -4.0722 (-3.2501)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0008 (0.0007)\tD repr loss -0.2356 (-0.1683)\n",
            "Epoch: [2][304/781]\tTime  0.635 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.2391 (3.4993)\tD(fake)1 -3.0618 (-3.2785)\tD(fake)2 -3.1892 (-3.2375)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0041 (0.0007)\tD repr loss -0.2078 (-0.1682)\n",
            "Epoch: [2][354/781]\tTime  0.641 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 2.9707 (3.5357)\tD(fake)1 -3.0322 (-3.2368)\tD(fake)2 -3.3554 (-3.1870)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0030 (0.0006)\tD repr loss -0.2059 (-0.1682)\n",
            "Epoch: [2][404/781]\tTime  0.639 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.7442 (3.5658)\tD(fake)1 -3.6183 (-3.2562)\tD(fake)2 -4.1886 (-3.2182)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0006 (0.0005)\tD repr loss -0.1667 (-0.1687)\n",
            "Epoch: [2][454/781]\tTime  0.632 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 4.3071 (3.6113)\tD(fake)1 -3.6308 (-3.2811)\tD(fake)2 -3.3008 (-3.2380)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0004 (0.0003)\tD repr loss -0.2703 (-0.1704)\n",
            "Epoch: [2][504/781]\tTime  0.647 ( 0.641)\tData  0.000 ( 0.000)\tD(real) 3.5894 (3.6200)\tD(fake)1 -3.6495 (-3.2879)\tD(fake)2 -3.5672 (-3.2605)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0020 (0.0004)\tD repr loss -0.2189 (-0.1707)\n",
            "Epoch: [2][554/781]\tTime  0.652 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.2723 (3.6375)\tD(fake)1 -3.4282 (-3.3139)\tD(fake)2 -3.2951 (-3.2789)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0037 (0.0004)\tD repr loss -0.1546 (-0.1695)\n",
            "Epoch: [2][604/781]\tTime  0.632 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 2.7552 (3.6601)\tD(fake)1 -3.8108 (-3.3212)\tD(fake)2 -4.1639 (-3.2898)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0044 (0.0004)\tD repr loss -0.1635 (-0.1691)\n",
            "Epoch: [2][654/781]\tTime  0.650 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.6119 (3.6670)\tD(fake)1 -3.2388 (-3.3264)\tD(fake)2 -3.4906 (-3.2940)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0013 (0.0004)\tD repr loss -0.2252 (-0.1694)\n",
            "Epoch: [2][704/781]\tTime  0.631 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.4095 (3.6887)\tD(fake)1 -3.3815 (-3.3308)\tD(fake)2 -3.3197 (-3.2967)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0055 (0.0003)\tD repr loss -0.1777 (-0.1691)\n",
            "Epoch: [2][754/781]\tTime  0.635 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.6108 (3.7147)\tD(fake)1 -3.4370 (-3.3341)\tD(fake)2 -3.3423 (-3.3026)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0040 (0.0002)\tD repr loss -0.1619 (-0.1692)\n",
            "Epoch: [2][780/781]\tTime  0.643 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.6879 (3.7142)\tD(fake)1 -3.4292 (-3.3401)\tD(fake)2 -3.2634 (-3.3037)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0023 (0.0002)\tD repr loss -0.1885 (-0.1692)\n",
            "Epoch: [3][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.8997 (3.6252)\tD(fake)1 -3.4169 (-3.4169)\tD(fake)2 -3.7375 (-3.4667)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0015 (-0.0015)\tD repr loss -0.1120 (-0.1756)\n",
            "Epoch: [3][ 54/781]\tTime  0.647 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 3.2977 (3.7249)\tD(fake)1 -3.4825 (-3.8260)\tD(fake)2 -3.8587 (-3.8460)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0001 (-0.0001)\tD repr loss -0.1790 (-0.1720)\n",
            "Epoch: [3][104/781]\tTime  0.650 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.1415 (3.6456)\tD(fake)1 -3.8446 (-3.7043)\tD(fake)2 -3.8440 (-3.7155)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0019 (0.0005)\tD repr loss -0.2488 (-0.1749)\n",
            "Epoch: [3][154/781]\tTime  0.639 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.5225 (3.6653)\tD(fake)1 -3.4164 (-3.7035)\tD(fake)2 -3.6278 (-3.6598)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0028 (0.0005)\tD repr loss -0.2014 (-0.1750)\n",
            "Epoch: [3][204/781]\tTime  0.630 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.9957 (3.7123)\tD(fake)1 -3.3999 (-3.6656)\tD(fake)2 -3.4073 (-3.6192)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0030 (0.0007)\tD repr loss -0.1496 (-0.1733)\n",
            "Epoch: [3][254/781]\tTime  0.651 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.8310 (3.7489)\tD(fake)1 -3.4380 (-3.6723)\tD(fake)2 -3.6565 (-3.6212)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0004 (0.0005)\tD repr loss -0.2024 (-0.1737)\n",
            "Epoch: [3][304/781]\tTime  0.650 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.9436 (3.7524)\tD(fake)1 -3.4514 (-3.6310)\tD(fake)2 -3.3624 (-3.5918)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0029 (0.0004)\tD repr loss -0.1939 (-0.1724)\n",
            "Epoch: [3][354/781]\tTime  0.645 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.2238 (3.7300)\tD(fake)1 -4.0952 (-3.6276)\tD(fake)2 -3.7743 (-3.5802)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0008 (0.0006)\tD repr loss -0.2265 (-0.1714)\n",
            "Epoch: [3][404/781]\tTime  0.653 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.1005 (3.7304)\tD(fake)1 -3.6769 (-3.6054)\tD(fake)2 -2.6092 (-3.5513)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0010 (0.0007)\tD repr loss -0.2696 (-0.1710)\n",
            "Epoch: [3][454/781]\tTime  0.637 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.6757 (3.6967)\tD(fake)1 -3.3193 (-3.5438)\tD(fake)2 -3.7057 (-3.5154)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0042 (0.0007)\tD repr loss -0.2210 (-0.1718)\n",
            "Epoch: [3][504/781]\tTime  0.630 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.4047 (3.6834)\tD(fake)1 -2.9736 (-3.5210)\tD(fake)2 -3.1236 (-3.4949)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0018 (0.0007)\tD repr loss -0.2443 (-0.1718)\n",
            "Epoch: [3][554/781]\tTime  0.649 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.0910 (3.6642)\tD(fake)1 -3.1995 (-3.5014)\tD(fake)2 -3.2154 (-3.4766)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0015 (0.0006)\tD repr loss -0.1885 (-0.1717)\n",
            "Epoch: [3][604/781]\tTime  0.634 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.5097 (3.6458)\tD(fake)1 -3.8079 (-3.5000)\tD(fake)2 -3.5168 (-3.4782)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0019 (0.0005)\tD repr loss -0.1678 (-0.1719)\n",
            "Epoch: [3][654/781]\tTime  0.640 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.7414 (3.6401)\tD(fake)1 -3.2528 (-3.4868)\tD(fake)2 -3.3199 (-3.4621)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0006 (0.0005)\tD repr loss -0.2227 (-0.1717)\n",
            "Epoch: [3][704/781]\tTime  0.646 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.3211 (3.6492)\tD(fake)1 -3.3545 (-3.4851)\tD(fake)2 -3.0408 (-3.4565)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0020 (0.0005)\tD repr loss -0.2276 (-0.1718)\n",
            "Epoch: [3][754/781]\tTime  0.639 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.8539 (3.6411)\tD(fake)1 -3.8194 (-3.4996)\tD(fake)2 -3.8142 (-3.4685)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0048 (0.0005)\tD repr loss -0.0964 (-0.1720)\n",
            "Epoch: [3][780/781]\tTime  0.634 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.6889 (3.6524)\tD(fake)1 -3.6665 (-3.5094)\tD(fake)2 -3.6668 (-3.4784)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0041 (0.0006)\tD repr loss -0.2140 (-0.1719)\n",
            "Epoch: [4][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.6884 (3.8223)\tD(fake)1 -3.5288 (-3.5288)\tD(fake)2 -3.8151 (-3.7955)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0004 (0.0004)\tD repr loss -0.2083 (-0.1776)\n",
            "Epoch: [4][ 54/781]\tTime  0.641 ( 0.648)\tData  0.000 ( 0.000)\tD(real) 3.5277 (3.6153)\tD(fake)1 -3.5308 (-3.5688)\tD(fake)2 -3.7037 (-3.6122)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0046 (0.0018)\tD repr loss -0.1843 (-0.1763)\n",
            "Epoch: [4][104/781]\tTime  0.645 ( 0.648)\tData  0.000 ( 0.000)\tD(real) 4.7252 (3.6980)\tD(fake)1 -3.7154 (-3.6137)\tD(fake)2 -3.9870 (-3.6364)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0010 (0.0014)\tD repr loss -0.2879 (-0.1736)\n",
            "Epoch: [4][154/781]\tTime  0.644 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 4.0738 (3.7357)\tD(fake)1 -3.3352 (-3.6391)\tD(fake)2 -3.0644 (-3.6357)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0051 (0.0006)\tD repr loss -0.1999 (-0.1714)\n",
            "Epoch: [4][204/781]\tTime  0.648 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.9458 (3.7606)\tD(fake)1 -3.2630 (-3.5109)\tD(fake)2 -3.0655 (-3.5199)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0029 (0.0004)\tD repr loss -0.1509 (-0.1731)\n",
            "Epoch: [4][254/781]\tTime  0.648 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.7921 (3.7514)\tD(fake)1 -3.5874 (-3.4750)\tD(fake)2 -3.6412 (-3.4897)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0045 (-0.0001)\tD repr loss -0.1985 (-0.1720)\n",
            "Epoch: [4][304/781]\tTime  0.640 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 4.4712 (3.7667)\tD(fake)1 -3.5538 (-3.4555)\tD(fake)2 -3.5737 (-3.4721)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0039 (0.0000)\tD repr loss -0.2550 (-0.1714)\n",
            "Epoch: [4][354/781]\tTime  0.656 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.9109 (3.7922)\tD(fake)1 -3.2949 (-3.4585)\tD(fake)2 -3.4471 (-3.4664)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0037 (0.0001)\tD repr loss -0.2116 (-0.1725)\n",
            "Epoch: [4][404/781]\tTime  0.640 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.5116 (3.7884)\tD(fake)1 -3.3730 (-3.4269)\tD(fake)2 -3.2820 (-3.4327)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0005 (0.0001)\tD repr loss -0.2345 (-0.1721)\n",
            "Epoch: [4][454/781]\tTime  0.652 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.9456 (3.7919)\tD(fake)1 -3.3878 (-3.4011)\tD(fake)2 -3.5624 (-3.4077)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0008 (0.0001)\tD repr loss -0.2607 (-0.1724)\n",
            "Epoch: [4][504/781]\tTime  0.636 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.8992 (3.7625)\tD(fake)1 -3.9378 (-3.4089)\tD(fake)2 -3.9062 (-3.4052)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0004 (0.0001)\tD repr loss -0.1767 (-0.1730)\n",
            "Epoch: [4][554/781]\tTime  0.641 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.5360 (3.7472)\tD(fake)1 -3.1025 (-3.3988)\tD(fake)2 -2.9018 (-3.3894)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0003 (0.0001)\tD repr loss -0.1884 (-0.1733)\n",
            "Epoch: [4][604/781]\tTime  0.647 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.7786 (3.7428)\tD(fake)1 -2.8615 (-3.3772)\tD(fake)2 -3.9351 (-3.3851)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0027 (0.0001)\tD repr loss -0.2513 (-0.1735)\n",
            "Epoch: [4][654/781]\tTime  0.629 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.9693 (3.7552)\tD(fake)1 -3.4955 (-3.3946)\tD(fake)2 -3.5540 (-3.4043)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0010 (0.0000)\tD repr loss -0.1672 (-0.1725)\n",
            "Epoch: [4][704/781]\tTime  0.631 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.2795 (3.7676)\tD(fake)1 -3.6241 (-3.4016)\tD(fake)2 -4.0025 (-3.4077)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0003 (0.0000)\tD repr loss -0.1947 (-0.1726)\n",
            "Epoch: [4][754/781]\tTime  0.636 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.4654 (3.7725)\tD(fake)1 -3.0947 (-3.4115)\tD(fake)2 -3.4702 (-3.4141)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0002 (0.0001)\tD repr loss -0.2030 (-0.1727)\n",
            "Epoch: [4][780/781]\tTime  0.637 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.4079 (3.7686)\tD(fake)1 -3.4004 (-3.4114)\tD(fake)2 -3.4353 (-3.4165)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0009 (0.0001)\tD repr loss -0.2319 (-0.1721)\n",
            "Epoch: [5][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.9536 (4.2152)\tD(fake)1 -3.4652 (-3.4652)\tD(fake)2 -3.4792 (-3.4313)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0018 (-0.0018)\tD repr loss -0.2317 (-0.1796)\n",
            "Epoch: [5][ 54/781]\tTime  0.643 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 2.9735 (3.8989)\tD(fake)1 -2.9390 (-3.2898)\tD(fake)2 -3.2041 (-3.3201)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0033 (0.0011)\tD repr loss -0.2210 (-0.1815)\n",
            "Epoch: [5][104/781]\tTime  0.648 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 4.0683 (3.7644)\tD(fake)1 -3.1344 (-3.2540)\tD(fake)2 -3.2589 (-3.2182)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0052 (0.0011)\tD repr loss -0.2122 (-0.1781)\n",
            "Epoch: [5][154/781]\tTime  0.641 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.5345 (3.7245)\tD(fake)1 -3.2366 (-3.2640)\tD(fake)2 -3.0953 (-3.2522)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0024 (0.0005)\tD repr loss -0.1593 (-0.1769)\n",
            "Epoch: [5][204/781]\tTime  0.640 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.3912 (3.6443)\tD(fake)1 -3.3145 (-3.2480)\tD(fake)2 -4.0428 (-3.2492)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0031 (-0.0001)\tD repr loss -0.2924 (-0.1769)\n",
            "Epoch: [5][254/781]\tTime  0.647 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 4.0608 (3.6369)\tD(fake)1 -2.8398 (-3.2234)\tD(fake)2 -2.5102 (-3.2219)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0034 (-0.0002)\tD repr loss -0.2074 (-0.1767)\n",
            "Epoch: [5][304/781]\tTime  0.665 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.6046 (3.6127)\tD(fake)1 -2.7181 (-3.1846)\tD(fake)2 -2.9351 (-3.1865)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0022 (-0.0002)\tD repr loss -0.1616 (-0.1758)\n",
            "Epoch: [5][354/781]\tTime  0.633 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 2.9938 (3.6040)\tD(fake)1 -3.6230 (-3.1840)\tD(fake)2 -3.7541 (-3.1803)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0024 (-0.0001)\tD repr loss -0.2210 (-0.1760)\n",
            "Epoch: [5][404/781]\tTime  0.646 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.7444 (3.6212)\tD(fake)1 -2.8836 (-3.2061)\tD(fake)2 -3.0898 (-3.2044)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0019 (-0.0000)\tD repr loss -0.2102 (-0.1753)\n",
            "Epoch: [5][454/781]\tTime  0.635 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.0640 (3.5903)\tD(fake)1 -3.2950 (-3.1955)\tD(fake)2 -3.1723 (-3.1886)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0016 (0.0001)\tD repr loss -0.2041 (-0.1752)\n",
            "Epoch: [5][504/781]\tTime  0.637 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.8150 (3.5642)\tD(fake)1 -3.6381 (-3.2013)\tD(fake)2 -3.0310 (-3.1919)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0043 (0.0002)\tD repr loss -0.2063 (-0.1748)\n",
            "Epoch: [5][554/781]\tTime  0.629 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.5662 (3.5637)\tD(fake)1 -2.8622 (-3.1848)\tD(fake)2 -2.9135 (-3.1759)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0037 (0.0003)\tD repr loss -0.1639 (-0.1749)\n",
            "Epoch: [5][604/781]\tTime  0.629 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.8972 (3.5693)\tD(fake)1 -3.5669 (-3.1832)\tD(fake)2 -2.8017 (-3.1759)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0032 (0.0003)\tD repr loss -0.1317 (-0.1751)\n",
            "Epoch: [5][654/781]\tTime  0.638 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.6395 (3.5669)\tD(fake)1 -3.0563 (-3.1561)\tD(fake)2 -3.1424 (-3.1536)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0005 (0.0002)\tD repr loss -0.1938 (-0.1751)\n",
            "Epoch: [5][704/781]\tTime  0.643 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.9288 (3.5846)\tD(fake)1 -2.7024 (-3.1404)\tD(fake)2 -2.7073 (-3.1391)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0034 (0.0003)\tD repr loss -0.1735 (-0.1754)\n",
            "Epoch: [5][754/781]\tTime  0.635 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.6447 (3.5899)\tD(fake)1 -2.6592 (-3.1280)\tD(fake)2 -3.4355 (-3.1307)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0015 (0.0003)\tD repr loss -0.1959 (-0.1749)\n",
            "Epoch: [5][780/781]\tTime  0.633 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.3721 (3.5826)\tD(fake)1 -2.5815 (-3.1222)\tD(fake)2 -2.9230 (-3.1316)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0043 (0.0003)\tD repr loss -0.1620 (-0.1751)\n",
            "Epoch: [6][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.4482 (3.4448)\tD(fake)1 -3.0432 (-3.0432)\tD(fake)2 -2.7641 (-2.6723)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0011 (0.0011)\tD repr loss -0.2345 (-0.1755)\n",
            "Epoch: [6][ 54/781]\tTime  0.651 ( 0.650)\tData  0.000 ( 0.000)\tD(real) 4.0702 (3.3720)\tD(fake)1 -3.0791 (-3.0186)\tD(fake)2 -3.1436 (-3.0508)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0054 (0.0006)\tD repr loss -0.1644 (-0.1680)\n",
            "Epoch: [6][104/781]\tTime  0.630 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.6341 (3.5033)\tD(fake)1 -3.3418 (-3.0988)\tD(fake)2 -3.5381 (-3.1008)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0018 (0.0003)\tD repr loss -0.3017 (-0.1747)\n",
            "Epoch: [6][154/781]\tTime  0.645 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.1354 (3.5542)\tD(fake)1 -2.6866 (-3.0281)\tD(fake)2 -2.9006 (-3.0611)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0008 (0.0004)\tD repr loss -0.2204 (-0.1745)\n",
            "Epoch: [6][204/781]\tTime  0.630 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.4187 (3.5801)\tD(fake)1 -2.9581 (-3.0269)\tD(fake)2 -3.3353 (-3.0631)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0016 (0.0003)\tD repr loss -0.2262 (-0.1745)\n",
            "Epoch: [6][254/781]\tTime  0.636 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.1740 (3.5603)\tD(fake)1 -3.2046 (-3.0709)\tD(fake)2 -3.1904 (-3.0932)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0004 (0.0004)\tD repr loss -0.3023 (-0.1755)\n",
            "Epoch: [6][304/781]\tTime  0.677 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.9589 (3.5643)\tD(fake)1 -2.9705 (-3.1136)\tD(fake)2 -3.7096 (-3.1276)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0009 (0.0004)\tD repr loss -0.2518 (-0.1740)\n",
            "Epoch: [6][354/781]\tTime  0.648 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.8153 (3.5778)\tD(fake)1 -2.9991 (-3.1371)\tD(fake)2 -3.0169 (-3.1600)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0038 (0.0002)\tD repr loss -0.2586 (-0.1746)\n",
            "Epoch: [6][404/781]\tTime  0.640 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.1738 (3.6054)\tD(fake)1 -2.9948 (-3.1483)\tD(fake)2 -3.1147 (-3.1637)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0015 (0.0002)\tD repr loss -0.2481 (-0.1754)\n",
            "Epoch: [6][454/781]\tTime  0.635 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.0281 (3.6024)\tD(fake)1 -2.9968 (-3.1593)\tD(fake)2 -3.1240 (-3.1686)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0037 (0.0001)\tD repr loss -0.1455 (-0.1746)\n",
            "Epoch: [6][504/781]\tTime  0.646 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.6910 (3.6019)\tD(fake)1 -3.1125 (-3.1466)\tD(fake)2 -2.9805 (-3.1676)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0008 (0.0001)\tD repr loss -0.3163 (-0.1754)\n",
            "Epoch: [6][554/781]\tTime  0.653 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.9083 (3.6285)\tD(fake)1 -3.8436 (-3.1680)\tD(fake)2 -3.2154 (-3.1821)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0038 (0.0001)\tD repr loss -0.2737 (-0.1763)\n",
            "Epoch: [6][604/781]\tTime  0.632 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.4046 (3.6455)\tD(fake)1 -3.0619 (-3.1672)\tD(fake)2 -3.3086 (-3.1796)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0014 (0.0001)\tD repr loss -0.1952 (-0.1761)\n",
            "Epoch: [6][654/781]\tTime  0.644 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.5828 (3.6698)\tD(fake)1 -2.5821 (-3.1434)\tD(fake)2 -3.0324 (-3.1627)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0005 (0.0000)\tD repr loss -0.2261 (-0.1767)\n",
            "Epoch: [6][704/781]\tTime  0.674 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.5016 (3.6859)\tD(fake)1 -2.8327 (-3.1286)\tD(fake)2 -2.7446 (-3.1568)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0008 (-0.0000)\tD repr loss -0.1824 (-0.1765)\n",
            "Epoch: [6][754/781]\tTime  0.653 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.1788 (3.6842)\tD(fake)1 -3.1175 (-3.1126)\tD(fake)2 -2.9289 (-3.1331)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0031 (-0.0001)\tD repr loss -0.2168 (-0.1765)\n",
            "Epoch: [6][780/781]\tTime  0.655 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 4.3520 (3.6909)\tD(fake)1 -3.1800 (-3.1073)\tD(fake)2 -3.0818 (-3.1274)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0028 (-0.0001)\tD repr loss -0.2138 (-0.1765)\n",
            "Epoch: [7][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 4.1783 (4.3644)\tD(fake)1 -3.1398 (-3.1398)\tD(fake)2 -3.0558 (-2.9959)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0009 (-0.0009)\tD repr loss -0.1977 (-0.1682)\n",
            "Epoch: [7][ 54/781]\tTime  0.635 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 3.6611 (3.7358)\tD(fake)1 -2.4433 (-2.9646)\tD(fake)2 -3.2371 (-2.9943)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0048 (0.0013)\tD repr loss -0.2438 (-0.1811)\n",
            "Epoch: [7][104/781]\tTime  0.661 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 4.2472 (3.9043)\tD(fake)1 -3.3878 (-3.0106)\tD(fake)2 -3.2303 (-3.0455)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0012 (0.0007)\tD repr loss -0.1644 (-0.1752)\n",
            "Epoch: [7][154/781]\tTime  0.644 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.3968 (3.9629)\tD(fake)1 -2.6653 (-3.0941)\tD(fake)2 -3.1877 (-3.1369)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0005 (0.0009)\tD repr loss -0.2331 (-0.1770)\n",
            "Epoch: [7][204/781]\tTime  0.642 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.5386 (3.9772)\tD(fake)1 -3.5456 (-3.1658)\tD(fake)2 -3.3572 (-3.2050)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0033 (0.0007)\tD repr loss -0.2259 (-0.1760)\n",
            "Epoch: [7][254/781]\tTime  0.645 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.2284 (3.9340)\tD(fake)1 -3.6704 (-3.1723)\tD(fake)2 -3.8873 (-3.2007)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0010 (0.0006)\tD repr loss -0.2022 (-0.1757)\n",
            "Epoch: [7][304/781]\tTime  0.638 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.1985 (3.9484)\tD(fake)1 -3.1613 (-3.2105)\tD(fake)2 -3.1441 (-3.2435)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0009 (0.0005)\tD repr loss -0.1574 (-0.1751)\n",
            "Epoch: [7][354/781]\tTime  0.638 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.3172 (3.9815)\tD(fake)1 -3.2305 (-3.2648)\tD(fake)2 -2.7302 (-3.2868)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0000 (0.0004)\tD repr loss -0.1955 (-0.1749)\n",
            "Epoch: [7][404/781]\tTime  0.664 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.5159 (3.9823)\tD(fake)1 -3.5683 (-3.2373)\tD(fake)2 -2.5973 (-3.2708)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0002 (0.0005)\tD repr loss -0.2452 (-0.1746)\n",
            "Epoch: [7][454/781]\tTime  0.659 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.8549 (3.9662)\tD(fake)1 -3.1131 (-3.2181)\tD(fake)2 -3.2197 (-3.2386)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0019 (0.0003)\tD repr loss -0.2458 (-0.1756)\n",
            "Epoch: [7][504/781]\tTime  0.636 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.0477 (3.9585)\tD(fake)1 -3.2867 (-3.2147)\tD(fake)2 -3.2812 (-3.2373)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0052 (0.0004)\tD repr loss -0.2809 (-0.1765)\n",
            "Epoch: [7][554/781]\tTime  0.646 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.1876 (3.9457)\tD(fake)1 -3.4079 (-3.2067)\tD(fake)2 -3.2137 (-3.2271)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0014 (0.0003)\tD repr loss -0.2049 (-0.1763)\n",
            "Epoch: [7][604/781]\tTime  0.646 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.0880 (3.9590)\tD(fake)1 -3.2376 (-3.2125)\tD(fake)2 -2.9227 (-3.2226)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0000 (0.0004)\tD repr loss -0.1845 (-0.1762)\n",
            "Epoch: [7][654/781]\tTime  0.639 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 2.7623 (3.9452)\tD(fake)1 -2.8995 (-3.2022)\tD(fake)2 -3.0949 (-3.2102)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0036 (0.0003)\tD repr loss -0.1932 (-0.1762)\n",
            "Epoch: [7][704/781]\tTime  0.644 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.8228 (3.9194)\tD(fake)1 -3.6091 (-3.1970)\tD(fake)2 -3.6678 (-3.2044)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0029 (0.0003)\tD repr loss -0.0980 (-0.1763)\n",
            "Epoch: [7][754/781]\tTime  0.651 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.0790 (3.8826)\tD(fake)1 -2.2099 (-3.1795)\tD(fake)2 -2.9881 (-3.1849)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0018 (0.0003)\tD repr loss -0.1703 (-0.1762)\n",
            "Epoch: [7][780/781]\tTime  0.643 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.5804 (3.8721)\tD(fake)1 -2.7769 (-3.1601)\tD(fake)2 -2.7122 (-3.1722)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0040 (0.0004)\tD repr loss -0.2002 (-0.1763)\n",
            "Epoch: [8][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.0472 (3.3369)\tD(fake)1 -2.6709 (-2.6709)\tD(fake)2 -3.0806 (-2.8788)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0069 (0.0069)\tD repr loss -0.1907 (-0.1442)\n",
            "Epoch: [8][ 54/781]\tTime  0.650 ( 0.658)\tData  0.000 ( 0.000)\tD(real) 3.8524 (3.7536)\tD(fake)1 -3.0705 (-2.9607)\tD(fake)2 -3.5660 (-2.9618)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0029 (0.0007)\tD repr loss -0.2015 (-0.1685)\n",
            "Epoch: [8][104/781]\tTime  0.640 ( 0.652)\tData  0.000 ( 0.000)\tD(real) 4.7599 (3.8822)\tD(fake)1 -3.0661 (-3.1306)\tD(fake)2 -2.7913 (-3.1261)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0013 (0.0006)\tD repr loss -0.2157 (-0.1739)\n",
            "Epoch: [8][154/781]\tTime  0.648 ( 0.649)\tData  0.000 ( 0.000)\tD(real) 3.6909 (3.8416)\tD(fake)1 -2.8114 (-3.1006)\tD(fake)2 -3.2836 (-3.1088)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0039 (0.0000)\tD repr loss -0.2198 (-0.1765)\n",
            "Epoch: [8][204/781]\tTime  0.638 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.5826 (3.8169)\tD(fake)1 -2.8237 (-3.0840)\tD(fake)2 -3.0076 (-3.1134)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0021 (0.0001)\tD repr loss -0.2907 (-0.1797)\n",
            "Epoch: [8][254/781]\tTime  0.639 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.7390 (3.8103)\tD(fake)1 -2.6202 (-3.0806)\tD(fake)2 -3.0278 (-3.1026)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0044 (-0.0003)\tD repr loss -0.1874 (-0.1801)\n",
            "Epoch: [8][304/781]\tTime  0.640 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 4.5131 (3.8533)\tD(fake)1 -2.6675 (-3.0925)\tD(fake)2 -3.0760 (-3.0994)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0008 (-0.0002)\tD repr loss -0.2300 (-0.1796)\n",
            "Epoch: [8][354/781]\tTime  0.642 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 2.8709 (3.8524)\tD(fake)1 -2.8903 (-3.0818)\tD(fake)2 -3.3325 (-3.0906)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0010 (-0.0001)\tD repr loss -0.1285 (-0.1808)\n",
            "Epoch: [8][404/781]\tTime  0.646 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.9889 (3.8279)\tD(fake)1 -2.8233 (-3.0423)\tD(fake)2 -2.9240 (-3.0530)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0068 (-0.0001)\tD repr loss -0.1228 (-0.1802)\n",
            "Epoch: [8][454/781]\tTime  0.636 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 2.0136 (3.8376)\tD(fake)1 -2.9888 (-3.0466)\tD(fake)2 -3.3002 (-3.0601)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0035 (-0.0002)\tD repr loss -0.1845 (-0.1792)\n",
            "Epoch: [8][504/781]\tTime  0.633 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.4492 (3.8169)\tD(fake)1 -3.4054 (-3.0420)\tD(fake)2 -2.8228 (-3.0517)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0011 (-0.0002)\tD repr loss -0.1687 (-0.1794)\n",
            "Epoch: [8][554/781]\tTime  0.652 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.0318 (3.8216)\tD(fake)1 -3.3159 (-3.0584)\tD(fake)2 -3.2760 (-3.0691)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0013 (-0.0003)\tD repr loss -0.1350 (-0.1795)\n",
            "Epoch: [8][604/781]\tTime  0.646 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.8306 (3.8122)\tD(fake)1 -3.4699 (-3.0944)\tD(fake)2 -3.3615 (-3.1040)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0013 (-0.0002)\tD repr loss -0.2367 (-0.1794)\n",
            "Epoch: [8][654/781]\tTime  0.637 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.5428 (3.8153)\tD(fake)1 -3.5073 (-3.1137)\tD(fake)2 -3.3089 (-3.1148)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0022 (-0.0001)\tD repr loss -0.1950 (-0.1788)\n",
            "Epoch: [8][704/781]\tTime  0.642 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.1210 (3.8178)\tD(fake)1 -3.2836 (-3.1186)\tD(fake)2 -3.1095 (-3.1176)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0011 (-0.0001)\tD repr loss -0.2193 (-0.1785)\n",
            "Epoch: [8][754/781]\tTime  0.642 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.8770 (3.8138)\tD(fake)1 -3.6547 (-3.1273)\tD(fake)2 -3.5230 (-3.1195)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0016 (-0.0001)\tD repr loss -0.2622 (-0.1783)\n",
            "Epoch: [8][780/781]\tTime  0.642 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.2656 (3.8238)\tD(fake)1 -2.8768 (-3.1262)\tD(fake)2 -2.7410 (-3.1192)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0030 (-0.0000)\tD repr loss -0.2196 (-0.1782)\n",
            "Epoch: [9][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.9026 (3.6995)\tD(fake)1 -3.0783 (-3.0783)\tD(fake)2 -2.9910 (-2.9711)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0031 (0.0031)\tD repr loss -0.2492 (-0.2087)\n",
            "Epoch: [9][ 54/781]\tTime  0.646 ( 0.654)\tData  0.000 ( 0.000)\tD(real) 3.6391 (3.8050)\tD(fake)1 -2.9490 (-3.2907)\tD(fake)2 -3.0826 (-3.1657)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0016 (0.0011)\tD repr loss -0.1315 (-0.1847)\n",
            "Epoch: [9][104/781]\tTime  0.636 ( 0.651)\tData  0.000 ( 0.000)\tD(real) 4.5386 (3.7977)\tD(fake)1 -3.0084 (-3.1787)\tD(fake)2 -3.1587 (-3.1131)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0009 (0.0014)\tD repr loss -0.1828 (-0.1784)\n",
            "Epoch: [9][154/781]\tTime  0.656 ( 0.651)\tData  0.000 ( 0.000)\tD(real) 3.6110 (3.8247)\tD(fake)1 -3.5385 (-3.1828)\tD(fake)2 -3.2931 (-3.1431)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0070 (0.0010)\tD repr loss -0.1845 (-0.1785)\n",
            "Epoch: [9][204/781]\tTime  0.645 ( 0.652)\tData  0.000 ( 0.000)\tD(real) 4.3955 (3.8937)\tD(fake)1 -3.2184 (-3.2313)\tD(fake)2 -3.3881 (-3.1989)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0034 (0.0008)\tD repr loss -0.1434 (-0.1813)\n",
            "Epoch: [9][254/781]\tTime  0.645 ( 0.653)\tData  0.000 ( 0.000)\tD(real) 3.8806 (3.8878)\tD(fake)1 -3.1927 (-3.2387)\tD(fake)2 -3.3786 (-3.2095)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0010 (0.0004)\tD repr loss -0.1830 (-0.1816)\n",
            "Epoch: [9][304/781]\tTime  0.640 ( 0.652)\tData  0.000 ( 0.000)\tD(real) 4.0980 (3.8974)\tD(fake)1 -3.5440 (-3.2650)\tD(fake)2 -3.0901 (-3.2373)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0014 (0.0003)\tD repr loss -0.2478 (-0.1809)\n",
            "Epoch: [9][354/781]\tTime  0.641 ( 0.651)\tData  0.000 ( 0.000)\tD(real) 4.3944 (3.9114)\tD(fake)1 -3.3090 (-3.2346)\tD(fake)2 -3.0015 (-3.2129)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0040 (0.0003)\tD repr loss -0.2150 (-0.1794)\n",
            "Epoch: [9][404/781]\tTime  0.640 ( 0.650)\tData  0.000 ( 0.000)\tD(real) 3.6922 (3.8909)\tD(fake)1 -3.2340 (-3.2170)\tD(fake)2 -3.2988 (-3.1975)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0047 (0.0003)\tD repr loss -0.1780 (-0.1795)\n",
            "Epoch: [9][454/781]\tTime  0.646 ( 0.649)\tData  0.000 ( 0.000)\tD(real) 3.8656 (3.8743)\tD(fake)1 -3.0881 (-3.1971)\tD(fake)2 -3.0050 (-3.1802)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0051 (0.0002)\tD repr loss -0.2554 (-0.1784)\n",
            "Epoch: [9][504/781]\tTime  0.637 ( 0.648)\tData  0.000 ( 0.000)\tD(real) 4.0353 (3.8922)\tD(fake)1 -3.4138 (-3.1927)\tD(fake)2 -3.5672 (-3.1785)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0016 (0.0000)\tD repr loss -0.1737 (-0.1779)\n",
            "Epoch: [9][554/781]\tTime  0.641 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 4.3365 (3.9003)\tD(fake)1 -3.3391 (-3.2036)\tD(fake)2 -3.1714 (-3.1851)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0001 (0.0000)\tD repr loss -0.2065 (-0.1787)\n",
            "Epoch: [9][604/781]\tTime  0.635 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 3.5598 (3.8858)\tD(fake)1 -2.7602 (-3.2019)\tD(fake)2 -2.6579 (-3.1830)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0005 (0.0001)\tD repr loss -0.2162 (-0.1780)\n",
            "Epoch: [9][654/781]\tTime  0.639 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 3.4994 (3.8731)\tD(fake)1 -2.9334 (-3.1816)\tD(fake)2 -2.8418 (-3.1631)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0017 (0.0002)\tD repr loss -0.1949 (-0.1780)\n",
            "Epoch: [9][704/781]\tTime  0.646 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 4.1662 (3.8620)\tD(fake)1 -2.8242 (-3.1609)\tD(fake)2 -2.9667 (-3.1474)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0025 (0.0002)\tD repr loss -0.1903 (-0.1778)\n",
            "Epoch: [9][754/781]\tTime  0.640 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 3.2594 (3.8410)\tD(fake)1 -2.9426 (-3.1531)\tD(fake)2 -3.4137 (-3.1403)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0011 (0.0001)\tD repr loss -0.1766 (-0.1774)\n",
            "Epoch: [9][780/781]\tTime  0.630 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.9154 (3.8369)\tD(fake)1 -3.2018 (-3.1590)\tD(fake)2 -3.3018 (-3.1438)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0044 (0.0002)\tD repr loss -0.2552 (-0.1777)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nUGjJ-6Pf3xE",
        "outputId": "ef525364-b235-47ce-e98f-86a4d2b28f0a"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.7846 (3.6550)\tD(fake)1 -3.3851 (-3.3851)\tD(fake)2 -3.0702 (-3.1549)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0022 (0.0022)\tD repr loss -0.1921 (-0.1686)\n",
            "Epoch: [0][ 54/781]\tTime  0.644 ( 0.652)\tData  0.000 ( 0.000)\tD(real) 4.2348 (3.7681)\tD(fake)1 -2.9805 (-3.1984)\tD(fake)2 -2.6303 (-3.1813)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0030 (0.0006)\tD repr loss -0.1839 (-0.1703)\n",
            "Epoch: [0][104/781]\tTime  0.637 ( 0.649)\tData  0.000 ( 0.000)\tD(real) 3.4767 (3.7537)\tD(fake)1 -3.0847 (-3.0837)\tD(fake)2 -2.8520 (-3.0217)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0053 (0.0008)\tD repr loss -0.2170 (-0.1757)\n",
            "Epoch: [0][154/781]\tTime  0.635 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.1425 (3.6950)\tD(fake)1 -2.6433 (-2.9799)\tD(fake)2 -2.5460 (-2.9452)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0044 (0.0004)\tD repr loss -0.2222 (-0.1779)\n",
            "Epoch: [0][204/781]\tTime  0.642 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.3127 (3.6664)\tD(fake)1 -3.0211 (-2.9370)\tD(fake)2 -3.0986 (-2.9156)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0017 (0.0003)\tD repr loss -0.2179 (-0.1791)\n",
            "Epoch: [0][254/781]\tTime  0.635 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.6439 (3.6536)\tD(fake)1 -2.3847 (-2.9562)\tD(fake)2 -3.0592 (-2.9478)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0007 (0.0003)\tD repr loss -0.2051 (-0.1794)\n",
            "Epoch: [0][304/781]\tTime  0.639 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.0005 (3.6712)\tD(fake)1 -2.9284 (-2.9498)\tD(fake)2 -2.8613 (-2.9549)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0021 (0.0003)\tD repr loss -0.2419 (-0.1794)\n",
            "Epoch: [0][354/781]\tTime  0.643 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.1132 (3.6755)\tD(fake)1 -2.3723 (-2.9732)\tD(fake)2 -2.5416 (-2.9708)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0000 (0.0001)\tD repr loss -0.2617 (-0.1794)\n",
            "Epoch: [0][404/781]\tTime  0.645 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.4382 (3.6638)\tD(fake)1 -3.0448 (-2.9679)\tD(fake)2 -2.4483 (-2.9605)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0025 (-0.0000)\tD repr loss -0.1923 (-0.1800)\n",
            "Epoch: [0][454/781]\tTime  0.641 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.1078 (3.6516)\tD(fake)1 -3.2355 (-2.9607)\tD(fake)2 -2.8889 (-2.9454)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0002 (-0.0001)\tD repr loss -0.2709 (-0.1792)\n",
            "Epoch: [0][504/781]\tTime  0.640 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.6281 (3.6496)\tD(fake)1 -3.2619 (-2.9880)\tD(fake)2 -3.6352 (-2.9642)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0049 (-0.0000)\tD repr loss -0.2253 (-0.1792)\n",
            "Epoch: [0][554/781]\tTime  0.642 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.5921 (3.6630)\tD(fake)1 -3.1953 (-3.0079)\tD(fake)2 -3.2306 (-2.9785)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0008 (-0.0001)\tD repr loss -0.1869 (-0.1787)\n",
            "Epoch: [0][604/781]\tTime  0.641 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 5.3858 (3.6938)\tD(fake)1 -3.3336 (-2.9940)\tD(fake)2 -2.8101 (-2.9744)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0008 (-0.0001)\tD repr loss -0.2080 (-0.1787)\n",
            "Epoch: [0][654/781]\tTime  0.646 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.8681 (3.7164)\tD(fake)1 -2.9128 (-3.0051)\tD(fake)2 -2.9291 (-2.9900)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0004 (-0.0001)\tD repr loss -0.2004 (-0.1782)\n",
            "Epoch: [0][704/781]\tTime  0.633 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.8607 (3.7453)\tD(fake)1 -3.1330 (-3.0166)\tD(fake)2 -2.9122 (-2.9997)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0022 (-0.0001)\tD repr loss -0.2160 (-0.1788)\n",
            "Epoch: [0][754/781]\tTime  0.646 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.9163 (3.7563)\tD(fake)1 -3.1353 (-3.0294)\tD(fake)2 -2.9729 (-3.0094)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0006 (0.0000)\tD repr loss -0.2217 (-0.1785)\n",
            "Epoch: [0][780/781]\tTime  0.646 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.6591 (3.7648)\tD(fake)1 -3.3491 (-3.0374)\tD(fake)2 -3.1124 (-3.0188)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0048 (0.0001)\tD repr loss -0.2750 (-0.1783)\n",
            "Epoch: [1][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.1521 (3.6170)\tD(fake)1 -3.2764 (-3.2764)\tD(fake)2 -3.7071 (-3.3471)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0061 (-0.0061)\tD repr loss -0.1295 (-0.1637)\n",
            "Epoch: [1][ 54/781]\tTime  0.692 ( 0.657)\tData  0.000 ( 0.000)\tD(real) 4.1876 (3.8059)\tD(fake)1 -3.0512 (-3.2751)\tD(fake)2 -3.2808 (-3.2742)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0029 (-0.0005)\tD repr loss -0.2578 (-0.1854)\n",
            "Epoch: [1][104/781]\tTime  0.635 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 3.6524 (3.7916)\tD(fake)1 -3.2803 (-3.2275)\tD(fake)2 -3.1564 (-3.2449)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0023 (0.0006)\tD repr loss -0.1672 (-0.1852)\n",
            "Epoch: [1][154/781]\tTime  0.636 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.8612 (3.8590)\tD(fake)1 -2.9188 (-3.2108)\tD(fake)2 -2.7698 (-3.1993)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0001 (0.0005)\tD repr loss -0.1974 (-0.1837)\n",
            "Epoch: [1][204/781]\tTime  0.641 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.9658 (3.8820)\tD(fake)1 -3.0286 (-3.2031)\tD(fake)2 -3.4287 (-3.1601)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0027 (0.0005)\tD repr loss -0.2128 (-0.1824)\n",
            "Epoch: [1][254/781]\tTime  0.641 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.6942 (3.8625)\tD(fake)1 -3.6923 (-3.2826)\tD(fake)2 -3.4411 (-3.2324)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0037 (0.0002)\tD repr loss -0.1303 (-0.1814)\n",
            "Epoch: [1][304/781]\tTime  0.656 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.9371 (3.8598)\tD(fake)1 -3.2848 (-3.2912)\tD(fake)2 -3.3799 (-3.2525)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0044 (-0.0000)\tD repr loss -0.2589 (-0.1820)\n",
            "Epoch: [1][354/781]\tTime  0.636 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.6539 (3.8417)\tD(fake)1 -3.0019 (-3.2858)\tD(fake)2 -3.2870 (-3.2590)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0011 (0.0000)\tD repr loss -0.2876 (-0.1810)\n",
            "Epoch: [1][404/781]\tTime  0.645 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.8763 (3.8388)\tD(fake)1 -3.1234 (-3.2815)\tD(fake)2 -3.1862 (-3.2561)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0003 (0.0001)\tD repr loss -0.2293 (-0.1820)\n",
            "Epoch: [1][454/781]\tTime  0.632 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.6665 (3.8468)\tD(fake)1 -2.8812 (-3.2448)\tD(fake)2 -2.9853 (-3.2217)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0015 (0.0002)\tD repr loss -0.2312 (-0.1807)\n",
            "Epoch: [1][504/781]\tTime  0.629 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.0197 (3.8435)\tD(fake)1 -3.1649 (-3.2361)\tD(fake)2 -3.3212 (-3.2130)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0002 (0.0002)\tD repr loss -0.2284 (-0.1812)\n",
            "Epoch: [1][554/781]\tTime  0.640 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.0915 (3.8577)\tD(fake)1 -3.5078 (-3.2564)\tD(fake)2 -3.6018 (-3.2422)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0079 (0.0003)\tD repr loss -0.2166 (-0.1816)\n",
            "Epoch: [1][604/781]\tTime  0.640 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.7689 (3.8703)\tD(fake)1 -3.7593 (-3.2643)\tD(fake)2 -3.8870 (-3.2481)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0037 (0.0004)\tD repr loss -0.1651 (-0.1809)\n",
            "Epoch: [1][654/781]\tTime  0.642 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.8802 (3.8809)\tD(fake)1 -3.2673 (-3.2684)\tD(fake)2 -3.2908 (-3.2527)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0006 (0.0004)\tD repr loss -0.2199 (-0.1811)\n",
            "Epoch: [1][704/781]\tTime  0.637 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.6445 (3.8868)\tD(fake)1 -2.8823 (-3.2574)\tD(fake)2 -2.7422 (-3.2383)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0018 (0.0004)\tD repr loss -0.2856 (-0.1811)\n",
            "Epoch: [1][754/781]\tTime  0.641 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.0456 (3.8710)\tD(fake)1 -2.4954 (-3.2278)\tD(fake)2 -2.8369 (-3.2145)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0003 (0.0003)\tD repr loss -0.0917 (-0.1805)\n",
            "Epoch: [1][780/781]\tTime  0.628 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.2333 (3.8621)\tD(fake)1 -2.6530 (-3.2208)\tD(fake)2 -2.7890 (-3.2112)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0015 (0.0003)\tD repr loss -0.1889 (-0.1802)\n",
            "Epoch: [2][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 2.9765 (3.1886)\tD(fake)1 -2.9955 (-2.9955)\tD(fake)2 -3.3738 (-3.2370)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0035 (0.0035)\tD repr loss -0.2574 (-0.1931)\n",
            "Epoch: [2][ 54/781]\tTime  0.635 ( 0.652)\tData  0.000 ( 0.000)\tD(real) 3.6680 (3.6021)\tD(fake)1 -3.1039 (-2.9251)\tD(fake)2 -2.8719 (-2.8822)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0056 (0.0007)\tD repr loss -0.1473 (-0.1819)\n",
            "Epoch: [2][104/781]\tTime  0.647 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 3.1626 (3.7159)\tD(fake)1 -3.2910 (-3.0592)\tD(fake)2 -3.2159 (-3.0200)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0078 (0.0015)\tD repr loss -0.2516 (-0.1816)\n",
            "Epoch: [2][154/781]\tTime  0.637 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 4.1296 (3.7831)\tD(fake)1 -3.0470 (-3.1425)\tD(fake)2 -3.0465 (-3.1082)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0035 (0.0010)\tD repr loss -0.1561 (-0.1799)\n",
            "Epoch: [2][204/781]\tTime  0.657 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.8101 (3.8663)\tD(fake)1 -3.2605 (-3.1712)\tD(fake)2 -3.5437 (-3.1641)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0002 (0.0007)\tD repr loss -0.2561 (-0.1798)\n",
            "Epoch: [2][254/781]\tTime  0.646 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.8994 (3.8882)\tD(fake)1 -3.2526 (-3.2053)\tD(fake)2 -3.2812 (-3.1751)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0026 (0.0006)\tD repr loss -0.3103 (-0.1800)\n",
            "Epoch: [2][304/781]\tTime  0.639 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.9805 (3.9043)\tD(fake)1 -3.4539 (-3.2318)\tD(fake)2 -3.4013 (-3.1992)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0051 (0.0003)\tD repr loss -0.2199 (-0.1800)\n",
            "Epoch: [2][354/781]\tTime  0.647 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.9720 (3.9146)\tD(fake)1 -3.3463 (-3.2309)\tD(fake)2 -3.1028 (-3.2054)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0013 (0.0003)\tD repr loss -0.2261 (-0.1802)\n",
            "Epoch: [2][404/781]\tTime  0.646 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.0863 (3.9097)\tD(fake)1 -2.9195 (-3.2230)\tD(fake)2 -3.1976 (-3.2095)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0007 (0.0001)\tD repr loss -0.1975 (-0.1795)\n",
            "Epoch: [2][454/781]\tTime  0.656 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.7987 (3.9118)\tD(fake)1 -3.5575 (-3.2217)\tD(fake)2 -2.9647 (-3.2072)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0007 (0.0001)\tD repr loss -0.1435 (-0.1801)\n",
            "Epoch: [2][504/781]\tTime  0.635 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.9216 (3.9087)\tD(fake)1 -2.9880 (-3.2104)\tD(fake)2 -2.4951 (-3.1999)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0039 (0.0001)\tD repr loss -0.2325 (-0.1800)\n",
            "Epoch: [2][554/781]\tTime  0.641 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.4466 (3.9240)\tD(fake)1 -3.2508 (-3.1835)\tD(fake)2 -3.2746 (-3.1849)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0043 (0.0002)\tD repr loss -0.1845 (-0.1803)\n",
            "Epoch: [2][604/781]\tTime  0.653 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.3294 (3.9225)\tD(fake)1 -2.7508 (-3.1683)\tD(fake)2 -3.2207 (-3.1836)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0003 (0.0003)\tD repr loss -0.2084 (-0.1799)\n",
            "Epoch: [2][654/781]\tTime  0.638 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.6105 (3.9272)\tD(fake)1 -3.4443 (-3.1893)\tD(fake)2 -3.3869 (-3.1980)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0025 (0.0001)\tD repr loss -0.2050 (-0.1799)\n",
            "Epoch: [2][704/781]\tTime  0.644 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.6295 (3.9366)\tD(fake)1 -3.1494 (-3.2030)\tD(fake)2 -3.5196 (-3.2152)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0023 (0.0001)\tD repr loss -0.1920 (-0.1803)\n",
            "Epoch: [2][754/781]\tTime  0.634 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.4515 (3.9501)\tD(fake)1 -3.3455 (-3.2130)\tD(fake)2 -2.8431 (-3.2235)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0001 (0.0001)\tD repr loss -0.2500 (-0.1803)\n",
            "Epoch: [2][780/781]\tTime  0.640 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.4803 (3.9488)\tD(fake)1 -3.1869 (-3.2081)\tD(fake)2 -3.3074 (-3.2225)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0048 (0.0001)\tD repr loss -0.2439 (-0.1800)\n",
            "Epoch: [3][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.8421 (4.1897)\tD(fake)1 -3.1969 (-3.1969)\tD(fake)2 -3.1744 (-3.0026)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0014 (-0.0014)\tD repr loss -0.2617 (-0.1967)\n",
            "Epoch: [3][ 54/781]\tTime  0.637 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.9811 (4.0765)\tD(fake)1 -3.0243 (-3.2431)\tD(fake)2 -3.1771 (-3.2018)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0021 (-0.0004)\tD repr loss -0.2699 (-0.1801)\n",
            "Epoch: [3][104/781]\tTime  0.649 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.9198 (4.0698)\tD(fake)1 -2.8553 (-3.2212)\tD(fake)2 -2.7461 (-3.1786)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0041 (-0.0000)\tD repr loss -0.2705 (-0.1854)\n",
            "Epoch: [3][154/781]\tTime  0.632 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.5101 (4.0814)\tD(fake)1 -3.1519 (-3.2787)\tD(fake)2 -3.3828 (-3.2126)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0012 (-0.0002)\tD repr loss -0.2470 (-0.1845)\n",
            "Epoch: [3][204/781]\tTime  0.647 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.7159 (4.0410)\tD(fake)1 -3.4747 (-3.3080)\tD(fake)2 -3.1788 (-3.2364)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0006 (0.0000)\tD repr loss -0.2021 (-0.1808)\n",
            "Epoch: [3][254/781]\tTime  0.655 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.1901 (4.0461)\tD(fake)1 -3.7623 (-3.3165)\tD(fake)2 -3.3468 (-3.2297)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0017 (-0.0002)\tD repr loss -0.1898 (-0.1803)\n",
            "Epoch: [3][304/781]\tTime  0.653 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 5.9531 (4.0277)\tD(fake)1 -2.6993 (-3.2740)\tD(fake)2 -2.4513 (-3.2033)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0051 (-0.0001)\tD repr loss -0.1480 (-0.1786)\n",
            "Epoch: [3][354/781]\tTime  0.634 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.5435 (4.0306)\tD(fake)1 -3.1089 (-3.2843)\tD(fake)2 -3.5078 (-3.2056)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0002 (-0.0000)\tD repr loss -0.1855 (-0.1790)\n",
            "Epoch: [3][404/781]\tTime  0.641 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.3481 (3.9786)\tD(fake)1 -3.0228 (-3.3036)\tD(fake)2 -3.0017 (-3.2195)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0033 (0.0000)\tD repr loss -0.1989 (-0.1789)\n",
            "Epoch: [3][454/781]\tTime  0.643 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.8518 (3.9659)\tD(fake)1 -2.6392 (-3.2609)\tD(fake)2 -2.7645 (-3.1896)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0003 (0.0002)\tD repr loss -0.2326 (-0.1794)\n",
            "Epoch: [3][504/781]\tTime  0.631 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.7752 (3.9616)\tD(fake)1 -3.0746 (-3.2167)\tD(fake)2 -3.0467 (-3.1656)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0030 (0.0002)\tD repr loss -0.2301 (-0.1804)\n",
            "Epoch: [3][554/781]\tTime  0.637 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.9136 (3.9612)\tD(fake)1 -2.9117 (-3.1988)\tD(fake)2 -3.0704 (-3.1556)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0057 (0.0003)\tD repr loss -0.2167 (-0.1802)\n",
            "Epoch: [3][604/781]\tTime  0.642 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.7699 (3.9628)\tD(fake)1 -3.6533 (-3.2132)\tD(fake)2 -3.5411 (-3.1748)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0056 (0.0003)\tD repr loss -0.2238 (-0.1802)\n",
            "Epoch: [3][654/781]\tTime  0.638 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 2.8476 (3.9628)\tD(fake)1 -3.5475 (-3.2397)\tD(fake)2 -3.6059 (-3.1953)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0001 (0.0003)\tD repr loss -0.2014 (-0.1804)\n",
            "Epoch: [3][704/781]\tTime  0.656 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.1354 (3.9603)\tD(fake)1 -3.3163 (-3.2438)\tD(fake)2 -3.1432 (-3.1961)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0051 (0.0003)\tD repr loss -0.1242 (-0.1805)\n",
            "Epoch: [3][754/781]\tTime  0.638 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.5973 (3.9571)\tD(fake)1 -3.4661 (-3.2520)\tD(fake)2 -2.9624 (-3.2070)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0041 (0.0003)\tD repr loss -0.1621 (-0.1803)\n",
            "Epoch: [3][780/781]\tTime  0.642 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.9875 (3.9625)\tD(fake)1 -2.7639 (-3.2462)\tD(fake)2 -2.7384 (-3.2079)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0048 (0.0003)\tD repr loss -0.1853 (-0.1802)\n",
            "Epoch: [4][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.7053 (3.8007)\tD(fake)1 -3.0064 (-3.0064)\tD(fake)2 -2.8359 (-3.1529)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0015 (0.0015)\tD repr loss -0.1820 (-0.1748)\n",
            "Epoch: [4][ 54/781]\tTime  0.653 ( 0.649)\tData  0.000 ( 0.000)\tD(real) 4.0284 (4.1017)\tD(fake)1 -3.0435 (-3.3084)\tD(fake)2 -3.4196 (-3.3712)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0020 (0.0010)\tD repr loss -0.2053 (-0.1780)\n",
            "Epoch: [4][104/781]\tTime  0.655 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.3706 (4.1218)\tD(fake)1 -3.3436 (-3.3410)\tD(fake)2 -3.1732 (-3.3843)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0004 (0.0007)\tD repr loss -0.2778 (-0.1817)\n",
            "Epoch: [4][154/781]\tTime  0.634 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.9055 (4.1246)\tD(fake)1 -3.7339 (-3.3087)\tD(fake)2 -3.5655 (-3.3140)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0037 (0.0006)\tD repr loss -0.1667 (-0.1858)\n",
            "Epoch: [4][204/781]\tTime  0.639 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.1723 (4.1466)\tD(fake)1 -3.0014 (-3.2770)\tD(fake)2 -3.0707 (-3.2980)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0043 (0.0005)\tD repr loss -0.2742 (-0.1842)\n",
            "Epoch: [4][254/781]\tTime  0.636 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.3394 (4.1032)\tD(fake)1 -2.8163 (-3.2258)\tD(fake)2 -3.3337 (-3.2648)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0001 (0.0004)\tD repr loss -0.1887 (-0.1834)\n",
            "Epoch: [4][304/781]\tTime  0.641 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 3.7021 (4.1045)\tD(fake)1 -3.0633 (-3.1889)\tD(fake)2 -3.3361 (-3.2361)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0030 (0.0005)\tD repr loss -0.2331 (-0.1829)\n",
            "Epoch: [4][354/781]\tTime  0.651 ( 0.642)\tData  0.000 ( 0.000)\tD(real) 4.2440 (4.0840)\tD(fake)1 -3.1915 (-3.1796)\tD(fake)2 -3.2293 (-3.2128)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0014 (0.0004)\tD repr loss -0.2025 (-0.1824)\n",
            "Epoch: [4][404/781]\tTime  0.629 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.7358 (4.0574)\tD(fake)1 -3.6207 (-3.1463)\tD(fake)2 -3.4110 (-3.1662)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0007 (0.0004)\tD repr loss -0.2052 (-0.1820)\n",
            "Epoch: [4][454/781]\tTime  0.655 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.5161 (4.0507)\tD(fake)1 -3.0834 (-3.1396)\tD(fake)2 -3.1957 (-3.1544)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0031 (0.0004)\tD repr loss -0.2824 (-0.1817)\n",
            "Epoch: [4][504/781]\tTime  0.638 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.6880 (4.0220)\tD(fake)1 -3.5092 (-3.1601)\tD(fake)2 -3.2614 (-3.1742)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0008 (0.0004)\tD repr loss -0.2014 (-0.1818)\n",
            "Epoch: [4][554/781]\tTime  0.651 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.7779 (3.9956)\tD(fake)1 -3.3513 (-3.1662)\tD(fake)2 -3.1893 (-3.1827)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0022 (0.0003)\tD repr loss -0.1526 (-0.1810)\n",
            "Epoch: [4][604/781]\tTime  0.643 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.8458 (3.9822)\tD(fake)1 -3.2029 (-3.1638)\tD(fake)2 -3.1065 (-3.1747)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0033 (0.0003)\tD repr loss -0.1261 (-0.1811)\n",
            "Epoch: [4][654/781]\tTime  0.659 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.2107 (3.9667)\tD(fake)1 -3.4221 (-3.1612)\tD(fake)2 -3.5115 (-3.1735)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0017 (0.0003)\tD repr loss -0.2541 (-0.1814)\n",
            "Epoch: [4][704/781]\tTime  0.633 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.9197 (3.9698)\tD(fake)1 -3.6489 (-3.1734)\tD(fake)2 -3.4672 (-3.1859)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0001 (0.0004)\tD repr loss -0.2810 (-0.1811)\n",
            "Epoch: [4][754/781]\tTime  0.635 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.8789 (3.9689)\tD(fake)1 -3.3876 (-3.1844)\tD(fake)2 -3.1459 (-3.1858)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0032 (0.0004)\tD repr loss -0.1982 (-0.1812)\n",
            "Epoch: [4][780/781]\tTime  0.646 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.7823 (3.9639)\tD(fake)1 -2.7005 (-3.1840)\tD(fake)2 -2.9258 (-3.1892)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0022 (0.0004)\tD repr loss -0.2089 (-0.1809)\n",
            "Epoch: [5][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.6572 (3.7553)\tD(fake)1 -2.9769 (-2.9769)\tD(fake)2 -3.2825 (-2.9431)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0047 (-0.0047)\tD repr loss -0.2157 (-0.1871)\n",
            "Epoch: [5][ 54/781]\tTime  0.649 ( 0.648)\tData  0.000 ( 0.000)\tD(real) 4.1314 (3.9086)\tD(fake)1 -3.4475 (-3.1343)\tD(fake)2 -3.0013 (-3.1016)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0004 (-0.0018)\tD repr loss -0.1545 (-0.1738)\n",
            "Epoch: [5][104/781]\tTime  0.652 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.0383 (3.9351)\tD(fake)1 -3.2648 (-3.1208)\tD(fake)2 -3.0908 (-3.0824)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0029 (-0.0008)\tD repr loss -0.2370 (-0.1765)\n",
            "Epoch: [5][154/781]\tTime  0.646 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.0826 (3.9144)\tD(fake)1 -3.2250 (-3.0941)\tD(fake)2 -3.0291 (-3.0631)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0067 (-0.0001)\tD repr loss -0.2255 (-0.1823)\n",
            "Epoch: [5][204/781]\tTime  0.649 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.3908 (3.9177)\tD(fake)1 -2.9505 (-3.0615)\tD(fake)2 -3.1039 (-3.0604)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0006 (0.0002)\tD repr loss -0.2007 (-0.1824)\n",
            "Epoch: [5][254/781]\tTime  0.633 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.2770 (3.9082)\tD(fake)1 -2.8138 (-3.0125)\tD(fake)2 -2.4923 (-3.0154)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0028 (0.0003)\tD repr loss -0.1869 (-0.1813)\n",
            "Epoch: [5][304/781]\tTime  0.640 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.3294 (3.9114)\tD(fake)1 -2.5337 (-2.9820)\tD(fake)2 -2.4932 (-2.9937)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0031 (0.0004)\tD repr loss -0.1823 (-0.1810)\n",
            "Epoch: [5][354/781]\tTime  0.643 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.2274 (3.9142)\tD(fake)1 -3.0920 (-2.9945)\tD(fake)2 -2.9518 (-3.0077)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0016 (0.0002)\tD repr loss -0.2426 (-0.1816)\n",
            "Epoch: [5][404/781]\tTime  0.645 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.7056 (3.9283)\tD(fake)1 -3.4790 (-3.0175)\tD(fake)2 -3.3768 (-3.0271)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0055 (0.0000)\tD repr loss -0.2610 (-0.1825)\n",
            "Epoch: [5][454/781]\tTime  0.637 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.2590 (3.9286)\tD(fake)1 -3.4742 (-3.0401)\tD(fake)2 -3.1956 (-3.0479)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0032 (0.0000)\tD repr loss -0.2024 (-0.1822)\n",
            "Epoch: [5][504/781]\tTime  0.636 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.2148 (3.9454)\tD(fake)1 -3.3344 (-3.0685)\tD(fake)2 -3.3551 (-3.0648)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0028 (0.0001)\tD repr loss -0.2847 (-0.1817)\n",
            "Epoch: [5][554/781]\tTime  0.635 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.7674 (3.9646)\tD(fake)1 -2.8583 (-3.0785)\tD(fake)2 -3.3241 (-3.0779)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0026 (0.0001)\tD repr loss -0.2081 (-0.1816)\n",
            "Epoch: [5][604/781]\tTime  0.660 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 3.6147 (3.9624)\tD(fake)1 -2.7747 (-3.0735)\tD(fake)2 -3.0061 (-3.0802)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0030 (0.0001)\tD repr loss -0.2421 (-0.1808)\n",
            "Epoch: [5][654/781]\tTime  0.657 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.6003 (3.9507)\tD(fake)1 -3.3500 (-3.0691)\tD(fake)2 -3.2224 (-3.0794)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0008 (0.0001)\tD repr loss -0.2954 (-0.1805)\n",
            "Epoch: [5][704/781]\tTime  0.630 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.2191 (3.9410)\tD(fake)1 -2.6769 (-3.0535)\tD(fake)2 -3.2360 (-3.0684)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0002 (0.0000)\tD repr loss -0.1799 (-0.1804)\n",
            "Epoch: [5][754/781]\tTime  0.649 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.0353 (3.9347)\tD(fake)1 -3.0432 (-3.0519)\tD(fake)2 -3.3730 (-3.0671)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0055 (-0.0000)\tD repr loss -0.2416 (-0.1805)\n",
            "Epoch: [5][780/781]\tTime  0.637 ( 0.643)\tData  0.000 ( 0.000)\tD(real) 4.1422 (3.9280)\tD(fake)1 -2.6558 (-3.0412)\tD(fake)2 -2.5146 (-3.0640)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0016 (0.0000)\tD repr loss -0.2619 (-0.1804)\n",
            "Epoch: [6][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.7397 (3.8441)\tD(fake)1 -2.5593 (-2.5593)\tD(fake)2 -2.3382 (-2.4520)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0038 (-0.0038)\tD repr loss -0.2017 (-0.1780)\n",
            "Epoch: [6][ 54/781]\tTime  0.639 ( 0.649)\tData  0.000 ( 0.000)\tD(real) 3.0826 (3.6664)\tD(fake)1 -2.8538 (-2.6716)\tD(fake)2 -2.9167 (-2.6869)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0044 (-0.0007)\tD repr loss -0.2242 (-0.1770)\n",
            "Epoch: [6][104/781]\tTime  0.642 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 4.2585 (3.7150)\tD(fake)1 -3.1605 (-2.8093)\tD(fake)2 -2.9306 (-2.7869)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0046 (0.0007)\tD repr loss -0.2288 (-0.1808)\n",
            "Epoch: [6][154/781]\tTime  0.635 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 4.1585 (3.7761)\tD(fake)1 -3.0259 (-2.8981)\tD(fake)2 -3.1225 (-2.8752)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0010 (0.0006)\tD repr loss -0.1481 (-0.1821)\n",
            "Epoch: [6][204/781]\tTime  0.636 ( 0.644)\tData  0.000 ( 0.000)\tD(real) 3.8571 (3.7886)\tD(fake)1 -2.9392 (-2.9633)\tD(fake)2 -3.3211 (-2.9115)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0004 (-0.0000)\tD repr loss -0.2604 (-0.1799)\n",
            "Epoch: [6][254/781]\tTime  0.652 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.8839 (3.8283)\tD(fake)1 -2.6786 (-3.0064)\tD(fake)2 -3.0494 (-2.9654)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0038 (-0.0001)\tD repr loss -0.1503 (-0.1791)\n",
            "Epoch: [6][304/781]\tTime  0.650 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.9543 (3.8925)\tD(fake)1 -2.5857 (-3.0299)\tD(fake)2 -3.4277 (-3.0004)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0033 (-0.0001)\tD repr loss -0.1989 (-0.1799)\n",
            "Epoch: [6][354/781]\tTime  0.632 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.8883 (3.9082)\tD(fake)1 -3.2437 (-3.0452)\tD(fake)2 -3.4180 (-3.0275)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0013 (-0.0000)\tD repr loss -0.1972 (-0.1807)\n",
            "Epoch: [6][404/781]\tTime  0.671 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.9585 (3.9155)\tD(fake)1 -3.3844 (-3.0625)\tD(fake)2 -3.0138 (-3.0442)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0013 (0.0001)\tD repr loss -0.1473 (-0.1798)\n",
            "Epoch: [6][454/781]\tTime  0.635 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 4.6826 (3.9424)\tD(fake)1 -3.5589 (-3.0870)\tD(fake)2 -2.6862 (-3.0660)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0027 (0.0001)\tD repr loss -0.1491 (-0.1802)\n",
            "Epoch: [6][504/781]\tTime  0.637 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.5752 (3.9474)\tD(fake)1 -3.1392 (-3.1042)\tD(fake)2 -3.3205 (-3.0850)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0037 (-0.0001)\tD repr loss -0.2185 (-0.1794)\n",
            "Epoch: [6][554/781]\tTime  0.651 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.8395 (3.9445)\tD(fake)1 -3.4972 (-3.1110)\tD(fake)2 -3.1045 (-3.0874)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0013 (0.0000)\tD repr loss -0.1683 (-0.1798)\n",
            "Epoch: [6][604/781]\tTime  0.644 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.4699 (3.9383)\tD(fake)1 -3.0835 (-3.1053)\tD(fake)2 -3.0334 (-3.0882)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0000 (-0.0000)\tD repr loss -0.2420 (-0.1800)\n",
            "Epoch: [6][654/781]\tTime  0.632 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 5.0135 (3.9324)\tD(fake)1 -2.6870 (-3.1046)\tD(fake)2 -2.7061 (-3.0951)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0003 (0.0001)\tD repr loss -0.2103 (-0.1799)\n",
            "Epoch: [6][704/781]\tTime  0.641 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.6352 (3.9335)\tD(fake)1 -3.1967 (-3.1064)\tD(fake)2 -3.4429 (-3.1067)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0006 (0.0001)\tD repr loss -0.2103 (-0.1801)\n",
            "Epoch: [6][754/781]\tTime  0.641 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.6826 (3.9353)\tD(fake)1 -3.8812 (-3.1279)\tD(fake)2 -3.4217 (-3.1249)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0038 (0.0002)\tD repr loss -0.2024 (-0.1795)\n",
            "Epoch: [6][780/781]\tTime  0.641 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.5876 (3.9326)\tD(fake)1 -3.7109 (-3.1392)\tD(fake)2 -3.1903 (-3.1304)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0032 (0.0002)\tD repr loss -0.3146 (-0.1794)\n",
            "Epoch: [7][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 4.5048 (3.9560)\tD(fake)1 -3.3231 (-3.3231)\tD(fake)2 -3.1549 (-3.2311)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0018 (0.0018)\tD repr loss -0.2173 (-0.1758)\n",
            "Epoch: [7][ 54/781]\tTime  0.648 ( 0.654)\tData  0.000 ( 0.000)\tD(real) 4.0947 (3.9502)\tD(fake)1 -3.7328 (-3.6269)\tD(fake)2 -3.5768 (-3.5277)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0028 (0.0009)\tD repr loss -0.1896 (-0.1794)\n",
            "Epoch: [7][104/781]\tTime  0.650 ( 0.656)\tData  0.000 ( 0.000)\tD(real) 4.3006 (3.8826)\tD(fake)1 -3.2044 (-3.4900)\tD(fake)2 -3.0810 (-3.3743)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0006 (0.0003)\tD repr loss -0.2497 (-0.1802)\n",
            "Epoch: [7][154/781]\tTime  0.653 ( 0.656)\tData  0.000 ( 0.000)\tD(real) 4.4902 (3.8903)\tD(fake)1 -2.5741 (-3.3266)\tD(fake)2 -2.8714 (-3.2452)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0029 (0.0000)\tD repr loss -0.1541 (-0.1787)\n",
            "Epoch: [7][204/781]\tTime  0.651 ( 0.653)\tData  0.000 ( 0.000)\tD(real) 4.4708 (3.9126)\tD(fake)1 -3.4252 (-3.2814)\tD(fake)2 -3.0200 (-3.2202)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0010 (-0.0002)\tD repr loss -0.2720 (-0.1787)\n",
            "Epoch: [7][254/781]\tTime  0.643 ( 0.651)\tData  0.000 ( 0.000)\tD(real) 3.8661 (3.8934)\tD(fake)1 -3.0331 (-3.2963)\tD(fake)2 -2.6177 (-3.2353)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0004 (-0.0003)\tD repr loss -0.2532 (-0.1805)\n",
            "Epoch: [7][304/781]\tTime  0.631 ( 0.649)\tData  0.000 ( 0.000)\tD(real) 3.2366 (3.8997)\tD(fake)1 -3.2355 (-3.2815)\tD(fake)2 -3.5070 (-3.2418)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0006 (-0.0000)\tD repr loss -0.2907 (-0.1816)\n",
            "Epoch: [7][354/781]\tTime  0.646 ( 0.648)\tData  0.000 ( 0.000)\tD(real) 3.4699 (3.8983)\tD(fake)1 -3.5978 (-3.2826)\tD(fake)2 -3.7662 (-3.2272)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0011 (-0.0000)\tD repr loss -0.2022 (-0.1813)\n",
            "Epoch: [7][404/781]\tTime  0.632 ( 0.648)\tData  0.000 ( 0.000)\tD(real) 3.6981 (3.9063)\tD(fake)1 -2.9056 (-3.2667)\tD(fake)2 -3.4632 (-3.2171)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0007 (0.0002)\tD repr loss -0.2651 (-0.1814)\n",
            "Epoch: [7][454/781]\tTime  0.637 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 4.1006 (3.9164)\tD(fake)1 -2.8468 (-3.2649)\tD(fake)2 -3.2038 (-3.2286)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0051 (0.0002)\tD repr loss -0.1870 (-0.1807)\n",
            "Epoch: [7][504/781]\tTime  0.643 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 4.0061 (3.9244)\tD(fake)1 -2.8775 (-3.2296)\tD(fake)2 -3.1530 (-3.2090)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0001 (0.0002)\tD repr loss -0.2318 (-0.1813)\n",
            "Epoch: [7][554/781]\tTime  0.648 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 4.4401 (3.9325)\tD(fake)1 -2.6875 (-3.2292)\tD(fake)2 -3.5504 (-3.2023)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0041 (0.0002)\tD repr loss -0.1675 (-0.1824)\n",
            "Epoch: [7][604/781]\tTime  0.642 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.6071 (3.9419)\tD(fake)1 -2.8909 (-3.2300)\tD(fake)2 -3.0808 (-3.1900)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0030 (0.0002)\tD repr loss -0.1483 (-0.1820)\n",
            "Epoch: [7][654/781]\tTime  0.633 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.0511 (3.9452)\tD(fake)1 -2.7490 (-3.2210)\tD(fake)2 -2.9550 (-3.1786)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0018 (0.0002)\tD repr loss -0.2406 (-0.1819)\n",
            "Epoch: [7][704/781]\tTime  0.655 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 4.1740 (3.9443)\tD(fake)1 -3.2731 (-3.2137)\tD(fake)2 -2.9993 (-3.1691)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0034 (0.0002)\tD repr loss -0.2196 (-0.1820)\n",
            "Epoch: [7][754/781]\tTime  0.643 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 4.3282 (3.9484)\tD(fake)1 -3.6839 (-3.2107)\tD(fake)2 -2.9776 (-3.1632)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0010 (0.0003)\tD repr loss -0.2671 (-0.1822)\n",
            "Epoch: [7][780/781]\tTime  0.630 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.4702 (3.9451)\tD(fake)1 -3.4299 (-3.2130)\tD(fake)2 -3.4216 (-3.1648)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0045 (0.0002)\tD repr loss -0.2741 (-0.1825)\n",
            "Epoch: [8][  4/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 3.7159 (3.9685)\tD(fake)1 -3.5728 (-3.5728)\tD(fake)2 -3.6252 (-3.4878)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0003 (-0.0003)\tD repr loss -0.2099 (-0.1900)\n",
            "Epoch: [8][ 54/781]\tTime  0.643 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.6486 (3.8612)\tD(fake)1 -3.6594 (-3.3479)\tD(fake)2 -3.5264 (-3.3198)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0013 (-0.0007)\tD repr loss -0.1650 (-0.1882)\n",
            "Epoch: [8][104/781]\tTime  0.651 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 4.3880 (3.8240)\tD(fake)1 -2.7338 (-3.2721)\tD(fake)2 -3.2130 (-3.3222)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0008 (-0.0002)\tD repr loss -0.2163 (-0.1886)\n",
            "Epoch: [8][154/781]\tTime  0.632 ( 0.648)\tData  0.000 ( 0.000)\tD(real) 4.4920 (3.8146)\tD(fake)1 -2.8633 (-3.2345)\tD(fake)2 -2.7866 (-3.2875)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0029 (0.0003)\tD repr loss -0.2650 (-0.1890)\n",
            "Epoch: [8][204/781]\tTime  0.650 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.1197 (3.8280)\tD(fake)1 -2.9337 (-3.1665)\tD(fake)2 -3.2396 (-3.1984)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0056 (0.0002)\tD repr loss -0.2050 (-0.1886)\n",
            "Epoch: [8][254/781]\tTime  0.639 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.9656 (3.8587)\tD(fake)1 -3.3440 (-3.1668)\tD(fake)2 -3.1010 (-3.1868)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0002 (0.0003)\tD repr loss -0.2046 (-0.1863)\n",
            "Epoch: [8][304/781]\tTime  0.654 ( 0.645)\tData  0.000 ( 0.000)\tD(real) 3.4773 (3.8249)\tD(fake)1 -3.2074 (-3.1680)\tD(fake)2 -3.2895 (-3.1691)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0037 (0.0002)\tD repr loss -0.1938 (-0.1868)\n",
            "Epoch: [8][354/781]\tTime  0.649 ( 0.646)\tData  0.000 ( 0.000)\tD(real) 3.9800 (3.8063)\tD(fake)1 -2.9103 (-3.1893)\tD(fake)2 -2.8819 (-3.1770)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0011 (0.0002)\tD repr loss -0.1688 (-0.1857)\n",
            "Epoch: [8][404/781]\tTime  0.645 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 3.8137 (3.7951)\tD(fake)1 -3.5049 (-3.1723)\tD(fake)2 -3.2267 (-3.1635)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0026 (0.0001)\tD repr loss -0.2100 (-0.1851)\n",
            "Epoch: [8][454/781]\tTime  0.663 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 3.9021 (3.8156)\tD(fake)1 -3.0620 (-3.1837)\tD(fake)2 -3.0737 (-3.1635)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0032 (-0.0000)\tD repr loss -0.2273 (-0.1846)\n",
            "Epoch: [8][504/781]\tTime  0.644 ( 0.647)\tData  0.000 ( 0.000)\tD(real) 5.0184 (3.8211)\tD(fake)1 -3.0573 (-3.1948)\tD(fake)2 -2.4245 (-3.1752)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0011 (0.0001)\tD repr loss -0.1852 (-0.1841)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-213-23fcf3414282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_per_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msave_vid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-209-a02757e7183e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         train(train_loader, model, simsiam,\n\u001b[0;32m---> 19\u001b[0;31m             D_criterion, G_criterion, D_optimizer, G_optimizer, epoch, args)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mD_sched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mG_sched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-208-db1766d3b85f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, simsiam, D_criterion, G_criterion, D_optimizer, G_optimizer, epoch, args)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;31m# Calculate gradient and minimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mD_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;31m# Update average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhsj8_owgDSl"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNxlmBIutPfw"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iepY_Go7tf7Y"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5V-0_k5tpiW"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdIo707ZyUic"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9naW-A2OGLMg"
      },
      "source": [
        "#args.G_consistency *= 10\n",
        "args.D_consistency *= 10\n",
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzUaCS1tS8YV"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKWZPniqS5uX"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYyP-fb7S-WE"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jC0_2Vt4Yd6"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogpaUfV94ZJI"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5Bh97MA4aCW"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMSlrEvie7Pa"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvYdbtKQe_2-"
      },
      "source": [
        "def show_grid(grid):\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(grid.permute(1,2,0))\n",
        "\n",
        "def sample_repr(real1, real2):\n",
        "    repr1, repr2 = get_repr(real1), get_repr(real2)\n",
        "    repr = slerp(repr1, repr2)\n",
        "    #repr = lerp(repr1, repr2)\n",
        "    return repr\n",
        "\n",
        "def show_sample(data_sampler):\n",
        "    x, _ = next(data_sampler)\n",
        "    #plt.hist(F.normalize(simsiam.encoder(x)).detach()[0].cpu().numpy()); return\n",
        "    x1 = x.cuda(args.gpu)[:16]\n",
        "    x2 = x.cuda(args.gpu)[16:32]\n",
        "    show_grid(vutils.make_grid(inv_normalize(x1).cpu(), padding=2, nrow=4))\n",
        "    show_grid(vutils.make_grid(inv_normalize(x2).cpu(), padding=2, nrow=4))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_fake1 = sample_G(sample_repr(x1, x2))\n",
        "        x_fake2 = sample_G(sample_repr(x1, x2))\n",
        "    show_grid(vutils.make_grid(inv_normalize(x_fake1).cpu(), padding=2, nrow=4))\n",
        "    show_grid(vutils.make_grid(inv_normalize(x_fake2).cpu(), padding=2, nrow=4))\n",
        "\n",
        "data_sampler = iter(train_loader)\n",
        "show_sample(data_sampler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gfx3T5m2wah"
      },
      "source": [
        "def show_sample2(data_sampler):\n",
        "    x, _ = next(data_sampler)\n",
        "    x = x.cuda(args.gpu)[:16]\n",
        "    show_grid(vutils.make_grid(inv_normalize(x).cpu(), padding=2, nrow=4))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_fake = sample_G(get_repr(x))\n",
        "    show_grid(vutils.make_grid(inv_normalize(x_fake).cpu(), padding=2, nrow=4))\n",
        "\n",
        "show_sample2(data_sampler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKfockIQPyzO"
      },
      "source": [
        "def show_sample3(data_sampler):\n",
        "    x, _ = next(data_sampler)\n",
        "    x = x.cuda(args.gpu)[:16]\n",
        "    show_grid(vutils.make_grid(inv_normalize(x).cpu(), padding=2, nrow=4))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        repr = -1+2*torch.rand(16, args.repr_dim).cuda(args.gpu)\n",
        "        #repr = torch.randn(16, args.repr_dim).cuda(args.gpu)\n",
        "        x_fake = sample_G(repr)\n",
        "    show_grid(vutils.make_grid(inv_normalize(x_fake).cpu(), padding=2, nrow=4))\n",
        "\n",
        "show_sample3(data_sampler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtDpMGCMid7O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}