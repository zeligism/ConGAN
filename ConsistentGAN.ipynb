{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConsistentGAN",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeligism/ConGAN/blob/main/ConsistentGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxx3Jy_8qsPE"
      },
      "source": [
        "### Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MFx20xTNkpQ",
        "outputId": "36e3c17b-fa3f-47cd-9a48-2ca2403de4a0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-QNzdq01hSb"
      },
      "source": [
        "# Header"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSlF68ff2K8L"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf_Qrpq7z3iJ"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import glob\n",
        "import random\n",
        "import datetime\n",
        "import yaml\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "import torchvision\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.tensorboard as tensorboard\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "from math import log2\n",
        "from pprint import pformat\n",
        "from collections import defaultdict"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USDduLe1Qkd9"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiRxrufxw1cm"
      },
      "source": [
        "### Report Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmEyNG58w2kJ"
      },
      "source": [
        "def plot_lines(losses_dict, filename=None, title=\"\"):\n",
        "    \"\"\"\n",
        "    Plots the losses of the discriminator and the generator.\n",
        "\n",
        "    Args:\n",
        "        filename: The plot's filename. If None, plot won't be saved.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(title)\n",
        "    for label, losses in losses_dict.items():\n",
        "        plt.plot(losses, label=label)\n",
        "    plt.xlabel(\"t\")\n",
        "    plt.legend()\n",
        "    \n",
        "    if filename is not None:\n",
        "        plt.savefig(filename)\n",
        "    \n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def create_progress_animation(frames, filename):\n",
        "    \"\"\"\n",
        "    Creates a video of the progress of the generator on a fixed latent vector.\n",
        "\n",
        "    Args:\n",
        "        filename: The animation's filename.\n",
        "    \"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    plt.axis(\"off\")\n",
        "    ims = [[plt.imshow(img.permute(1,2,0), animated=True)]\n",
        "           for img in frames]\n",
        "    ani = animation.ArtistAnimation(fig, ims, blit=True)\n",
        "    \n",
        "    ani.save(filename)\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def generate_grid(generator, latent):\n",
        "    \"\"\"\n",
        "    Check generator's output on latent vectors and return it.\n",
        "\n",
        "    Args:\n",
        "        generator: The generator.\n",
        "        latent: Latent vector from which an image grid will be generated.\n",
        "\n",
        "    Returns:\n",
        "        A grid of images generated by `generator` from `latent`.\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake = generator(latent).detach()\n",
        "\n",
        "    image_grid = vutils.make_grid(fake.cpu(), padding=2, normalize=True, range=(-1,1))\n",
        "\n",
        "    return image_grid\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUzwGurc1qOx"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPhc2oS53G4e"
      },
      "source": [
        "## PyTorch Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU7HFc6t5N8w"
      },
      "source": [
        "### DCGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJHPo8w13JmH"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Default stride and padding half the size of features,\n",
        "    e.g. if input is [in_channels, 64, 64], output will be [out_channels, 32, 32].\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1,\n",
        "                 use_batchnorm=True, use_spectralnorm=False, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                              stride=stride, padding=padding, bias=False)\n",
        "        if use_spectralnorm:\n",
        "            self.conv = nn.utils.parametrizations.spectral_norm(self.conv)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels) if use_batchnorm else None\n",
        "        self.activation = nn.LeakyReLU(0.2, inplace=True) if activation is None else activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.batchnorm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Default stride and padding double the size of features,\n",
        "    e.g. if input is [in_channels, 32, 32], output will be [out_channels, 64, 64].\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1,\n",
        "                 use_batchnorm=True, use_spectralnorm=False, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convT = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                                        stride=stride, padding=padding, bias=False)\n",
        "        if use_spectralnorm:\n",
        "            self.convT = nn.utils.parametrizations.spectral_norm(self.convT)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels) if use_batchnorm else None\n",
        "        self.activation = nn.ReLU(inplace=True) if activation is None else activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convT(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.batchnorm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCGAN_Discriminator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_latents=16,\n",
        "                 num_features=64,\n",
        "                 image_channels=3,\n",
        "                 image_size=64,\n",
        "                 feature_multiplier=2,\n",
        "                 max_features=512,\n",
        "                 gan_type=\"gan\",\n",
        "                 fully_convolutional=True,\n",
        "                 activation=None,\n",
        "                 use_batchnorm=True,\n",
        "                 use_spectralnorm=False,\n",
        "                 output_sigmoid=True,\n",
        "                 D_block=ConvBlock):\n",
        "        super().__init__()\n",
        "\n",
        "        using_grad_penalty = gan_type in (\"gan-gp\", \"wgan-gp\")\n",
        "        output_sigmoid = output_sigmoid and gan_type in (\"gan\", \"gan-gp\")\n",
        "\n",
        "        block_config = {\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": use_batchnorm and not using_grad_penalty,\n",
        "            \"use_spectralnorm\": use_spectralnorm,\n",
        "        }\n",
        "\n",
        "        # Calculate intermediate image sizes\n",
        "        image_sizes = [image_size]\n",
        "        while image_sizes[-1] > 5:\n",
        "            image_sizes.append(image_sizes[-1] // 2)\n",
        "        latent_kernel = image_sizes[-1]  # should be either 3, 4, or 5\n",
        "        num_layers = len(image_sizes) - 1\n",
        "\n",
        "        # Calculate feature sizes\n",
        "        features = [min(max_features, round(num_features * feature_multiplier**i))\n",
        "                    for i in range(num_layers)]\n",
        "\n",
        "        # Input layer\n",
        "        self.input_layer = D_block(image_channels, features[0], **block_config)\n",
        "\n",
        "        # Intermediate layers\n",
        "        self.main_layers = nn.Sequential(*[\n",
        "            D_block(in_features, out_features, **block_config)\n",
        "            for in_features, out_features in zip(features, features[1:])\n",
        "        ])\n",
        "\n",
        "        # Output layer (feature_size = 3, 4, or 5 -> 1)\n",
        "        if fully_convolutional:\n",
        "            conv = nn.Conv2d(features[-1], num_latents, latent_kernel, bias=False)\n",
        "            if use_spectralnorm:\n",
        "                conv = nn.utils.parametrizations.spectral_norm(conv)\n",
        "            self.output_layer = nn.Sequential(conv, nn.Flatten())\n",
        "        else:\n",
        "            linear = nn.Linear(features[-1] * latent_kernel**2, num_latents, bias=False)\n",
        "            if use_spectralnorm:\n",
        "                linear = nn.utils.parametrizations.spectral_norm(linear)\n",
        "            self.output_layer = nn.Sequential(nn.Flatten(), linear)\n",
        "        \n",
        "        self.hidden_dim = features[-1] * latent_kernel**2\n",
        "\n",
        "        # Add sigmoid activation if using regular GAN loss\n",
        "        self.output_activation = nn.Sigmoid() if output_sigmoid else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.main_layers(x)\n",
        "        x = self.output_layer(x)\n",
        "        if self.output_activation:\n",
        "            x = self.output_activation(x)\n",
        "        # Remove H and W dimensions, infer channels dim (remove if 1)\n",
        "        x = x.view(x.size(0), -1).squeeze(1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCGAN_Generator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_latents=100,\n",
        "                 num_features=64,\n",
        "                 image_channels=3,\n",
        "                 image_size=64,\n",
        "                 feature_multiplier=2,\n",
        "                 max_features=512,\n",
        "                 gan_type=\"gan\",\n",
        "                 fully_convolutional=True,\n",
        "                 activation=None,\n",
        "                 use_batchnorm=True,\n",
        "                 use_spectralnorm=False,\n",
        "                 G_block=ConvTBlock):\n",
        "        super().__init__()\n",
        "\n",
        "        block_config = {\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": use_batchnorm,\n",
        "            \"use_spectralnorm\": use_spectralnorm\n",
        "        }\n",
        "\n",
        "        # Calculate intermediate image sizes\n",
        "        image_sizes = [image_size]\n",
        "        while image_sizes[-1] > 5:\n",
        "            image_sizes.append(image_sizes[-1] // 2)\n",
        "        latent_kernel = image_sizes[-1]  # should be either 3, 4, or 5\n",
        "        num_layers = len(image_sizes) - 1\n",
        "\n",
        "        # Calculate feature sizes\n",
        "        features = [min(max_features, round(num_features * feature_multiplier**i))\n",
        "                    for i in range(num_layers)]\n",
        "\n",
        "        # Reverse order of image sizes and features for generator\n",
        "        image_sizes = image_sizes[::-1]\n",
        "        features = features[::-1]\n",
        "\n",
        "        # Input layer\n",
        "        if fully_convolutional:\n",
        "            self.input_layer = G_block(num_latents, features[0], kernel_size=latent_kernel,\n",
        "                                       stride=1, padding=0, **block_config)\n",
        "        else:\n",
        "            self.input_layer = nn.Sequential(\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(num_latents, features[0] * image_sizes[0]**2, bias=False),\n",
        "                View(features[0], image_sizes[0], image_sizes[0])\n",
        "            )\n",
        "\n",
        "        # Intermediate layers\n",
        "        self.main_layers = nn.Sequential(*[\n",
        "            G_block(in_features, out_features, kernel_size=4+(expected_size%2), **block_config)\n",
        "            for in_features, out_features, expected_size in zip(features, features[1:], image_sizes[1:])\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.ConvTranspose2d(features[-1], image_channels, kernel_size=4+(image_size%2),\n",
        "                                               stride=2, padding=1, bias=False)\n",
        "        self.output_activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add H and W dimensions, infer channels dim (add if none)\n",
        "        x = x.view(x.size(0), -1, 1, 1)\n",
        "        x = self.input_layer(x)\n",
        "        x = self.main_layers(x)\n",
        "        x = self.output_layer(x)\n",
        "        x = self.output_activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCGAN(nn.Module):\n",
        "    \"\"\"Deep Convolutional Generative Adversarial Network\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_latents=100,\n",
        "                 D_num_features=64,\n",
        "                 G_num_features=64,\n",
        "                 image_channels=3,\n",
        "                 image_size=64,\n",
        "                 feature_multiplier=2,\n",
        "                 gan_type=\"gan\",\n",
        "                 fully_convolutional=True,\n",
        "                 activation=None,\n",
        "                 use_batchnorm=True,\n",
        "                 use_spectralnorm=False,\n",
        "                 output_sigmoid=True,):\n",
        "        \"\"\"\n",
        "        Initializes DCGAN.\n",
        "\n",
        "        Args:\n",
        "            num_latents: Number of latent factors.\n",
        "            num_features: Number of features in the convolutions.\n",
        "            image_channels: Number of channels in the input image.\n",
        "            image_size: Size (i.e. height or width) of image.\n",
        "            gan_type: Type of GAN (e.g. \"gan\" or \"wgan-gp\").\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_latents = num_latents\n",
        "        self.D_num_features = D_num_features\n",
        "        self.G_num_features = G_num_features\n",
        "        self.image_channels = image_channels\n",
        "        self.image_size = image_size\n",
        "        self.feature_multiplier = feature_multiplier\n",
        "        self.gan_type = gan_type\n",
        "        self.fully_convolutional = fully_convolutional\n",
        "        self.activation = activation\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        self.use_spectralnorm = use_spectralnorm\n",
        "\n",
        "        D_params = {\n",
        "            \"num_latents\": 1,  # XXX\n",
        "            \"num_features\": D_num_features,\n",
        "            \"image_channels\": image_channels,\n",
        "            \"image_size\": image_size,\n",
        "            \"feature_multiplier\": feature_multiplier,\n",
        "            \"gan_type\": gan_type,\n",
        "            \"fully_convolutional\": fully_convolutional,\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": use_batchnorm,\n",
        "            \"use_spectralnorm\": use_spectralnorm,\n",
        "            \"output_sigmoid\": output_sigmoid,\n",
        "        }\n",
        "        G_params = {\n",
        "            \"num_latents\": num_latents,\n",
        "            \"num_features\": G_num_features,\n",
        "            \"image_channels\": image_channels,\n",
        "            \"image_size\": image_size,\n",
        "            \"feature_multiplier\": feature_multiplier,\n",
        "            \"gan_type\": gan_type,\n",
        "            \"fully_convolutional\": fully_convolutional,\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": True,\n",
        "            \"use_spectralnorm\": False,  # XXX\n",
        "        }\n",
        "\n",
        "        self.D = DCGAN_Discriminator(**D_params)\n",
        "        self.G = DCGAN_Generator(**G_params)\n",
        "\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self, *shape, including_batch=False):\n",
        "        super().__init__()\n",
        "        self.shape = shape\n",
        "        self.including_batch = including_batch\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.including_batch:\n",
        "            return x.view(*self.shape)\n",
        "        else:\n",
        "            return x.view(x.size(0), *self.shape)\n",
        "\n",
        "class ChannelNoise(nn.Module):\n",
        "    \"\"\"\n",
        "    Channel noise injection module.\n",
        "    Adds a linearly transformed noise to a convolution layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, std=0.02):\n",
        "        super().__init__()\n",
        "        self.std = std\n",
        "        self.scale = nn.Parameter(torch.ones(1, num_channels, 1, 1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        noise_size = [x.size()[0], 1, *x.size()[2:]]  # single channel\n",
        "        noise = self.std * torch.randn(noise_size).to(x)\n",
        "\n",
        "        return x + self.scale * noise"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSPvaklIYvwT"
      },
      "source": [
        "### Third-party modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS9B8z4ZY4oX"
      },
      "source": [
        "#### DCGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLNQ90KUY_Is"
      },
      "source": [
        "#https://github.com/christiancosgrove/pytorch-spectral-normalization-gan/blob/master/model.py\n",
        "\n",
        "from torch.nn.utils.parametrizations import spectral_norm\n",
        "\n",
        "class SNDCGAN_Generator(nn.Module):\n",
        "    def __init__(self, z_dim, num_features=64, channels=3):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, 8*num_features, 4, stride=1),\n",
        "            nn.BatchNorm2d(8*num_features),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(8*num_features, 4*num_features, 4, stride=2, padding=(1,1)),\n",
        "            nn.BatchNorm2d(4*num_features),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(4*num_features, 2*num_features, 4, stride=2, padding=(1,1)),\n",
        "            nn.BatchNorm2d(2*num_features),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(2*num_features, num_features, 4, stride=2, padding=(1,1)),\n",
        "            nn.BatchNorm2d(num_features),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(num_features, channels, 3, stride=1, padding=(1,1)),\n",
        "            # use this instead of last line for 64:\n",
        "            # nn.ConvTranspose2d(64, 32, 4, stride=2, padding=(1,1)),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z.view(-1, self.z_dim, 1, 1))\n",
        "\n",
        "class SNDCGAN_Discriminator(nn.Module):\n",
        "    def __init__(self, num_features=64, channels=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            spectral_norm(nn.Conv2d(channels, num_features, 3, stride=1, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(num_features, num_features, 4, stride=2, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(num_features, 2*num_features, 3, stride=1, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(2*num_features, 2*num_features, 4, stride=2, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(2*num_features, 4*num_features, 3, stride=1, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(4*num_features, 4*num_features, 4, stride=2, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(4*num_features, 8*num_features, 3, stride=1, padding=(1,1))),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # use this instead of last 2 lines for 64:\n",
        "            # spectral_norm(nn.Conv2d(256, 256, 3, stride=1, padding=(1,1))),\n",
        "            # nn.LeakyReLU(0.1, inplace=True),\n",
        "            # spectral_norm(nn.Conv2d(256, 512, 3, stride=1, padding=(1,1))),\n",
        "            # nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        self.hidden_dim = 4*4 * 8*num_features\n",
        "        self.fc = spectral_norm(nn.Linear(self.hidden_dim, 1))\n",
        "\n",
        "    def forward(self, x, return_h=False):\n",
        "        h = self.main(x)\n",
        "        out = self.fc(h).squeeze(1)\n",
        "        if return_h:\n",
        "            return out, h\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class SNDCGAN(nn.Module):\n",
        "    def __init__(self, num_latents, num_features=64, channels=3):\n",
        "        super().__init__()\n",
        "        self.num_latents = num_latents\n",
        "        self.channels = channels\n",
        "        self.D = SNDCGAN_Discriminator(channels=channels, num_features=num_features)\n",
        "        self.G = SNDCGAN_Generator(num_latents, channels=channels, num_features=num_features)\n",
        "    "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eiu_Ri-XY6yy"
      },
      "source": [
        "#### ResNet GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFjWBbXzdlSF"
      },
      "source": [
        "# https://github.com/christiancosgrove/pytorch-spectral-normalization-gan/blob/master/model_resnet.py\n",
        "\n",
        "from torch.nn.utils.parametrizations import spectral_norm\n",
        "\n",
        "class ResBlockGenerator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResBlockGenerator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n",
        "        nn.init.xavier_uniform_(self.conv1.weight.data, 1.)\n",
        "        nn.init.xavier_uniform_(self.conv2.weight.data, 1.)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            self.conv1,\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            self.conv2\n",
        "            )\n",
        "        self.bypass = nn.Sequential()\n",
        "        if stride != 1:\n",
        "            self.bypass = nn.Upsample(scale_factor=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x) + self.bypass(x)\n",
        "\n",
        "\n",
        "class ResBlockDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResBlockDiscriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n",
        "        nn.init.xavier_uniform_(self.conv1.weight.data, 1.)\n",
        "        nn.init.xavier_uniform_(self.conv2.weight.data, 1.)\n",
        "\n",
        "        if stride == 1:\n",
        "            self.model = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                spectral_norm(self.conv1),\n",
        "                nn.ReLU(),\n",
        "                spectral_norm(self.conv2)\n",
        "                )\n",
        "        else:\n",
        "            self.model = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                spectral_norm(self.conv1),\n",
        "                nn.ReLU(),\n",
        "                spectral_norm(self.conv2),\n",
        "                nn.AvgPool2d(2, stride=stride, padding=0)\n",
        "                )\n",
        "        self.bypass = nn.Sequential()\n",
        "        if stride != 1:\n",
        "\n",
        "            self.bypass_conv = nn.Conv2d(in_channels,out_channels, 1, 1, padding=0)\n",
        "            nn.init.xavier_uniform_(self.bypass_conv.weight.data, np.sqrt(2))\n",
        "\n",
        "            self.bypass = nn.Sequential(\n",
        "                spectral_norm(self.bypass_conv),\n",
        "                nn.AvgPool2d(2, stride=stride, padding=0)\n",
        "            )\n",
        "            # if in_channels == out_channels:\n",
        "            #     self.bypass = nn.AvgPool2d(2, stride=stride, padding=0)\n",
        "            # else:\n",
        "            #     self.bypass = nn.Sequential(\n",
        "            #         spectral_norm(nn.Conv2d(in_channels,out_channels, 1, 1, padding=0)),\n",
        "            #         nn.AvgPool2d(2, stride=stride, padding=0)\n",
        "            #     )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x) + self.bypass(x)\n",
        "\n",
        "# special ResBlock just for the first layer of the discriminator\n",
        "class FirstResBlockDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(FirstResBlockDiscriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n",
        "        self.bypass_conv = nn.Conv2d(in_channels, out_channels, 1, 1, padding=0)\n",
        "        nn.init.xavier_uniform_(self.conv1.weight.data, 1.)\n",
        "        nn.init.xavier_uniform_(self.conv2.weight.data, 1.)\n",
        "        nn.init.xavier_uniform_(self.bypass_conv.weight.data, np.sqrt(2))\n",
        "\n",
        "        # we don't want to apply ReLU activation to raw image before convolution transformation.\n",
        "        self.model = nn.Sequential(\n",
        "            spectral_norm(self.conv1),\n",
        "            nn.ReLU(),\n",
        "            spectral_norm(self.conv2),\n",
        "            nn.AvgPool2d(2)\n",
        "            )\n",
        "        self.bypass = nn.Sequential(\n",
        "            nn.AvgPool2d(2),\n",
        "            spectral_norm(self.bypass_conv),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x) + self.bypass(x)\n",
        "\n",
        "class SNResNet_Generator(nn.Module):\n",
        "    def __init__(self, z_dim, image_size=64, channels=3):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.dense = nn.Linear(self.z_dim, 4 * 4 * image_size)\n",
        "        self.final = nn.Conv2d(image_size, channels, 3, stride=1, padding=1)\n",
        "        nn.init.xavier_uniform_(self.dense.weight.data, 1.)\n",
        "        nn.init.xavier_uniform_(self.final.weight.data, 1.)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            ResBlockGenerator(image_size, image_size, stride=2),\n",
        "            ResBlockGenerator(image_size, image_size, stride=2),\n",
        "            ResBlockGenerator(image_size, image_size, stride=2),\n",
        "            nn.BatchNorm2d(image_size),\n",
        "            nn.ReLU(),\n",
        "            self.final,\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(self.dense(z).view(-1, self.image_size, 4, 4))\n",
        "\n",
        "class SNResNet_Discriminator(nn.Module):\n",
        "    def __init__(self, image_size=64, channels=3):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "                FirstResBlockDiscriminator(channels, image_size, stride=2),\n",
        "                ResBlockDiscriminator(image_size, image_size, stride=2),\n",
        "                ResBlockDiscriminator(image_size, image_size),\n",
        "                ResBlockDiscriminator(image_size, image_size),\n",
        "                nn.ReLU(),\n",
        "                nn.AvgPool2d(8),\n",
        "            )\n",
        "        self.fc = nn.Linear(image_size, 1)\n",
        "        nn.init.xavier_uniform_(self.fc.weight.data, 1.)\n",
        "        self.fc = spectral_norm(self.fc)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.model(x).view(-1, self.image_size))\n",
        "\n",
        "\n",
        "class SNResNetGAN(nn.Module):\n",
        "    def __init__(self, num_latents, image_size=64, channels=3):\n",
        "        super().__init__()\n",
        "        self.num_latents = num_latents\n",
        "        self.channels = channels\n",
        "        self.D = SNResNet_Discriminator(image_size=image_size, channels=channels)\n",
        "        self.G = SNResNet_Generator(num_latents, image_size=image_size, channels=channels)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYujBEzC7EOO"
      },
      "source": [
        "#### SimSiam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-YcNut27F-v"
      },
      "source": [
        "class SimSiam(nn.Module):\n",
        "    \"\"\"\n",
        "    Build a SimSiam model.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_encoder, dim=2048, pred_dim=512):\n",
        "        \"\"\"\n",
        "        dim: feature dimension (default: 2048)\n",
        "        pred_dim: hidden dimension of the predictor (default: 512)\n",
        "        \"\"\"\n",
        "        super(SimSiam, self).__init__()\n",
        "\n",
        "        # create the encoder\n",
        "        # num_classes is the output fc dimension, zero-initialize last BNs\n",
        "        self.encoder = base_encoder(num_classes=dim, zero_init_residual=True)\n",
        "\n",
        "        # build a 3-layer projector\n",
        "        prev_dim = self.encoder.fc.weight.shape[1]\n",
        "        self.encoder.fc = nn.Sequential(nn.Linear(prev_dim, prev_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(prev_dim),\n",
        "                                        nn.ReLU(inplace=True), # first layer\n",
        "                                        nn.Linear(prev_dim, prev_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(prev_dim),\n",
        "                                        nn.ReLU(inplace=True), # second layer\n",
        "                                        self.encoder.fc,\n",
        "                                        nn.BatchNorm1d(dim, affine=False)) # output layer\n",
        "        self.encoder.fc[6].bias.requires_grad = False # hack: not use bias as it is followed by BN\n",
        "\n",
        "        # build a 2-layer predictor\n",
        "        self.predictor = nn.Sequential(nn.Linear(dim, pred_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(pred_dim),\n",
        "                                        nn.ReLU(inplace=True), # hidden layer\n",
        "                                        nn.Linear(pred_dim, dim)) # output layer\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            x1: first views of images\n",
        "            x2: second views of images\n",
        "        Output:\n",
        "            p1, p2, z1, z2: predictors and targets of the network\n",
        "            See Sec. 3 of https://arxiv.org/abs/2011.10566 for detailed notations\n",
        "        \"\"\"\n",
        "\n",
        "        # compute features for one view\n",
        "        z1 = self.encoder(x1) # NxC\n",
        "        z2 = self.encoder(x2) # NxC\n",
        "\n",
        "        p1 = self.predictor(z1) # NxC\n",
        "        p2 = self.predictor(z2) # NxC\n",
        "\n",
        "        return p1, p2, z1.detach(), z2.detach()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEuurkcmLLd3"
      },
      "source": [
        "### Latent Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eTlMYVgLN5E"
      },
      "source": [
        "class LatentTransform(nn.Module):\n",
        "    def __init__(self, repr_dim, latent_dim, hidden_dim, full_transform=True, noop=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.repr_dim = repr_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.full_transform = full_transform\n",
        "        self.noop = noop\n",
        "\n",
        "        if self.noop:\n",
        "            self.output_dim = self.latent_dim\n",
        "            return\n",
        "        elif self.full_transform:\n",
        "            self.input_dim = self.repr_dim + self.latent_dim\n",
        "            self.output_dim = self.hidden_dim\n",
        "        else:\n",
        "            self.input_dim = self.repr_dim\n",
        "            self.output_dim = self.hidden_dim + self.latent_dim\n",
        "\n",
        "        #self.transform = nn.Linear(self.input_dim, self.hidden_dim)\n",
        "        self.transform = nn.Sequential(nn.Linear(self.input_dim, self.hidden_dim, bias=False),\n",
        "                                       nn.BatchNorm1d(self.hidden_dim),\n",
        "                                       nn.ReLU(inplace=True),\n",
        "                                       nn.Linear(self.hidden_dim, self.hidden_dim))\n",
        "    \n",
        "    def forward(self, repr, noise):\n",
        "        if self.noop:\n",
        "            return noise\n",
        "\n",
        "        # assuming latent is concat as [repr,noise] XXX\n",
        "        if self.full_transform:\n",
        "            latent = torch.cat([repr, noise], dim=1)\n",
        "            latent = self.transform(latent)\n",
        "        else:\n",
        "            repr = self.transform(repr)\n",
        "            latent = torch.cat([repr, noise], dim=1)\n",
        "\n",
        "        return latent\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yRivPV9BwFk"
      },
      "source": [
        "# Training v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5rpQp2E9rE5"
      },
      "source": [
        "### Imports and globals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51zFNn509xLz"
      },
      "source": [
        "import argparse\n",
        "import builtins\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.multiprocessing as mp\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "GANSIAM_DIR = \"/content/drive/My Drive/gansiam/\"\n",
        "SIMSIAM_PATH = os.path.join(GANSIAM_DIR, \"pretrained_batch256.tar\")\n",
        "TINYIMAGENET_DIR = \"tiny-imagenet-200\""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9X_JYE2Vwxd"
      },
      "source": [
        "### Download Tiny Imagenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "559H2an_V03M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ecd29fd-07a6-4468-92d2-bd623c933d48"
      },
      "source": [
        "%%bash\n",
        "if [[ -d  \"tiny-imagenet-200\" ]]; then\n",
        "    echo \"Tiny Imagenet exists.\"\n",
        "else\n",
        "    wget -q \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "    unzip -qq \"tiny-imagenet-200.zip\" && rm \"tiny-imagenet-200.zip\"\n",
        "    echo \"Downloaded Tiny Imagenet.\"\n",
        "fi"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiny Imagenet exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbo7T6blPVTc"
      },
      "source": [
        "### Load pre-trained SimSiam model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyzHmINsryyh"
      },
      "source": [
        "#### SimSiam Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8KJUUeWr1dI"
      },
      "source": [
        "from PIL import ImageFilter\n",
        "import random\n",
        "\n",
        "\n",
        "class TwoCropsTransform:\n",
        "    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n",
        "\n",
        "    def __init__(self, base_transform):\n",
        "        self.base_transform = base_transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        q = self.base_transform(x)\n",
        "        k = self.base_transform(x)\n",
        "        return [q, k]\n",
        "\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
        "\n",
        "    def __init__(self, sigma=[.1, 2.]):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, x):\n",
        "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
        "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
        "        return x\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nr8clgi_AzY",
        "outputId": "5e2b5d03-0057-4eef-aac1-116ef306b79c"
      },
      "source": [
        "checkpoint = torch.load(SIMSIAM_PATH, map_location=\"cuda:0\")\n",
        "# remove 'module.' from dict keys\n",
        "model_dict = OrderedDict((k[7:], v) for k, v in checkpoint[\"state_dict\"].items())\n",
        "\n",
        "# Load model\n",
        "simsiam = SimSiam(models.__dict__[\"resnet50\"])\n",
        "simsiam.load_state_dict(model_dict)\n",
        "#print(simsiam)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kckB_xSVX8kB"
      },
      "source": [
        "# Training v3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nouxldrYb0r"
      },
      "source": [
        "## Args"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUP5vn8OX--p"
      },
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.load = False\n",
        "        self.print_freq = 10\n",
        "        self.seed = None\n",
        "        self.gpu = 0\n",
        "        self.workers = 2\n",
        "        self.epochs = 100\n",
        "\n",
        "        ### lr is about 2e-4 for batch size of 64\n",
        "        # we scale according to our choice of batch size\n",
        "        self.batch_size = 64\n",
        "        self.D_lr = 2e-4 * (64 / self.batch_size)\n",
        "        self.G_lr = 2e-4 * (64 / self.batch_size)\n",
        "        self.Q_lr = self.D_lr\n",
        "        self.latent_transform_lr = self.G_lr\n",
        "        self.lr_decay = 0.02\n",
        "        self.betas = (0.5, 0.999)\n",
        "\n",
        "        # SimSiam (_don't change_ if loading pre-trained)\n",
        "        self.dim = 2048\n",
        "        self.pred_dim = 512\n",
        "\n",
        "        # GAN\n",
        "        self.repr_dim = self.dim  # don't change\n",
        "        self.latent_full_transform = True\n",
        "        self.latent_noise_dim = 256\n",
        "        self.latent_hidden_dim = 512  # dim of transform output\n",
        "        self.Q_hidden_dim = 512\n",
        "        self.num_features = 64\n",
        "        self.D_iters = 3\n",
        "\n",
        "        self.gan_type = \"gan\"  # ignore this\n",
        "        self.wgan = False  # if False, use spectral norm\n",
        "        self.grad_penalty = 0.  # 0 if wgan is False\n",
        "        self.grad_center = 1.  # not important\n",
        "\n",
        "        self.generate_grid_interval = 100\n",
        "\n",
        "        # make noise proportional to sd(data)\n",
        "        self.im_noise = 1e-3  # image sd is about 1.0\n",
        "        self.repr_noise = 0. #1e-6  # (normalized) repr sd is about 0.001\n",
        "\n",
        "        self.G_consistency = 0.1\n",
        "        self.D_consistency = 0.\n",
        "\n",
        "\n",
        "GENERATED_GRIDS = []\n",
        "IMAGE_SIZE = 32\n",
        "DATASET = \"CIFAR10\"\n",
        "args = Args()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xihRlU0PYhiJ"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW_P-JfeYiOe",
        "outputId": "e17479cb-0c91-47ba-cba0-8015beb76a6a"
      },
      "source": [
        "# image normalization\n",
        "#mean = [0.485, 0.456, 0.406]\n",
        "#std = [0.229, 0.224, 0.225]\n",
        "mean = [0.5]\n",
        "std = [0.5]\n",
        "normalize = transforms.Normalize(mean=mean, std=std)\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean= [-m/s for m, s in zip(mean, std)],\n",
        "   std= [1/s for s in std]\n",
        ")\n",
        "\n",
        "augmentation = [\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "]\n",
        "\n",
        "# MoCo v2's aug: similar to SimCLR https://arxiv.org/abs/2002.05709\n",
        "_augmentation = [\n",
        "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.2, 1.)),\n",
        "    transforms.RandomApply([\n",
        "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
        "    ], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "]\n",
        "\n",
        "if DATASET == \"MNIST\":\n",
        "    augmentation = [transforms.Grayscale(3)] + augmentation\n",
        "    train_dataset = datasets.MNIST(\n",
        "        root=os.path.join(GANSIAM_DIR, \"mnist/train\"), train=True, download=True,\n",
        "        transform=transforms.Compose(augmentation))\n",
        "elif DATASET == \"CelebA\":\n",
        "    train_dataset = datasets.CelebA(\n",
        "        root=os.path.join(GANSIAM_DIR, \"celeba\"), download=True,\n",
        "        transform=transforms.Compose(augmentation))\n",
        "elif DATASET == \"CIFAR10\":\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=os.path.join(GANSIAM_DIR, \"cifar10/train\"), train=True, download=True,\n",
        "        transform=transforms.Compose(augmentation))\n",
        "        #transform=TwoCropsTransform(transforms.Compose(augmentation)))\n",
        "elif DATASET == \"Tiny Imagenet\":\n",
        "    train_dataset = datasets.ImageFolder(\n",
        "        root=os.path.join(TINYIMAGENET_DIR, 'train'),\n",
        "        transform=transforms.Compose(augmentation))\n",
        "else:\n",
        "    raise Exception(f\"Dataset '{DATASET}' not found\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "    num_workers=args.workers, pin_memory=True, sampler=None, drop_last=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oQ0_BfjmAHf"
      },
      "source": [
        "### Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmLiHFvBimb3"
      },
      "source": [
        "def D_criterion_NS(D_real, D_fake):\n",
        "    d_loss = F.softplus(-D_real) + F.softplus(D_fake)\n",
        "    return d_loss.mean()\n",
        "\n",
        "def G_criterion_NS(D_fake):\n",
        "    return F.softplus(-D_fake).mean()\n",
        "\n",
        "def D_criterion_LS(D_real, D_fake):\n",
        "    d_loss = 0.5 * (D_real - torch.ones_like(D_real))**2 + 0.5 * (D_fake)**2\n",
        "    return d_loss.mean()\n",
        "\n",
        "def G_criterion_LS(D_fake):\n",
        "    gen_loss = 0.5 * (D_fake - torch.ones_like(D_fake))**2\n",
        "    return gen_loss.mean()\n",
        "\n",
        "def D_criterion_hinge(D_real, D_fake):\n",
        "    return torch.mean(F.relu(1. - D_real)) + torch.mean(F.relu(1. + D_fake))\n",
        "\n",
        "def G_criterion_hinge(D_fake):\n",
        "    return -torch.mean(D_fake)\n",
        "\n",
        "def D_criterion_wasserstein(D_real, D_fake):\n",
        "    return torch.mean(D_fake - D_real)\n",
        "\n",
        "def G_criterion_wasserstein(D_fake):\n",
        "    return -torch.mean(D_fake)\n",
        "\n",
        "def interpolate(real, fake):\n",
        "    eps_size = [1] * len(real.size())\n",
        "    eps_size[0] = real.size(0)\n",
        "    eps = torch.rand(eps_size).to(real)\n",
        "    return eps * real + (1 - eps) * fake\n",
        "\n",
        "def simple_gradient_penalty(D, x, center=0.):\n",
        "    x.requires_grad_()\n",
        "    D_x = D(x)\n",
        "    D_grad = torch.autograd.grad(D_x, x, torch.ones_like(D_x), create_graph=True)\n",
        "    D_grad_norm = D_grad[0].view(x.size(0), -1).norm(dim=1)\n",
        "    return (D_grad_norm - center).pow(2).mean()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0Z4Ke6DYkDg"
      },
      "source": [
        "## Model + Opt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rO_MvGzYldx",
        "outputId": "b8eef414-1b22-419d-8cf7-a38b66d3a2e8"
      },
      "source": [
        "if args.seed is not None:\n",
        "    random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    cudnn.deterministic = True\n",
        "\n",
        "torch.cuda.set_device(args.gpu)\n",
        "\n",
        "latent_transform = LatentTransform(repr_dim=args.repr_dim,\n",
        "                                   latent_dim=args.latent_noise_dim,\n",
        "                                   hidden_dim=args.latent_hidden_dim,\n",
        "                                   full_transform=args.latent_full_transform,\n",
        "                                   )\n",
        "\n",
        "model = DCGAN(num_latents=latent_transform.output_dim,\n",
        "              image_size=IMAGE_SIZE,\n",
        "              gan_type=args.gan_type,  # doesn't make a difference\n",
        "              D_num_features=args.num_features,\n",
        "              G_num_features=args.num_features,\n",
        "              use_batchnorm=False,  # for D only\n",
        "              output_sigmoid=False,  # for D only\n",
        "              use_spectralnorm=not args.wgan,  # for spectral norm, use the below model\n",
        "              )\n",
        "\n",
        "\n",
        "if not args.wgan:\n",
        "    model = SNDCGAN(num_latents=latent_transform.output_dim,\n",
        "                    num_features=args.num_features)\n",
        "\n",
        "\n",
        "\n",
        "Q_hidden_dim = args.Q_hidden_dim\n",
        "if args.D_consistency == 0.:\n",
        "    Q = nn.Module()\n",
        "else:\n",
        "    Q = nn.Sequential(nn.Linear(model.D.hidden_dim, Q_hidden_dim, bias=False),\n",
        "                    nn.BatchNorm1d(Q_hidden_dim),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Linear(Q_hidden_dim, args.repr_dim))\n",
        "\n",
        "model = model.cuda(args.gpu)\n",
        "Q = Q.cuda(args.gpu)\n",
        "latent_transform = latent_transform.cuda(args.gpu)\n",
        "simsiam = simsiam.cuda(args.gpu)\n",
        "\n",
        "print(\"Num of params in D:\", sum(map(torch.numel, model.D.parameters())))\n",
        "print(\"Num of params in G:\", sum(map(torch.numel, model.G.parameters())))\n",
        "print(\"Num of params in Q:\", sum(map(torch.numel, Q.parameters())))\n",
        "print(\"Num of params in L:\", sum(map(torch.numel, latent_transform.parameters())))\n",
        "\n",
        "# Define D and G loss functions\n",
        "if args.wgan:\n",
        "    args.grad_penalty = 10.\n",
        "    D_criterion = D_criterion_wasserstein\n",
        "    G_criterion = G_criterion_wasserstein\n",
        "else:\n",
        "    args.grad_penalty = 0.\n",
        "    D_criterion = D_criterion_LS\n",
        "    G_criterion = G_criterion_LS\n",
        "\n",
        "# Optimizers\n",
        "D_optimizer = torch.optim.Adam(model.D.parameters(), args.D_lr, betas=args.betas)\n",
        "G_optimizer = torch.optim.Adam(\n",
        "    [{\"params\": model.G.parameters()},\n",
        "     {\"params\": latent_transform.parameters(), \"lr\": args.latent_transform_lr}],\n",
        "     args.G_lr, betas=args.betas)\n",
        "net_optimizer = torch.optim.Adam(\n",
        "    [{\"params\": Q.parameters(), \"lr\": args.Q_lr},\n",
        "     {\"params\": model.D.parameters(), \"lr\": args.D_lr},\n",
        "     {\"params\": model.G.parameters(), \"lr\": args.G_lr},\n",
        "     {\"params\": latent_transform.parameters(), \"lr\": args.latent_transform_lr}],\n",
        "     args.Q_lr, betas=args.betas)\n",
        "\n",
        "sched = torch.optim.lr_scheduler.ExponentialLR(net_optimizer, 1. - args.lr_decay)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "if args.load:\n",
        "    model.load_state_dict(torch.load(f\"{GANSIAM_DIR}/results/model.pth.tar\"))\n",
        "    latent_transform.load_state_dict(torch.load(f\"{GANSIAM_DIR}/results/latent_transform.pth.tar\"))\n",
        "    Q.load_state_dict(torch.load(f\"{GANSIAM_DIR}/results/Q.pth.tar\"))\n",
        "    sched.load_state_dict(torch.load(f\"{GANSIAM_DIR}/results/sched.pth.tar\"))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of params in D: 2935873\n",
            "Num of params in G: 6951427\n",
            "Num of params in Q: 0\n",
            "Num of params in L: 1443328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUTqwby4cX0i",
        "outputId": "a188a956-9581-4f53-a64f-a4d06af2cf0a"
      },
      "source": [
        "print(model)\n",
        "print(Q)\n",
        "print(latent_transform)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SNDCGAN(\n",
            "  (D): SNDCGAN_Discriminator(\n",
            "    (main): Sequential(\n",
            "      (0): ParametrizedConv2d(\n",
            "        3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (2): ParametrizedConv2d(\n",
            "        64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (4): ParametrizedConv2d(\n",
            "        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (6): ParametrizedConv2d(\n",
            "        128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (7): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (8): ParametrizedConv2d(\n",
            "        128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (9): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (10): ParametrizedConv2d(\n",
            "        256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (11): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (12): ParametrizedConv2d(\n",
            "        256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _SpectralNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (13): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (14): Flatten(start_dim=1, end_dim=-1)\n",
            "    )\n",
            "    (fc): ParametrizedLinear(\n",
            "      in_features=8192, out_features=1, bias=True\n",
            "      (parametrizations): ModuleDict(\n",
            "        (weight): ParametrizationList(\n",
            "          (0): _SpectralNorm()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (G): SNDCGAN_Generator(\n",
            "    (model): Sequential(\n",
            "      (0): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "      (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): ReLU()\n",
            "      (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (11): ReLU()\n",
            "      (12): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (13): Tanh()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Module()\n",
            "LatentTransform(\n",
            "  (transform): Sequential(\n",
            "    (0): Linear(in_features=2304, out_features=512, bias=False)\n",
            "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeDicP6QZNQ2"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckj9x-2fNtnp"
      },
      "source": [
        "def sample_noise(num_samples):\n",
        "    return torch.randn(num_samples, args.latent_noise_dim)\n",
        "\n",
        "def get_repr(img):\n",
        "    with torch.no_grad():\n",
        "        repr = simsiam.encoder(img)\n",
        "        repr = F.normalize(repr + args.repr_noise * torch.randn_like(repr))\n",
        "    return repr\n",
        "\n",
        "def sample_G(repr):\n",
        "    noise = sample_noise(repr.size(0)).cuda(args.gpu)\n",
        "    z = latent_transform(repr, noise)\n",
        "    fake = model.G(z)\n",
        "    fake = fake + args.im_noise * torch.randn_like(fake)\n",
        "    return fake"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEVERm6alJt6"
      },
      "source": [
        "# Sample a global latent for reuse\n",
        "fixed_x, _ = next(iter(train_loader))\n",
        "fixed_x = fixed_x[:32].cuda(args.gpu)\n",
        "fixed_repr = get_repr(fixed_x)\n",
        "fixed_noise = sample_noise(32).cuda(args.gpu)\n",
        "\n",
        "def check_G_progress(G):\n",
        "    with torch.no_grad():\n",
        "        z = latent_transform(fixed_repr, fixed_noise)\n",
        "        fake_progress = G(z)\n",
        "    im_grid = torch.cat([fixed_x, fake_progress], dim=0)\n",
        "    grid = vutils.make_grid(im_grid.cpu(), padding=2, normalize=True, range=(-1,1))\n",
        "    return grid"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7gU4I8OZOer"
      },
      "source": [
        "def train(train_loader, model, simsiam,\n",
        "          D_criterion, G_criterion, D_optimizer, G_optimizer, epoch, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    data_time = AverageMeter('Data', ':6.3f')\n",
        "    D_on_reals = AverageMeter('D(real)', ':.4f')\n",
        "    D_on_fakes1 = AverageMeter('D(fake)1', ':.4f')\n",
        "    D_on_fakes2 = AverageMeter('D(fake)2', ':.4f')\n",
        "    D_grads = AverageMeter('grad(D)', ':.4f')\n",
        "    G_repr_losses = AverageMeter('G repr loss', ':.4f')\n",
        "    D_repr_losses = AverageMeter('D repr loss', ':.4f')\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader),\n",
        "        [batch_time, data_time,\n",
        "         D_on_reals, D_on_fakes1, D_on_fakes2, D_grads, G_repr_losses, D_repr_losses],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    \n",
        "    # Create dataset sampler\n",
        "    data_iter = iter(enumerate(train_loader))\n",
        "    batch_idx = [0]  # just an ugly hack\n",
        "    def sample_data():\n",
        "        i, (x, y) = next(data_iter)\n",
        "        batch_idx[0] = i\n",
        "        x = x.cuda(args.gpu, non_blocking=True)\n",
        "        real = x + args.im_noise * torch.randn_like(x)\n",
        "        return real\n",
        "\n",
        "    end = time.time()\n",
        "    # Train until data_iter is exhausted\n",
        "    try:\n",
        "        i = -1\n",
        "        while True:\n",
        "            i += 1\n",
        "            # measure data loading time\n",
        "            data_time.update(time.time() - end)\n",
        "\n",
        "            ### Train discriminator\n",
        "            for _ in range(args.D_iters):\n",
        "                # Sample data and get representation\n",
        "                real = sample_data()\n",
        "                repr = get_repr(real)\n",
        "                # Sample from generator given repr\n",
        "                with torch.no_grad():\n",
        "                    fake = sample_G(repr)\n",
        "                # Classify real and fake data\n",
        "                D_real = model.D(real)\n",
        "                D_fake = model.D(fake)\n",
        "                # Calculate loss\n",
        "                D_loss = D_criterion(D_real, D_fake)\n",
        "                # Gradient penalty\n",
        "                if args.grad_penalty != 0.:\n",
        "                    D_grad_penalty = simple_gradient_penalty(\n",
        "                        model.D, interpolate(real, fake), center=args.grad_center)\n",
        "                    D_loss = D_loss + args.grad_penalty * D_grad_penalty\n",
        "                    D_grads.update(D_grad_penalty.mean().item(), real.size(0))\n",
        "                # Calculate gradient and minimize\n",
        "                D_optimizer.zero_grad()\n",
        "                D_loss.backward()\n",
        "                D_optimizer.step()\n",
        "                # Update average\n",
        "                D_on_reals.update(D_real.mean().item(), real.size(0))\n",
        "                D_on_fakes1.update(D_fake.mean().item(), real.size(0))\n",
        "\n",
        "            ### Train generator\n",
        "            # Sample data and get representation\n",
        "            real = sample_data()\n",
        "            repr = get_repr(real)\n",
        "            # Sample from generator given repr\n",
        "            fake = sample_G(repr)\n",
        "            # Classify fake data\n",
        "            D_fake = model.D(fake)\n",
        "            # Calculate adversarial loss\n",
        "            G_loss = G_criterion(D_fake)\n",
        "            # Calculate consistency loss\n",
        "            if args.G_consistency != 0.:\n",
        "                G_repr = simsiam.encoder(fake)\n",
        "                G_repr_loss = -F.cosine_similarity(G_repr, repr).mean()\n",
        "                G_loss = G_loss + args.G_consistency * G_repr_loss\n",
        "                G_repr_losses.update(G_repr_loss.mean().item(), real.size(0))\n",
        "            # Calculate gradient and minimize\n",
        "            G_optimizer.zero_grad()\n",
        "            G_loss.backward()\n",
        "            G_optimizer.step()\n",
        "            # Update average\n",
        "            D_on_fakes2.update(D_fake.mean().item(), real.size(0))\n",
        "            \n",
        "            ### InfoGAN training\n",
        "            if args.D_consistency != 0.:\n",
        "                # Sample random representation XXX\n",
        "                repr = torch.rand(real.size(0), args.repr_dim).cuda(args.gpu)\n",
        "                repr = F.normalize(repr)\n",
        "                # Sample from generator given repr\n",
        "                fake = sample_G(repr)\n",
        "                # Classify fake and find similarity\n",
        "                D_fake, h_fake = model.D(fake, return_h=True)\n",
        "                D_repr = Q(h_fake)\n",
        "                D_repr_loss = -F.cosine_similarity(D_repr, repr).mean()\n",
        "                # Train whole GAN\n",
        "                net_optimizer.zero_grad()\n",
        "                (args.D_consistency * D_repr_loss).backward()\n",
        "                net_optimizer.step()\n",
        "                # Update average\n",
        "                D_repr_losses.update(D_repr_loss.mean().item(), real.size(0))\n",
        "\n",
        "            # Check generator's progress by recording its output on a fixed input\n",
        "            if i % args.generate_grid_interval == 0:\n",
        "                grid = check_G_progress(model.G)\n",
        "                GENERATED_GRIDS.append(grid)\n",
        "\n",
        "            if i % args.print_freq == 0:\n",
        "                progress.display(batch_idx[0])\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "    \n",
        "    except StopIteration:\n",
        "        progress.display(batch_idx[0])\n",
        "        return"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v24ES94Re55W"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDMPwe-ae6q-"
      },
      "source": [
        "def save():\n",
        "    torch.save({'state_dict': model.state_dict()},\n",
        "                f\"{GANSIAM_DIR}/results/model.pth.tar\")\n",
        "    torch.save({'state_dict': latent_transform.state_dict()},\n",
        "                f\"{GANSIAM_DIR}/results/latent_transform.pth.tar\")\n",
        "    torch.save({'state_dict': Q.state_dict()},\n",
        "                f\"{GANSIAM_DIR}/results/Q.pth.tar\")\n",
        "    torch.save({'state_dict': sched.state_dict()},\n",
        "                f\"{GANSIAM_DIR}/results/sched.pth.tar\")\n",
        "\n",
        "def save_vid():\n",
        "    vidname = f\"grids_per_{args.generate_grid_interval}_iters.mp4\"\n",
        "    vidname = os.path.join(GANSIAM_DIR, \"results\", \"progress\", vidname)\n",
        "    create_progress_animation(GENERATED_GRIDS, vidname)\n",
        "\n",
        "def run(epochs):\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # train for one epoch\n",
        "        train(train_loader, model, simsiam,\n",
        "            D_criterion, G_criterion, D_optimizer, G_optimizer, epoch, args)\n",
        "        sched.step()\n",
        "\n",
        "        # Check G's progress evey epoch by generating an image\n",
        "        grid = check_G_progress(model.G)\n",
        "        imname = f'{GANSIAM_DIR}/results/progress/grid_{epoch:04d}.png'\n",
        "        plt.imsave(imname, grid.permute(1,2,0).numpy())\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            save()\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qjya-BRT_cJ"
      },
      "source": [
        "epochs_per_cell = 10"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmNNZfXWe5Rr",
        "outputId": "8890e9ee-ed0b-46e6-9abe-b6b5c375dd78"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][  2/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 0.1174 (0.0592)\tD(fake)1 0.1319 (0.0648)\tD(fake)2 0.4993 (0.4993)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0217 (0.0217)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][ 32/781]\tTime  0.323 ( 0.436)\tData  0.000 ( 0.000)\tD(real) 0.8618 (0.5296)\tD(fake)1 -0.0147 (0.1255)\tD(fake)2 0.0243 (0.1192)\tgrad(D) 0.0000 (0.0000)\tG repr loss 0.0450 (-0.0310)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][ 62/781]\tTime  0.326 ( 0.378)\tData  0.000 ( 0.000)\tD(real) 0.9907 (0.7408)\tD(fake)1 0.0036 (0.0758)\tD(fake)2 -0.0032 (0.0593)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4817 (-0.1042)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][ 92/781]\tTime  0.324 ( 0.359)\tData  0.000 ( 0.000)\tD(real) 1.0082 (0.8234)\tD(fake)1 0.0049 (0.0521)\tD(fake)2 0.0024 (0.0395)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6494 (-0.2474)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][122/781]\tTime  0.313 ( 0.348)\tData  0.000 ( 0.000)\tD(real) 1.0013 (0.8661)\tD(fake)1 0.0010 (0.0398)\tD(fake)2 0.0066 (0.0292)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6621 (-0.3412)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][152/781]\tTime  0.308 ( 0.342)\tData  0.000 ( 0.000)\tD(real) 0.9826 (0.8921)\tD(fake)1 -0.0031 (0.0322)\tD(fake)2 -0.0024 (0.0230)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7439 (-0.4008)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][182/781]\tTime  0.314 ( 0.337)\tData  0.000 ( 0.000)\tD(real) 1.0135 (0.9097)\tD(fake)1 0.0163 (0.0270)\tD(fake)2 0.0085 (0.0185)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7356 (-0.4578)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][212/781]\tTime  0.316 ( 0.335)\tData  0.000 ( 0.000)\tD(real) 0.9911 (0.9222)\tD(fake)1 -0.0042 (0.0234)\tD(fake)2 -0.0021 (0.0158)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.2845 (-0.4844)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][242/781]\tTime  0.328 ( 0.333)\tData  0.000 ( 0.000)\tD(real) 1.0197 (0.9313)\tD(fake)1 0.0029 (0.0207)\tD(fake)2 0.0011 (0.0129)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7976 (-0.5179)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][272/781]\tTime  0.321 ( 0.331)\tData  0.000 ( 0.000)\tD(real) 1.0016 (0.9388)\tD(fake)1 0.0003 (0.0184)\tD(fake)2 -0.0002 (0.0115)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8543 (-0.5504)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][302/781]\tTime  0.319 ( 0.330)\tData  0.000 ( 0.000)\tD(real) 0.9876 (0.9448)\tD(fake)1 -0.0005 (0.0167)\tD(fake)2 0.0005 (0.0104)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8017 (-0.5739)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][332/781]\tTime  0.333 ( 0.330)\tData  0.000 ( 0.000)\tD(real) 0.9985 (0.9497)\tD(fake)1 0.0002 (0.0153)\tD(fake)2 -0.0017 (0.0094)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8222 (-0.5930)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][362/781]\tTime  0.343 ( 0.330)\tData  0.000 ( 0.000)\tD(real) 0.6952 (0.9530)\tD(fake)1 0.0279 (0.0142)\tD(fake)2 0.0087 (0.0088)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7699 (-0.6071)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][392/781]\tTime  0.320 ( 0.329)\tData  0.000 ( 0.000)\tD(real) 0.9931 (0.9558)\tD(fake)1 -0.0001 (0.0131)\tD(fake)2 -0.0000 (0.0081)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8712 (-0.6204)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][422/781]\tTime  0.321 ( 0.329)\tData  0.000 ( 0.000)\tD(real) 1.0030 (0.9589)\tD(fake)1 0.0005 (0.0123)\tD(fake)2 -0.0001 (0.0075)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.9111 (-0.6315)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][452/781]\tTime  0.325 ( 0.328)\tData  0.000 ( 0.000)\tD(real) 0.9962 (0.9616)\tD(fake)1 -0.0000 (0.0114)\tD(fake)2 0.0003 (0.0070)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7584 (-0.6407)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][482/781]\tTime  0.332 ( 0.328)\tData  0.000 ( 0.000)\tD(real) 0.9979 (0.9640)\tD(fake)1 0.0012 (0.0107)\tD(fake)2 -0.0007 (0.0066)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6428 (-0.6468)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][512/781]\tTime  0.316 ( 0.327)\tData  0.000 ( 0.000)\tD(real) 1.0022 (0.9661)\tD(fake)1 -0.0003 (0.0101)\tD(fake)2 -0.0004 (0.0062)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8474 (-0.6531)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][542/781]\tTime  0.312 ( 0.327)\tData  0.000 ( 0.000)\tD(real) 0.9976 (0.9680)\tD(fake)1 0.0005 (0.0096)\tD(fake)2 -0.0008 (0.0058)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8540 (-0.6636)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][572/781]\tTime  0.309 ( 0.326)\tData  0.000 ( 0.000)\tD(real) 0.9980 (0.9696)\tD(fake)1 -0.0005 (0.0091)\tD(fake)2 -0.0005 (0.0055)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.9097 (-0.6720)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][602/781]\tTime  0.311 ( 0.326)\tData  0.000 ( 0.000)\tD(real) 1.0001 (0.9712)\tD(fake)1 -0.0001 (0.0087)\tD(fake)2 0.0001 (0.0053)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8714 (-0.6808)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][632/781]\tTime  0.330 ( 0.325)\tData  0.000 ( 0.000)\tD(real) 0.9884 (0.9725)\tD(fake)1 0.0062 (0.0084)\tD(fake)2 -0.0000 (0.0051)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.2567 (-0.6827)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][662/781]\tTime  0.311 ( 0.325)\tData  0.000 ( 0.000)\tD(real) 1.0033 (0.9737)\tD(fake)1 0.0002 (0.0080)\tD(fake)2 0.0003 (0.0048)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7186 (-0.6885)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][692/781]\tTime  0.322 ( 0.325)\tData  0.000 ( 0.000)\tD(real) 1.0024 (0.9749)\tD(fake)1 -0.0000 (0.0076)\tD(fake)2 -0.0001 (0.0046)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8609 (-0.6951)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][722/781]\tTime  0.310 ( 0.324)\tData  0.000 ( 0.000)\tD(real) 1.0013 (0.9759)\tD(fake)1 0.0005 (0.0073)\tD(fake)2 -0.0002 (0.0044)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8736 (-0.7000)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][752/781]\tTime  0.329 ( 0.324)\tData  0.000 ( 0.000)\tD(real) 0.9943 (0.9768)\tD(fake)1 -0.0012 (0.0070)\tD(fake)2 -0.0005 (0.0042)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5255 (-0.7041)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [0][780/781]\tTime  0.312 ( 0.324)\tData  0.000 ( 0.000)\tD(real) 1.0011 (0.9777)\tD(fake)1 0.0012 (0.0068)\tD(fake)2 -0.0002 (0.0041)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.2625 (-0.7074)\tD repr loss 0.0000 (0.0000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][  2/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 1.0027 (1.0004)\tD(fake)1 -0.0006 (-0.0008)\tD(fake)2 0.0002 (0.0002)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.9158 (-0.9158)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][ 32/781]\tTime  0.314 ( 0.329)\tData  0.000 ( 0.000)\tD(real) 0.9958 (0.9997)\tD(fake)1 -0.0018 (0.0002)\tD(fake)2 0.0009 (0.0004)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8564 (-0.8596)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][ 62/781]\tTime  0.374 ( 0.328)\tData  0.000 ( 0.000)\tD(real) 0.0629 (0.9470)\tD(fake)1 0.0643 (0.0336)\tD(fake)2 0.0811 (0.0017)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8899 (-0.8251)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][ 92/781]\tTime  0.317 ( 0.327)\tData  0.000 ( 0.000)\tD(real) 0.2053 (0.8123)\tD(fake)1 0.1207 (0.1085)\tD(fake)2 0.2200 (0.0694)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8983 (-0.8318)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][122/781]\tTime  0.319 ( 0.325)\tData  0.000 ( 0.000)\tD(real) 0.7668 (0.7594)\tD(fake)1 0.0391 (0.1356)\tD(fake)2 0.0316 (0.1045)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8128 (-0.8357)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][152/781]\tTime  0.317 ( 0.323)\tData  0.000 ( 0.000)\tD(real) 0.8674 (0.7922)\tD(fake)1 0.3526 (0.1247)\tD(fake)2 0.0234 (0.0894)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7363 (-0.8148)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][182/781]\tTime  0.318 ( 0.323)\tData  0.000 ( 0.000)\tD(real) 0.6156 (0.7737)\tD(fake)1 0.3925 (0.1674)\tD(fake)2 0.4035 (0.1357)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8465 (-0.8043)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][212/781]\tTime  0.315 ( 0.323)\tData  0.000 ( 0.000)\tD(real) 0.6370 (0.7506)\tD(fake)1 0.3979 (0.2014)\tD(fake)2 0.3662 (0.1717)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8343 (-0.8061)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][242/781]\tTime  0.330 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.7571 (0.7369)\tD(fake)1 0.3722 (0.2237)\tD(fake)2 0.3358 (0.1961)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7798 (-0.8073)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][272/781]\tTime  0.314 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.5235 (0.7280)\tD(fake)1 0.3809 (0.2314)\tD(fake)2 0.3813 (0.2024)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7946 (-0.8043)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][302/781]\tTime  0.336 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.7062 (0.7170)\tD(fake)1 0.4106 (0.2475)\tD(fake)2 0.4307 (0.2201)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4419 (-0.7942)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][332/781]\tTime  0.310 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.4856 (0.7107)\tD(fake)1 0.2905 (0.2585)\tD(fake)2 0.3260 (0.2309)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8760 (-0.7976)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][362/781]\tTime  0.312 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.9359 (0.7109)\tD(fake)1 0.0715 (0.2578)\tD(fake)2 0.0618 (0.2251)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5714 (-0.7955)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][392/781]\tTime  0.327 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 1.0352 (0.7261)\tD(fake)1 0.1921 (0.2455)\tD(fake)2 -0.0569 (0.2106)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8340 (-0.7951)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][422/781]\tTime  0.321 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8738 (0.7336)\tD(fake)1 0.0728 (0.2380)\tD(fake)2 0.1217 (0.2016)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8813 (-0.7917)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][452/781]\tTime  0.317 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.6797 (0.7296)\tD(fake)1 0.3377 (0.2437)\tD(fake)2 0.2508 (0.2036)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8479 (-0.7943)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][482/781]\tTime  0.311 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8259 (0.7299)\tD(fake)1 0.2227 (0.2461)\tD(fake)2 0.1792 (0.2069)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8759 (-0.7945)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][512/781]\tTime  0.335 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7146 (0.7294)\tD(fake)1 0.1951 (0.2477)\tD(fake)2 0.2842 (0.2095)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7663 (-0.7949)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][542/781]\tTime  0.315 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.4255 (0.7228)\tD(fake)1 0.4120 (0.2558)\tD(fake)2 0.5150 (0.2162)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7551 (-0.7931)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][572/781]\tTime  0.315 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.5460 (0.7153)\tD(fake)1 0.4429 (0.2613)\tD(fake)2 0.4574 (0.2232)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7656 (-0.7934)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][602/781]\tTime  0.322 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.6336 (0.7086)\tD(fake)1 0.3620 (0.2666)\tD(fake)2 0.3608 (0.2272)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8372 (-0.7951)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][632/781]\tTime  0.312 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7929 (0.7080)\tD(fake)1 0.2274 (0.2701)\tD(fake)2 0.0986 (0.2282)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8221 (-0.7954)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][662/781]\tTime  0.317 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.9449 (0.7118)\tD(fake)1 0.0785 (0.2675)\tD(fake)2 0.0759 (0.2259)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5448 (-0.7909)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][692/781]\tTime  0.319 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.8098 (0.7152)\tD(fake)1 0.1722 (0.2641)\tD(fake)2 0.1588 (0.2252)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7854 (-0.7877)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][722/781]\tTime  0.322 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.8677 (0.7214)\tD(fake)1 0.0875 (0.2577)\tD(fake)2 0.0790 (0.2198)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8328 (-0.7875)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][752/781]\tTime  0.314 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.9144 (0.7295)\tD(fake)1 0.0433 (0.2499)\tD(fake)2 0.0393 (0.2125)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8222 (-0.7883)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [1][780/781]\tTime  0.317 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.9125 (0.7376)\tD(fake)1 0.0970 (0.2429)\tD(fake)2 0.0262 (0.2061)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8604 (-0.7886)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][  2/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 0.9783 (0.9496)\tD(fake)1 0.0469 (0.0361)\tD(fake)2 0.1032 (0.1032)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8627 (-0.8627)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][ 32/781]\tTime  0.316 ( 0.330)\tData  0.000 ( 0.000)\tD(real) 0.9084 (0.8986)\tD(fake)1 0.0662 (0.0760)\tD(fake)2 0.0755 (0.0397)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4457 (-0.8005)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][ 62/781]\tTime  0.324 ( 0.325)\tData  0.000 ( 0.000)\tD(real) 0.7393 (0.9203)\tD(fake)1 -0.0500 (0.0745)\tD(fake)2 -0.0301 (0.0339)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6634 (-0.7958)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][ 92/781]\tTime  0.318 ( 0.323)\tData  0.000 ( 0.000)\tD(real) 1.0126 (0.9250)\tD(fake)1 0.0526 (0.0702)\tD(fake)2 0.0293 (0.0388)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8340 (-0.8078)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][122/781]\tTime  0.311 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.9288 (0.9323)\tD(fake)1 0.0599 (0.0660)\tD(fake)2 0.0229 (0.0415)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8722 (-0.8094)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][152/781]\tTime  0.310 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.9236 (0.9312)\tD(fake)1 0.1120 (0.0668)\tD(fake)2 0.0434 (0.0506)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8611 (-0.8155)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][182/781]\tTime  0.327 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.6936 (0.9168)\tD(fake)1 0.1317 (0.0821)\tD(fake)2 0.3079 (0.0656)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8525 (-0.8091)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][212/781]\tTime  0.317 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.5675 (0.8801)\tD(fake)1 0.3885 (0.1107)\tD(fake)2 0.3702 (0.1005)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8557 (-0.8009)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][242/781]\tTime  0.325 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7118 (0.8505)\tD(fake)1 0.3740 (0.1409)\tD(fake)2 0.2596 (0.1258)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5156 (-0.7998)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][272/781]\tTime  0.320 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.5224 (0.8346)\tD(fake)1 0.2311 (0.1577)\tD(fake)2 0.2786 (0.1357)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3496 (-0.7967)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][302/781]\tTime  0.311 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.8157 (0.8177)\tD(fake)1 0.1351 (0.1730)\tD(fake)2 0.1556 (0.1472)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7018 (-0.7886)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][332/781]\tTime  0.327 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7792 (0.8164)\tD(fake)1 0.1253 (0.1759)\tD(fake)2 0.0801 (0.1463)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8462 (-0.7831)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][362/781]\tTime  0.322 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.5430 (0.8158)\tD(fake)1 0.2291 (0.1778)\tD(fake)2 0.1531 (0.1456)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8737 (-0.7841)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][392/781]\tTime  0.322 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.6557 (0.8054)\tD(fake)1 0.1786 (0.1864)\tD(fake)2 0.3445 (0.1544)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5360 (-0.7793)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][422/781]\tTime  0.309 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.6019 (0.8005)\tD(fake)1 0.1866 (0.1936)\tD(fake)2 0.3959 (0.1612)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8525 (-0.7798)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][452/781]\tTime  0.317 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.4886 (0.7954)\tD(fake)1 0.1336 (0.1980)\tD(fake)2 0.2034 (0.1648)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8087 (-0.7805)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][482/781]\tTime  0.351 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.8703 (0.7926)\tD(fake)1 0.1111 (0.2002)\tD(fake)2 0.1490 (0.1633)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6676 (-0.7760)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][512/781]\tTime  0.325 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7905 (0.7954)\tD(fake)1 0.1960 (0.1993)\tD(fake)2 0.1451 (0.1605)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8244 (-0.7697)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][542/781]\tTime  0.328 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.2735 (0.7925)\tD(fake)1 0.0027 (0.2031)\tD(fake)2 0.5444 (0.1672)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8364 (-0.7703)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][572/781]\tTime  0.313 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.6388 (0.7869)\tD(fake)1 0.1104 (0.2101)\tD(fake)2 0.1910 (0.1699)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6981 (-0.7678)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][602/781]\tTime  0.325 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8242 (0.7853)\tD(fake)1 0.1360 (0.2098)\tD(fake)2 0.1265 (0.1710)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7384 (-0.7597)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][632/781]\tTime  0.321 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.5538 (0.7841)\tD(fake)1 0.0632 (0.2115)\tD(fake)2 0.2417 (0.1718)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6325 (-0.7559)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][662/781]\tTime  0.317 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.6509 (0.7824)\tD(fake)1 0.1916 (0.2144)\tD(fake)2 0.2320 (0.1725)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6787 (-0.7548)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][692/781]\tTime  0.315 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7295 (0.7811)\tD(fake)1 0.0313 (0.2156)\tD(fake)2 0.1189 (0.1721)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8699 (-0.7539)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][722/781]\tTime  0.312 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7042 (0.7793)\tD(fake)1 0.1769 (0.2180)\tD(fake)2 0.1905 (0.1730)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.8097 (-0.7521)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][752/781]\tTime  0.324 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7316 (0.7783)\tD(fake)1 0.1540 (0.2191)\tD(fake)2 0.2733 (0.1744)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7154 (-0.7497)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [2][780/781]\tTime  0.317 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.6385 (0.7780)\tD(fake)1 0.2521 (0.2196)\tD(fake)2 0.1419 (0.1733)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6993 (-0.7460)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][  2/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 0.9742 (0.8644)\tD(fake)1 0.2839 (0.2312)\tD(fake)2 0.0839 (0.0839)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7459 (-0.7459)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][ 32/781]\tTime  0.313 ( 0.329)\tData  0.000 ( 0.000)\tD(real) 0.8844 (0.7635)\tD(fake)1 0.2267 (0.2422)\tD(fake)2 0.1437 (0.1318)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6419 (-0.6861)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][ 62/781]\tTime  0.315 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.4555 (0.7538)\tD(fake)1 0.1452 (0.2418)\tD(fake)2 0.1604 (0.1501)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4530 (-0.6633)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][ 92/781]\tTime  0.318 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.6681 (0.7540)\tD(fake)1 0.3589 (0.2366)\tD(fake)2 0.3072 (0.1522)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4732 (-0.6607)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][122/781]\tTime  0.316 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.6415 (0.7562)\tD(fake)1 0.1287 (0.2342)\tD(fake)2 0.2299 (0.1475)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7240 (-0.6679)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][152/781]\tTime  0.325 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.4947 (0.7580)\tD(fake)1 -0.0315 (0.2349)\tD(fake)2 0.1876 (0.1506)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6466 (-0.6703)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][182/781]\tTime  0.318 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.9164 (0.7661)\tD(fake)1 0.1816 (0.2294)\tD(fake)2 0.0357 (0.1447)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7563 (-0.6653)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][212/781]\tTime  0.314 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7755 (0.7681)\tD(fake)1 0.1266 (0.2247)\tD(fake)2 0.0993 (0.1437)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.2387 (-0.6615)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][242/781]\tTime  0.322 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7261 (0.7725)\tD(fake)1 0.3769 (0.2188)\tD(fake)2 0.3065 (0.1426)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4808 (-0.6640)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][272/781]\tTime  0.311 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.6278 (0.7724)\tD(fake)1 0.0770 (0.2194)\tD(fake)2 0.0794 (0.1399)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6122 (-0.6659)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][302/781]\tTime  0.310 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8681 (0.7753)\tD(fake)1 0.1971 (0.2182)\tD(fake)2 0.1036 (0.1388)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5295 (-0.6520)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][332/781]\tTime  0.324 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.5728 (0.7714)\tD(fake)1 0.1066 (0.2195)\tD(fake)2 0.1879 (0.1407)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7873 (-0.6549)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][362/781]\tTime  0.316 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.7310 (0.7721)\tD(fake)1 0.1099 (0.2200)\tD(fake)2 0.1287 (0.1413)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6809 (-0.6571)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][392/781]\tTime  0.315 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.7566 (0.7684)\tD(fake)1 0.1578 (0.2244)\tD(fake)2 0.1903 (0.1454)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4963 (-0.6602)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][422/781]\tTime  0.338 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.9753 (0.7693)\tD(fake)1 0.2326 (0.2245)\tD(fake)2 0.1110 (0.1478)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7446 (-0.6602)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][452/781]\tTime  0.318 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.8365 (0.7711)\tD(fake)1 0.2152 (0.2226)\tD(fake)2 0.1297 (0.1468)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7154 (-0.6560)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][482/781]\tTime  0.316 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8383 (0.7721)\tD(fake)1 0.2452 (0.2218)\tD(fake)2 0.1275 (0.1470)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5437 (-0.6497)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][512/781]\tTime  0.317 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7595 (0.7729)\tD(fake)1 0.1263 (0.2209)\tD(fake)2 0.1289 (0.1466)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6271 (-0.6450)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][542/781]\tTime  0.313 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8025 (0.7717)\tD(fake)1 0.2063 (0.2221)\tD(fake)2 0.1133 (0.1452)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5111 (-0.6361)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][572/781]\tTime  0.323 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.8140 (0.7712)\tD(fake)1 0.2925 (0.2227)\tD(fake)2 0.1576 (0.1459)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5873 (-0.6310)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][602/781]\tTime  0.316 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8433 (0.7724)\tD(fake)1 0.1195 (0.2217)\tD(fake)2 0.1067 (0.1442)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5691 (-0.6250)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][632/781]\tTime  0.309 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7894 (0.7738)\tD(fake)1 0.0628 (0.2200)\tD(fake)2 0.1219 (0.1430)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5304 (-0.6203)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][662/781]\tTime  0.319 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.7056 (0.7741)\tD(fake)1 0.0208 (0.2195)\tD(fake)2 0.1294 (0.1436)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6875 (-0.6159)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][692/781]\tTime  0.313 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.3690 (0.7743)\tD(fake)1 -0.1593 (0.2189)\tD(fake)2 0.4798 (0.1433)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4344 (-0.6132)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][722/781]\tTime  0.316 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.7932 (0.7741)\tD(fake)1 0.2272 (0.2202)\tD(fake)2 0.1449 (0.1449)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.2916 (-0.6067)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][752/781]\tTime  0.315 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8999 (0.7753)\tD(fake)1 0.1681 (0.2204)\tD(fake)2 -0.0082 (0.1421)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6779 (-0.6054)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [3][780/781]\tTime  0.307 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.7838 (0.7741)\tD(fake)1 0.2113 (0.2211)\tD(fake)2 0.1152 (0.1428)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5378 (-0.6032)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][  2/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 0.7887 (0.8250)\tD(fake)1 -0.0124 (0.1098)\tD(fake)2 0.1060 (0.1060)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5968 (-0.5968)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][ 32/781]\tTime  0.320 ( 0.331)\tData  0.000 ( 0.000)\tD(real) 0.8962 (0.8162)\tD(fake)1 0.2539 (0.1807)\tD(fake)2 0.1916 (0.1274)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6927 (-0.6217)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][ 62/781]\tTime  0.323 ( 0.325)\tData  0.000 ( 0.000)\tD(real) 0.7073 (0.7795)\tD(fake)1 0.0758 (0.2146)\tD(fake)2 0.0705 (0.1379)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6682 (-0.6141)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][ 92/781]\tTime  0.315 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.7906 (0.7756)\tD(fake)1 0.2509 (0.2185)\tD(fake)2 0.2293 (0.1327)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6393 (-0.6212)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][122/781]\tTime  0.336 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.5351 (0.7664)\tD(fake)1 0.1267 (0.2236)\tD(fake)2 0.1667 (0.1406)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3731 (-0.6066)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][152/781]\tTime  0.316 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7512 (0.7652)\tD(fake)1 0.0749 (0.2267)\tD(fake)2 0.1836 (0.1400)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5260 (-0.5921)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][182/781]\tTime  0.323 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7362 (0.7651)\tD(fake)1 0.1096 (0.2290)\tD(fake)2 0.3032 (0.1396)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4826 (-0.5841)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][212/781]\tTime  0.320 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7783 (0.7674)\tD(fake)1 0.3170 (0.2263)\tD(fake)2 0.3982 (0.1448)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6029 (-0.5973)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][242/781]\tTime  0.314 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.5395 (0.7650)\tD(fake)1 0.1552 (0.2277)\tD(fake)2 0.2187 (0.1413)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5938 (-0.5904)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][272/781]\tTime  0.320 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7875 (0.7672)\tD(fake)1 0.0408 (0.2266)\tD(fake)2 0.0363 (0.1355)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6916 (-0.5880)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][302/781]\tTime  0.323 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8222 (0.7734)\tD(fake)1 0.1995 (0.2209)\tD(fake)2 0.1060 (0.1314)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3748 (-0.5837)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][332/781]\tTime  0.311 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7711 (0.7734)\tD(fake)1 0.1504 (0.2200)\tD(fake)2 0.0768 (0.1312)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5743 (-0.5723)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][362/781]\tTime  0.314 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.6585 (0.7742)\tD(fake)1 0.0166 (0.2189)\tD(fake)2 0.0854 (0.1281)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5260 (-0.5688)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][392/781]\tTime  0.318 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.6360 (0.7765)\tD(fake)1 0.0222 (0.2171)\tD(fake)2 0.3189 (0.1263)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5702 (-0.5653)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][422/781]\tTime  0.324 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.7295 (0.7772)\tD(fake)1 0.0320 (0.2170)\tD(fake)2 0.1004 (0.1257)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5198 (-0.5616)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][452/781]\tTime  0.318 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7421 (0.7790)\tD(fake)1 0.0303 (0.2147)\tD(fake)2 0.1264 (0.1261)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5921 (-0.5549)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][482/781]\tTime  0.317 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.7819 (0.7804)\tD(fake)1 0.1647 (0.2143)\tD(fake)2 0.0556 (0.1234)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6052 (-0.5516)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][512/781]\tTime  0.313 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.7729 (0.7837)\tD(fake)1 0.0573 (0.2109)\tD(fake)2 0.1381 (0.1202)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3383 (-0.5476)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][542/781]\tTime  0.318 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.7283 (0.7840)\tD(fake)1 0.1282 (0.2104)\tD(fake)2 0.1468 (0.1206)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5335 (-0.5438)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][572/781]\tTime  0.314 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8388 (0.7868)\tD(fake)1 0.2141 (0.2079)\tD(fake)2 0.0262 (0.1184)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3922 (-0.5363)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][602/781]\tTime  0.318 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.5230 (0.7908)\tD(fake)1 -0.1320 (0.2034)\tD(fake)2 0.1783 (0.1155)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.2887 (-0.5268)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][632/781]\tTime  0.317 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8717 (0.7928)\tD(fake)1 0.0204 (0.2018)\tD(fake)2 0.0609 (0.1138)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4804 (-0.5194)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][662/781]\tTime  0.310 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8255 (0.7944)\tD(fake)1 0.0471 (0.2002)\tD(fake)2 0.1521 (0.1131)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4304 (-0.5164)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][692/781]\tTime  0.312 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8901 (0.7993)\tD(fake)1 0.0779 (0.1963)\tD(fake)2 0.1529 (0.1091)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3406 (-0.5113)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][722/781]\tTime  0.320 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.7526 (0.7998)\tD(fake)1 0.0297 (0.1955)\tD(fake)2 0.1471 (0.1075)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4412 (-0.5099)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][752/781]\tTime  0.314 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8778 (0.8012)\tD(fake)1 0.1360 (0.1944)\tD(fake)2 0.2033 (0.1063)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5663 (-0.5131)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [4][780/781]\tTime  0.310 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.9270 (0.8018)\tD(fake)1 0.3207 (0.1941)\tD(fake)2 0.2017 (0.1058)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5689 (-0.5117)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][  2/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 0.8367 (0.8113)\tD(fake)1 0.0379 (0.0029)\tD(fake)2 0.2466 (0.2466)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4870 (-0.4870)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][ 32/781]\tTime  0.323 ( 0.329)\tData  0.000 ( 0.000)\tD(real) 0.7673 (0.8372)\tD(fake)1 0.0351 (0.1387)\tD(fake)2 0.1071 (0.0871)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6875 (-0.5775)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][ 62/781]\tTime  0.313 ( 0.323)\tData  0.000 ( 0.000)\tD(real) 0.6446 (0.8316)\tD(fake)1 -0.1787 (0.1510)\tD(fake)2 -0.0192 (0.1015)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4871 (-0.5921)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][ 92/781]\tTime  0.322 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8136 (0.8343)\tD(fake)1 0.0926 (0.1560)\tD(fake)2 0.0383 (0.0959)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5093 (-0.5719)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][122/781]\tTime  0.333 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8046 (0.8351)\tD(fake)1 0.0206 (0.1568)\tD(fake)2 0.0408 (0.0938)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5073 (-0.5454)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][152/781]\tTime  0.312 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.8397 (0.8388)\tD(fake)1 0.0128 (0.1542)\tD(fake)2 0.0136 (0.0898)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4490 (-0.5476)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][182/781]\tTime  0.317 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8765 (0.8380)\tD(fake)1 0.1234 (0.1555)\tD(fake)2 0.0592 (0.0881)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5599 (-0.5468)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][212/781]\tTime  0.317 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.6788 (0.8360)\tD(fake)1 -0.0041 (0.1578)\tD(fake)2 0.1433 (0.0872)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7065 (-0.5499)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][242/781]\tTime  0.310 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.8067 (0.8347)\tD(fake)1 0.0278 (0.1599)\tD(fake)2 0.2180 (0.0895)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4817 (-0.5584)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][272/781]\tTime  0.310 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.7542 (0.8348)\tD(fake)1 0.0695 (0.1599)\tD(fake)2 0.0884 (0.0879)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4228 (-0.5572)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][302/781]\tTime  0.310 ( 0.317)\tData  0.000 ( 0.000)\tD(real) 0.8163 (0.8322)\tD(fake)1 0.1345 (0.1620)\tD(fake)2 0.0655 (0.0852)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.1162 (-0.5582)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][332/781]\tTime  0.319 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.8439 (0.8325)\tD(fake)1 0.0621 (0.1630)\tD(fake)2 0.0738 (0.0839)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6112 (-0.5595)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][362/781]\tTime  0.328 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.6951 (0.8320)\tD(fake)1 -0.0640 (0.1632)\tD(fake)2 0.1157 (0.0852)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5837 (-0.5607)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][392/781]\tTime  0.321 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.9156 (0.8347)\tD(fake)1 0.1102 (0.1612)\tD(fake)2 0.0450 (0.0815)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6561 (-0.5607)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][422/781]\tTime  0.315 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.8935 (0.8376)\tD(fake)1 0.0602 (0.1584)\tD(fake)2 0.0231 (0.0791)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7218 (-0.5656)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][452/781]\tTime  0.325 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.7751 (0.8394)\tD(fake)1 0.0079 (0.1562)\tD(fake)2 0.0151 (0.0776)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4781 (-0.5664)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][482/781]\tTime  0.321 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.8833 (0.8408)\tD(fake)1 0.0755 (0.1554)\tD(fake)2 0.0670 (0.0760)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4193 (-0.5598)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][512/781]\tTime  0.314 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.9123 (0.8413)\tD(fake)1 0.0440 (0.1549)\tD(fake)2 0.0954 (0.0767)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6287 (-0.5603)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][542/781]\tTime  0.312 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.9349 (0.8419)\tD(fake)1 0.0613 (0.1545)\tD(fake)2 0.0761 (0.0758)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4763 (-0.5644)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][572/781]\tTime  0.314 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.7764 (0.8420)\tD(fake)1 0.0189 (0.1544)\tD(fake)2 0.1300 (0.0750)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4916 (-0.5608)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][602/781]\tTime  0.313 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 1.0024 (0.8432)\tD(fake)1 0.2346 (0.1538)\tD(fake)2 -0.1468 (0.0742)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5935 (-0.5593)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][632/781]\tTime  0.322 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.8666 (0.8443)\tD(fake)1 0.0766 (0.1517)\tD(fake)2 0.0715 (0.0729)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4935 (-0.5573)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][662/781]\tTime  0.327 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.9050 (0.8463)\tD(fake)1 0.1146 (0.1501)\tD(fake)2 0.1669 (0.0715)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.2248 (-0.5529)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][692/781]\tTime  0.319 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.7481 (0.8474)\tD(fake)1 -0.0826 (0.1490)\tD(fake)2 0.0460 (0.0704)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5495 (-0.5494)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][722/781]\tTime  0.313 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.8476 (0.8491)\tD(fake)1 -0.0689 (0.1473)\tD(fake)2 0.2032 (0.0704)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6520 (-0.5522)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][752/781]\tTime  0.321 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.8096 (0.8506)\tD(fake)1 -0.0267 (0.1457)\tD(fake)2 0.0877 (0.0695)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5818 (-0.5548)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [5][780/781]\tTime  0.315 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.9121 (0.8530)\tD(fake)1 0.1179 (0.1437)\tD(fake)2 -0.0162 (0.0686)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5954 (-0.5548)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][  2/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 0.9451 (0.9436)\tD(fake)1 -0.0147 (0.0431)\tD(fake)2 0.0302 (0.0302)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5171 (-0.5171)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][ 32/781]\tTime  0.312 ( 0.328)\tData  0.000 ( 0.000)\tD(real) 0.7663 (0.8901)\tD(fake)1 -0.0996 (0.1061)\tD(fake)2 0.1115 (0.0519)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6047 (-0.4762)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][ 62/781]\tTime  0.318 ( 0.323)\tData  0.000 ( 0.000)\tD(real) 0.9529 (0.8911)\tD(fake)1 0.0030 (0.1121)\tD(fake)2 0.0183 (0.0518)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6966 (-0.5196)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][ 92/781]\tTime  0.323 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8054 (0.8755)\tD(fake)1 0.0166 (0.1236)\tD(fake)2 0.0401 (0.0591)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5719 (-0.5502)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][122/781]\tTime  0.312 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7580 (0.8730)\tD(fake)1 -0.0702 (0.1242)\tD(fake)2 0.0068 (0.0568)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5420 (-0.5622)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][152/781]\tTime  0.321 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.8793 (0.8657)\tD(fake)1 0.0781 (0.1314)\tD(fake)2 0.0806 (0.0599)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4906 (-0.5603)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][182/781]\tTime  0.321 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.8991 (0.8664)\tD(fake)1 0.0322 (0.1329)\tD(fake)2 0.0528 (0.0602)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4302 (-0.5410)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][212/781]\tTime  0.321 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7388 (0.8608)\tD(fake)1 0.0461 (0.1361)\tD(fake)2 0.1003 (0.0631)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.0932 (-0.5215)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][242/781]\tTime  0.332 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.8888 (0.8607)\tD(fake)1 -0.0800 (0.1369)\tD(fake)2 -0.0301 (0.0621)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.2640 (-0.4975)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][272/781]\tTime  0.314 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.8049 (0.8628)\tD(fake)1 -0.0450 (0.1354)\tD(fake)2 0.0347 (0.0608)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4259 (-0.4875)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][302/781]\tTime  0.325 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.8100 (0.8602)\tD(fake)1 0.0051 (0.1370)\tD(fake)2 0.0979 (0.0638)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3847 (-0.4831)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][332/781]\tTime  0.312 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.9120 (0.8614)\tD(fake)1 0.2743 (0.1364)\tD(fake)2 0.0911 (0.0615)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4419 (-0.4826)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][362/781]\tTime  0.315 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.4591 (0.8611)\tD(fake)1 -0.1455 (0.1361)\tD(fake)2 -0.0183 (0.0614)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3732 (-0.4791)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][392/781]\tTime  0.313 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.9249 (0.8610)\tD(fake)1 0.0434 (0.1366)\tD(fake)2 -0.0348 (0.0596)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6700 (-0.4806)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][422/781]\tTime  0.312 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7526 (0.8636)\tD(fake)1 -0.0524 (0.1338)\tD(fake)2 0.0716 (0.0570)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6217 (-0.4863)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][452/781]\tTime  0.333 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.9245 (0.8631)\tD(fake)1 0.2479 (0.1344)\tD(fake)2 0.0582 (0.0561)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6510 (-0.4914)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][482/781]\tTime  0.325 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7010 (0.8640)\tD(fake)1 -0.0700 (0.1334)\tD(fake)2 0.0576 (0.0543)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5901 (-0.4948)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][512/781]\tTime  0.319 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.8265 (0.8645)\tD(fake)1 -0.0010 (0.1331)\tD(fake)2 0.0492 (0.0529)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4455 (-0.4947)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][542/781]\tTime  0.321 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.9558 (0.8659)\tD(fake)1 0.1339 (0.1317)\tD(fake)2 0.1051 (0.0526)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4733 (-0.4967)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][572/781]\tTime  0.318 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.8702 (0.8667)\tD(fake)1 0.1224 (0.1312)\tD(fake)2 0.1334 (0.0518)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6162 (-0.5021)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][602/781]\tTime  0.314 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.8524 (0.8668)\tD(fake)1 0.0663 (0.1312)\tD(fake)2 0.1376 (0.0546)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.2694 (-0.5043)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][632/781]\tTime  0.320 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.6068 (0.8655)\tD(fake)1 -0.0665 (0.1318)\tD(fake)2 0.1119 (0.0550)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3258 (-0.5056)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][662/781]\tTime  0.313 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.6785 (0.8658)\tD(fake)1 0.0664 (0.1322)\tD(fake)2 0.0159 (0.0556)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5527 (-0.5069)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][692/781]\tTime  0.330 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7713 (0.8646)\tD(fake)1 0.0023 (0.1331)\tD(fake)2 0.1163 (0.0562)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5193 (-0.5084)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][722/781]\tTime  0.316 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.8325 (0.8649)\tD(fake)1 0.0043 (0.1328)\tD(fake)2 -0.0109 (0.0555)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6305 (-0.5105)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][752/781]\tTime  0.323 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.9429 (0.8671)\tD(fake)1 0.0829 (0.1312)\tD(fake)2 -0.0490 (0.0538)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5635 (-0.5137)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [6][780/781]\tTime  0.310 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.9529 (0.8677)\tD(fake)1 0.2520 (0.1306)\tD(fake)2 0.1295 (0.0554)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7503 (-0.5162)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][  2/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 0.9350 (0.8834)\tD(fake)1 0.0139 (0.0057)\tD(fake)2 0.0963 (0.0963)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7707 (-0.7707)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][ 32/781]\tTime  0.313 ( 0.329)\tData  0.000 ( 0.000)\tD(real) 0.7570 (0.8818)\tD(fake)1 -0.0728 (0.1073)\tD(fake)2 0.1957 (0.0862)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3466 (-0.5661)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][ 62/781]\tTime  0.316 ( 0.324)\tData  0.000 ( 0.000)\tD(real) 0.8624 (0.8751)\tD(fake)1 0.0633 (0.1212)\tD(fake)2 0.0670 (0.0743)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7003 (-0.5972)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][ 92/781]\tTime  0.325 ( 0.325)\tData  0.000 ( 0.000)\tD(real) 0.9075 (0.8777)\tD(fake)1 0.0070 (0.1170)\tD(fake)2 -0.0154 (0.0616)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6476 (-0.6100)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][122/781]\tTime  0.335 ( 0.326)\tData  0.000 ( 0.000)\tD(real) 0.5351 (0.8750)\tD(fake)1 -0.1077 (0.1165)\tD(fake)2 0.1074 (0.0554)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5770 (-0.6219)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][152/781]\tTime  0.312 ( 0.325)\tData  0.000 ( 0.000)\tD(real) 0.8920 (0.8729)\tD(fake)1 0.2160 (0.1222)\tD(fake)2 0.1437 (0.0616)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6706 (-0.6285)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][182/781]\tTime  0.322 ( 0.325)\tData  0.000 ( 0.000)\tD(real) 0.9172 (0.8699)\tD(fake)1 0.1345 (0.1262)\tD(fake)2 0.0632 (0.0599)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6532 (-0.6290)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][212/781]\tTime  0.318 ( 0.324)\tData  0.000 ( 0.000)\tD(real) 0.9577 (0.8699)\tD(fake)1 0.1172 (0.1268)\tD(fake)2 -0.0282 (0.0582)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6600 (-0.6304)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][242/781]\tTime  0.318 ( 0.323)\tData  0.000 ( 0.000)\tD(real) 0.7547 (0.8721)\tD(fake)1 0.0269 (0.1243)\tD(fake)2 0.1940 (0.0573)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6706 (-0.6378)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][272/781]\tTime  0.318 ( 0.323)\tData  0.000 ( 0.000)\tD(real) 0.9485 (0.8699)\tD(fake)1 0.1041 (0.1271)\tD(fake)2 0.0022 (0.0574)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5707 (-0.6336)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][302/781]\tTime  0.314 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.9150 (0.8683)\tD(fake)1 0.0483 (0.1281)\tD(fake)2 -0.0453 (0.0573)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5845 (-0.6181)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][332/781]\tTime  0.315 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.9318 (0.8705)\tD(fake)1 0.0012 (0.1265)\tD(fake)2 -0.0662 (0.0547)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6066 (-0.6159)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][362/781]\tTime  0.315 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.8915 (0.8705)\tD(fake)1 0.1329 (0.1265)\tD(fake)2 0.0166 (0.0563)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5696 (-0.6148)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][392/781]\tTime  0.315 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8172 (0.8707)\tD(fake)1 -0.0088 (0.1265)\tD(fake)2 0.0131 (0.0543)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5689 (-0.6150)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][422/781]\tTime  0.329 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.9213 (0.8719)\tD(fake)1 0.0605 (0.1254)\tD(fake)2 0.0244 (0.0531)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5024 (-0.6095)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][452/781]\tTime  0.328 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.8906 (0.8719)\tD(fake)1 0.1312 (0.1252)\tD(fake)2 0.0966 (0.0557)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5954 (-0.6075)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][482/781]\tTime  0.325 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.5926 (0.8705)\tD(fake)1 -0.0836 (0.1263)\tD(fake)2 0.0294 (0.0565)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5137 (-0.6053)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][512/781]\tTime  0.332 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7485 (0.8702)\tD(fake)1 -0.0980 (0.1268)\tD(fake)2 0.0497 (0.0566)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5718 (-0.6031)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][542/781]\tTime  0.313 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.5439 (0.8692)\tD(fake)1 -0.1314 (0.1271)\tD(fake)2 0.2208 (0.0585)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6092 (-0.6012)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][572/781]\tTime  0.326 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7973 (0.8687)\tD(fake)1 0.0169 (0.1287)\tD(fake)2 0.1037 (0.0599)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6147 (-0.5994)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][602/781]\tTime  0.313 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7212 (0.8680)\tD(fake)1 -0.0201 (0.1291)\tD(fake)2 0.1432 (0.0597)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5388 (-0.6001)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][632/781]\tTime  0.310 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8925 (0.8678)\tD(fake)1 -0.0064 (0.1292)\tD(fake)2 0.0806 (0.0601)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6615 (-0.6014)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][662/781]\tTime  0.321 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7653 (0.8681)\tD(fake)1 -0.0199 (0.1289)\tD(fake)2 0.0036 (0.0595)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5769 (-0.5981)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][692/781]\tTime  0.318 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.9274 (0.8695)\tD(fake)1 0.0313 (0.1278)\tD(fake)2 0.0450 (0.0585)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4689 (-0.5983)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][722/781]\tTime  0.313 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.9608 (0.8701)\tD(fake)1 0.1461 (0.1273)\tD(fake)2 0.0088 (0.0581)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6662 (-0.5969)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][752/781]\tTime  0.321 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.9685 (0.8701)\tD(fake)1 0.1441 (0.1270)\tD(fake)2 -0.0274 (0.0580)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5844 (-0.5947)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [7][780/781]\tTime  0.312 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8874 (0.8708)\tD(fake)1 0.1619 (0.1265)\tD(fake)2 0.0051 (0.0577)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6249 (-0.5942)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][  2/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 0.9652 (0.9143)\tD(fake)1 0.0963 (0.0900)\tD(fake)2 0.0147 (0.0147)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.1927 (-0.1927)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][ 32/781]\tTime  0.318 ( 0.326)\tData  0.000 ( 0.000)\tD(real) 0.7459 (0.8751)\tD(fake)1 -0.0720 (0.1130)\tD(fake)2 0.0695 (0.0561)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6183 (-0.5046)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][ 62/781]\tTime  0.320 ( 0.325)\tData  0.000 ( 0.000)\tD(real) 0.7385 (0.8716)\tD(fake)1 -0.0340 (0.1230)\tD(fake)2 0.0187 (0.0624)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5854 (-0.5674)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][ 92/781]\tTime  0.311 ( 0.324)\tData  0.000 ( 0.000)\tD(real) 0.8254 (0.8695)\tD(fake)1 0.0341 (0.1265)\tD(fake)2 0.0030 (0.0576)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4866 (-0.5619)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][122/781]\tTime  0.313 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.9037 (0.8731)\tD(fake)1 0.0443 (0.1252)\tD(fake)2 0.0135 (0.0565)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5921 (-0.5697)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][152/781]\tTime  0.309 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.7440 (0.8750)\tD(fake)1 -0.0987 (0.1218)\tD(fake)2 0.0154 (0.0561)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6794 (-0.5800)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][182/781]\tTime  0.318 ( 0.320)\tData  0.000 ( 0.000)\tD(real) 0.8491 (0.8736)\tD(fake)1 0.0831 (0.1235)\tD(fake)2 0.2853 (0.0611)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6681 (-0.5872)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][212/781]\tTime  0.316 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8174 (0.8693)\tD(fake)1 0.1407 (0.1272)\tD(fake)2 0.0677 (0.0623)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5284 (-0.5817)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][242/781]\tTime  0.320 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8284 (0.8709)\tD(fake)1 0.0357 (0.1259)\tD(fake)2 0.0999 (0.0605)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3219 (-0.5765)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][272/781]\tTime  0.319 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.9094 (0.8707)\tD(fake)1 0.1137 (0.1268)\tD(fake)2 0.0621 (0.0632)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5697 (-0.5747)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][302/781]\tTime  0.321 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8141 (0.8693)\tD(fake)1 0.0295 (0.1274)\tD(fake)2 0.1341 (0.0633)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6154 (-0.5792)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][332/781]\tTime  0.316 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.7980 (0.8702)\tD(fake)1 0.0243 (0.1273)\tD(fake)2 0.0075 (0.0611)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5532 (-0.5821)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][362/781]\tTime  0.317 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8738 (0.8713)\tD(fake)1 0.0254 (0.1264)\tD(fake)2 0.0057 (0.0606)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4616 (-0.5753)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][392/781]\tTime  0.316 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.7925 (0.8712)\tD(fake)1 -0.0290 (0.1262)\tD(fake)2 0.0340 (0.0608)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6542 (-0.5760)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][422/781]\tTime  0.319 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.8247 (0.8716)\tD(fake)1 -0.0582 (0.1263)\tD(fake)2 0.0099 (0.0591)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6519 (-0.5758)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][452/781]\tTime  0.323 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.7365 (0.8734)\tD(fake)1 -0.0841 (0.1245)\tD(fake)2 -0.0017 (0.0561)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6515 (-0.5748)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][482/781]\tTime  0.315 ( 0.319)\tData  0.000 ( 0.000)\tD(real) 0.9195 (0.8749)\tD(fake)1 0.0396 (0.1237)\tD(fake)2 -0.0186 (0.0566)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6499 (-0.5777)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][512/781]\tTime  0.315 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.9101 (0.8744)\tD(fake)1 0.1249 (0.1237)\tD(fake)2 0.0842 (0.0560)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5343 (-0.5815)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][542/781]\tTime  0.315 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.8241 (0.8739)\tD(fake)1 0.0339 (0.1237)\tD(fake)2 0.0011 (0.0538)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6235 (-0.5825)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][572/781]\tTime  0.316 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.7764 (0.8738)\tD(fake)1 -0.0855 (0.1239)\tD(fake)2 -0.0089 (0.0540)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6730 (-0.5812)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][602/781]\tTime  0.311 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.5703 (0.8733)\tD(fake)1 -0.1285 (0.1239)\tD(fake)2 -0.0069 (0.0523)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.4858 (-0.5765)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][632/781]\tTime  0.317 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.8101 (0.8731)\tD(fake)1 -0.0439 (0.1242)\tD(fake)2 0.0058 (0.0523)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5911 (-0.5790)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][662/781]\tTime  0.317 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.5423 (0.8739)\tD(fake)1 -0.1129 (0.1230)\tD(fake)2 -0.0107 (0.0509)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3620 (-0.5799)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][692/781]\tTime  0.318 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.9280 (0.8750)\tD(fake)1 -0.0103 (0.1225)\tD(fake)2 0.0780 (0.0499)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6374 (-0.5808)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][722/781]\tTime  0.326 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.9494 (0.8756)\tD(fake)1 0.1267 (0.1221)\tD(fake)2 0.0536 (0.0487)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5613 (-0.5813)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][752/781]\tTime  0.314 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.9179 (0.8748)\tD(fake)1 -0.0183 (0.1228)\tD(fake)2 -0.0690 (0.0481)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5053 (-0.5787)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [8][780/781]\tTime  0.315 ( 0.318)\tData  0.000 ( 0.000)\tD(real) 0.8506 (0.8755)\tD(fake)1 0.1621 (0.1218)\tD(fake)2 0.0480 (0.0470)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6039 (-0.5779)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][  2/781]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tD(real) 0.8923 (0.9430)\tD(fake)1 -0.0090 (0.0925)\tD(fake)2 0.0035 (0.0035)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5311 (-0.5311)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][ 32/781]\tTime  0.330 ( 0.339)\tData  0.000 ( 0.000)\tD(real) 0.9646 (0.9209)\tD(fake)1 0.0855 (0.0898)\tD(fake)2 0.0113 (0.0021)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6111 (-0.5566)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][ 62/781]\tTime  0.321 ( 0.331)\tData  0.000 ( 0.000)\tD(real) 0.8911 (0.9045)\tD(fake)1 -0.0318 (0.0959)\tD(fake)2 0.0672 (0.0130)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5696 (-0.5450)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][ 92/781]\tTime  0.318 ( 0.328)\tData  0.000 ( 0.000)\tD(real) 0.8838 (0.8955)\tD(fake)1 0.0580 (0.1018)\tD(fake)2 -0.0542 (0.0149)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5804 (-0.5607)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][122/781]\tTime  0.318 ( 0.326)\tData  0.000 ( 0.000)\tD(real) 0.8286 (0.8915)\tD(fake)1 -0.0502 (0.1053)\tD(fake)2 -0.0404 (0.0247)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6496 (-0.5631)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][152/781]\tTime  0.311 ( 0.324)\tData  0.000 ( 0.000)\tD(real) 0.8375 (0.8885)\tD(fake)1 -0.0745 (0.1080)\tD(fake)2 -0.0837 (0.0274)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3647 (-0.5479)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][182/781]\tTime  0.324 ( 0.323)\tData  0.000 ( 0.000)\tD(real) 0.8024 (0.8874)\tD(fake)1 0.0606 (0.1084)\tD(fake)2 0.0526 (0.0238)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5852 (-0.5599)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][212/781]\tTime  0.323 ( 0.324)\tData  0.000 ( 0.000)\tD(real) 0.6831 (0.8843)\tD(fake)1 -0.0698 (0.1108)\tD(fake)2 -0.0089 (0.0246)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5888 (-0.5691)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][242/781]\tTime  0.313 ( 0.324)\tData  0.000 ( 0.000)\tD(real) 0.9062 (0.8878)\tD(fake)1 -0.0013 (0.1106)\tD(fake)2 0.0226 (0.0232)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.1187 (-0.5633)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][272/781]\tTime  0.317 ( 0.323)\tData  0.000 ( 0.000)\tD(real) 0.8088 (0.8871)\tD(fake)1 0.0021 (0.1106)\tD(fake)2 0.1002 (0.0246)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.1647 (-0.5557)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][302/781]\tTime  0.311 ( 0.323)\tData  0.000 ( 0.000)\tD(real) 0.7866 (0.8836)\tD(fake)1 -0.0160 (0.1129)\tD(fake)2 0.0249 (0.0266)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7336 (-0.5597)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][332/781]\tTime  0.325 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.8025 (0.8827)\tD(fake)1 0.0374 (0.1134)\tD(fake)2 0.1229 (0.0271)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3289 (-0.5650)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][362/781]\tTime  0.311 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.9614 (0.8821)\tD(fake)1 0.1806 (0.1141)\tD(fake)2 0.1106 (0.0286)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6063 (-0.5694)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][392/781]\tTime  0.320 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.8851 (0.8833)\tD(fake)1 0.1158 (0.1139)\tD(fake)2 0.0317 (0.0297)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7462 (-0.5768)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][422/781]\tTime  0.319 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.8997 (0.8818)\tD(fake)1 0.1203 (0.1153)\tD(fake)2 0.0045 (0.0307)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6742 (-0.5836)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][452/781]\tTime  0.313 ( 0.322)\tData  0.000 ( 0.000)\tD(real) 0.8046 (0.8826)\tD(fake)1 -0.0256 (0.1146)\tD(fake)2 0.0772 (0.0294)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6881 (-0.5827)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][482/781]\tTime  0.317 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8905 (0.8826)\tD(fake)1 -0.0621 (0.1147)\tD(fake)2 0.0863 (0.0311)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5935 (-0.5836)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][512/781]\tTime  0.320 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.6408 (0.8824)\tD(fake)1 -0.0621 (0.1149)\tD(fake)2 0.0281 (0.0327)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5252 (-0.5836)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][542/781]\tTime  0.320 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.7978 (0.8821)\tD(fake)1 -0.0549 (0.1152)\tD(fake)2 0.0985 (0.0347)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7062 (-0.5838)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][572/781]\tTime  0.321 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8404 (0.8811)\tD(fake)1 0.0958 (0.1163)\tD(fake)2 0.0759 (0.0372)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6022 (-0.5824)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][602/781]\tTime  0.319 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8246 (0.8800)\tD(fake)1 0.0537 (0.1170)\tD(fake)2 0.0888 (0.0378)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6332 (-0.5861)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][632/781]\tTime  0.327 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.9015 (0.8796)\tD(fake)1 0.0536 (0.1179)\tD(fake)2 0.1217 (0.0385)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6036 (-0.5893)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][662/781]\tTime  0.315 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8675 (0.8778)\tD(fake)1 0.0822 (0.1192)\tD(fake)2 0.0298 (0.0397)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.6724 (-0.5907)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][692/781]\tTime  0.330 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8142 (0.8767)\tD(fake)1 0.2309 (0.1193)\tD(fake)2 0.2523 (0.0415)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.7329 (-0.5952)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][722/781]\tTime  0.315 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.8681 (0.8767)\tD(fake)1 -0.0064 (0.1205)\tD(fake)2 -0.0499 (0.0417)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.3726 (-0.5960)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][752/781]\tTime  0.320 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.9896 (0.8768)\tD(fake)1 0.1471 (0.1208)\tD(fake)2 -0.0443 (0.0406)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5228 (-0.5935)\tD repr loss 0.0000 (0.0000)\n",
            "Epoch: [9][780/781]\tTime  0.312 ( 0.321)\tData  0.000 ( 0.000)\tD(real) 0.9266 (0.8769)\tD(fake)1 0.1248 (0.1207)\tD(fake)2 -0.0557 (0.0409)\tgrad(D) 0.0000 (0.0000)\tG repr loss -0.5360 (-0.5911)\tD repr loss 0.0000 (0.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-p4wAlbftUb"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUGjJ-6Pf3xE"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhsj8_owgDSl"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNxlmBIutPfw"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iepY_Go7tf7Y"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5V-0_k5tpiW"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdIo707ZyUic"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9naW-A2OGLMg"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzUaCS1tS8YV"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKWZPniqS5uX"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYyP-fb7S-WE"
      },
      "source": [
        "run(epochs_per_cell)\n",
        "save_vid()\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMSlrEvie7Pa"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvYdbtKQe_2-"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "def show_sample(x, num_samples=16, show_x=False):\n",
        "    x = x.cuda(args.gpu)[:num_samples]\n",
        "    if show_x:\n",
        "        x_grid = vutils.make_grid(inv_normalize(x).cpu(), padding=2, nrow=4)\n",
        "        plt.figure(figsize=(8,8))\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(x_grid.permute(1,2,0))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        noise = sample_noise(num_samples).cuda(args.gpu)\n",
        "        repr = simsiam.encoder(x)\n",
        "        repr = F.normalize(repr + args.repr_noise * torch.randn_like(repr))\n",
        "        z = latent_transform(repr, noise)\n",
        "        x_fake = model.G(z)\n",
        "    im_grid = vutils.make_grid(x_fake.cpu(), padding=2, nrow=4, normalize=True, range=(-1,1))\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(im_grid.permute(1,2,0))\n",
        "\n",
        "x, _ = next(iter(train_loader))\n",
        "show_sample(x, show_x=True)\n",
        "show_sample(x)\n",
        "show_sample(x)\n",
        "show_sample(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gfx3T5m2wah"
      },
      "source": [
        "save_vid()\n",
        "save()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5P0BestcMER"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}