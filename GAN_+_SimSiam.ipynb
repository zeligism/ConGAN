{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN + SimSiam",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeligism/ConGAN/blob/main/GAN_%2B_SimSiam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxx3Jy_8qsPE"
      },
      "source": [
        "### Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MFx20xTNkpQ",
        "outputId": "c3a1f9b9-d070-4497-8ecd-fc83b2c69849"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-QNzdq01hSb"
      },
      "source": [
        "# Header"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSlF68ff2K8L"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf_Qrpq7z3iJ"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import glob\n",
        "import random\n",
        "import datetime\n",
        "import yaml\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "import torchvision\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.tensorboard as tensorboard\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "from math import log2\n",
        "from pprint import pformat\n",
        "from collections import defaultdict"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USDduLe1Qkd9"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiRxrufxw1cm"
      },
      "source": [
        "### Report Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmEyNG58w2kJ"
      },
      "source": [
        "def plot_lines(losses_dict, filename=None, title=\"\"):\n",
        "    \"\"\"\n",
        "    Plots the losses of the discriminator and the generator.\n",
        "\n",
        "    Args:\n",
        "        filename: The plot's filename. If None, plot won't be saved.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(title)\n",
        "    for label, losses in losses_dict.items():\n",
        "        plt.plot(losses, label=label)\n",
        "    plt.xlabel(\"t\")\n",
        "    plt.legend()\n",
        "    \n",
        "    if filename is not None:\n",
        "        plt.savefig(filename)\n",
        "    \n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def create_progress_animation(frames, filename):\n",
        "    \"\"\"\n",
        "    Creates a video of the progress of the generator on a fixed latent vector.\n",
        "\n",
        "    Args:\n",
        "        filename: The animation's filename.\n",
        "    \"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    plt.axis(\"off\")\n",
        "    ims = [[plt.imshow(img.permute(1,2,0), animated=True)]\n",
        "           for img in frames]\n",
        "    ani = animation.ArtistAnimation(fig, ims, blit=True)\n",
        "    \n",
        "    ani.save(filename)\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def generate_grid(generator, latent):\n",
        "    \"\"\"\n",
        "    Check generator's output on latent vectors and return it.\n",
        "\n",
        "    Args:\n",
        "        generator: The generator.\n",
        "        latent: Latent vector from which an image grid will be generated.\n",
        "\n",
        "    Returns:\n",
        "        A grid of images generated by `generator` from `latent`.\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake = generator(latent).detach()\n",
        "\n",
        "    image_grid = vutils.make_grid(fake.cpu(), padding=2, normalize=True, range=(-1,1))\n",
        "\n",
        "    return image_grid\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUzwGurc1qOx"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPhc2oS53G4e"
      },
      "source": [
        "## PyTorch Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU7HFc6t5N8w"
      },
      "source": [
        "### DCGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJHPo8w13JmH"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Default stride and padding half the size of features,\n",
        "    e.g. if input is [in_channels, 64, 64], output will be [out_channels, 32, 32].\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1,\n",
        "                 use_batchnorm=True, use_spectralnorm=False, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                              stride=stride, padding=padding, bias=False)\n",
        "        if use_spectralnorm:\n",
        "            self.conv = nn.utils.spectral_norm(self.conv)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels) if use_batchnorm else None\n",
        "        self.activation = nn.LeakyReLU(0.2, inplace=True) if activation is None else activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.batchnorm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Default stride and padding double the size of features,\n",
        "    e.g. if input is [in_channels, 32, 32], output will be [out_channels, 64, 64].\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1,\n",
        "                 use_batchnorm=True, use_spectralnorm=False, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convT = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                                        stride=stride, padding=padding, bias=False)\n",
        "        if use_spectralnorm:\n",
        "            self.convT = nn.utils.spectral_norm(self.convT)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels) if use_batchnorm else None\n",
        "        self.activation = nn.ReLU(inplace=True) if activation is None else activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convT(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.batchnorm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCGAN_Discriminator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_latents=16,\n",
        "                 num_features=64,\n",
        "                 image_channels=3,\n",
        "                 image_size=64,\n",
        "                 feature_multiplier=2,\n",
        "                 max_features=512,\n",
        "                 gan_type=\"gan\",\n",
        "                 fully_convolutional=True,\n",
        "                 activation=None,\n",
        "                 use_batchnorm=True,\n",
        "                 use_spectralnorm=False,\n",
        "                 D_block=ConvBlock):\n",
        "        super().__init__()\n",
        "\n",
        "        using_grad_penalty = gan_type in (\"gan-gp\", \"wgan-gp\")\n",
        "        output_sigmoid = gan_type in (\"gan\", \"gan-gp\")\n",
        "\n",
        "        block_config = {\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": use_batchnorm and not using_grad_penalty,\n",
        "            \"use_spectralnorm\": use_spectralnorm,\n",
        "        }\n",
        "\n",
        "        # Calculate intermediate image sizes\n",
        "        image_sizes = [image_size]\n",
        "        while image_sizes[-1] > 5:\n",
        "            image_sizes.append(image_sizes[-1] // 2)\n",
        "        latent_kernel = image_sizes[-1]  # should be either 3, 4, or 5\n",
        "        num_layers = len(image_sizes) - 1\n",
        "\n",
        "        # Calculate feature sizes\n",
        "        features = [min(max_features, round(num_features * feature_multiplier**i))\n",
        "                    for i in range(num_layers)]\n",
        "\n",
        "        # Input layer\n",
        "        self.input_layer = D_block(image_channels, features[0], **block_config)\n",
        "\n",
        "        # Intermediate layers\n",
        "        self.main_layers = nn.Sequential(*[\n",
        "            D_block(in_features, out_features, **block_config)\n",
        "            for in_features, out_features in zip(features, features[1:])\n",
        "        ])\n",
        "\n",
        "        # Output layer (feature_size = 3, 4, or 5 -> 1)\n",
        "        if fully_convolutional:\n",
        "            self.output_layer = nn.Sequential(\n",
        "                nn.Conv2d(features[-1], num_latents, latent_kernel, bias=False),\n",
        "                nn.Flatten(),\n",
        "            )\n",
        "        else:\n",
        "            self.output_layer = nn.Sequential(\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(features[-1] * latent_kernel**2, num_latents, bias=False)\n",
        "            )\n",
        "\n",
        "        # Add sigmoid activation if using regular GAN loss\n",
        "        self.output_activation = nn.Sigmoid() if output_sigmoid else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.main_layers(x)\n",
        "        x = self.output_layer(x)\n",
        "        if self.output_activation:\n",
        "            x = self.output_activation(x)\n",
        "        # Remove H and W dimensions, infer channels dim (remove if 1)\n",
        "        x = x.view(x.size(0), -1).squeeze(1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCGAN_Generator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_latents=100,\n",
        "                 num_features=64,\n",
        "                 image_channels=3,\n",
        "                 image_size=64,\n",
        "                 feature_multiplier=2,\n",
        "                 max_features=512,\n",
        "                 gan_type=\"gan\",\n",
        "                 fully_convolutional=True,\n",
        "                 activation=None,\n",
        "                 use_batchnorm=True,\n",
        "                 use_spectralnorm=False,\n",
        "                 G_block=ConvTBlock):\n",
        "        super().__init__()\n",
        "\n",
        "        block_config = {\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": use_batchnorm,\n",
        "            \"use_spectralnorm\": use_spectralnorm\n",
        "        }\n",
        "\n",
        "        # Calculate intermediate image sizes\n",
        "        image_sizes = [image_size]\n",
        "        while image_sizes[-1] > 5:\n",
        "            image_sizes.append(image_sizes[-1] // 2)\n",
        "        latent_kernel = image_sizes[-1]  # should be either 3, 4, or 5\n",
        "        num_layers = len(image_sizes) - 1\n",
        "\n",
        "        # Calculate feature sizes\n",
        "        features = [min(max_features, round(num_features * feature_multiplier**i))\n",
        "                    for i in range(num_layers)]\n",
        "\n",
        "        # Reverse order of image sizes and features for generator\n",
        "        image_sizes = image_sizes[::-1]\n",
        "        features = features[::-1]\n",
        "\n",
        "        # Input layer\n",
        "        if fully_convolutional:\n",
        "            self.input_layer = G_block(num_latents, features[0], kernel_size=latent_kernel,\n",
        "                                       stride=1, padding=0, **block_config)\n",
        "        else:\n",
        "            self.input_layer = nn.Sequential(\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(num_latents, features[0] * image_sizes[0]**2, bias=False),\n",
        "                View(features[0], image_sizes[0], image_sizes[0])\n",
        "            )\n",
        "\n",
        "        # Intermediate layers\n",
        "        self.main_layers = nn.Sequential(*[\n",
        "            G_block(in_features, out_features, kernel_size=4+(expected_size%2), **block_config)\n",
        "            for in_features, out_features, expected_size in zip(features, features[1:], image_sizes[1:])\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.ConvTranspose2d(features[-1], image_channels, kernel_size=4+(image_size%2),\n",
        "                                               stride=2, padding=1, bias=False)\n",
        "        self.output_activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add H and W dimensions, infer channels dim (add if none)\n",
        "        x = x.view(x.size(0), -1, 1, 1)\n",
        "        x = self.input_layer(x)\n",
        "        x = self.main_layers(x)\n",
        "        x = self.output_layer(x)\n",
        "        x = self.output_activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCGAN(nn.Module):\n",
        "    \"\"\"Deep Convolutional Generative Adversarial Network\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_latents=100,\n",
        "                 num_features=64,\n",
        "                 image_channels=3,\n",
        "                 image_size=64,\n",
        "                 feature_multiplier=2,\n",
        "                 gan_type=\"gan\",\n",
        "                 fully_convolutional=True,\n",
        "                 activation=None,\n",
        "                 use_batchnorm=True,\n",
        "                 use_spectralnorm=False,):\n",
        "        \"\"\"\n",
        "        Initializes DCGAN.\n",
        "\n",
        "        Args:\n",
        "            num_latents: Number of latent factors.\n",
        "            num_features: Number of features in the convolutions.\n",
        "            image_channels: Number of channels in the input image.\n",
        "            image_size: Size (i.e. height or width) of image.\n",
        "            gan_type: Type of GAN (e.g. \"gan\" or \"wgan-gp\").\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_latents = num_latents\n",
        "        self.num_features = num_features\n",
        "        self.image_channels = image_channels\n",
        "        self.image_size = image_size\n",
        "        self.feature_multiplier = feature_multiplier\n",
        "        self.gan_type = gan_type\n",
        "        self.fully_convolutional = fully_convolutional\n",
        "        self.activation = activation\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        self.use_spectralnorm = use_spectralnorm\n",
        "\n",
        "        D_params = {\n",
        "            \"num_latents\": 1,  # XXX\n",
        "            \"num_features\": num_features,\n",
        "            \"image_channels\": image_channels,\n",
        "            \"image_size\": image_size,\n",
        "            \"feature_multiplier\": feature_multiplier,\n",
        "            \"gan_type\": gan_type,\n",
        "            \"fully_convolutional\": fully_convolutional,\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": use_batchnorm,\n",
        "            \"use_spectralnorm\": use_spectralnorm,\n",
        "        }\n",
        "        G_params = {\n",
        "            \"num_latents\": num_latents,\n",
        "            \"num_features\": num_features,\n",
        "            \"image_channels\": image_channels,\n",
        "            \"image_size\": image_size,\n",
        "            \"feature_multiplier\": feature_multiplier,\n",
        "            \"gan_type\": gan_type,\n",
        "            \"fully_convolutional\": fully_convolutional,\n",
        "            \"activation\": activation,\n",
        "            \"use_batchnorm\": use_batchnorm,\n",
        "            \"use_spectralnorm\": use_spectralnorm,\n",
        "        }\n",
        "\n",
        "        self.D = DCGAN_Discriminator(**D_params)\n",
        "        self.G = DCGAN_Generator(**G_params)\n",
        "\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self, *shape, including_batch=False):\n",
        "        super().__init__()\n",
        "        self.shape = shape\n",
        "        self.including_batch = including_batch\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.including_batch:\n",
        "            return x.view(*self.shape)\n",
        "        else:\n",
        "            return x.view(x.size(0), *self.shape)\n",
        "\n",
        "class ChannelNoise(nn.Module):\n",
        "    \"\"\"\n",
        "    Channel noise injection module.\n",
        "    Adds a linearly transformed noise to a convolution layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, std=0.02):\n",
        "        super().__init__()\n",
        "        self.std = std\n",
        "        self.scale = nn.Parameter(torch.ones(1, num_channels, 1, 1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        noise_size = [x.size()[0], 1, *x.size()[2:]]  # single channel\n",
        "        noise = self.std * torch.randn(noise_size).to(x)\n",
        "\n",
        "        return x + self.scale * noise"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYujBEzC7EOO"
      },
      "source": [
        "### SimSiam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-YcNut27F-v"
      },
      "source": [
        "class SimSiam(nn.Module):\n",
        "    \"\"\"\n",
        "    Build a SimSiam model.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_encoder, dim=2048, pred_dim=512):\n",
        "        \"\"\"\n",
        "        dim: feature dimension (default: 2048)\n",
        "        pred_dim: hidden dimension of the predictor (default: 512)\n",
        "        \"\"\"\n",
        "        super(SimSiam, self).__init__()\n",
        "\n",
        "        # create the encoder\n",
        "        # num_classes is the output fc dimension, zero-initialize last BNs\n",
        "        self.encoder = base_encoder(num_classes=dim, zero_init_residual=True)\n",
        "\n",
        "        # build a 3-layer projector\n",
        "        prev_dim = self.encoder.fc.weight.shape[1]\n",
        "        self.encoder.fc = nn.Sequential(nn.Linear(prev_dim, prev_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(prev_dim),\n",
        "                                        nn.ReLU(inplace=True), # first layer\n",
        "                                        nn.Linear(prev_dim, prev_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(prev_dim),\n",
        "                                        nn.ReLU(inplace=True), # second layer\n",
        "                                        self.encoder.fc,\n",
        "                                        nn.BatchNorm1d(dim, affine=False)) # output layer\n",
        "        self.encoder.fc[6].bias.requires_grad = False # hack: not use bias as it is followed by BN\n",
        "\n",
        "        # build a 2-layer predictor\n",
        "        self.predictor = nn.Sequential(nn.Linear(dim, pred_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(pred_dim),\n",
        "                                        nn.ReLU(inplace=True), # hidden layer\n",
        "                                        nn.Linear(pred_dim, dim)) # output layer\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            x1: first views of images\n",
        "            x2: second views of images\n",
        "        Output:\n",
        "            p1, p2, z1, z2: predictors and targets of the network\n",
        "            See Sec. 3 of https://arxiv.org/abs/2011.10566 for detailed notations\n",
        "        \"\"\"\n",
        "\n",
        "        # compute features for one view\n",
        "        z1 = self.encoder(x1) # NxC\n",
        "        z2 = self.encoder(x2) # NxC\n",
        "\n",
        "        p1 = self.predictor(z1) # NxC\n",
        "        p2 = self.predictor(z2) # NxC\n",
        "\n",
        "        return p1, p2, z1.detach(), z2.detach()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQcvNDLQ5niT"
      },
      "source": [
        "### ConsistentGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeUqwjQ95mMV"
      },
      "source": [
        "class ConsistentGAN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 base_encoder,\n",
        "                 repr_dim,\n",
        "                 pred_dim,\n",
        "                 latent_dim,\n",
        "                 image_size=64,\n",
        "                 *args, **kwargs):\n",
        "        \"\"\"\n",
        "        s ~ S is representation/encoding space, e.g. s = Enc(x).\n",
        "        z ~ Z is latent/seed space for G, e.g. x_fake ~ G(z).\n",
        "\n",
        "        Case A:\n",
        "            - Sample x_real ~ X\n",
        "            - s <- Encoder(x_real)\n",
        "            - Sample z ~ Z (coupled with s somehow? e.g. z = f(s) + noise)\n",
        "            - x_fake <- G(z)\n",
        "            - Test D on x_real, x_fake, i.e. test whether x in X}\n",
        "            - Calculate GAN loss\n",
        "            - Calculate SimSiam loss with Predictor(s)\n",
        "\n",
        "        Case B: \n",
        "            - Sample x_real ~ X\n",
        "            - s_real <- Encoder(x_real)\n",
        "            - x_recon <- Decoder(s_real)\n",
        "            - Sample z ~ Z (e.g. N(0,1) or U(-1,1))\n",
        "            - s_fake <- G(z)\n",
        "            - Test D on s_real, s_fake, i.e. test whether Dec(s) in X (how?)\n",
        "            - Calculate GAN loss\n",
        "            - Calculate Contrastive/SimSiam loss with Predictor(s)\n",
        "            - Calculate reconsruction loss, e.g. || Enc(x_recon) - Enc(x_real) ||\n",
        "\n",
        "        We choose Case B for now.\n",
        "\n",
        "        Adversarial learning:\n",
        "            Case A:\n",
        "                Train D and G so that we can disciminate x based on its representation.\n",
        "                Train G so that representation has enough info for reconstructing x.\n",
        "                Ideally, G would generate different views of x given similar s's.\n",
        "            Case B:\n",
        "                Train D and G so that we can disciminate representations.\n",
        "                Train G so that it produces representations as real (close to Enc(x)) as possible.\n",
        "                This assumes that Enc(x) is the real representation.\n",
        "                (There is no such thing as any random transformation can be real enough, but\n",
        "                we should try to mitigate this problem nonetheless. For example, we can at least\n",
        "                make it stable enough by pre-training Enc/Dec.)\n",
        "                Ideally, G would produce accurate representations that can be decoded later.\n",
        "\n",
        "        Contrastive learning:\n",
        "            If x1 and x2 are views of same x (e.g. x1, x2 = rand_aug(x), rand_aug(x))\n",
        "            Then, Encoder(x1) should be similar to Encoder(x2), so we do this (SimSiam algorithm):\n",
        "            min 0.5*{ sim(Predictor(s1), s2.detach()) + sim(Predictor(s2), s1.detach()) }\n",
        "            where sim = CosineSimilarity(dim=1).\n",
        "        \n",
        "        Reconstruction learning:\n",
        "            We can add a (deterministic) Decoder that learns to decode s to x.\n",
        "            This does not take into account the invariance of representation to augmentations.\n",
        "            It simply learns the inverse of Encoder. G will learn to produce represenations\n",
        "            that are as real as possible, where the Decoder will learn to decode them into their\n",
        "            corresponding x's such that Enc(Dec(s)) = s (do we stop grad at s for this loss?)\n",
        "\n",
        "        Architecture:\n",
        "            We assume x comes from an image dataset, e.g. CIFAR10.\n",
        "            Encoder-Decoder pair can have a DCGAN-style architecture.\n",
        "            The generator can also have an arch similar to the decoder.\n",
        "            Other networks can be simple FCNs.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.repr_dim = repr_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.pred_dim = pred_dim\n",
        "\n",
        "        ### Copied from SimSiam repo >>>>>>>>\n",
        "        # create the encoder\n",
        "        # num_classes is the output fc dimension, zero-initialize last BNs\n",
        "        self.encoder = base_encoder(num_classes=repr_dim, zero_init_residual=True)\n",
        "\n",
        "        # build a 3-layer projector\n",
        "        prev_dim = self.encoder.fc.weight.shape[1]\n",
        "        self.encoder.fc = nn.Sequential(nn.Linear(prev_dim, prev_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(prev_dim),\n",
        "                                        nn.ReLU(inplace=True), # first layer\n",
        "                                        nn.Linear(prev_dim, prev_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(prev_dim),\n",
        "                                        nn.ReLU(inplace=True), # second layer\n",
        "                                        self.encoder.fc,\n",
        "                                        nn.BatchNorm1d(repr_dim, affine=False)) # output layer\n",
        "        self.encoder.fc[6].bias.requires_grad = False # hack: not use bias as it is followed by BN\n",
        "\n",
        "        # build a 2-layer predictor\n",
        "        self.predictor = nn.Sequential(nn.Linear(repr_dim, pred_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(pred_dim),\n",
        "                                        nn.ReLU(inplace=True), # hidden layer\n",
        "                                        nn.Linear(pred_dim, repr_dim)) # output layer\n",
        "        ### <<<<<<<<\n",
        "        \n",
        "        # Make D's architecture kinda similar to predictor @TODO\n",
        "        # @XXX: Shouldn't use batchnorm with grad_penalty\n",
        "        D_hidden_dim = repr_dim // 2\n",
        "        self.D = nn.Sequential(nn.Linear(repr_dim, D_hidden_dim, bias=False),\n",
        "                               nn.BatchNorm1d(D_hidden_dim),\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               nn.Linear(D_hidden_dim, D_hidden_dim, bias=False),\n",
        "                               nn.BatchNorm1d(D_hidden_dim),\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               nn.Linear(D_hidden_dim, D_hidden_dim, bias=False),\n",
        "                               nn.BatchNorm1d(D_hidden_dim),\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               nn.Linear(D_hidden_dim, 1),\n",
        "                               nn.Sigmoid())\n",
        "\n",
        "        # Same for generator (latent -> representations)\n",
        "        self.G = nn.Sequential(nn.Linear(latent_dim, repr_dim, bias=False),\n",
        "                               nn.BatchNorm1d(repr_dim),\n",
        "                               nn.ReLU(inplace=True), # hidden layer\n",
        "                               nn.Linear(repr_dim, repr_dim, bias=False),\n",
        "                               nn.BatchNorm1d(repr_dim),\n",
        "                               nn.ReLU(inplace=True), # hidden layer\n",
        "                               nn.Linear(repr_dim, repr_dim, bias=False),\n",
        "                               nn.BatchNorm1d(repr_dim),\n",
        "                               nn.ReLU(inplace=True), # hidden layer\n",
        "                               nn.Linear(repr_dim, repr_dim))\n",
        "\n",
        "        # Decoder should be a ConvT net. We'll use DCGAN's G for now @XXX\n",
        "        self.decoder = DCGAN_Generator(num_latents=repr_dim, image_size=image_size)\n",
        "\n",
        "        # To check progress of G\n",
        "        self.fixed_latent = self.sample_latent(8*8)\n",
        "    \n",
        "\n",
        "    def sample_latent(self, batch_size):\n",
        "\n",
        "        latent_size = [batch_size, self.latent_dim]\n",
        "        latent = torch.randn(latent_size)\n",
        "\n",
        "        return latent\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            x1: first views of images\n",
        "            x2: second views of images\n",
        "        Output:\n",
        "            p1, p2, z1, z2: predictors and targets of the network\n",
        "            See Sec. 3 of https://arxiv.org/abs/2011.10566 for detailed notations\n",
        "        \"\"\"\n",
        "\n",
        "        # compute features for one view\n",
        "        z1 = self.encoder(x1) # NxC\n",
        "        z2 = self.encoder(x2) # NxC\n",
        "\n",
        "        p1 = self.predictor(z1) # NxC\n",
        "        p2 = self.predictor(z2) # NxC\n",
        "\n",
        "        return p1, p2, z1.detach(), z2.detach()\n",
        "\n",
        "\n",
        "class ConsistentGAN2(nn.Module):\n",
        "    def __init__(self, repr_dim, latent_dim,\n",
        "                 D_batchnorm=True, image_size=64, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.repr_dim = repr_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        \n",
        "        # Make D's architecture kinda similar to predictor\n",
        "        D_hidden_dim = repr_dim // 10\n",
        "        if D_batchnorm:\n",
        "            self.D = nn.Sequential(nn.Linear(repr_dim, D_hidden_dim, bias=False),\n",
        "                                   nn.LeakyReLU(0.2, inplace=True),\n",
        "                                   nn.Linear(D_hidden_dim, D_hidden_dim, bias=False),\n",
        "                                   nn.BatchNorm1d(D_hidden_dim),\n",
        "                                   nn.LeakyReLU(0.2, inplace=True),\n",
        "                                   nn.Linear(D_hidden_dim, 1),\n",
        "                                   nn.Sigmoid())\n",
        "        else:\n",
        "            self.D = nn.Sequential(nn.Linear(repr_dim, D_hidden_dim, bias=False),\n",
        "                                   nn.LeakyReLU(0.2, inplace=True),\n",
        "                                   nn.Linear(D_hidden_dim, D_hidden_dim, bias=False),\n",
        "                                   nn.LeakyReLU(0.2, inplace=True),\n",
        "                                   nn.Linear(D_hidden_dim, 1),\n",
        "                                   nn.Sigmoid())\n",
        "\n",
        "\n",
        "        # Same for generator (latent -> representations)\n",
        "        G_hidden_dim = repr_dim // 10\n",
        "        self.G = nn.Sequential(nn.Linear(latent_dim, G_hidden_dim, bias=False),\n",
        "                               nn.BatchNorm1d(G_hidden_dim),\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               #nn.Linear(G_hidden_dim, G_hidden_dim, bias=False),\n",
        "                               #nn.BatchNorm1d(G_hidden_dim),\n",
        "                               #nn.LeakyReLU(0.2, inplace=True),\n",
        "                               #nn.Linear(G_hidden_dim, G_hidden_dim, bias=False),\n",
        "                               #nn.BatchNorm1d(G_hidden_dim),\n",
        "                               #nn.LeakyReLU(0.2, inplace=True),\n",
        "                               nn.Linear(G_hidden_dim, G_hidden_dim, bias=False),\n",
        "                               nn.BatchNorm1d(G_hidden_dim),\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               nn.Linear(G_hidden_dim, repr_dim),\n",
        "                               )\n",
        "\n",
        "        # Encodes x to context\n",
        "        # XXX\n",
        "        ctx_dim = repr_dim // 10\n",
        "        self.ctx_encoder = DCGAN_Discriminator(num_latents=ctx_dim, image_size=image_size)\n",
        "        # Projections to decoding space\n",
        "        repr_dec_dim = repr_dim // 10\n",
        "        ctx_dec_dim = ctx_dim // 10\n",
        "        num_latents = repr_dec_dim + ctx_dec_dim\n",
        "        self.repr_proj = nn.Linear(repr_dim, repr_dec_dim)\n",
        "        self.ctx_proj = nn.Linear(ctx_dim, ctx_dec_dim)\n",
        "        # Decodes representation + context projection to an image\n",
        "        self.decoder = DCGAN_Generator(num_latents=num_latents, image_size=image_size)\n",
        "\n",
        "        # To check progress of G\n",
        "        self.fixed_latent = self.sample_latent(8*8)\n",
        "    \n",
        "    # XXX temp function\n",
        "    def proj(self, repr, ctx):\n",
        "        return torch.cat([self.repr_proj(repr), self.ctx_proj(ctx)], dim=1)\n",
        "\n",
        "    def sample_latent(self, batch_size):\n",
        "        latent_size = [batch_size, self.latent_dim]\n",
        "        latent = torch.randn(latent_size)\n",
        "        return latent\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yRivPV9BwFk"
      },
      "source": [
        "# Training v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5rpQp2E9rE5"
      },
      "source": [
        "### Imports and globals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51zFNn509xLz"
      },
      "source": [
        "import argparse\n",
        "import builtins\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.multiprocessing as mp\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "GANSIAM_DIR = \"/content/drive/My Drive/gansiam/\"\n",
        "SIMSIAM_PATH = os.path.join(GANSIAM_DIR, \"pretrained_batch256.tar\")\n",
        "TINYIMAGENET_DIR = \"tiny-imagenet-200\"\n",
        "IMAGE_SIZE = 64\n",
        "GENERATED_GRIDS = []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9X_JYE2Vwxd"
      },
      "source": [
        "### Download Tiny Imagenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "559H2an_V03M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abc78ee-7d72-4d4f-d3fb-9d2bf64f41e3"
      },
      "source": [
        "%%bash\n",
        "if [[ -d  \"tiny-imagenet-200\" ]]; then\n",
        "    echo \"Tiny Imagenet exists.\"\n",
        "else\n",
        "    wget -q \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "    unzip -qq \"tiny-imagenet-200.zip\" && rm \"tiny-imagenet-200.zip\"\n",
        "    echo \"Downloaded Tiny Imagenet.\"\n",
        "fi"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded Tiny Imagenet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwyRrBEGq9jh"
      },
      "source": [
        "### Utility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlbiU7QhrETa"
      },
      "source": [
        "#### GAN Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-TnvLE0q-1U"
      },
      "source": [
        "def get_D_loss(gan_type=\"gan\"):\n",
        "    if gan_type in (\"gan\", \"gan-gp\"):\n",
        "        return D_loss_GAN\n",
        "    elif gan_type in (\"wgan\", \"wgan-gp\"):\n",
        "        return D_loss_WGAN\n",
        "    else:\n",
        "        raise ValueError(f\"gan_type {gan_type} not supported\")\n",
        "\n",
        "\n",
        "def get_G_loss(gan_type=\"gan\"):\n",
        "    if gan_type in (\"gan\", \"gan-gp\"):\n",
        "        return G_loss_GAN\n",
        "    elif gan_type in (\"wgan\", \"wgan-gp\"):\n",
        "        return G_loss_WGAN\n",
        "    else:\n",
        "        raise ValueError(f\"gan_type {gan_type} not supported\")\n",
        "\n",
        "\n",
        "def D_loss_GAN(D_real, D_fake, label_smoothing=True):\n",
        "    \n",
        "    # Create (noisy) real and fake labels XXX\n",
        "    if label_smoothing:\n",
        "        real_label = 0.7 + 0.5 * torch.rand_like(D_real)\n",
        "    else:\n",
        "        real_label = torch.ones_like(D_real) - 0.1\n",
        "    fake_label = torch.zeros_like(D_fake)\n",
        "\n",
        "    # Calculate binary cross entropy loss\n",
        "    D_loss_real = F.binary_cross_entropy(D_real, real_label)\n",
        "    D_loss_fake = F.binary_cross_entropy(D_fake, fake_label)\n",
        "\n",
        "    # Loss is: - log(D(x)) - log(1 - D(x_g)),\n",
        "    # which is equiv. to maximizing: log(D(x)) + log(1 - D(x_g))\n",
        "    D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "    return D_loss.mean()\n",
        "\n",
        "\n",
        "def D_loss_WGAN(D_real, D_fake):\n",
        "\n",
        "    # Maximize: D(x) - D(x_g) - const * (|| grad of D(x_i) wrt x_i || - 1)^2,\n",
        "    # where x_i <- eps * x + (1 - eps) * x_g, and eps ~ rand(0,1)\n",
        "    D_loss = -1 * (D_real - D_fake)\n",
        "\n",
        "    return D_loss.mean()\n",
        "\n",
        "\n",
        "def G_loss_GAN(D_fake):\n",
        "\n",
        "    # Calculate binary cross entropy loss with a fake binary label\n",
        "    fake_label = torch.zeros_like(D_fake)\n",
        "\n",
        "    # Loss is: -log(D(G(z))), which is equiv. to minimizing log(1-D(G(z)))\n",
        "    # We use this loss vs. the original one for stability only.\n",
        "    G_loss = F.binary_cross_entropy(D_fake, 1 - fake_label)\n",
        "\n",
        "    return G_loss.mean()\n",
        "\n",
        "\n",
        "def G_loss_WGAN(D_fake):\n",
        "\n",
        "    # Minimize: -D(G(z))\n",
        "    G_loss = -D_fake\n",
        "    \n",
        "    return G_loss.mean()\n",
        "\n",
        "\n",
        "def interpolate(real, fake):\n",
        "    eps_size = [1] * len(real.size())\n",
        "    eps_size[0] = real.size(0)\n",
        "    eps = torch.rand(eps_size).to(real)\n",
        "    return eps * real + (1 - eps) * fake\n",
        "\n",
        "def simple_gradient_penalty(D, x, center=0.):\n",
        "    x.requires_grad_()\n",
        "    D_x = D(x)\n",
        "    D_grad = torch.autograd.grad(D_x, x, torch.ones_like(D_x), create_graph=True)\n",
        "    D_grad_norm = D_grad[0].view(x.size(0), -1).norm(dim=1)\n",
        "    return (D_grad_norm - center).pow(2).mean()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esZGRUlerHNX"
      },
      "source": [
        "#### Data Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-K3c9WlrJDD"
      },
      "source": [
        "from PIL import ImageFilter\n",
        "import random\n",
        "\n",
        "\n",
        "class TwoCropsTransform:\n",
        "    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n",
        "\n",
        "    def __init__(self, base_transform):\n",
        "        self.base_transform = base_transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        q = self.base_transform(x)\n",
        "        k = self.base_transform(x)\n",
        "        return [q, k]\n",
        "\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
        "\n",
        "    def __init__(self, sigma=[.1, 2.]):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, x):\n",
        "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
        "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
        "        return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyzHmINsryyh"
      },
      "source": [
        "#### Train Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8KJUUeWr1dI"
      },
      "source": [
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    filename = os.path.join(GANSIAM_DIR, \"results\", filename)\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, init_lr, epoch, args):\n",
        "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
        "    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / args.epochs))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        if 'fix_lr' in param_group and param_group['fix_lr']:\n",
        "            param_group['lr'] = init_lr\n",
        "        else:\n",
        "            param_group['lr'] = cur_lr\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbo7T6blPVTc"
      },
      "source": [
        "### Load pre-trained SimSiam model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nr8clgi_AzY",
        "outputId": "ec8932a3-ef19-4176-a2e2-8f5fa704c413"
      },
      "source": [
        "checkpoint = torch.load(SIMSIAM_PATH, map_location=\"cuda:0\")\n",
        "# remove 'module.' from dict keys\n",
        "model_dict = OrderedDict((k[7:], v) for k, v in checkpoint[\"state_dict\"].items())\n",
        "\n",
        "# Load model\n",
        "simsiam = SimSiam(models.__dict__[\"resnet50\"])\n",
        "simsiam.load_state_dict(model_dict)\n",
        "#print(simsiam)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rBd2nl-XNbD"
      },
      "source": [
        "### Declare arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLbu8XmXXPYC"
      },
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.data = TINYIMAGENET_DIR\n",
        "        self.workers = 2\n",
        "        self.epochs = 200\n",
        "        self.batch_size = 256\n",
        "        self.D_lr = 1e-4\n",
        "        self.G_lr = 2e-4\n",
        "        self.decoder_lr = 1e-5\n",
        "        self.momentum = 0.9\n",
        "        self.weight_decay = 1e-4\n",
        "        self.print_freq = 10\n",
        "        self.seed = None\n",
        "        self.gpu = 0\n",
        "\n",
        "        # SimSiam\n",
        "        self.dim = 2048\n",
        "        self.pred_dim = 512\n",
        "\n",
        "        # GAN\n",
        "        self.gan_type = \"wgan-gp\"\n",
        "        self.repr_dim = self.dim  # don't change\n",
        "        self.latent_dim = 256\n",
        "        self.D_iters = 3\n",
        "        self.noise = 1e-6\n",
        "        self.grad_penalty = 10.\n",
        "        self.grad_center = 1.\n",
        "        self.generate_grid_interval = 50\n",
        "\n",
        "        self.norm_penalty = 10.\n",
        "\n",
        "GENERATED_GRIDS = []\n",
        "args = Args()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbP0FbPDqQtL"
      },
      "source": [
        "### Init model and optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OfO92x6_UGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e259e45f-f277-48ee-89f5-a64af020c953"
      },
      "source": [
        "if args.seed is not None:\n",
        "    random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    cudnn.deterministic = True\n",
        "\n",
        "torch.cuda.set_device(args.gpu)\n",
        "\n",
        "model = ConsistentGAN2(args.repr_dim, args.latent_dim,\n",
        "                       D_batchnorm=args.gan_type in (\"gan\", \"wgan\"),\n",
        "                       image_size=IMAGE_SIZE)\n",
        "model = model.cuda(args.gpu)\n",
        "simsiam = simsiam.cuda(args.gpu)\n",
        "#print(model)\n",
        "\n",
        "# Define D and G loss functions\n",
        "D_criterion = get_D_loss(args.gan_type)\n",
        "G_criterion = get_G_loss(args.gan_type)\n",
        "def rec_criterion(x, y):\n",
        "    return (x - y).pow(2).flatten(start_dim=1).sum(dim=1).mean()\n",
        "\n",
        "D_optimizer = torch.optim.SGD(\n",
        "    model.D.parameters(), args.D_lr, momentum=args.momentum)\n",
        "G_optimizer = torch.optim.SGD(\n",
        "    model.G.parameters(), args.G_lr, momentum=args.momentum)\n",
        "decoder_optimizer = torch.optim.SGD(\n",
        "    model.decoder.parameters(), args.decoder_lr,\n",
        "    momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "\n",
        "ctx_encoder_optimizer = torch.optim.SGD(\n",
        "    model.ctx_encoder.parameters(), args.decoder_lr,\n",
        "    momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "print(\"Num of params:\", sum(map(torch.numel, model.parameters())))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of params: 10417389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYHY5awRqqoX"
      },
      "source": [
        "### Data loading and transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0qqoOtDqsJU",
        "outputId": "8e0cd878-3d0a-48cd-ed0e-c8429247d65d"
      },
      "source": [
        "# image normalization\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "normalize = transforms.Normalize(mean=mean, std=std)\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean= [-m/s for m, s in zip(mean, std)],\n",
        "   std= [1/s for s in std]\n",
        ")\n",
        "\n",
        "\n",
        "# MoCo v2's aug: similar to SimCLR https://arxiv.org/abs/2002.05709\n",
        "\"\"\"\n",
        "augmentation = [\n",
        "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.2, 1.)),\n",
        "    transforms.RandomApply([\n",
        "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
        "    ], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "augmentation = [\n",
        "    #transforms.RandomResizedCrop(IMAGE_SIZE),\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Using CIFAR10\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root=os.path.join(GANSIAM_DIR, \"cifar10/train\"), train=True, download=True,\n",
        "    transform=transforms.Compose(augmentation))\n",
        "    #transform=TwoCropsTransform(transforms.Compose(augmentation)))\n",
        "\"\"\"\n",
        "\n",
        "# Using Tiny Imagenet\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    root=os.path.join(TINYIMAGENET_DIR, 'train'),\n",
        "    transform=transforms.Compose(augmentation))\n",
        "\"\"\"\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "    num_workers=args.workers, pin_memory=True, sampler=None, drop_last=True)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0mLBr5XrfoV"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sawTiX_zqt1e"
      },
      "source": [
        "def train(train_loader, model, simsiam,\n",
        "          rec_criterion, D_criterion, G_criterion,\n",
        "          D_optimizer, G_optimizer,\n",
        "          decoder_optimizer, ctx_encoder_optimizer,\n",
        "          epoch, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    data_time = AverageMeter('Data', ':6.3f')\n",
        "    D_on_reals = AverageMeter('D(real)', ':.4f')\n",
        "    D_on_fakes = AverageMeter('D(fake)', ':.4f')\n",
        "    D_grads = AverageMeter('grad(D) penalty', ':.4f')\n",
        "    rec_losses = AverageMeter('Rec loss', ':.4f')\n",
        "    norms = AverageMeter('norm', ':.4f')\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader),\n",
        "        [batch_time, data_time,\n",
        "         D_on_reals, D_on_fakes, D_grads, rec_losses, norms],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        x = x.cuda(args.gpu, non_blocking=True)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # compute output and loss\n",
        "        with torch.no_grad():\n",
        "            repr = simsiam.encoder(x)\n",
        "\n",
        "        ### train GAN\n",
        "        if (i+1) % (args.D_iters+1) > 0:\n",
        "            latent = model.sample_latent(batch_size).cuda(args.gpu)\n",
        "            \n",
        "            # Add noise to real sample\n",
        "            real = repr + args.noise * torch.randn_like(repr)\n",
        "\n",
        "            # Sample from generator\n",
        "            with torch.no_grad():\n",
        "                fake = model.G(latent)\n",
        "                # Add noise to fake sample as well\n",
        "                fake += torch.randn_like(fake) * args.noise\n",
        "\n",
        "            # Classify real and fake data\n",
        "            D_real = model.D(real)\n",
        "            D_fake = model.D(fake)\n",
        "\n",
        "            # Calculate loss\n",
        "            D_loss = D_criterion(D_real, D_fake)\n",
        "            # Gradient penalty\n",
        "            D_grad_penalty = simple_gradient_penalty(\n",
        "                model.D, interpolate(real, fake), center=args.grad_center)\n",
        "            D_loss += args.grad_penalty * D_grad_penalty\n",
        "\n",
        "            # Calculate gradient and minimize\n",
        "            D_optimizer.zero_grad()\n",
        "            D_loss.backward()\n",
        "            D_optimizer.step()\n",
        "\n",
        "        else:\n",
        "            # Sample from generators\n",
        "            latent = model.sample_latent(batch_size).cuda(args.gpu)\n",
        "            fake = model.G(latent)\n",
        "            fake += args.noise * torch.randn_like(fake)\n",
        "            # Classify fake images\n",
        "            D_fake = model.D(fake)\n",
        "            # Calculate loss\n",
        "            G_loss = G_criterion(D_fake)\n",
        "            # Calculate gradient and minimize\n",
        "            G_optimizer.zero_grad()\n",
        "            G_loss.backward()\n",
        "            G_optimizer.step()\n",
        "\n",
        "            # Save D's evaluations on real and fake\n",
        "            D_on_reals.update(D_real.mean().item(), batch_size)\n",
        "            D_on_fakes.update(D_fake.mean().item(), batch_size)\n",
        "            D_grads.update(D_grad_penalty.mean().item(), batch_size)\n",
        "\n",
        "        # Calculate Reconstruction loss\n",
        "        ctx = model.ctx_encoder(x)\n",
        "        proj = model.proj(repr, ctx)\n",
        "        x_rec = model.decoder(proj + 1e-5 * torch.randn_like(proj))\n",
        "        # loss\n",
        "        rec_loss = rec_criterion(x, x_rec)\n",
        "        proj_norm = proj.flatten(start_dim=1).norm(dim=1).mean()\n",
        "        \n",
        "        decoder_optimizer.zero_grad()\n",
        "        ctx_encoder_optimizer.zero_grad()\n",
        "        (rec_loss + args.norm_penalty * proj_norm).backward()\n",
        "        decoder_optimizer.step()\n",
        "        ctx_encoder_optimizer.step()\n",
        "\n",
        "        rec_losses.update(rec_loss.item(), batch_size)\n",
        "        norms.update(proj_norm.item(), batch_size)\n",
        "\n",
        "        # Check generator's progress by recording its output on a fixed input\n",
        "        if i % args.generate_grid_interval == 0:\n",
        "            def decode_latent(z):\n",
        "                repr = model.G(z)\n",
        "                ctx = model.ctx_encoder(x[:model.fixed_latent.size(0)])  # ref x\n",
        "                proj = model.proj(repr, ctx)\n",
        "                return model.decoder(proj)\n",
        "\n",
        "            grid = generate_grid(decode_latent, model.fixed_latent.cuda(args.gpu))\n",
        "            GENERATED_GRIDS.append(grid)\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            progress.display(i)\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE7JOEPnsoBw",
        "outputId": "db72a694-4512-4799-a6df-af98c0a8deea"
      },
      "source": [
        "for epoch in range(args.epochs):\n",
        "    # train for one epoch\n",
        "    train(train_loader, model, simsiam,\n",
        "          rec_criterion, D_criterion, G_criterion,\n",
        "          D_optimizer, G_optimizer, \n",
        "          decoder_optimizer, ctx_encoder_optimizer,\n",
        "          epoch, args)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        save_checkpoint({\n",
        "            'state_dict': model.state_dict(),\n",
        "        }, is_best=False, filename='checkpoint_{:04d}.pth.tar'.format(epoch))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][  0/195]\tTime  0.394 ( 0.394)\tData  0.207 ( 0.207)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 19063.2539 (19063.2539)\tnorm 1.2087 (1.2087)\n",
            "Epoch: [0][ 10/195]\tTime  0.151 ( 0.174)\tData  0.000 ( 0.019)\tD(real) 0.4852 (0.4852)\tD(fake) 0.4901 (0.4903)\tgrad(D) penalty 0.9476 (0.9478)\tRec loss 12014.6602 (14027.4885)\tnorm 1.6422 (1.3665)\n",
            "Epoch: [0][ 20/195]\tTime  0.147 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.4853 (0.4852)\tD(fake) 0.4904 (0.4903)\tgrad(D) penalty 0.9466 (0.9475)\tRec loss 9657.6230 (12401.6411)\tnorm 1.9530 (1.5992)\n",
            "Epoch: [0][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.4853 (0.4853)\tD(fake) 0.4891 (0.4900)\tgrad(D) penalty 0.9457 (0.9471)\tRec loss 8894.6113 (11546.1828)\tnorm 2.2423 (1.7648)\n",
            "Epoch: [0][ 40/195]\tTime  0.150 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.4853 (0.4853)\tD(fake) 0.4887 (0.4897)\tgrad(D) penalty 0.9439 (0.9463)\tRec loss 8633.0205 (10891.4995)\tnorm 2.2369 (1.8677)\n",
            "Epoch: [0][ 50/195]\tTime  0.165 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.4853 (0.4853)\tD(fake) 0.4887 (0.4896)\tgrad(D) penalty 0.9427 (0.9458)\tRec loss 7871.7563 (10358.1751)\tnorm 2.5474 (1.9817)\n",
            "Epoch: [0][ 60/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.4854 (0.4853)\tD(fake) 0.4867 (0.4891)\tgrad(D) penalty 0.9408 (0.9449)\tRec loss 7511.4038 (9913.2589)\tnorm 2.5318 (2.0747)\n",
            "Epoch: [0][ 70/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.4854 (0.4853)\tD(fake) 0.4864 (0.4888)\tgrad(D) penalty 0.9390 (0.9442)\tRec loss 7044.9219 (9532.0002)\tnorm 2.4582 (2.1313)\n",
            "Epoch: [0][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.4854 (0.4853)\tD(fake) 0.4853 (0.4884)\tgrad(D) penalty 0.9368 (0.9432)\tRec loss 6767.4683 (9208.1960)\tnorm 2.4243 (2.1671)\n",
            "Epoch: [0][ 90/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.4854 (0.4853)\tD(fake) 0.4843 (0.4880)\tgrad(D) penalty 0.9352 (0.9426)\tRec loss 6524.8213 (8929.9089)\tnorm 2.3328 (2.1884)\n",
            "Epoch: [0][100/195]\tTime  0.170 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.4855 (0.4853)\tD(fake) 0.4828 (0.4875)\tgrad(D) penalty 0.9327 (0.9415)\tRec loss 6017.6172 (8686.2026)\tnorm 2.2267 (2.1972)\n",
            "Epoch: [0][110/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.4855 (0.4854)\tD(fake) 0.4820 (0.4871)\tgrad(D) penalty 0.9311 (0.9407)\tRec loss 6102.3467 (8475.8532)\tnorm 2.1304 (2.1969)\n",
            "Epoch: [0][120/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.4856 (0.4854)\tD(fake) 0.4804 (0.4865)\tgrad(D) penalty 0.9285 (0.9396)\tRec loss 6537.4121 (8271.2335)\tnorm 2.0841 (2.1903)\n",
            "Epoch: [0][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.4856 (0.4854)\tD(fake) 0.4790 (0.4860)\tgrad(D) penalty 0.9267 (0.9389)\tRec loss 6139.9316 (8103.7953)\tnorm 2.0876 (2.1825)\n",
            "Epoch: [0][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.4856 (0.4854)\tD(fake) 0.4771 (0.4853)\tgrad(D) penalty 0.9235 (0.9376)\tRec loss 6425.2305 (7948.0922)\tnorm 2.0507 (2.1740)\n",
            "Epoch: [0][150/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.4857 (0.4854)\tD(fake) 0.4750 (0.4848)\tgrad(D) penalty 0.9216 (0.9368)\tRec loss 5918.6660 (7811.9630)\tnorm 2.0214 (2.1650)\n",
            "Epoch: [0][160/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.4857 (0.4854)\tD(fake) 0.4735 (0.4840)\tgrad(D) penalty 0.9182 (0.9355)\tRec loss 5183.4756 (7684.4919)\tnorm 1.9947 (2.1553)\n",
            "Epoch: [0][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.4858 (0.4855)\tD(fake) 0.4721 (0.4834)\tgrad(D) penalty 0.9154 (0.9345)\tRec loss 5921.1680 (7563.3809)\tnorm 1.9306 (2.1440)\n",
            "Epoch: [0][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.4859 (0.4855)\tD(fake) 0.4680 (0.4825)\tgrad(D) penalty 0.9121 (0.9331)\tRec loss 5856.7002 (7453.0136)\tnorm 1.9041 (2.1312)\n",
            "Epoch: [0][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.4859 (0.4855)\tD(fake) 0.4665 (0.4818)\tgrad(D) penalty 0.9094 (0.9321)\tRec loss 5032.0000 (7349.3098)\tnorm 1.8671 (2.1177)\n",
            "Epoch: [1][  0/195]\tTime  0.387 ( 0.387)\tData  0.194 ( 0.194)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 5320.4893 (5320.4893)\tnorm 1.8400 (1.8400)\n",
            "Epoch: [1][ 10/195]\tTime  0.148 ( 0.172)\tData  0.000 ( 0.018)\tD(real) 0.4861 (0.4860)\tD(fake) 0.4616 (0.4622)\tgrad(D) penalty 0.9043 (0.9047)\tRec loss 5196.4189 (5350.3172)\tnorm 1.8219 (1.8313)\n",
            "Epoch: [1][ 20/195]\tTime  0.148 ( 0.160)\tData  0.000 ( 0.009)\tD(real) 0.4862 (0.4861)\tD(fake) 0.4587 (0.4604)\tgrad(D) penalty 0.8997 (0.9027)\tRec loss 5513.7026 (5342.9578)\tnorm 1.7865 (1.8185)\n",
            "Epoch: [1][ 30/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.4862 (0.4861)\tD(fake) 0.4557 (0.4593)\tgrad(D) penalty 0.8958 (0.9010)\tRec loss 5362.9595 (5303.4338)\tnorm 1.8211 (1.8147)\n",
            "Epoch: [1][ 40/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.4864 (0.4862)\tD(fake) 0.4510 (0.4572)\tgrad(D) penalty 0.8912 (0.8986)\tRec loss 4989.3535 (5268.0931)\tnorm 1.8283 (1.8120)\n",
            "Epoch: [1][ 50/195]\tTime  0.164 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.4865 (0.4862)\tD(fake) 0.4480 (0.4558)\tgrad(D) penalty 0.8889 (0.8971)\tRec loss 4932.8223 (5195.4011)\tnorm 1.8128 (1.8119)\n",
            "Epoch: [1][ 60/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.4866 (0.4863)\tD(fake) 0.4423 (0.4535)\tgrad(D) penalty 0.8822 (0.8945)\tRec loss 4708.6914 (5149.9775)\tnorm 1.7794 (1.8082)\n",
            "Epoch: [1][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.4867 (0.4863)\tD(fake) 0.4395 (0.4519)\tgrad(D) penalty 0.8787 (0.8927)\tRec loss 4770.4966 (5101.1773)\tnorm 1.7204 (1.8007)\n",
            "Epoch: [1][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.4869 (0.4864)\tD(fake) 0.4328 (0.4494)\tgrad(D) penalty 0.8717 (0.8899)\tRec loss 4875.2510 (5061.7511)\tnorm 1.6910 (1.7909)\n",
            "Epoch: [1][ 90/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.4871 (0.4865)\tD(fake) 0.4286 (0.4476)\tgrad(D) penalty 0.8686 (0.8880)\tRec loss 4995.4434 (5040.1098)\tnorm 1.6956 (1.7800)\n",
            "Epoch: [1][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.4873 (0.4866)\tD(fake) 0.4211 (0.4447)\tgrad(D) penalty 0.8601 (0.8849)\tRec loss 4687.9243 (5016.2601)\tnorm 1.7093 (1.7715)\n",
            "Epoch: [1][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.4875 (0.4866)\tD(fake) 0.4166 (0.4427)\tgrad(D) penalty 0.8558 (0.8828)\tRec loss 4687.8164 (4983.9739)\tnorm 1.6824 (1.7646)\n",
            "Epoch: [1][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.4878 (0.4867)\tD(fake) 0.4086 (0.4396)\tgrad(D) penalty 0.8475 (0.8795)\tRec loss 4548.7744 (4969.3734)\tnorm 1.6667 (1.7589)\n",
            "Epoch: [1][130/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.4880 (0.4868)\tD(fake) 0.4037 (0.4374)\tgrad(D) penalty 0.8406 (0.8772)\tRec loss 4703.7441 (4953.9837)\tnorm 1.6716 (1.7538)\n",
            "Epoch: [1][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.4884 (0.4869)\tD(fake) 0.3947 (0.4340)\tgrad(D) penalty 0.8317 (0.8735)\tRec loss 4840.8809 (4938.2371)\tnorm 1.6806 (1.7491)\n",
            "Epoch: [1][150/195]\tTime  0.167 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.4886 (0.4870)\tD(fake) 0.3890 (0.4317)\tgrad(D) penalty 0.8248 (0.8710)\tRec loss 4521.3125 (4923.7520)\tnorm 1.6551 (1.7438)\n",
            "Epoch: [1][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.4891 (0.4872)\tD(fake) 0.3800 (0.4280)\tgrad(D) penalty 0.8138 (0.8670)\tRec loss 4759.5708 (4911.4278)\tnorm 1.6774 (1.7391)\n",
            "Epoch: [1][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.4894 (0.4873)\tD(fake) 0.3730 (0.4255)\tgrad(D) penalty 0.8081 (0.8642)\tRec loss 4629.5151 (4897.8641)\tnorm 1.6445 (1.7340)\n",
            "Epoch: [1][180/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.4900 (0.4874)\tD(fake) 0.3638 (0.4216)\tgrad(D) penalty 0.7951 (0.8598)\tRec loss 4593.5029 (4887.2175)\tnorm 1.6479 (1.7290)\n",
            "Epoch: [1][190/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.4904 (0.4876)\tD(fake) 0.3580 (0.4190)\tgrad(D) penalty 0.7885 (0.8569)\tRec loss 4650.6201 (4872.7386)\tnorm 1.6499 (1.7240)\n",
            "Epoch: [2][  0/195]\tTime  0.375 ( 0.375)\tData  0.193 ( 0.193)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4657.9307 (4657.9307)\tnorm 1.6445 (1.6445)\n",
            "Epoch: [2][ 10/195]\tTime  0.150 ( 0.169)\tData  0.000 ( 0.018)\tD(real) 0.4913 (0.4912)\tD(fake) 0.3455 (0.3469)\tgrad(D) penalty 0.7695 (0.7714)\tRec loss 4353.9326 (4592.1626)\tnorm 1.6289 (1.6339)\n",
            "Epoch: [2][ 20/195]\tTime  0.147 ( 0.159)\tData  0.000 ( 0.010)\tD(real) 0.4921 (0.4916)\tD(fake) 0.3360 (0.3425)\tgrad(D) penalty 0.7553 (0.7646)\tRec loss 4486.7598 (4599.6524)\tnorm 1.6190 (1.6368)\n",
            "Epoch: [2][ 30/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.4927 (0.4919)\tD(fake) 0.3314 (0.3397)\tgrad(D) penalty 0.7434 (0.7592)\tRec loss 4807.2949 (4610.8083)\tnorm 1.5984 (1.6344)\n",
            "Epoch: [2][ 40/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.4937 (0.4923)\tD(fake) 0.3252 (0.3359)\tgrad(D) penalty 0.7300 (0.7519)\tRec loss 4689.9639 (4624.7589)\tnorm 1.5968 (1.6282)\n",
            "Epoch: [2][ 50/195]\tTime  0.163 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.4943 (0.4926)\tD(fake) 0.3208 (0.3335)\tgrad(D) penalty 0.7182 (0.7469)\tRec loss 4566.9014 (4624.3688)\tnorm 1.5988 (1.6249)\n",
            "Epoch: [2][ 60/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.4954 (0.4931)\tD(fake) 0.3170 (0.3305)\tgrad(D) penalty 0.7020 (0.7387)\tRec loss 4501.4824 (4615.3092)\tnorm 1.5939 (1.6214)\n",
            "Epoch: [2][ 70/195]\tTime  0.156 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.4962 (0.4935)\tD(fake) 0.3148 (0.3288)\tgrad(D) penalty 0.6945 (0.7336)\tRec loss 4960.8955 (4627.6283)\tnorm 1.5838 (1.6176)\n",
            "Epoch: [2][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.4974 (0.4940)\tD(fake) 0.3152 (0.3269)\tgrad(D) penalty 0.6726 (0.7253)\tRec loss 4346.1548 (4619.4078)\tnorm 1.6315 (1.6162)\n",
            "Epoch: [2][ 90/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.4983 (0.4944)\tD(fake) 0.3191 (0.3261)\tgrad(D) penalty 0.6608 (0.7196)\tRec loss 4363.5732 (4614.4399)\tnorm 1.5761 (1.6138)\n",
            "Epoch: [2][100/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.4996 (0.4949)\tD(fake) 0.3236 (0.3255)\tgrad(D) penalty 0.6395 (0.7107)\tRec loss 4659.2588 (4613.8595)\tnorm 1.5754 (1.6116)\n",
            "Epoch: [2][110/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5005 (0.4953)\tD(fake) 0.3265 (0.3255)\tgrad(D) penalty 0.6261 (0.7046)\tRec loss 4508.8115 (4615.9499)\tnorm 1.6036 (1.6094)\n",
            "Epoch: [2][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5019 (0.4959)\tD(fake) 0.3375 (0.3263)\tgrad(D) penalty 0.6058 (0.6957)\tRec loss 4486.9668 (4614.2424)\tnorm 1.5897 (1.6068)\n",
            "Epoch: [2][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5027 (0.4963)\tD(fake) 0.3439 (0.3272)\tgrad(D) penalty 0.5952 (0.6896)\tRec loss 4904.3379 (4613.3069)\tnorm 1.5499 (1.6037)\n",
            "Epoch: [2][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5041 (0.4970)\tD(fake) 0.3557 (0.3293)\tgrad(D) penalty 0.5762 (0.6803)\tRec loss 4718.5981 (4611.3776)\tnorm 1.5584 (1.6003)\n",
            "Epoch: [2][150/195]\tTime  0.171 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5049 (0.4974)\tD(fake) 0.3631 (0.3310)\tgrad(D) penalty 0.5682 (0.6743)\tRec loss 4588.2378 (4613.8143)\tnorm 1.5483 (1.5971)\n",
            "Epoch: [2][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5062 (0.4980)\tD(fake) 0.3795 (0.3343)\tgrad(D) penalty 0.5502 (0.6655)\tRec loss 4775.2871 (4610.3026)\tnorm 1.5051 (1.5933)\n",
            "Epoch: [2][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5069 (0.4984)\tD(fake) 0.3888 (0.3367)\tgrad(D) penalty 0.5392 (0.6597)\tRec loss 4468.1504 (4605.4159)\tnorm 1.4944 (1.5896)\n",
            "Epoch: [2][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5080 (0.4990)\tD(fake) 0.4001 (0.3407)\tgrad(D) penalty 0.5277 (0.6511)\tRec loss 4841.9136 (4601.1669)\tnorm 1.5214 (1.5858)\n",
            "Epoch: [2][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5086 (0.4994)\tD(fake) 0.4123 (0.3437)\tgrad(D) penalty 0.5175 (0.6455)\tRec loss 4637.3218 (4594.3754)\tnorm 1.4981 (1.5816)\n",
            "Epoch: [3][  0/195]\tTime  0.398 ( 0.398)\tData  0.200 ( 0.200)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4411.1748 (4411.1748)\tnorm 1.5082 (1.5082)\n",
            "Epoch: [3][ 10/195]\tTime  0.150 ( 0.172)\tData  0.000 ( 0.018)\tD(real) 0.5098 (0.5097)\tD(fake) 0.4226 (0.4213)\tgrad(D) penalty 0.5005 (0.5027)\tRec loss 4556.8408 (4508.9308)\tnorm 1.4985 (1.5015)\n",
            "Epoch: [3][ 20/195]\tTime  0.147 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.5106 (0.5101)\tD(fake) 0.4323 (0.4265)\tgrad(D) penalty 0.4897 (0.4969)\tRec loss 4465.1445 (4505.9093)\tnorm 1.4833 (1.4973)\n",
            "Epoch: [3][ 30/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.5110 (0.5103)\tD(fake) 0.4389 (0.4298)\tgrad(D) penalty 0.4824 (0.4930)\tRec loss 4516.1304 (4534.4282)\tnorm 1.4925 (1.4987)\n",
            "Epoch: [3][ 40/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.5117 (0.5107)\tD(fake) 0.4491 (0.4349)\tgrad(D) penalty 0.4742 (0.4877)\tRec loss 4726.7061 (4546.1356)\tnorm 1.5123 (1.4985)\n",
            "Epoch: [3][ 50/195]\tTime  0.168 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.5121 (0.5109)\tD(fake) 0.4531 (0.4375)\tgrad(D) penalty 0.4620 (0.4838)\tRec loss 4443.8857 (4525.1985)\tnorm 1.5018 (1.4988)\n",
            "Epoch: [3][ 60/195]\tTime  0.154 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.5126 (0.5112)\tD(fake) 0.4615 (0.4416)\tgrad(D) penalty 0.4482 (0.4774)\tRec loss 4445.8613 (4518.5249)\tnorm 1.5089 (1.4992)\n",
            "Epoch: [3][ 70/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.5128 (0.5114)\tD(fake) 0.4626 (0.4440)\tgrad(D) penalty 0.4416 (0.4734)\tRec loss 4421.9619 (4523.1061)\tnorm 1.5002 (1.5002)\n",
            "Epoch: [3][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5132 (0.5116)\tD(fake) 0.4652 (0.4473)\tgrad(D) penalty 0.4254 (0.4672)\tRec loss 4168.5464 (4522.8358)\tnorm 1.4717 (1.4990)\n",
            "Epoch: [3][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5134 (0.5118)\tD(fake) 0.4678 (0.4491)\tgrad(D) penalty 0.4194 (0.4633)\tRec loss 4526.4570 (4526.9587)\tnorm 1.4820 (1.4978)\n",
            "Epoch: [3][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5137 (0.5120)\tD(fake) 0.4706 (0.4515)\tgrad(D) penalty 0.4016 (0.4568)\tRec loss 4719.4297 (4528.6022)\tnorm 1.4934 (1.4973)\n",
            "Epoch: [3][110/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5139 (0.5121)\tD(fake) 0.4738 (0.4531)\tgrad(D) penalty 0.3971 (0.4523)\tRec loss 4751.8784 (4527.7142)\tnorm 1.4809 (1.4962)\n",
            "Epoch: [3][120/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5142 (0.5123)\tD(fake) 0.4707 (0.4549)\tgrad(D) penalty 0.3821 (0.4456)\tRec loss 4688.8828 (4537.3343)\tnorm 1.4762 (1.4950)\n",
            "Epoch: [3][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5144 (0.5125)\tD(fake) 0.4765 (0.4562)\tgrad(D) penalty 0.3788 (0.4415)\tRec loss 4498.0020 (4537.1457)\tnorm 1.4999 (1.4944)\n",
            "Epoch: [3][140/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5146 (0.5126)\tD(fake) 0.4766 (0.4577)\tgrad(D) penalty 0.3583 (0.4351)\tRec loss 4419.7363 (4530.9109)\tnorm 1.4848 (1.4935)\n",
            "Epoch: [3][150/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5147 (0.5127)\tD(fake) 0.4784 (0.4588)\tgrad(D) penalty 0.3610 (0.4311)\tRec loss 4621.9863 (4528.2629)\tnorm 1.4665 (1.4920)\n",
            "Epoch: [3][160/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5149 (0.5129)\tD(fake) 0.4729 (0.4600)\tgrad(D) penalty 0.3434 (0.4247)\tRec loss 4570.5596 (4526.7562)\tnorm 1.4851 (1.4911)\n",
            "Epoch: [3][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5150 (0.5130)\tD(fake) 0.4740 (0.4607)\tgrad(D) penalty 0.3355 (0.4204)\tRec loss 4748.0244 (4528.1089)\tnorm 1.4804 (1.4910)\n",
            "Epoch: [3][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5151 (0.5131)\tD(fake) 0.4731 (0.4617)\tgrad(D) penalty 0.3286 (0.4145)\tRec loss 4820.4209 (4533.0131)\tnorm 1.4776 (1.4898)\n",
            "Epoch: [3][190/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5152 (0.5132)\tD(fake) 0.4792 (0.4624)\tgrad(D) penalty 0.3177 (0.4105)\tRec loss 4155.9219 (4528.2448)\tnorm 1.4660 (1.4890)\n",
            "Epoch: [4][  0/195]\tTime  0.389 ( 0.389)\tData  0.198 ( 0.198)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4451.4219 (4451.4219)\tnorm 1.4558 (1.4558)\n",
            "Epoch: [4][ 10/195]\tTime  0.148 ( 0.171)\tData  0.000 ( 0.018)\tD(real) 0.5154 (0.5154)\tD(fake) 0.4679 (0.4694)\tgrad(D) penalty 0.2893 (0.2910)\tRec loss 4307.6958 (4550.6674)\tnorm 1.4459 (1.4444)\n",
            "Epoch: [4][ 20/195]\tTime  0.149 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.5155 (0.5154)\tD(fake) 0.4719 (0.4695)\tgrad(D) penalty 0.2829 (0.2888)\tRec loss 4456.6367 (4517.4684)\tnorm 1.4689 (1.4506)\n",
            "Epoch: [4][ 30/195]\tTime  0.149 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.5156 (0.5155)\tD(fake) 0.4738 (0.4707)\tgrad(D) penalty 0.2813 (0.2873)\tRec loss 4606.7227 (4521.4428)\tnorm 1.4720 (1.4522)\n",
            "Epoch: [4][ 40/195]\tTime  0.153 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.5157 (0.5155)\tD(fake) 0.4734 (0.4722)\tgrad(D) penalty 0.2767 (0.2836)\tRec loss 4604.6963 (4519.1151)\tnorm 1.4672 (1.4553)\n",
            "Epoch: [4][ 50/195]\tTime  0.166 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.5158 (0.5156)\tD(fake) 0.4742 (0.4725)\tgrad(D) penalty 0.2664 (0.2808)\tRec loss 4253.1992 (4498.9982)\tnorm 1.4608 (1.4548)\n",
            "Epoch: [4][ 60/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.5159 (0.5156)\tD(fake) 0.4760 (0.4731)\tgrad(D) penalty 0.2577 (0.2767)\tRec loss 4480.9092 (4506.1309)\tnorm 1.4526 (1.4539)\n",
            "Epoch: [4][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.5160 (0.5157)\tD(fake) 0.4753 (0.4735)\tgrad(D) penalty 0.2576 (0.2744)\tRec loss 4818.1523 (4512.4462)\tnorm 1.4811 (1.4567)\n",
            "Epoch: [4][ 80/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5160 (0.5157)\tD(fake) 0.4733 (0.4738)\tgrad(D) penalty 0.2425 (0.2703)\tRec loss 4238.8413 (4509.2816)\tnorm 1.4517 (1.4569)\n",
            "Epoch: [4][ 90/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5160 (0.5157)\tD(fake) 0.4780 (0.4740)\tgrad(D) penalty 0.2412 (0.2674)\tRec loss 4254.9878 (4502.0295)\tnorm 1.4570 (1.4564)\n",
            "Epoch: [4][100/195]\tTime  0.169 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5161 (0.5158)\tD(fake) 0.4712 (0.4739)\tgrad(D) penalty 0.2267 (0.2630)\tRec loss 4309.6719 (4508.1641)\tnorm 1.4370 (1.4553)\n",
            "Epoch: [4][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5161 (0.5158)\tD(fake) 0.4735 (0.4739)\tgrad(D) penalty 0.2244 (0.2602)\tRec loss 4434.3486 (4495.6500)\tnorm 1.4207 (1.4540)\n",
            "Epoch: [4][120/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5161 (0.5158)\tD(fake) 0.4698 (0.4735)\tgrad(D) penalty 0.2188 (0.2558)\tRec loss 4256.6553 (4496.2033)\tnorm 1.4299 (1.4522)\n",
            "Epoch: [4][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5161 (0.5159)\tD(fake) 0.4693 (0.4733)\tgrad(D) penalty 0.1979 (0.2524)\tRec loss 4282.2173 (4497.4766)\tnorm 1.4609 (1.4509)\n",
            "Epoch: [4][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5162 (0.5159)\tD(fake) 0.4717 (0.4728)\tgrad(D) penalty 0.1928 (0.2478)\tRec loss 4902.5713 (4508.3357)\tnorm 1.4353 (1.4505)\n",
            "Epoch: [4][150/195]\tTime  0.164 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5162 (0.5159)\tD(fake) 0.4716 (0.4728)\tgrad(D) penalty 0.1913 (0.2448)\tRec loss 4352.5933 (4502.4046)\tnorm 1.4397 (1.4500)\n",
            "Epoch: [4][160/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5162 (0.5159)\tD(fake) 0.4704 (0.4725)\tgrad(D) penalty 0.1825 (0.2403)\tRec loss 4801.7324 (4503.0717)\tnorm 1.4051 (1.4481)\n",
            "Epoch: [4][170/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5163 (0.5159)\tD(fake) 0.4696 (0.4723)\tgrad(D) penalty 0.1778 (0.2373)\tRec loss 4566.2500 (4498.9596)\tnorm 1.4093 (1.4458)\n",
            "Epoch: [4][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5163 (0.5160)\tD(fake) 0.4648 (0.4718)\tgrad(D) penalty 0.1677 (0.2330)\tRec loss 4644.0400 (4494.2487)\tnorm 1.3884 (1.4434)\n",
            "Epoch: [4][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5163 (0.5160)\tD(fake) 0.4682 (0.4716)\tgrad(D) penalty 0.1641 (0.2302)\tRec loss 4387.0723 (4492.5658)\tnorm 1.3848 (1.4415)\n",
            "Epoch: [5][  0/195]\tTime  0.398 ( 0.398)\tData  0.195 ( 0.195)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4575.2705 (4575.2705)\tnorm 1.4048 (1.4048)\n",
            "Epoch: [5][ 10/195]\tTime  0.147 ( 0.171)\tData  0.000 ( 0.018)\tD(real) 0.5164 (0.5164)\tD(fake) 0.4584 (0.4572)\tgrad(D) penalty 0.1480 (0.1454)\tRec loss 4565.7920 (4507.1220)\tnorm 1.3901 (1.3856)\n",
            "Epoch: [5][ 20/195]\tTime  0.151 ( 0.159)\tData  0.000 ( 0.010)\tD(real) 0.5165 (0.5164)\tD(fake) 0.4584 (0.4583)\tgrad(D) penalty 0.1466 (0.1456)\tRec loss 4663.6816 (4528.5093)\tnorm 1.3973 (1.3869)\n",
            "Epoch: [5][ 30/195]\tTime  0.152 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.5165 (0.5165)\tD(fake) 0.4634 (0.4594)\tgrad(D) penalty 0.1413 (0.1450)\tRec loss 4590.7920 (4506.3380)\tnorm 1.3816 (1.3876)\n",
            "Epoch: [5][ 40/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.5166 (0.5165)\tD(fake) 0.4649 (0.4608)\tgrad(D) penalty 0.1274 (0.1416)\tRec loss 4183.0000 (4472.2652)\tnorm 1.4020 (1.3876)\n",
            "Epoch: [5][ 50/195]\tTime  0.168 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.5166 (0.5165)\tD(fake) 0.4617 (0.4610)\tgrad(D) penalty 0.1343 (0.1398)\tRec loss 4172.1318 (4461.2352)\tnorm 1.3634 (1.3877)\n",
            "Epoch: [5][ 60/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5166 (0.5165)\tD(fake) 0.4573 (0.4604)\tgrad(D) penalty 0.1215 (0.1367)\tRec loss 4207.5200 (4456.7313)\tnorm 1.3879 (1.3862)\n",
            "Epoch: [5][ 70/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.5166 (0.5165)\tD(fake) 0.4602 (0.4601)\tgrad(D) penalty 0.1131 (0.1341)\tRec loss 4137.0210 (4450.7938)\tnorm 1.3784 (1.3838)\n",
            "Epoch: [5][ 80/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.5167 (0.5166)\tD(fake) 0.4573 (0.4602)\tgrad(D) penalty 0.1082 (0.1308)\tRec loss 4252.3281 (4448.6830)\tnorm 1.3713 (1.3811)\n",
            "Epoch: [5][ 90/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5167 (0.5166)\tD(fake) 0.4554 (0.4600)\tgrad(D) penalty 0.1100 (0.1290)\tRec loss 4482.0566 (4453.8642)\tnorm 1.3742 (1.3792)\n",
            "Epoch: [5][100/195]\tTime  0.171 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5167 (0.5166)\tD(fake) 0.4581 (0.4599)\tgrad(D) penalty 0.1029 (0.1261)\tRec loss 4609.3926 (4451.5719)\tnorm 1.3567 (1.3784)\n",
            "Epoch: [5][110/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5167 (0.5166)\tD(fake) 0.4590 (0.4599)\tgrad(D) penalty 0.0972 (0.1241)\tRec loss 4498.7578 (4455.0931)\tnorm 1.3739 (1.3786)\n",
            "Epoch: [5][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5167 (0.5166)\tD(fake) 0.4548 (0.4593)\tgrad(D) penalty 0.0940 (0.1209)\tRec loss 4647.5356 (4464.8813)\tnorm 1.3723 (1.3774)\n",
            "Epoch: [5][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5167 (0.5166)\tD(fake) 0.4539 (0.4591)\tgrad(D) penalty 0.0877 (0.1191)\tRec loss 4127.6816 (4467.4794)\tnorm 1.3743 (1.3764)\n",
            "Epoch: [5][140/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5167 (0.5166)\tD(fake) 0.4567 (0.4587)\tgrad(D) penalty 0.0856 (0.1163)\tRec loss 4441.9990 (4465.1731)\tnorm 1.3548 (1.3757)\n",
            "Epoch: [5][150/195]\tTime  0.169 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5168 (0.5166)\tD(fake) 0.4557 (0.4584)\tgrad(D) penalty 0.0814 (0.1143)\tRec loss 4267.2480 (4458.4501)\tnorm 1.3539 (1.3749)\n",
            "Epoch: [5][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5168 (0.5166)\tD(fake) 0.4531 (0.4580)\tgrad(D) penalty 0.0798 (0.1118)\tRec loss 4626.5928 (4455.7393)\tnorm 1.3649 (1.3735)\n",
            "Epoch: [5][170/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5168 (0.5166)\tD(fake) 0.4539 (0.4578)\tgrad(D) penalty 0.0734 (0.1100)\tRec loss 4206.1631 (4456.3161)\tnorm 1.3692 (1.3722)\n",
            "Epoch: [5][180/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5168 (0.5167)\tD(fake) 0.4530 (0.4573)\tgrad(D) penalty 0.0702 (0.1074)\tRec loss 4643.5015 (4456.0837)\tnorm 1.3765 (1.3718)\n",
            "Epoch: [5][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5168 (0.5167)\tD(fake) 0.4512 (0.4570)\tgrad(D) penalty 0.0670 (0.1057)\tRec loss 4493.6289 (4456.3410)\tnorm 1.3750 (1.3715)\n",
            "Epoch: [6][  0/195]\tTime  0.391 ( 0.391)\tData  0.188 ( 0.188)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3979.6526 (3979.6526)\tnorm 1.3453 (1.3453)\n",
            "Epoch: [6][ 10/195]\tTime  0.152 ( 0.170)\tData  0.000 ( 0.017)\tD(real) 0.5169 (0.5169)\tD(fake) 0.4316 (0.4314)\tgrad(D) penalty 0.0563 (0.0568)\tRec loss 4525.7246 (4461.6923)\tnorm 1.3439 (1.3513)\n",
            "Epoch: [6][ 20/195]\tTime  0.149 ( 0.159)\tData  0.000 ( 0.009)\tD(real) 0.5170 (0.5169)\tD(fake) 0.4359 (0.4348)\tgrad(D) penalty 0.0516 (0.0565)\tRec loss 4319.7031 (4422.3887)\tnorm 1.3434 (1.3519)\n",
            "Epoch: [6][ 30/195]\tTime  0.150 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.5171 (0.5169)\tD(fake) 0.4436 (0.4362)\tgrad(D) penalty 0.0558 (0.0556)\tRec loss 4195.0156 (4445.5126)\tnorm 1.3626 (1.3514)\n",
            "Epoch: [6][ 40/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.5171 (0.5170)\tD(fake) 0.4484 (0.4390)\tgrad(D) penalty 0.0544 (0.0545)\tRec loss 4603.7705 (4459.8859)\tnorm 1.3339 (1.3510)\n",
            "Epoch: [6][ 50/195]\tTime  0.164 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.5172 (0.5170)\tD(fake) 0.4431 (0.4396)\tgrad(D) penalty 0.0483 (0.0535)\tRec loss 4278.8481 (4439.7646)\tnorm 1.3462 (1.3489)\n",
            "Epoch: [6][ 60/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5172 (0.5171)\tD(fake) 0.4420 (0.4413)\tgrad(D) penalty 0.0439 (0.0519)\tRec loss 4309.3779 (4425.9465)\tnorm 1.3508 (1.3488)\n",
            "Epoch: [6][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5172 (0.5171)\tD(fake) 0.4459 (0.4418)\tgrad(D) penalty 0.0398 (0.0505)\tRec loss 4513.4199 (4427.8915)\tnorm 1.3353 (1.3476)\n",
            "Epoch: [6][ 80/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.5173 (0.5171)\tD(fake) 0.4427 (0.4414)\tgrad(D) penalty 0.0400 (0.0490)\tRec loss 4457.5034 (4425.7700)\tnorm 1.3335 (1.3452)\n",
            "Epoch: [6][ 90/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5173 (0.5171)\tD(fake) 0.4427 (0.4414)\tgrad(D) penalty 0.0362 (0.0479)\tRec loss 4490.7700 (4422.3335)\tnorm 1.3562 (1.3453)\n",
            "Epoch: [6][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5175 (0.5172)\tD(fake) 0.4451 (0.4415)\tgrad(D) penalty 0.0343 (0.0463)\tRec loss 4335.6924 (4427.2628)\tnorm 1.3568 (1.3454)\n",
            "Epoch: [6][110/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5175 (0.5172)\tD(fake) 0.4473 (0.4420)\tgrad(D) penalty 0.0307 (0.0452)\tRec loss 4650.8511 (4438.8514)\tnorm 1.3459 (1.3466)\n",
            "Epoch: [6][120/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5176 (0.5172)\tD(fake) 0.4500 (0.4425)\tgrad(D) penalty 0.0300 (0.0436)\tRec loss 4369.1221 (4438.6662)\tnorm 1.3599 (1.3480)\n",
            "Epoch: [6][130/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5176 (0.5173)\tD(fake) 0.4518 (0.4430)\tgrad(D) penalty 0.0298 (0.0427)\tRec loss 4421.0249 (4436.2178)\tnorm 1.3344 (1.3478)\n",
            "Epoch: [6][140/195]\tTime  0.154 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5177 (0.5173)\tD(fake) 0.4512 (0.4435)\tgrad(D) penalty 0.0257 (0.0413)\tRec loss 4620.1616 (4440.4669)\tnorm 1.3430 (1.3473)\n",
            "Epoch: [6][150/195]\tTime  0.170 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5178 (0.5173)\tD(fake) 0.4462 (0.4438)\tgrad(D) penalty 0.0229 (0.0404)\tRec loss 4486.4736 (4438.4982)\tnorm 1.3464 (1.3467)\n",
            "Epoch: [6][160/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5179 (0.5174)\tD(fake) 0.4441 (0.4440)\tgrad(D) penalty 0.0220 (0.0390)\tRec loss 4233.0889 (4431.2506)\tnorm 1.3276 (1.3465)\n",
            "Epoch: [6][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5179 (0.5174)\tD(fake) 0.4454 (0.4440)\tgrad(D) penalty 0.0180 (0.0381)\tRec loss 4403.9507 (4431.1628)\tnorm 1.3372 (1.3458)\n",
            "Epoch: [6][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5180 (0.5174)\tD(fake) 0.4453 (0.4443)\tgrad(D) penalty 0.0179 (0.0367)\tRec loss 4131.6016 (4431.8230)\tnorm 1.3372 (1.3446)\n",
            "Epoch: [6][190/195]\tTime  0.151 ( 0.149)\tData  0.000 ( 0.001)\tD(real) 0.5181 (0.5175)\tD(fake) 0.4542 (0.4447)\tgrad(D) penalty 0.0214 (0.0360)\tRec loss 4534.3789 (4433.3118)\tnorm 1.3141 (1.3436)\n",
            "Epoch: [7][  0/195]\tTime  0.394 ( 0.394)\tData  0.197 ( 0.197)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4281.1758 (4281.1758)\tnorm 1.3394 (1.3394)\n",
            "Epoch: [7][ 10/195]\tTime  0.152 ( 0.172)\tData  0.000 ( 0.018)\tD(real) 0.5183 (0.5183)\tD(fake) 0.4230 (0.4221)\tgrad(D) penalty 0.0110 (0.0135)\tRec loss 4520.7153 (4369.2792)\tnorm 1.3167 (1.3284)\n",
            "Epoch: [7][ 20/195]\tTime  0.148 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.5186 (0.5184)\tD(fake) 0.4461 (0.4316)\tgrad(D) penalty 0.0138 (0.0140)\tRec loss 4622.2812 (4375.5684)\tnorm 1.3203 (1.3241)\n",
            "Epoch: [7][ 30/195]\tTime  0.153 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.5187 (0.5185)\tD(fake) 0.4559 (0.4377)\tgrad(D) penalty 0.0120 (0.0140)\tRec loss 4480.5244 (4391.3427)\tnorm 1.3230 (1.3218)\n",
            "Epoch: [7][ 40/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.5189 (0.5186)\tD(fake) 0.4626 (0.4447)\tgrad(D) penalty 0.0132 (0.0136)\tRec loss 4630.9727 (4396.8767)\tnorm 1.3136 (1.3196)\n",
            "Epoch: [7][ 50/195]\tTime  0.163 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.5190 (0.5186)\tD(fake) 0.4601 (0.4476)\tgrad(D) penalty 0.0119 (0.0133)\tRec loss 4447.2002 (4400.0812)\tnorm 1.3062 (1.3160)\n",
            "Epoch: [7][ 60/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.5191 (0.5187)\tD(fake) 0.4592 (0.4503)\tgrad(D) penalty 0.0107 (0.0126)\tRec loss 4588.9004 (4406.4085)\tnorm 1.3021 (1.3135)\n",
            "Epoch: [7][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.5192 (0.5188)\tD(fake) 0.4592 (0.4514)\tgrad(D) penalty 0.0098 (0.0124)\tRec loss 4451.5439 (4401.5008)\tnorm 1.2835 (1.3108)\n",
            "Epoch: [7][ 80/195]\tTime  0.154 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5193 (0.5189)\tD(fake) 0.4577 (0.4522)\tgrad(D) penalty 0.0107 (0.0124)\tRec loss 4594.2988 (4399.3657)\tnorm 1.3090 (1.3098)\n",
            "Epoch: [7][ 90/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5195 (0.5189)\tD(fake) 0.4624 (0.4529)\tgrad(D) penalty 0.0093 (0.0122)\tRec loss 4641.1787 (4403.1717)\tnorm 1.2632 (1.3069)\n",
            "Epoch: [7][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5196 (0.5190)\tD(fake) 0.4636 (0.4539)\tgrad(D) penalty 0.0116 (0.0122)\tRec loss 4391.8818 (4407.2841)\tnorm 1.2711 (1.3051)\n",
            "Epoch: [7][110/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5198 (0.5191)\tD(fake) 0.4693 (0.4550)\tgrad(D) penalty 0.0092 (0.0120)\tRec loss 4625.8306 (4411.0723)\tnorm 1.2888 (1.3038)\n",
            "Epoch: [7][120/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5200 (0.5191)\tD(fake) 0.4730 (0.4565)\tgrad(D) penalty 0.0099 (0.0119)\tRec loss 4411.4849 (4413.8062)\tnorm 1.3043 (1.3027)\n",
            "Epoch: [7][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5202 (0.5192)\tD(fake) 0.4777 (0.4574)\tgrad(D) penalty 0.0111 (0.0118)\tRec loss 4774.2188 (4420.6170)\tnorm 1.3041 (1.3011)\n",
            "Epoch: [7][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5204 (0.5193)\tD(fake) 0.4764 (0.4591)\tgrad(D) penalty 0.0096 (0.0117)\tRec loss 4228.9829 (4413.9720)\tnorm 1.2781 (1.3005)\n",
            "Epoch: [7][150/195]\tTime  0.168 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5205 (0.5194)\tD(fake) 0.4766 (0.4601)\tgrad(D) penalty 0.0119 (0.0116)\tRec loss 4062.4160 (4409.5059)\tnorm 1.2768 (1.2994)\n",
            "Epoch: [7][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5207 (0.5195)\tD(fake) 0.4788 (0.4614)\tgrad(D) penalty 0.0099 (0.0115)\tRec loss 4561.7104 (4407.9543)\tnorm 1.2708 (1.2978)\n",
            "Epoch: [7][170/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5209 (0.5195)\tD(fake) 0.4799 (0.4623)\tgrad(D) penalty 0.0112 (0.0115)\tRec loss 4171.2300 (4406.4637)\tnorm 1.2698 (1.2967)\n",
            "Epoch: [7][180/195]\tTime  0.157 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5211 (0.5196)\tD(fake) 0.4795 (0.4637)\tgrad(D) penalty 0.0112 (0.0115)\tRec loss 4631.7295 (4404.8379)\tnorm 1.2670 (1.2958)\n",
            "Epoch: [7][190/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5213 (0.5197)\tD(fake) 0.4829 (0.4645)\tgrad(D) penalty 0.0118 (0.0114)\tRec loss 4139.3940 (4403.2664)\tnorm 1.2597 (1.2945)\n",
            "Epoch: [8][  0/195]\tTime  0.396 ( 0.396)\tData  0.203 ( 0.203)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4081.3157 (4081.3157)\tnorm 1.2964 (1.2964)\n",
            "Epoch: [8][ 10/195]\tTime  0.151 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.5216 (0.5216)\tD(fake) 0.4608 (0.4574)\tgrad(D) penalty 0.0127 (0.0119)\tRec loss 4018.1792 (4383.1996)\tnorm 1.2615 (1.2824)\n",
            "Epoch: [8][ 20/195]\tTime  0.148 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.5219 (0.5217)\tD(fake) 0.4714 (0.4621)\tgrad(D) penalty 0.0127 (0.0117)\tRec loss 4011.9404 (4352.6752)\tnorm 1.2894 (1.2824)\n",
            "Epoch: [8][ 30/195]\tTime  0.154 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.5221 (0.5218)\tD(fake) 0.4809 (0.4671)\tgrad(D) penalty 0.0096 (0.0114)\tRec loss 4561.9380 (4375.3900)\tnorm 1.2899 (1.2833)\n",
            "Epoch: [8][ 40/195]\tTime  0.153 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.5224 (0.5220)\tD(fake) 0.4891 (0.4731)\tgrad(D) penalty 0.0111 (0.0114)\tRec loss 4413.4414 (4373.8178)\tnorm 1.2751 (1.2843)\n",
            "Epoch: [8][ 50/195]\tTime  0.168 ( 0.156)\tData  0.000 ( 0.004)\tD(real) 0.5226 (0.5221)\tD(fake) 0.5019 (0.4771)\tgrad(D) penalty 0.0092 (0.0110)\tRec loss 4714.5479 (4385.7565)\tnorm 1.2819 (1.2829)\n",
            "Epoch: [8][ 60/195]\tTime  0.152 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.5230 (0.5222)\tD(fake) 0.5004 (0.4817)\tgrad(D) penalty 0.0105 (0.0109)\tRec loss 4395.2261 (4405.4458)\tnorm 1.2684 (1.2821)\n",
            "Epoch: [8][ 70/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.5231 (0.5223)\tD(fake) 0.5023 (0.4843)\tgrad(D) penalty 0.0096 (0.0110)\tRec loss 4175.8569 (4405.3088)\tnorm 1.2787 (1.2820)\n",
            "Epoch: [8][ 80/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.5234 (0.5225)\tD(fake) 0.5013 (0.4871)\tgrad(D) penalty 0.0128 (0.0111)\tRec loss 4303.5229 (4404.5460)\tnorm 1.2729 (1.2798)\n",
            "Epoch: [8][ 90/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.5236 (0.5226)\tD(fake) 0.5082 (0.4891)\tgrad(D) penalty 0.0100 (0.0110)\tRec loss 4337.1714 (4394.1078)\tnorm 1.2646 (1.2793)\n",
            "Epoch: [8][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5238 (0.5227)\tD(fake) 0.5106 (0.4913)\tgrad(D) penalty 0.0095 (0.0111)\tRec loss 4733.5273 (4398.8087)\tnorm 1.2739 (1.2774)\n",
            "Epoch: [8][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5240 (0.5228)\tD(fake) 0.5066 (0.4925)\tgrad(D) penalty 0.0136 (0.0112)\tRec loss 4343.5444 (4398.1986)\tnorm 1.2519 (1.2759)\n",
            "Epoch: [8][120/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5244 (0.5230)\tD(fake) 0.4969 (0.4934)\tgrad(D) penalty 0.0139 (0.0112)\tRec loss 4395.7080 (4392.5656)\tnorm 1.2630 (1.2754)\n",
            "Epoch: [8][130/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5246 (0.5231)\tD(fake) 0.5006 (0.4939)\tgrad(D) penalty 0.0123 (0.0113)\tRec loss 4208.9902 (4394.9960)\tnorm 1.2581 (1.2744)\n",
            "Epoch: [8][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5249 (0.5232)\tD(fake) 0.5008 (0.4944)\tgrad(D) penalty 0.0130 (0.0113)\tRec loss 4350.6851 (4388.9565)\tnorm 1.2516 (1.2731)\n",
            "Epoch: [8][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5251 (0.5233)\tD(fake) 0.4992 (0.4946)\tgrad(D) penalty 0.0114 (0.0113)\tRec loss 4473.8721 (4391.4238)\tnorm 1.2379 (1.2708)\n",
            "Epoch: [8][160/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5254 (0.5234)\tD(fake) 0.5077 (0.4952)\tgrad(D) penalty 0.0147 (0.0114)\tRec loss 4315.0762 (4382.2940)\tnorm 1.2525 (1.2695)\n",
            "Epoch: [8][170/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5256 (0.5236)\tD(fake) 0.5035 (0.4956)\tgrad(D) penalty 0.0108 (0.0114)\tRec loss 4582.2148 (4389.6663)\tnorm 1.2643 (1.2686)\n",
            "Epoch: [8][180/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5260 (0.5237)\tD(fake) 0.5110 (0.4964)\tgrad(D) penalty 0.0130 (0.0114)\tRec loss 4279.4868 (4389.6816)\tnorm 1.2680 (1.2683)\n",
            "Epoch: [8][190/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5262 (0.5238)\tD(fake) 0.5058 (0.4968)\tgrad(D) penalty 0.0109 (0.0115)\tRec loss 4393.0469 (4393.0915)\tnorm 1.2573 (1.2686)\n",
            "Epoch: [9][  0/195]\tTime  0.373 ( 0.373)\tData  0.188 ( 0.188)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4363.0742 (4363.0742)\tnorm 1.2642 (1.2642)\n",
            "Epoch: [9][ 10/195]\tTime  0.148 ( 0.169)\tData  0.000 ( 0.017)\tD(real) 0.5266 (0.5266)\tD(fake) 0.4816 (0.4767)\tgrad(D) penalty 0.0132 (0.0135)\tRec loss 4595.4810 (4389.4214)\tnorm 1.2390 (1.2471)\n",
            "Epoch: [9][ 20/195]\tTime  0.152 ( 0.159)\tData  0.000 ( 0.009)\tD(real) 0.5271 (0.5268)\tD(fake) 0.4919 (0.4838)\tgrad(D) penalty 0.0140 (0.0133)\tRec loss 4246.2598 (4375.8472)\tnorm 1.2573 (1.2500)\n",
            "Epoch: [9][ 30/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.5274 (0.5269)\tD(fake) 0.4996 (0.4883)\tgrad(D) penalty 0.0135 (0.0133)\tRec loss 4348.9731 (4372.9676)\tnorm 1.2579 (1.2466)\n",
            "Epoch: [9][ 40/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.5278 (0.5271)\tD(fake) 0.5096 (0.4938)\tgrad(D) penalty 0.0130 (0.0132)\tRec loss 4544.9482 (4357.8783)\tnorm 1.2113 (1.2451)\n",
            "Epoch: [9][ 50/195]\tTime  0.166 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.5279 (0.5273)\tD(fake) 0.5086 (0.4965)\tgrad(D) penalty 0.0142 (0.0132)\tRec loss 4220.3271 (4360.2838)\tnorm 1.2384 (1.2422)\n",
            "Epoch: [9][ 60/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5284 (0.5275)\tD(fake) 0.5132 (0.4998)\tgrad(D) penalty 0.0130 (0.0133)\tRec loss 4463.0493 (4369.5769)\tnorm 1.2520 (1.2431)\n",
            "Epoch: [9][ 70/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.5286 (0.5276)\tD(fake) 0.5197 (0.5023)\tgrad(D) penalty 0.0117 (0.0132)\tRec loss 4640.2603 (4371.8713)\tnorm 1.2510 (1.2443)\n",
            "Epoch: [9][ 80/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.5290 (0.5278)\tD(fake) 0.5157 (0.5044)\tgrad(D) penalty 0.0098 (0.0132)\tRec loss 4430.5166 (4381.8276)\tnorm 1.2395 (1.2441)\n",
            "Epoch: [9][ 90/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5293 (0.5279)\tD(fake) 0.5259 (0.5058)\tgrad(D) penalty 0.0150 (0.0133)\tRec loss 4272.3477 (4377.1905)\tnorm 1.2527 (1.2433)\n",
            "Epoch: [9][100/195]\tTime  0.167 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5297 (0.5281)\tD(fake) 0.5235 (0.5082)\tgrad(D) penalty 0.0143 (0.0133)\tRec loss 4475.0293 (4375.8425)\tnorm 1.2146 (1.2409)\n",
            "Epoch: [9][110/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5299 (0.5282)\tD(fake) 0.5266 (0.5094)\tgrad(D) penalty 0.0119 (0.0133)\tRec loss 4176.0127 (4371.5259)\tnorm 1.2311 (1.2395)\n",
            "Epoch: [9][120/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5304 (0.5284)\tD(fake) 0.5229 (0.5109)\tgrad(D) penalty 0.0138 (0.0133)\tRec loss 4412.4673 (4368.4990)\tnorm 1.2166 (1.2390)\n",
            "Epoch: [9][130/195]\tTime  0.153 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5306 (0.5286)\tD(fake) 0.5148 (0.5112)\tgrad(D) penalty 0.0113 (0.0132)\tRec loss 4276.1514 (4365.6929)\tnorm 1.2237 (1.2382)\n",
            "Epoch: [9][140/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5310 (0.5288)\tD(fake) 0.5080 (0.5112)\tgrad(D) penalty 0.0116 (0.0131)\tRec loss 4257.1855 (4366.5650)\tnorm 1.2348 (1.2380)\n",
            "Epoch: [9][150/195]\tTime  0.165 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5312 (0.5289)\tD(fake) 0.5084 (0.5109)\tgrad(D) penalty 0.0133 (0.0131)\tRec loss 4441.4199 (4368.2969)\tnorm 1.2433 (1.2385)\n",
            "Epoch: [9][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5317 (0.5291)\tD(fake) 0.5125 (0.5110)\tgrad(D) penalty 0.0115 (0.0130)\tRec loss 4451.6611 (4367.9259)\tnorm 1.2626 (1.2389)\n",
            "Epoch: [9][170/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5320 (0.5292)\tD(fake) 0.5141 (0.5111)\tgrad(D) penalty 0.0124 (0.0130)\tRec loss 4452.1670 (4365.9258)\tnorm 1.2551 (1.2392)\n",
            "Epoch: [9][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5324 (0.5294)\tD(fake) 0.5152 (0.5113)\tgrad(D) penalty 0.0129 (0.0130)\tRec loss 4081.3967 (4366.0585)\tnorm 1.2397 (1.2387)\n",
            "Epoch: [9][190/195]\tTime  0.154 ( 0.149)\tData  0.000 ( 0.001)\tD(real) 0.5327 (0.5296)\tD(fake) 0.5131 (0.5115)\tgrad(D) penalty 0.0144 (0.0130)\tRec loss 4365.4717 (4369.0964)\tnorm 1.2215 (1.2384)\n",
            "Epoch: [10][  0/195]\tTime  0.380 ( 0.380)\tData  0.192 ( 0.192)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4310.0078 (4310.0078)\tnorm 1.2191 (1.2191)\n",
            "Epoch: [10][ 10/195]\tTime  0.148 ( 0.169)\tData  0.000 ( 0.018)\tD(real) 0.5332 (0.5331)\tD(fake) 0.4801 (0.4799)\tgrad(D) penalty 0.0125 (0.0150)\tRec loss 4296.8652 (4293.7113)\tnorm 1.2116 (1.2196)\n",
            "Epoch: [10][ 20/195]\tTime  0.151 ( 0.160)\tData  0.000 ( 0.009)\tD(real) 0.5337 (0.5334)\tD(fake) 0.4972 (0.4865)\tgrad(D) penalty 0.0137 (0.0140)\tRec loss 4461.5381 (4299.0498)\tnorm 1.2258 (1.2181)\n",
            "Epoch: [10][ 30/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.5339 (0.5335)\tD(fake) 0.5079 (0.4919)\tgrad(D) penalty 0.0127 (0.0138)\tRec loss 4486.4863 (4302.4877)\tnorm 1.1986 (1.2162)\n",
            "Epoch: [10][ 40/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.005)\tD(real) 0.5345 (0.5338)\tD(fake) 0.5202 (0.4986)\tgrad(D) penalty 0.0124 (0.0138)\tRec loss 4187.4263 (4282.0470)\tnorm 1.2089 (1.2139)\n",
            "Epoch: [10][ 50/195]\tTime  0.166 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.5348 (0.5339)\tD(fake) 0.5258 (0.5028)\tgrad(D) penalty 0.0175 (0.0141)\tRec loss 4246.5132 (4303.8644)\tnorm 1.2361 (1.2150)\n",
            "Epoch: [10][ 60/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5352 (0.5342)\tD(fake) 0.5167 (0.5060)\tgrad(D) penalty 0.0107 (0.0138)\tRec loss 4273.5049 (4308.6194)\tnorm 1.2275 (1.2169)\n",
            "Epoch: [10][ 70/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.5355 (0.5343)\tD(fake) 0.5063 (0.5064)\tgrad(D) penalty 0.0128 (0.0137)\tRec loss 4268.5283 (4314.1077)\tnorm 1.2164 (1.2161)\n",
            "Epoch: [10][ 80/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.5359 (0.5345)\tD(fake) 0.4982 (0.5055)\tgrad(D) penalty 0.0123 (0.0135)\tRec loss 4224.8452 (4310.3037)\tnorm 1.2177 (1.2159)\n",
            "Epoch: [10][ 90/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5362 (0.5347)\tD(fake) 0.4926 (0.5045)\tgrad(D) penalty 0.0119 (0.0134)\tRec loss 4248.9473 (4304.0009)\tnorm 1.1941 (1.2147)\n",
            "Epoch: [10][100/195]\tTime  0.168 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5367 (0.5349)\tD(fake) 0.4942 (0.5034)\tgrad(D) penalty 0.0121 (0.0134)\tRec loss 4116.4038 (4312.4952)\tnorm 1.2040 (1.2136)\n",
            "Epoch: [10][110/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5370 (0.5351)\tD(fake) 0.4962 (0.5030)\tgrad(D) penalty 0.0150 (0.0135)\tRec loss 4347.3496 (4325.7966)\tnorm 1.2311 (1.2142)\n",
            "Epoch: [10][120/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5374 (0.5353)\tD(fake) 0.5009 (0.5027)\tgrad(D) penalty 0.0140 (0.0136)\tRec loss 4294.4453 (4334.3598)\tnorm 1.2275 (1.2151)\n",
            "Epoch: [10][130/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5378 (0.5354)\tD(fake) 0.5072 (0.5029)\tgrad(D) penalty 0.0153 (0.0137)\tRec loss 4338.3486 (4340.4601)\tnorm 1.2059 (1.2153)\n",
            "Epoch: [10][140/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5382 (0.5356)\tD(fake) 0.5057 (0.5033)\tgrad(D) penalty 0.0124 (0.0135)\tRec loss 4622.3164 (4345.2752)\tnorm 1.2055 (1.2145)\n",
            "Epoch: [10][150/195]\tTime  0.165 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5385 (0.5358)\tD(fake) 0.5021 (0.5032)\tgrad(D) penalty 0.0136 (0.0135)\tRec loss 4387.1484 (4342.8380)\tnorm 1.2287 (1.2142)\n",
            "Epoch: [10][160/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5389 (0.5360)\tD(fake) 0.4953 (0.5029)\tgrad(D) penalty 0.0112 (0.0134)\tRec loss 4493.2920 (4343.8408)\tnorm 1.2140 (1.2143)\n",
            "Epoch: [10][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5392 (0.5362)\tD(fake) 0.4914 (0.5024)\tgrad(D) penalty 0.0113 (0.0133)\tRec loss 4344.8545 (4347.9106)\tnorm 1.2212 (1.2142)\n",
            "Epoch: [10][180/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5397 (0.5364)\tD(fake) 0.4899 (0.5014)\tgrad(D) penalty 0.0121 (0.0133)\tRec loss 4402.9678 (4355.7397)\tnorm 1.1972 (1.2144)\n",
            "Epoch: [10][190/195]\tTime  0.156 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5400 (0.5365)\tD(fake) 0.4948 (0.5011)\tgrad(D) penalty 0.0117 (0.0132)\tRec loss 4421.9775 (4356.9659)\tnorm 1.2139 (1.2148)\n",
            "Epoch: [11][  0/195]\tTime  0.399 ( 0.399)\tData  0.201 ( 0.201)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4305.6543 (4305.6543)\tnorm 1.1969 (1.1969)\n",
            "Epoch: [11][ 10/195]\tTime  0.154 ( 0.173)\tData  0.000 ( 0.018)\tD(real) 0.5405 (0.5405)\tD(fake) 0.4654 (0.4657)\tgrad(D) penalty 0.0128 (0.0141)\tRec loss 4510.1318 (4326.7184)\tnorm 1.2048 (1.2149)\n",
            "Epoch: [11][ 20/195]\tTime  0.151 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.5410 (0.5407)\tD(fake) 0.4783 (0.4716)\tgrad(D) penalty 0.0128 (0.0141)\tRec loss 4571.3438 (4331.1986)\tnorm 1.1953 (1.2108)\n",
            "Epoch: [11][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.5414 (0.5409)\tD(fake) 0.4866 (0.4753)\tgrad(D) penalty 0.0117 (0.0133)\tRec loss 4243.3818 (4330.0846)\tnorm 1.2091 (1.2114)\n",
            "Epoch: [11][ 40/195]\tTime  0.151 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.5419 (0.5411)\tD(fake) 0.4960 (0.4809)\tgrad(D) penalty 0.0113 (0.0131)\tRec loss 4330.7710 (4336.1659)\tnorm 1.2166 (1.2125)\n",
            "Epoch: [11][ 50/195]\tTime  0.167 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.5422 (0.5413)\tD(fake) 0.4968 (0.4835)\tgrad(D) penalty 0.0127 (0.0130)\tRec loss 4276.4888 (4339.8192)\tnorm 1.2179 (1.2111)\n",
            "Epoch: [11][ 60/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.5426 (0.5415)\tD(fake) 0.5000 (0.4865)\tgrad(D) penalty 0.0111 (0.0127)\tRec loss 4192.7178 (4341.7629)\tnorm 1.2015 (1.2088)\n",
            "Epoch: [11][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5429 (0.5417)\tD(fake) 0.4984 (0.4878)\tgrad(D) penalty 0.0139 (0.0127)\tRec loss 4114.5088 (4331.0440)\tnorm 1.2085 (1.2083)\n",
            "Epoch: [11][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5434 (0.5419)\tD(fake) 0.4824 (0.4875)\tgrad(D) penalty 0.0102 (0.0124)\tRec loss 4231.9360 (4344.1312)\tnorm 1.2128 (1.2068)\n",
            "Epoch: [11][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5437 (0.5421)\tD(fake) 0.4859 (0.4874)\tgrad(D) penalty 0.0145 (0.0125)\tRec loss 4292.5664 (4345.2474)\tnorm 1.2155 (1.2076)\n",
            "Epoch: [11][100/195]\tTime  0.168 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5441 (0.5423)\tD(fake) 0.4799 (0.4870)\tgrad(D) penalty 0.0100 (0.0123)\tRec loss 4533.1953 (4339.6608)\tnorm 1.1806 (1.2065)\n",
            "Epoch: [11][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5444 (0.5424)\tD(fake) 0.4790 (0.4866)\tgrad(D) penalty 0.0096 (0.0122)\tRec loss 4481.8647 (4345.2672)\tnorm 1.1876 (1.2052)\n",
            "Epoch: [11][120/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5449 (0.5427)\tD(fake) 0.4775 (0.4858)\tgrad(D) penalty 0.0115 (0.0120)\tRec loss 4519.5273 (4342.7648)\tnorm 1.1962 (1.2051)\n",
            "Epoch: [11][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5451 (0.5428)\tD(fake) 0.4737 (0.4853)\tgrad(D) penalty 0.0088 (0.0119)\tRec loss 4480.6021 (4347.8485)\tnorm 1.2003 (1.2048)\n",
            "Epoch: [11][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5456 (0.5430)\tD(fake) 0.4663 (0.4839)\tgrad(D) penalty 0.0110 (0.0118)\tRec loss 4127.8198 (4341.6170)\tnorm 1.2078 (1.2041)\n",
            "Epoch: [11][150/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5458 (0.5432)\tD(fake) 0.4678 (0.4830)\tgrad(D) penalty 0.0139 (0.0118)\tRec loss 4242.8555 (4337.5760)\tnorm 1.1934 (1.2037)\n",
            "Epoch: [11][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5462 (0.5434)\tD(fake) 0.4651 (0.4816)\tgrad(D) penalty 0.0116 (0.0118)\tRec loss 4504.1973 (4343.3895)\tnorm 1.1890 (1.2029)\n",
            "Epoch: [11][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5466 (0.5436)\tD(fake) 0.4722 (0.4811)\tgrad(D) penalty 0.0118 (0.0118)\tRec loss 4527.6416 (4347.6316)\tnorm 1.1865 (1.2023)\n",
            "Epoch: [11][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5471 (0.5438)\tD(fake) 0.4756 (0.4807)\tgrad(D) penalty 0.0093 (0.0117)\tRec loss 4352.4238 (4352.0456)\tnorm 1.1922 (1.2025)\n",
            "Epoch: [11][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5474 (0.5439)\tD(fake) 0.4739 (0.4805)\tgrad(D) penalty 0.0101 (0.0117)\tRec loss 4330.5303 (4346.3679)\tnorm 1.1990 (1.2024)\n",
            "Epoch: [12][  0/195]\tTime  0.366 ( 0.366)\tData  0.187 ( 0.187)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4127.6777 (4127.6777)\tnorm 1.2038 (1.2038)\n",
            "Epoch: [12][ 10/195]\tTime  0.149 ( 0.169)\tData  0.000 ( 0.017)\tD(real) 0.5479 (0.5479)\tD(fake) 0.4314 (0.4317)\tgrad(D) penalty 0.0133 (0.0123)\tRec loss 4226.2915 (4323.9006)\tnorm 1.2181 (1.2031)\n",
            "Epoch: [12][ 20/195]\tTime  0.149 ( 0.160)\tData  0.000 ( 0.009)\tD(real) 0.5485 (0.5481)\tD(fake) 0.4455 (0.4374)\tgrad(D) penalty 0.0119 (0.0118)\tRec loss 4497.9277 (4327.3985)\tnorm 1.1975 (1.1997)\n",
            "Epoch: [12][ 30/195]\tTime  0.151 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.5488 (0.5483)\tD(fake) 0.4498 (0.4409)\tgrad(D) penalty 0.0080 (0.0113)\tRec loss 4242.2207 (4316.0302)\tnorm 1.2078 (1.1983)\n",
            "Epoch: [12][ 40/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.5493 (0.5485)\tD(fake) 0.4546 (0.4449)\tgrad(D) penalty 0.0103 (0.0113)\tRec loss 4188.0181 (4289.3245)\tnorm 1.1886 (1.1977)\n",
            "Epoch: [12][ 50/195]\tTime  0.165 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.5495 (0.5487)\tD(fake) 0.4546 (0.4469)\tgrad(D) penalty 0.0112 (0.0111)\tRec loss 4534.2686 (4285.9533)\tnorm 1.1811 (1.1963)\n",
            "Epoch: [12][ 60/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.5500 (0.5489)\tD(fake) 0.4535 (0.4484)\tgrad(D) penalty 0.0117 (0.0111)\tRec loss 4702.9482 (4319.1480)\tnorm 1.1848 (1.1942)\n",
            "Epoch: [12][ 70/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5503 (0.5491)\tD(fake) 0.4482 (0.4486)\tgrad(D) penalty 0.0100 (0.0110)\tRec loss 4128.3140 (4320.1845)\tnorm 1.2207 (1.1938)\n",
            "Epoch: [12][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5507 (0.5493)\tD(fake) 0.4444 (0.4486)\tgrad(D) penalty 0.0110 (0.0109)\tRec loss 4598.7847 (4318.0765)\tnorm 1.1654 (1.1929)\n",
            "Epoch: [12][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5511 (0.5495)\tD(fake) 0.4434 (0.4483)\tgrad(D) penalty 0.0116 (0.0109)\tRec loss 4151.1553 (4315.6873)\tnorm 1.1842 (1.1912)\n",
            "Epoch: [12][100/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5515 (0.5497)\tD(fake) 0.4414 (0.4477)\tgrad(D) penalty 0.0099 (0.0108)\tRec loss 4890.2441 (4321.7559)\tnorm 1.1640 (1.1898)\n",
            "Epoch: [12][110/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5517 (0.5498)\tD(fake) 0.4385 (0.4469)\tgrad(D) penalty 0.0111 (0.0108)\tRec loss 4376.3887 (4336.6295)\tnorm 1.2030 (1.1880)\n",
            "Epoch: [12][120/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5522 (0.5501)\tD(fake) 0.4349 (0.4460)\tgrad(D) penalty 0.0112 (0.0107)\tRec loss 4249.6270 (4336.7384)\tnorm 1.1921 (1.1872)\n",
            "Epoch: [12][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5525 (0.5502)\tD(fake) 0.4376 (0.4455)\tgrad(D) penalty 0.0103 (0.0107)\tRec loss 4563.4717 (4338.3544)\tnorm 1.1768 (1.1864)\n",
            "Epoch: [12][140/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5530 (0.5504)\tD(fake) 0.4425 (0.4450)\tgrad(D) penalty 0.0087 (0.0106)\tRec loss 4557.5073 (4336.3172)\tnorm 1.1683 (1.1854)\n",
            "Epoch: [12][150/195]\tTime  0.167 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5533 (0.5506)\tD(fake) 0.4394 (0.4446)\tgrad(D) penalty 0.0080 (0.0106)\tRec loss 4517.5400 (4337.5985)\tnorm 1.1697 (1.1842)\n",
            "Epoch: [12][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5537 (0.5508)\tD(fake) 0.4377 (0.4442)\tgrad(D) penalty 0.0089 (0.0105)\tRec loss 4425.9717 (4337.4850)\tnorm 1.1564 (1.1833)\n",
            "Epoch: [12][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5540 (0.5509)\tD(fake) 0.4401 (0.4441)\tgrad(D) penalty 0.0108 (0.0105)\tRec loss 4625.8853 (4336.2887)\tnorm 1.1844 (1.1824)\n",
            "Epoch: [12][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5545 (0.5512)\tD(fake) 0.4425 (0.4437)\tgrad(D) penalty 0.0087 (0.0105)\tRec loss 4254.7676 (4334.0184)\tnorm 1.1762 (1.1813)\n",
            "Epoch: [12][190/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5548 (0.5513)\tD(fake) 0.4390 (0.4435)\tgrad(D) penalty 0.0094 (0.0105)\tRec loss 4264.5332 (4331.2234)\tnorm 1.1498 (1.1799)\n",
            "Epoch: [13][  0/195]\tTime  0.396 ( 0.396)\tData  0.199 ( 0.199)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4299.2607 (4299.2607)\tnorm 1.1497 (1.1497)\n",
            "Epoch: [13][ 10/195]\tTime  0.152 ( 0.172)\tData  0.000 ( 0.018)\tD(real) 0.5554 (0.5553)\tD(fake) 0.4087 (0.4046)\tgrad(D) penalty 0.0109 (0.0118)\tRec loss 4404.8457 (4323.5246)\tnorm 1.1588 (1.1514)\n",
            "Epoch: [13][ 20/195]\tTime  0.148 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.5559 (0.5556)\tD(fake) 0.4198 (0.4111)\tgrad(D) penalty 0.0099 (0.0117)\tRec loss 4118.9819 (4282.5615)\tnorm 1.1329 (1.1508)\n",
            "Epoch: [13][ 30/195]\tTime  0.146 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.5562 (0.5557)\tD(fake) 0.4250 (0.4149)\tgrad(D) penalty 0.0088 (0.0110)\tRec loss 4327.3164 (4304.4078)\tnorm 1.1515 (1.1518)\n",
            "Epoch: [13][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.5567 (0.5560)\tD(fake) 0.4269 (0.4191)\tgrad(D) penalty 0.0070 (0.0102)\tRec loss 4653.0244 (4315.2303)\tnorm 1.1522 (1.1510)\n",
            "Epoch: [13][ 50/195]\tTime  0.164 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.5569 (0.5561)\tD(fake) 0.4229 (0.4199)\tgrad(D) penalty 0.0090 (0.0100)\tRec loss 3985.4482 (4310.2107)\tnorm 1.1628 (1.1509)\n",
            "Epoch: [13][ 60/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.5574 (0.5563)\tD(fake) 0.4177 (0.4200)\tgrad(D) penalty 0.0106 (0.0100)\tRec loss 4408.5688 (4320.3908)\tnorm 1.1570 (1.1526)\n",
            "Epoch: [13][ 70/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.5577 (0.5565)\tD(fake) 0.4235 (0.4201)\tgrad(D) penalty 0.0100 (0.0099)\tRec loss 4146.9341 (4324.0659)\tnorm 1.1735 (1.1532)\n",
            "Epoch: [13][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5581 (0.5567)\tD(fake) 0.4219 (0.4203)\tgrad(D) penalty 0.0118 (0.0101)\tRec loss 4406.0054 (4319.7944)\tnorm 1.1664 (1.1547)\n",
            "Epoch: [13][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5584 (0.5569)\tD(fake) 0.4293 (0.4211)\tgrad(D) penalty 0.0099 (0.0101)\tRec loss 3908.7625 (4312.0562)\tnorm 1.1751 (1.1551)\n",
            "Epoch: [13][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5589 (0.5571)\tD(fake) 0.4237 (0.4213)\tgrad(D) penalty 0.0084 (0.0100)\tRec loss 4112.1221 (4315.1777)\tnorm 1.1738 (1.1552)\n",
            "Epoch: [13][110/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5592 (0.5572)\tD(fake) 0.4146 (0.4210)\tgrad(D) penalty 0.0086 (0.0099)\tRec loss 4353.2373 (4317.4654)\tnorm 1.1505 (1.1566)\n",
            "Epoch: [13][120/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5596 (0.5575)\tD(fake) 0.4131 (0.4205)\tgrad(D) penalty 0.0103 (0.0099)\tRec loss 3999.1892 (4313.4167)\tnorm 1.1549 (1.1576)\n",
            "Epoch: [13][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5599 (0.5576)\tD(fake) 0.4079 (0.4199)\tgrad(D) penalty 0.0095 (0.0099)\tRec loss 4572.9297 (4324.9533)\tnorm 1.1656 (1.1580)\n",
            "Epoch: [13][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5604 (0.5578)\tD(fake) 0.4147 (0.4193)\tgrad(D) penalty 0.0103 (0.0100)\tRec loss 4588.8662 (4330.8075)\tnorm 1.1662 (1.1586)\n",
            "Epoch: [13][150/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5606 (0.5580)\tD(fake) 0.4143 (0.4190)\tgrad(D) penalty 0.0101 (0.0099)\tRec loss 4136.5703 (4325.3277)\tnorm 1.1567 (1.1590)\n",
            "Epoch: [13][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5612 (0.5582)\tD(fake) 0.4170 (0.4190)\tgrad(D) penalty 0.0084 (0.0099)\tRec loss 4197.2041 (4318.7230)\tnorm 1.1550 (1.1592)\n",
            "Epoch: [13][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5615 (0.5584)\tD(fake) 0.4142 (0.4188)\tgrad(D) penalty 0.0076 (0.0098)\tRec loss 4499.0786 (4322.2323)\tnorm 1.1414 (1.1584)\n",
            "Epoch: [13][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5619 (0.5586)\tD(fake) 0.4033 (0.4180)\tgrad(D) penalty 0.0085 (0.0097)\tRec loss 4099.6904 (4321.3636)\tnorm 1.1607 (1.1580)\n",
            "Epoch: [13][190/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5622 (0.5587)\tD(fake) 0.3973 (0.4171)\tgrad(D) penalty 0.0097 (0.0097)\tRec loss 4290.5513 (4320.5261)\tnorm 1.1545 (1.1572)\n",
            "Epoch: [14][  0/195]\tTime  0.386 ( 0.386)\tData  0.193 ( 0.193)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4054.9448 (4054.9448)\tnorm 1.1554 (1.1554)\n",
            "Epoch: [14][ 10/195]\tTime  0.153 ( 0.172)\tData  0.000 ( 0.018)\tD(real) 0.5628 (0.5627)\tD(fake) 0.3764 (0.3716)\tgrad(D) penalty 0.0110 (0.0106)\tRec loss 4094.7063 (4121.2010)\tnorm 1.1204 (1.1359)\n",
            "Epoch: [14][ 20/195]\tTime  0.147 ( 0.161)\tData  0.000 ( 0.009)\tD(real) 0.5633 (0.5630)\tD(fake) 0.4092 (0.3848)\tgrad(D) penalty 0.0094 (0.0104)\tRec loss 4389.6069 (4238.7293)\tnorm 1.1321 (1.1337)\n",
            "Epoch: [14][ 30/195]\tTime  0.154 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.5636 (0.5631)\tD(fake) 0.4098 (0.3917)\tgrad(D) penalty 0.0074 (0.0096)\tRec loss 3976.2131 (4278.0740)\tnorm 1.1770 (1.1397)\n",
            "Epoch: [14][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.5640 (0.5634)\tD(fake) 0.4074 (0.3966)\tgrad(D) penalty 0.0075 (0.0093)\tRec loss 4372.0654 (4273.3060)\tnorm 1.1683 (1.1445)\n",
            "Epoch: [14][ 50/195]\tTime  0.173 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.5644 (0.5635)\tD(fake) 0.4024 (0.3978)\tgrad(D) penalty 0.0071 (0.0091)\tRec loss 4273.7998 (4282.4363)\tnorm 1.1459 (1.1479)\n",
            "Epoch: [14][ 60/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.5648 (0.5637)\tD(fake) 0.3922 (0.3975)\tgrad(D) penalty 0.0095 (0.0091)\tRec loss 4504.6440 (4308.0987)\tnorm 1.1638 (1.1487)\n",
            "Epoch: [14][ 70/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.5650 (0.5639)\tD(fake) 0.3875 (0.3965)\tgrad(D) penalty 0.0078 (0.0089)\tRec loss 4226.3682 (4312.5867)\tnorm 1.1776 (1.1508)\n",
            "Epoch: [14][ 80/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5655 (0.5641)\tD(fake) 0.3839 (0.3948)\tgrad(D) penalty 0.0122 (0.0091)\tRec loss 4520.3154 (4318.4267)\tnorm 1.1679 (1.1532)\n",
            "Epoch: [14][ 90/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5658 (0.5643)\tD(fake) 0.3855 (0.3940)\tgrad(D) penalty 0.0109 (0.0092)\tRec loss 4151.2212 (4312.8015)\tnorm 1.1683 (1.1554)\n",
            "Epoch: [14][100/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5663 (0.5645)\tD(fake) 0.3920 (0.3939)\tgrad(D) penalty 0.0085 (0.0092)\tRec loss 4145.2168 (4311.1208)\tnorm 1.1846 (1.1576)\n",
            "Epoch: [14][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5665 (0.5646)\tD(fake) 0.3917 (0.3937)\tgrad(D) penalty 0.0083 (0.0091)\tRec loss 4007.7427 (4313.2455)\tnorm 1.1736 (1.1592)\n",
            "Epoch: [14][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5669 (0.5648)\tD(fake) 0.3850 (0.3931)\tgrad(D) penalty 0.0069 (0.0090)\tRec loss 4483.1030 (4309.5594)\tnorm 1.1868 (1.1604)\n",
            "Epoch: [14][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5673 (0.5650)\tD(fake) 0.3812 (0.3924)\tgrad(D) penalty 0.0092 (0.0090)\tRec loss 4102.1343 (4305.2315)\tnorm 1.1542 (1.1607)\n",
            "Epoch: [14][140/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5677 (0.5652)\tD(fake) 0.3771 (0.3911)\tgrad(D) penalty 0.0120 (0.0090)\tRec loss 4507.9692 (4307.2609)\tnorm 1.1625 (1.1606)\n",
            "Epoch: [14][150/195]\tTime  0.162 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5680 (0.5654)\tD(fake) 0.3759 (0.3902)\tgrad(D) penalty 0.0078 (0.0090)\tRec loss 4522.3271 (4312.7908)\tnorm 1.1510 (1.1603)\n",
            "Epoch: [14][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5684 (0.5656)\tD(fake) 0.3783 (0.3890)\tgrad(D) penalty 0.0088 (0.0089)\tRec loss 4410.2305 (4317.4382)\tnorm 1.1422 (1.1594)\n",
            "Epoch: [14][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5687 (0.5657)\tD(fake) 0.3781 (0.3885)\tgrad(D) penalty 0.0066 (0.0089)\tRec loss 4280.3804 (4311.5408)\tnorm 1.1666 (1.1592)\n",
            "Epoch: [14][180/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5691 (0.5659)\tD(fake) 0.3752 (0.3876)\tgrad(D) penalty 0.0051 (0.0087)\tRec loss 4238.3916 (4315.6435)\tnorm 1.1420 (1.1582)\n",
            "Epoch: [14][190/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5693 (0.5661)\tD(fake) 0.3656 (0.3868)\tgrad(D) penalty 0.0089 (0.0087)\tRec loss 4437.4111 (4312.1961)\tnorm 1.1566 (1.1572)\n",
            "Epoch: [15][  0/195]\tTime  0.412 ( 0.412)\tData  0.211 ( 0.211)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4147.7393 (4147.7393)\tnorm 1.1574 (1.1574)\n",
            "Epoch: [15][ 10/195]\tTime  0.146 ( 0.173)\tData  0.000 ( 0.020)\tD(real) 0.5700 (0.5700)\tD(fake) 0.3460 (0.3400)\tgrad(D) penalty 0.0125 (0.0134)\tRec loss 4049.6243 (4252.4201)\tnorm 1.1596 (1.1527)\n",
            "Epoch: [15][ 20/195]\tTime  0.145 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.5705 (0.5702)\tD(fake) 0.3766 (0.3564)\tgrad(D) penalty 0.0075 (0.0103)\tRec loss 4763.8066 (4306.5712)\tnorm 1.1570 (1.1526)\n",
            "Epoch: [15][ 30/195]\tTime  0.145 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.5708 (0.5704)\tD(fake) 0.3817 (0.3639)\tgrad(D) penalty 0.0071 (0.0092)\tRec loss 4113.6011 (4291.9697)\tnorm 1.1382 (1.1507)\n",
            "Epoch: [15][ 40/195]\tTime  0.150 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.5712 (0.5706)\tD(fake) 0.3730 (0.3673)\tgrad(D) penalty 0.0088 (0.0086)\tRec loss 4433.7256 (4300.9214)\tnorm 1.1325 (1.1512)\n",
            "Epoch: [15][ 50/195]\tTime  0.168 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.5715 (0.5707)\tD(fake) 0.3635 (0.3671)\tgrad(D) penalty 0.0060 (0.0083)\tRec loss 4230.8428 (4292.1482)\tnorm 1.1277 (1.1486)\n",
            "Epoch: [15][ 60/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.5719 (0.5709)\tD(fake) 0.3526 (0.3648)\tgrad(D) penalty 0.0068 (0.0081)\tRec loss 4322.6104 (4292.7810)\tnorm 1.1388 (1.1450)\n",
            "Epoch: [15][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5722 (0.5711)\tD(fake) 0.3516 (0.3632)\tgrad(D) penalty 0.0097 (0.0082)\tRec loss 4605.2886 (4294.4914)\tnorm 1.1275 (1.1435)\n",
            "Epoch: [15][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5726 (0.5713)\tD(fake) 0.3595 (0.3621)\tgrad(D) penalty 0.0075 (0.0081)\tRec loss 4237.5303 (4292.6128)\tnorm 1.1312 (1.1421)\n",
            "Epoch: [15][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5730 (0.5714)\tD(fake) 0.3593 (0.3619)\tgrad(D) penalty 0.0084 (0.0081)\tRec loss 4395.6816 (4299.0168)\tnorm 1.1551 (1.1411)\n",
            "Epoch: [15][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5734 (0.5717)\tD(fake) 0.3593 (0.3615)\tgrad(D) penalty 0.0100 (0.0083)\tRec loss 4463.6592 (4296.5755)\tnorm 1.1406 (1.1420)\n",
            "Epoch: [15][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5737 (0.5718)\tD(fake) 0.3595 (0.3614)\tgrad(D) penalty 0.0060 (0.0081)\tRec loss 4299.8652 (4298.0090)\tnorm 1.1473 (1.1431)\n",
            "Epoch: [15][120/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5741 (0.5720)\tD(fake) 0.3521 (0.3608)\tgrad(D) penalty 0.0068 (0.0079)\tRec loss 4675.9839 (4291.8896)\tnorm 1.1453 (1.1439)\n",
            "Epoch: [15][130/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5744 (0.5722)\tD(fake) 0.3496 (0.3602)\tgrad(D) penalty 0.0061 (0.0079)\tRec loss 4292.3574 (4290.3775)\tnorm 1.1261 (1.1433)\n",
            "Epoch: [15][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5748 (0.5724)\tD(fake) 0.3472 (0.3592)\tgrad(D) penalty 0.0071 (0.0079)\tRec loss 4210.7046 (4291.5209)\tnorm 1.1286 (1.1428)\n",
            "Epoch: [15][150/195]\tTime  0.168 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5751 (0.5725)\tD(fake) 0.3449 (0.3585)\tgrad(D) penalty 0.0083 (0.0079)\tRec loss 4546.0713 (4290.5087)\tnorm 1.1280 (1.1428)\n",
            "Epoch: [15][160/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5755 (0.5727)\tD(fake) 0.3430 (0.3574)\tgrad(D) penalty 0.0096 (0.0080)\tRec loss 4180.2285 (4291.1538)\tnorm 1.1308 (1.1422)\n",
            "Epoch: [15][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5758 (0.5729)\tD(fake) 0.3438 (0.3568)\tgrad(D) penalty 0.0085 (0.0080)\tRec loss 4315.2471 (4293.9198)\tnorm 1.1307 (1.1413)\n",
            "Epoch: [15][180/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5762 (0.5731)\tD(fake) 0.3393 (0.3559)\tgrad(D) penalty 0.0101 (0.0081)\tRec loss 4577.4023 (4298.0140)\tnorm 1.1317 (1.1404)\n",
            "Epoch: [15][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5765 (0.5732)\tD(fake) 0.3465 (0.3555)\tgrad(D) penalty 0.0100 (0.0081)\tRec loss 4448.1924 (4302.9761)\tnorm 1.1315 (1.1401)\n",
            "Epoch: [16][  0/195]\tTime  0.401 ( 0.401)\tData  0.205 ( 0.205)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4092.3384 (4092.3384)\tnorm 1.1250 (1.1250)\n",
            "Epoch: [16][ 10/195]\tTime  0.146 ( 0.173)\tData  0.000 ( 0.019)\tD(real) 0.5770 (0.5770)\tD(fake) 0.3241 (0.3194)\tgrad(D) penalty 0.0101 (0.0129)\tRec loss 4413.9199 (4301.7721)\tnorm 1.1380 (1.1260)\n",
            "Epoch: [16][ 20/195]\tTime  0.146 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.5775 (0.5772)\tD(fake) 0.3496 (0.3320)\tgrad(D) penalty 0.0091 (0.0117)\tRec loss 4147.1641 (4280.3659)\tnorm 1.1397 (1.1271)\n",
            "Epoch: [16][ 30/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.5778 (0.5773)\tD(fake) 0.3499 (0.3369)\tgrad(D) penalty 0.0157 (0.0120)\tRec loss 4416.6890 (4323.6784)\tnorm 1.1252 (1.1250)\n",
            "Epoch: [16][ 40/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.5781 (0.5775)\tD(fake) 0.3382 (0.3387)\tgrad(D) penalty 0.0172 (0.0130)\tRec loss 4335.3945 (4316.4312)\tnorm 1.1084 (1.1244)\n",
            "Epoch: [16][ 50/195]\tTime  0.167 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.5784 (0.5777)\tD(fake) 0.3369 (0.3382)\tgrad(D) penalty 0.0183 (0.0139)\tRec loss 4444.9482 (4312.4619)\tnorm 1.1154 (1.1229)\n",
            "Epoch: [16][ 60/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.5788 (0.5779)\tD(fake) 0.3340 (0.3376)\tgrad(D) penalty 0.0212 (0.0153)\tRec loss 4013.6572 (4298.9191)\tnorm 1.1171 (1.1209)\n",
            "Epoch: [16][ 70/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.5790 (0.5780)\tD(fake) 0.3408 (0.3378)\tgrad(D) penalty 0.0273 (0.0164)\tRec loss 4453.4111 (4297.0390)\tnorm 1.1240 (1.1202)\n",
            "Epoch: [16][ 80/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.5794 (0.5782)\tD(fake) 0.3430 (0.3384)\tgrad(D) penalty 0.0314 (0.0182)\tRec loss 4173.1621 (4287.3744)\tnorm 1.1308 (1.1202)\n",
            "Epoch: [16][ 90/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5796 (0.5783)\tD(fake) 0.3421 (0.3386)\tgrad(D) penalty 0.0330 (0.0196)\tRec loss 4186.2109 (4286.3917)\tnorm 1.1186 (1.1196)\n",
            "Epoch: [16][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5800 (0.5785)\tD(fake) 0.3367 (0.3387)\tgrad(D) penalty 0.0369 (0.0216)\tRec loss 4132.6318 (4287.5607)\tnorm 1.1242 (1.1192)\n",
            "Epoch: [16][110/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5802 (0.5786)\tD(fake) 0.3427 (0.3389)\tgrad(D) penalty 0.0372 (0.0227)\tRec loss 4682.5215 (4287.1497)\tnorm 1.1206 (1.1195)\n",
            "Epoch: [16][120/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5806 (0.5788)\tD(fake) 0.3480 (0.3393)\tgrad(D) penalty 0.0391 (0.0244)\tRec loss 4319.7812 (4295.3408)\tnorm 1.1330 (1.1207)\n",
            "Epoch: [16][130/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5808 (0.5789)\tD(fake) 0.3497 (0.3398)\tgrad(D) penalty 0.0370 (0.0252)\tRec loss 4350.7520 (4296.4360)\tnorm 1.1346 (1.1217)\n",
            "Epoch: [16][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5812 (0.5791)\tD(fake) 0.3441 (0.3404)\tgrad(D) penalty 0.0380 (0.0262)\tRec loss 4262.0781 (4297.3666)\tnorm 1.1295 (1.1222)\n",
            "Epoch: [16][150/195]\tTime  0.175 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5814 (0.5792)\tD(fake) 0.3495 (0.3408)\tgrad(D) penalty 0.0335 (0.0266)\tRec loss 4369.2832 (4298.7717)\tnorm 1.1386 (1.1226)\n",
            "Epoch: [16][160/195]\tTime  0.155 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5818 (0.5794)\tD(fake) 0.3536 (0.3417)\tgrad(D) penalty 0.0308 (0.0270)\tRec loss 4473.2588 (4296.0419)\tnorm 1.1154 (1.1228)\n",
            "Epoch: [16][170/195]\tTime  0.153 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5820 (0.5795)\tD(fake) 0.3598 (0.3424)\tgrad(D) penalty 0.0287 (0.0271)\tRec loss 4157.1777 (4296.0207)\tnorm 1.1194 (1.1230)\n",
            "Epoch: [16][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5823 (0.5797)\tD(fake) 0.3610 (0.3436)\tgrad(D) penalty 0.0237 (0.0270)\tRec loss 4145.9438 (4295.0061)\tnorm 1.1317 (1.1233)\n",
            "Epoch: [16][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5825 (0.5798)\tD(fake) 0.3536 (0.3441)\tgrad(D) penalty 0.0202 (0.0268)\tRec loss 4483.0156 (4299.0248)\tnorm 1.1154 (1.1228)\n",
            "Epoch: [17][  0/195]\tTime  0.391 ( 0.391)\tData  0.200 ( 0.200)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4228.0386 (4228.0386)\tnorm 1.1244 (1.1244)\n",
            "Epoch: [17][ 10/195]\tTime  0.151 ( 0.172)\tData  0.000 ( 0.018)\tD(real) 0.5831 (0.5830)\tD(fake) 0.3437 (0.3382)\tgrad(D) penalty 0.0183 (0.0188)\tRec loss 4649.3906 (4301.7297)\tnorm 1.1044 (1.1065)\n",
            "Epoch: [17][ 20/195]\tTime  0.153 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.5835 (0.5832)\tD(fake) 0.3702 (0.3534)\tgrad(D) penalty 0.0091 (0.0149)\tRec loss 4320.5410 (4337.1452)\tnorm 1.1067 (1.1060)\n",
            "Epoch: [17][ 30/195]\tTime  0.154 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.5837 (0.5833)\tD(fake) 0.3621 (0.3568)\tgrad(D) penalty 0.0082 (0.0132)\tRec loss 4215.7666 (4320.1874)\tnorm 1.1175 (1.1077)\n",
            "Epoch: [17][ 40/195]\tTime  0.149 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.5840 (0.5835)\tD(fake) 0.3372 (0.3536)\tgrad(D) penalty 0.0093 (0.0118)\tRec loss 4299.1787 (4304.6819)\tnorm 1.1313 (1.1100)\n",
            "Epoch: [17][ 50/195]\tTime  0.167 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.5842 (0.5836)\tD(fake) 0.3325 (0.3501)\tgrad(D) penalty 0.0089 (0.0115)\tRec loss 4347.2070 (4293.8151)\tnorm 1.1338 (1.1114)\n",
            "Epoch: [17][ 60/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.5846 (0.5838)\tD(fake) 0.3403 (0.3479)\tgrad(D) penalty 0.0062 (0.0107)\tRec loss 4612.6997 (4294.5422)\tnorm 1.0991 (1.1120)\n",
            "Epoch: [17][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5848 (0.5839)\tD(fake) 0.3351 (0.3466)\tgrad(D) penalty 0.0067 (0.0102)\tRec loss 4167.1904 (4276.2195)\tnorm 1.1208 (1.1125)\n",
            "Epoch: [17][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5853 (0.5841)\tD(fake) 0.3391 (0.3455)\tgrad(D) penalty 0.0054 (0.0096)\tRec loss 4231.0137 (4283.2191)\tnorm 1.1202 (1.1134)\n",
            "Epoch: [17][ 90/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5855 (0.5842)\tD(fake) 0.3336 (0.3445)\tgrad(D) penalty 0.0068 (0.0093)\tRec loss 4347.9072 (4281.2209)\tnorm 1.0914 (1.1130)\n",
            "Epoch: [17][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5859 (0.5844)\tD(fake) 0.3275 (0.3428)\tgrad(D) penalty 0.0054 (0.0089)\tRec loss 4218.8462 (4284.5949)\tnorm 1.1092 (1.1117)\n",
            "Epoch: [17][110/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5862 (0.5845)\tD(fake) 0.3233 (0.3414)\tgrad(D) penalty 0.0049 (0.0086)\tRec loss 4619.5234 (4278.7930)\tnorm 1.1080 (1.1116)\n",
            "Epoch: [17][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5866 (0.5847)\tD(fake) 0.3247 (0.3395)\tgrad(D) penalty 0.0073 (0.0084)\tRec loss 4491.6729 (4273.5718)\tnorm 1.1075 (1.1107)\n",
            "Epoch: [17][130/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5868 (0.5848)\tD(fake) 0.3214 (0.3384)\tgrad(D) penalty 0.0050 (0.0082)\tRec loss 4180.3921 (4282.2534)\tnorm 1.1032 (1.1101)\n",
            "Epoch: [17][140/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5873 (0.5850)\tD(fake) 0.3120 (0.3364)\tgrad(D) penalty 0.0109 (0.0081)\tRec loss 4322.6504 (4281.5909)\tnorm 1.1032 (1.1092)\n",
            "Epoch: [17][150/195]\tTime  0.164 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.5875 (0.5852)\tD(fake) 0.3109 (0.3349)\tgrad(D) penalty 0.0065 (0.0080)\tRec loss 4232.7012 (4279.0182)\tnorm 1.0981 (1.1079)\n",
            "Epoch: [17][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5879 (0.5854)\tD(fake) 0.3167 (0.3335)\tgrad(D) penalty 0.0055 (0.0079)\tRec loss 4584.5820 (4283.8742)\tnorm 1.0943 (1.1073)\n",
            "Epoch: [17][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5882 (0.5855)\tD(fake) 0.3169 (0.3327)\tgrad(D) penalty 0.0058 (0.0078)\tRec loss 4402.9248 (4281.5649)\tnorm 1.0886 (1.1069)\n",
            "Epoch: [17][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5886 (0.5857)\tD(fake) 0.3073 (0.3312)\tgrad(D) penalty 0.0061 (0.0076)\tRec loss 4356.2754 (4281.0073)\tnorm 1.0921 (1.1063)\n",
            "Epoch: [17][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.5889 (0.5858)\tD(fake) 0.3083 (0.3302)\tgrad(D) penalty 0.0080 (0.0076)\tRec loss 4405.3232 (4285.4969)\tnorm 1.0929 (1.1050)\n",
            "Epoch: [18][  0/195]\tTime  0.401 ( 0.401)\tData  0.203 ( 0.203)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4265.5410 (4265.5410)\tnorm 1.1028 (1.1028)\n",
            "Epoch: [18][ 10/195]\tTime  0.146 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.5894 (0.5894)\tD(fake) 0.3016 (0.2951)\tgrad(D) penalty 0.0077 (0.0083)\tRec loss 4297.1006 (4258.9073)\tnorm 1.0893 (1.0878)\n",
            "Epoch: [18][ 20/195]\tTime  0.147 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.5900 (0.5896)\tD(fake) 0.3235 (0.3102)\tgrad(D) penalty 0.0059 (0.0071)\tRec loss 4171.1621 (4295.1526)\tnorm 1.1139 (1.0875)\n",
            "Epoch: [18][ 30/195]\tTime  0.148 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.5902 (0.5898)\tD(fake) 0.3153 (0.3125)\tgrad(D) penalty 0.0057 (0.0065)\tRec loss 4021.7422 (4271.5933)\tnorm 1.0593 (1.0868)\n",
            "Epoch: [18][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.5906 (0.5900)\tD(fake) 0.3018 (0.3102)\tgrad(D) penalty 0.0072 (0.0068)\tRec loss 4446.6680 (4276.9196)\tnorm 1.0597 (1.0840)\n",
            "Epoch: [18][ 50/195]\tTime  0.166 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.5908 (0.5901)\tD(fake) 0.3017 (0.3086)\tgrad(D) penalty 0.0080 (0.0069)\tRec loss 4231.5732 (4278.9256)\tnorm 1.1001 (1.0835)\n",
            "Epoch: [18][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.5912 (0.5903)\tD(fake) 0.3137 (0.3090)\tgrad(D) penalty 0.0053 (0.0069)\tRec loss 4340.5259 (4276.3658)\tnorm 1.0807 (1.0837)\n",
            "Epoch: [18][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.5916 (0.5904)\tD(fake) 0.3063 (0.3089)\tgrad(D) penalty 0.0062 (0.0069)\tRec loss 4255.5049 (4266.7535)\tnorm 1.0828 (1.0830)\n",
            "Epoch: [18][ 80/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.5920 (0.5907)\tD(fake) 0.3014 (0.3079)\tgrad(D) penalty 0.0070 (0.0069)\tRec loss 4067.3005 (4262.1602)\tnorm 1.0814 (1.0833)\n",
            "Epoch: [18][ 90/195]\tTime  0.153 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5922 (0.5908)\tD(fake) 0.3005 (0.3074)\tgrad(D) penalty 0.0073 (0.0069)\tRec loss 4410.8643 (4271.4960)\tnorm 1.0848 (1.0834)\n",
            "Epoch: [18][100/195]\tTime  0.168 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5927 (0.5910)\tD(fake) 0.3088 (0.3071)\tgrad(D) penalty 0.0071 (0.0068)\tRec loss 4074.3596 (4276.4227)\tnorm 1.0745 (1.0829)\n",
            "Epoch: [18][110/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5929 (0.5911)\tD(fake) 0.3069 (0.3070)\tgrad(D) penalty 0.0050 (0.0067)\tRec loss 3953.1460 (4277.4715)\tnorm 1.0841 (1.0830)\n",
            "Epoch: [18][120/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5933 (0.5913)\tD(fake) 0.2995 (0.3064)\tgrad(D) penalty 0.0065 (0.0067)\tRec loss 4256.5410 (4279.1378)\tnorm 1.0773 (1.0835)\n",
            "Epoch: [18][130/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5936 (0.5915)\tD(fake) 0.3019 (0.3059)\tgrad(D) penalty 0.0064 (0.0068)\tRec loss 4352.3047 (4282.1884)\tnorm 1.0990 (1.0838)\n",
            "Epoch: [18][140/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5940 (0.5917)\tD(fake) 0.3025 (0.3055)\tgrad(D) penalty 0.0101 (0.0069)\tRec loss 4365.0107 (4283.0293)\tnorm 1.0847 (1.0847)\n",
            "Epoch: [18][150/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5943 (0.5918)\tD(fake) 0.3016 (0.3054)\tgrad(D) penalty 0.0086 (0.0070)\tRec loss 4266.3770 (4281.5574)\tnorm 1.0910 (1.0853)\n",
            "Epoch: [18][160/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.5947 (0.5920)\tD(fake) 0.3006 (0.3051)\tgrad(D) penalty 0.0081 (0.0071)\tRec loss 4077.3584 (4276.2116)\tnorm 1.1054 (1.0859)\n",
            "Epoch: [18][170/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5949 (0.5922)\tD(fake) 0.3071 (0.3051)\tgrad(D) penalty 0.0082 (0.0071)\tRec loss 4272.3330 (4273.5478)\tnorm 1.0890 (1.0862)\n",
            "Epoch: [18][180/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5953 (0.5924)\tD(fake) 0.3043 (0.3050)\tgrad(D) penalty 0.0107 (0.0074)\tRec loss 4387.2930 (4274.3937)\tnorm 1.0911 (1.0862)\n",
            "Epoch: [18][190/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.5956 (0.5925)\tD(fake) 0.3042 (0.3050)\tgrad(D) penalty 0.0091 (0.0075)\tRec loss 4535.4316 (4275.8543)\tnorm 1.1021 (1.0867)\n",
            "Epoch: [19][  0/195]\tTime  0.371 ( 0.371)\tData  0.192 ( 0.192)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4334.1353 (4334.1353)\tnorm 1.0726 (1.0726)\n",
            "Epoch: [19][ 10/195]\tTime  0.151 ( 0.171)\tData  0.000 ( 0.018)\tD(real) 0.5962 (0.5961)\tD(fake) 0.2923 (0.2873)\tgrad(D) penalty 0.0132 (0.0138)\tRec loss 3981.6763 (4285.0906)\tnorm 1.0878 (1.0914)\n",
            "Epoch: [19][ 20/195]\tTime  0.152 ( 0.160)\tData  0.000 ( 0.009)\tD(real) 0.5966 (0.5963)\tD(fake) 0.3183 (0.3028)\tgrad(D) penalty 0.0122 (0.0130)\tRec loss 4222.8262 (4284.7784)\tnorm 1.0953 (1.0928)\n",
            "Epoch: [19][ 30/195]\tTime  0.145 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.5969 (0.5965)\tD(fake) 0.3110 (0.3059)\tgrad(D) penalty 0.0113 (0.0126)\tRec loss 4475.9531 (4313.7002)\tnorm 1.0775 (1.0865)\n",
            "Epoch: [19][ 40/195]\tTime  0.145 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.5973 (0.5967)\tD(fake) 0.2980 (0.3047)\tgrad(D) penalty 0.0139 (0.0131)\tRec loss 4039.0522 (4302.7468)\tnorm 1.0892 (1.0867)\n",
            "Epoch: [19][ 50/195]\tTime  0.170 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.5975 (0.5968)\tD(fake) 0.3037 (0.3043)\tgrad(D) penalty 0.0155 (0.0135)\tRec loss 4326.5840 (4285.1907)\tnorm 1.0682 (1.0853)\n",
            "Epoch: [19][ 60/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.5980 (0.5970)\tD(fake) 0.3065 (0.3048)\tgrad(D) penalty 0.0140 (0.0138)\tRec loss 4654.6475 (4286.0648)\tnorm 1.0597 (1.0836)\n",
            "Epoch: [19][ 70/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5982 (0.5971)\tD(fake) 0.3014 (0.3046)\tgrad(D) penalty 0.0184 (0.0142)\tRec loss 4147.3525 (4280.8545)\tnorm 1.0746 (1.0821)\n",
            "Epoch: [19][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.5986 (0.5973)\tD(fake) 0.3127 (0.3051)\tgrad(D) penalty 0.0198 (0.0148)\tRec loss 3976.9912 (4266.1696)\tnorm 1.0872 (1.0815)\n",
            "Epoch: [19][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5988 (0.5975)\tD(fake) 0.3122 (0.3058)\tgrad(D) penalty 0.0164 (0.0149)\tRec loss 4322.5762 (4273.9682)\tnorm 1.0692 (1.0808)\n",
            "Epoch: [19][100/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5992 (0.5976)\tD(fake) 0.3070 (0.3063)\tgrad(D) penalty 0.0188 (0.0153)\tRec loss 4111.8589 (4264.4461)\tnorm 1.0752 (1.0809)\n",
            "Epoch: [19][110/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5994 (0.5978)\tD(fake) 0.3062 (0.3063)\tgrad(D) penalty 0.0211 (0.0157)\tRec loss 4315.7695 (4264.9436)\tnorm 1.0914 (1.0814)\n",
            "Epoch: [19][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.5999 (0.5980)\tD(fake) 0.3198 (0.3076)\tgrad(D) penalty 0.0197 (0.0162)\tRec loss 4707.8975 (4269.6182)\tnorm 1.0996 (1.0820)\n",
            "Epoch: [19][130/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6001 (0.5981)\tD(fake) 0.3124 (0.3081)\tgrad(D) penalty 0.0203 (0.0164)\tRec loss 4387.8838 (4268.5842)\tnorm 1.0922 (1.0830)\n",
            "Epoch: [19][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6005 (0.5983)\tD(fake) 0.3105 (0.3082)\tgrad(D) penalty 0.0214 (0.0169)\tRec loss 4589.4648 (4269.7219)\tnorm 1.0903 (1.0836)\n",
            "Epoch: [19][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6007 (0.5984)\tD(fake) 0.3182 (0.3085)\tgrad(D) penalty 0.0236 (0.0173)\tRec loss 4596.8271 (4273.3566)\tnorm 1.0944 (1.0840)\n",
            "Epoch: [19][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6011 (0.5986)\tD(fake) 0.3146 (0.3091)\tgrad(D) penalty 0.0238 (0.0177)\tRec loss 4416.6265 (4273.8077)\tnorm 1.0967 (1.0846)\n",
            "Epoch: [19][170/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6013 (0.5987)\tD(fake) 0.3274 (0.3098)\tgrad(D) penalty 0.0253 (0.0181)\tRec loss 4343.6113 (4275.5688)\tnorm 1.1060 (1.0851)\n",
            "Epoch: [19][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6017 (0.5989)\tD(fake) 0.3344 (0.3111)\tgrad(D) penalty 0.0243 (0.0185)\tRec loss 4073.2627 (4274.0349)\tnorm 1.0923 (1.0860)\n",
            "Epoch: [19][190/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6020 (0.5990)\tD(fake) 0.3327 (0.3121)\tgrad(D) penalty 0.0254 (0.0188)\tRec loss 3880.4966 (4275.4142)\tnorm 1.1096 (1.0866)\n",
            "Epoch: [20][  0/195]\tTime  0.401 ( 0.401)\tData  0.201 ( 0.201)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4193.3145 (4193.3145)\tnorm 1.0982 (1.0982)\n",
            "Epoch: [20][ 10/195]\tTime  0.146 ( 0.174)\tData  0.000 ( 0.018)\tD(real) 0.6024 (0.6023)\tD(fake) 0.3146 (0.3097)\tgrad(D) penalty 0.0259 (0.0281)\tRec loss 4265.4424 (4207.8428)\tnorm 1.0924 (1.1000)\n",
            "Epoch: [20][ 20/195]\tTime  0.152 ( 0.162)\tData  0.000 ( 0.010)\tD(real) 0.6028 (0.6025)\tD(fake) 0.3409 (0.3253)\tgrad(D) penalty 0.0334 (0.0305)\tRec loss 4297.8730 (4180.7597)\tnorm 1.0842 (1.0927)\n",
            "Epoch: [20][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.6030 (0.6027)\tD(fake) 0.3437 (0.3305)\tgrad(D) penalty 0.0314 (0.0310)\tRec loss 4319.3804 (4218.3131)\tnorm 1.0842 (1.0872)\n",
            "Epoch: [20][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.6033 (0.6028)\tD(fake) 0.3295 (0.3311)\tgrad(D) penalty 0.0347 (0.0322)\tRec loss 4589.0830 (4259.6610)\tnorm 1.0797 (1.0851)\n",
            "Epoch: [20][ 50/195]\tTime  0.167 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.6035 (0.6029)\tD(fake) 0.3285 (0.3307)\tgrad(D) penalty 0.0373 (0.0330)\tRec loss 4313.8936 (4255.1124)\tnorm 1.0886 (1.0831)\n",
            "Epoch: [20][ 60/195]\tTime  0.154 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.6039 (0.6031)\tD(fake) 0.3478 (0.3326)\tgrad(D) penalty 0.0431 (0.0347)\tRec loss 4216.0264 (4256.8520)\tnorm 1.0712 (1.0823)\n",
            "Epoch: [20][ 70/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6041 (0.6032)\tD(fake) 0.3419 (0.3340)\tgrad(D) penalty 0.0430 (0.0355)\tRec loss 4066.3579 (4257.3230)\tnorm 1.0641 (1.0803)\n",
            "Epoch: [20][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6045 (0.6034)\tD(fake) 0.3368 (0.3348)\tgrad(D) penalty 0.0432 (0.0366)\tRec loss 4481.3418 (4270.9690)\tnorm 1.0821 (1.0796)\n",
            "Epoch: [20][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6046 (0.6035)\tD(fake) 0.3388 (0.3351)\tgrad(D) penalty 0.0467 (0.0374)\tRec loss 4118.1533 (4275.4472)\tnorm 1.1048 (1.0804)\n",
            "Epoch: [20][100/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6050 (0.6037)\tD(fake) 0.3450 (0.3359)\tgrad(D) penalty 0.0446 (0.0384)\tRec loss 4259.0713 (4271.1935)\tnorm 1.0859 (1.0817)\n",
            "Epoch: [20][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6052 (0.6038)\tD(fake) 0.3443 (0.3365)\tgrad(D) penalty 0.0551 (0.0395)\tRec loss 4553.1143 (4268.7539)\tnorm 1.0774 (1.0819)\n",
            "Epoch: [20][120/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6055 (0.6040)\tD(fake) 0.3473 (0.3375)\tgrad(D) penalty 0.0559 (0.0410)\tRec loss 4575.1089 (4266.8909)\tnorm 1.0616 (1.0813)\n",
            "Epoch: [20][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6057 (0.6041)\tD(fake) 0.3421 (0.3379)\tgrad(D) penalty 0.0546 (0.0419)\tRec loss 4304.8965 (4271.4725)\tnorm 1.0621 (1.0806)\n",
            "Epoch: [20][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6059 (0.6042)\tD(fake) 0.3322 (0.3377)\tgrad(D) penalty 0.0560 (0.0429)\tRec loss 4378.8535 (4276.4323)\tnorm 1.0627 (1.0797)\n",
            "Epoch: [20][150/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6062 (0.6043)\tD(fake) 0.3410 (0.3378)\tgrad(D) penalty 0.0526 (0.0436)\tRec loss 4167.2812 (4268.5124)\tnorm 1.0680 (1.0790)\n",
            "Epoch: [20][160/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6065 (0.6045)\tD(fake) 0.3531 (0.3388)\tgrad(D) penalty 0.0541 (0.0444)\tRec loss 4359.7197 (4272.1781)\tnorm 1.0735 (1.0783)\n",
            "Epoch: [20][170/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6067 (0.6046)\tD(fake) 0.3533 (0.3394)\tgrad(D) penalty 0.0599 (0.0450)\tRec loss 4280.3506 (4271.2255)\tnorm 1.0686 (1.0782)\n",
            "Epoch: [20][180/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6070 (0.6047)\tD(fake) 0.3499 (0.3402)\tgrad(D) penalty 0.0547 (0.0459)\tRec loss 4040.5959 (4271.5653)\tnorm 1.0835 (1.0778)\n",
            "Epoch: [20][190/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6071 (0.6048)\tD(fake) 0.3519 (0.3405)\tgrad(D) penalty 0.0569 (0.0462)\tRec loss 4348.0420 (4271.3489)\tnorm 1.1066 (1.0783)\n",
            "Epoch: [21][  0/195]\tTime  0.399 ( 0.399)\tData  0.197 ( 0.197)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4185.4556 (4185.4556)\tnorm 1.1026 (1.1026)\n",
            "Epoch: [21][ 10/195]\tTime  0.150 ( 0.171)\tData  0.000 ( 0.018)\tD(real) 0.6075 (0.6074)\tD(fake) 0.3392 (0.3319)\tgrad(D) penalty 0.0482 (0.0495)\tRec loss 4324.2471 (4285.0459)\tnorm 1.0892 (1.0983)\n",
            "Epoch: [21][ 20/195]\tTime  0.151 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.6078 (0.6076)\tD(fake) 0.3545 (0.3452)\tgrad(D) penalty 0.0487 (0.0486)\tRec loss 4102.3530 (4279.5517)\tnorm 1.0874 (1.0970)\n",
            "Epoch: [21][ 30/195]\tTime  0.156 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.6079 (0.6076)\tD(fake) 0.3504 (0.3474)\tgrad(D) penalty 0.0424 (0.0472)\tRec loss 4557.3721 (4255.0799)\tnorm 1.0733 (1.0936)\n",
            "Epoch: [21][ 40/195]\tTime  0.146 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.6082 (0.6078)\tD(fake) 0.3413 (0.3460)\tgrad(D) penalty 0.0425 (0.0456)\tRec loss 4184.2227 (4255.8131)\tnorm 1.0688 (1.0890)\n",
            "Epoch: [21][ 50/195]\tTime  0.165 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.6083 (0.6079)\tD(fake) 0.3491 (0.3460)\tgrad(D) penalty 0.0411 (0.0447)\tRec loss 4444.7856 (4283.2389)\tnorm 1.0725 (1.0860)\n",
            "Epoch: [21][ 60/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.6086 (0.6080)\tD(fake) 0.3615 (0.3483)\tgrad(D) penalty 0.0322 (0.0426)\tRec loss 4172.3149 (4274.8416)\tnorm 1.0791 (1.0841)\n",
            "Epoch: [21][ 70/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6088 (0.6081)\tD(fake) 0.3464 (0.3485)\tgrad(D) penalty 0.0303 (0.0412)\tRec loss 3940.4868 (4258.3518)\tnorm 1.0569 (1.0819)\n",
            "Epoch: [21][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6090 (0.6082)\tD(fake) 0.3348 (0.3468)\tgrad(D) penalty 0.0268 (0.0394)\tRec loss 4136.2910 (4253.8632)\tnorm 1.0553 (1.0797)\n",
            "Epoch: [21][ 90/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6092 (0.6083)\tD(fake) 0.3503 (0.3468)\tgrad(D) penalty 0.0247 (0.0380)\tRec loss 4488.7520 (4248.7123)\tnorm 1.0723 (1.0778)\n",
            "Epoch: [21][100/195]\tTime  0.162 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6094 (0.6084)\tD(fake) 0.3473 (0.3475)\tgrad(D) penalty 0.0193 (0.0360)\tRec loss 4145.7129 (4248.9610)\tnorm 1.0770 (1.0765)\n",
            "Epoch: [21][110/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6097 (0.6085)\tD(fake) 0.3299 (0.3466)\tgrad(D) penalty 0.0191 (0.0347)\tRec loss 4428.0732 (4245.7684)\tnorm 1.0541 (1.0754)\n",
            "Epoch: [21][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6100 (0.6087)\tD(fake) 0.3329 (0.3450)\tgrad(D) penalty 0.0169 (0.0330)\tRec loss 4227.4238 (4248.9637)\tnorm 1.0724 (1.0753)\n",
            "Epoch: [21][130/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6102 (0.6087)\tD(fake) 0.3379 (0.3445)\tgrad(D) penalty 0.0195 (0.0321)\tRec loss 4096.8760 (4253.1457)\tnorm 1.0702 (1.0754)\n",
            "Epoch: [21][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6104 (0.6089)\tD(fake) 0.3352 (0.3439)\tgrad(D) penalty 0.0134 (0.0305)\tRec loss 4373.6782 (4252.8846)\tnorm 1.0720 (1.0749)\n",
            "Epoch: [21][150/195]\tTime  0.165 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6107 (0.6090)\tD(fake) 0.3318 (0.3432)\tgrad(D) penalty 0.0149 (0.0297)\tRec loss 4320.9092 (4253.9903)\tnorm 1.0749 (1.0740)\n",
            "Epoch: [21][160/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6109 (0.6091)\tD(fake) 0.3150 (0.3415)\tgrad(D) penalty 0.0125 (0.0284)\tRec loss 4012.2283 (4252.9437)\tnorm 1.0665 (1.0731)\n",
            "Epoch: [21][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6112 (0.6092)\tD(fake) 0.3261 (0.3405)\tgrad(D) penalty 0.0120 (0.0276)\tRec loss 4364.7051 (4254.8670)\tnorm 1.0509 (1.0720)\n",
            "Epoch: [21][180/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6116 (0.6094)\tD(fake) 0.3297 (0.3397)\tgrad(D) penalty 0.0119 (0.0266)\tRec loss 4299.3145 (4253.2394)\tnorm 1.0500 (1.0711)\n",
            "Epoch: [21][190/195]\tTime  0.153 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6118 (0.6095)\tD(fake) 0.3252 (0.3391)\tgrad(D) penalty 0.0134 (0.0260)\tRec loss 4317.5713 (4254.3369)\tnorm 1.0737 (1.0703)\n",
            "Epoch: [22][  0/195]\tTime  0.398 ( 0.398)\tData  0.202 ( 0.202)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3913.5339 (3913.5339)\tnorm 1.0611 (1.0611)\n",
            "Epoch: [22][ 10/195]\tTime  0.153 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.6123 (0.6122)\tD(fake) 0.3066 (0.3014)\tgrad(D) penalty 0.0137 (0.0138)\tRec loss 4216.6509 (4094.0754)\tnorm 1.0588 (1.0632)\n",
            "Epoch: [22][ 20/195]\tTime  0.155 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.6127 (0.6124)\tD(fake) 0.3314 (0.3168)\tgrad(D) penalty 0.0165 (0.0135)\tRec loss 4464.3696 (4171.5179)\tnorm 1.0640 (1.0580)\n",
            "Epoch: [22][ 30/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.6129 (0.6125)\tD(fake) 0.3289 (0.3205)\tgrad(D) penalty 0.0142 (0.0135)\tRec loss 4119.7764 (4188.8365)\tnorm 1.0618 (1.0597)\n",
            "Epoch: [22][ 40/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.6132 (0.6127)\tD(fake) 0.3248 (0.3218)\tgrad(D) penalty 0.0124 (0.0138)\tRec loss 4537.0615 (4196.7880)\tnorm 1.0672 (1.0616)\n",
            "Epoch: [22][ 50/195]\tTime  0.165 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.6134 (0.6128)\tD(fake) 0.3131 (0.3208)\tgrad(D) penalty 0.0170 (0.0142)\tRec loss 4312.7534 (4203.0806)\tnorm 1.0703 (1.0630)\n",
            "Epoch: [22][ 60/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.6138 (0.6130)\tD(fake) 0.3177 (0.3201)\tgrad(D) penalty 0.0157 (0.0145)\tRec loss 4501.6968 (4201.8669)\tnorm 1.0633 (1.0635)\n",
            "Epoch: [22][ 70/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6140 (0.6131)\tD(fake) 0.3234 (0.3201)\tgrad(D) penalty 0.0161 (0.0147)\tRec loss 4007.9890 (4212.5628)\tnorm 1.0593 (1.0635)\n",
            "Epoch: [22][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6143 (0.6132)\tD(fake) 0.3207 (0.3204)\tgrad(D) penalty 0.0164 (0.0148)\tRec loss 4296.1729 (4232.4875)\tnorm 1.0541 (1.0621)\n",
            "Epoch: [22][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6145 (0.6134)\tD(fake) 0.3211 (0.3206)\tgrad(D) penalty 0.0154 (0.0149)\tRec loss 4146.2969 (4236.2784)\tnorm 1.0479 (1.0607)\n",
            "Epoch: [22][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6149 (0.6135)\tD(fake) 0.3121 (0.3201)\tgrad(D) penalty 0.0157 (0.0150)\tRec loss 3922.0293 (4241.3741)\tnorm 1.0431 (1.0592)\n",
            "Epoch: [22][110/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6151 (0.6136)\tD(fake) 0.3132 (0.3197)\tgrad(D) penalty 0.0173 (0.0150)\tRec loss 4182.2305 (4248.2312)\tnorm 1.0529 (1.0576)\n",
            "Epoch: [22][120/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6153 (0.6138)\tD(fake) 0.3313 (0.3201)\tgrad(D) penalty 0.0131 (0.0150)\tRec loss 4326.5151 (4248.8785)\tnorm 1.0569 (1.0578)\n",
            "Epoch: [22][130/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6156 (0.6139)\tD(fake) 0.3225 (0.3203)\tgrad(D) penalty 0.0148 (0.0149)\tRec loss 4504.5215 (4249.9430)\tnorm 1.0422 (1.0575)\n",
            "Epoch: [22][140/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6159 (0.6141)\tD(fake) 0.3284 (0.3206)\tgrad(D) penalty 0.0151 (0.0149)\tRec loss 4075.0591 (4248.4550)\tnorm 1.0587 (1.0574)\n",
            "Epoch: [22][150/195]\tTime  0.167 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6162 (0.6142)\tD(fake) 0.3227 (0.3207)\tgrad(D) penalty 0.0131 (0.0148)\tRec loss 4327.5679 (4259.7554)\tnorm 1.0535 (1.0571)\n",
            "Epoch: [22][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6165 (0.6143)\tD(fake) 0.3172 (0.3207)\tgrad(D) penalty 0.0133 (0.0147)\tRec loss 4041.2842 (4257.8295)\tnorm 1.0610 (1.0572)\n",
            "Epoch: [22][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6167 (0.6145)\tD(fake) 0.3249 (0.3208)\tgrad(D) penalty 0.0130 (0.0146)\tRec loss 4166.4922 (4260.5196)\tnorm 1.0696 (1.0572)\n",
            "Epoch: [22][180/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6171 (0.6146)\tD(fake) 0.3205 (0.3210)\tgrad(D) penalty 0.0102 (0.0144)\tRec loss 4185.4600 (4255.4017)\tnorm 1.0701 (1.0575)\n",
            "Epoch: [22][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6172 (0.6147)\tD(fake) 0.3228 (0.3209)\tgrad(D) penalty 0.0116 (0.0142)\tRec loss 4584.6431 (4255.7679)\tnorm 1.0514 (1.0576)\n",
            "Epoch: [23][  0/195]\tTime  0.393 ( 0.393)\tData  0.205 ( 0.205)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4053.7825 (4053.7825)\tnorm 1.0707 (1.0707)\n",
            "Epoch: [23][ 10/195]\tTime  0.146 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.6178 (0.6177)\tD(fake) 0.3083 (0.3043)\tgrad(D) penalty 0.0105 (0.0111)\tRec loss 4529.3286 (4230.4430)\tnorm 1.0649 (1.0656)\n",
            "Epoch: [23][ 20/195]\tTime  0.145 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.6182 (0.6179)\tD(fake) 0.3157 (0.3128)\tgrad(D) penalty 0.0107 (0.0106)\tRec loss 4273.0723 (4280.2236)\tnorm 1.0717 (1.0658)\n",
            "Epoch: [23][ 30/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.6184 (0.6180)\tD(fake) 0.3153 (0.3140)\tgrad(D) penalty 0.0090 (0.0100)\tRec loss 4398.4805 (4269.2112)\tnorm 1.0636 (1.0686)\n",
            "Epoch: [23][ 40/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.6188 (0.6182)\tD(fake) 0.3064 (0.3134)\tgrad(D) penalty 0.0087 (0.0098)\tRec loss 4444.8779 (4261.3894)\tnorm 1.0557 (1.0676)\n",
            "Epoch: [23][ 50/195]\tTime  0.167 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.6190 (0.6183)\tD(fake) 0.2992 (0.3114)\tgrad(D) penalty 0.0102 (0.0098)\tRec loss 3980.1367 (4254.9172)\tnorm 1.0560 (1.0654)\n",
            "Epoch: [23][ 60/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.6192 (0.6185)\tD(fake) 0.3076 (0.3097)\tgrad(D) penalty 0.0084 (0.0097)\tRec loss 4094.9790 (4255.7518)\tnorm 1.0446 (1.0635)\n",
            "Epoch: [23][ 70/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6197 (0.6186)\tD(fake) 0.3145 (0.3102)\tgrad(D) penalty 0.0079 (0.0096)\tRec loss 4618.4966 (4251.8005)\tnorm 1.0471 (1.0604)\n",
            "Epoch: [23][ 80/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.6199 (0.6188)\tD(fake) 0.3033 (0.3097)\tgrad(D) penalty 0.0099 (0.0096)\tRec loss 4309.8984 (4253.8351)\tnorm 1.0316 (1.0579)\n",
            "Epoch: [23][ 90/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.6203 (0.6189)\tD(fake) 0.3098 (0.3093)\tgrad(D) penalty 0.0094 (0.0097)\tRec loss 4223.6338 (4257.7578)\tnorm 1.0578 (1.0565)\n",
            "Epoch: [23][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6206 (0.6191)\tD(fake) 0.3115 (0.3096)\tgrad(D) penalty 0.0080 (0.0096)\tRec loss 4659.8516 (4265.1752)\tnorm 1.0303 (1.0553)\n",
            "Epoch: [23][110/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6209 (0.6192)\tD(fake) 0.3110 (0.3097)\tgrad(D) penalty 0.0099 (0.0096)\tRec loss 3941.6543 (4262.0651)\tnorm 1.0588 (1.0546)\n",
            "Epoch: [23][120/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6212 (0.6194)\tD(fake) 0.3039 (0.3095)\tgrad(D) penalty 0.0088 (0.0095)\tRec loss 4156.1436 (4258.2187)\tnorm 1.0336 (1.0536)\n",
            "Epoch: [23][130/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6215 (0.6196)\tD(fake) 0.2965 (0.3087)\tgrad(D) penalty 0.0128 (0.0096)\tRec loss 4106.5522 (4252.3477)\tnorm 1.0469 (1.0527)\n",
            "Epoch: [23][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6219 (0.6197)\tD(fake) 0.2994 (0.3077)\tgrad(D) penalty 0.0116 (0.0098)\tRec loss 4141.1885 (4253.3674)\tnorm 1.0351 (1.0521)\n",
            "Epoch: [23][150/195]\tTime  0.164 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6222 (0.6199)\tD(fake) 0.3110 (0.3077)\tgrad(D) penalty 0.0111 (0.0098)\tRec loss 4223.0850 (4252.4597)\tnorm 1.0386 (1.0509)\n",
            "Epoch: [23][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6225 (0.6201)\tD(fake) 0.3079 (0.3078)\tgrad(D) penalty 0.0086 (0.0098)\tRec loss 4225.3267 (4253.1249)\tnorm 1.0422 (1.0504)\n",
            "Epoch: [23][170/195]\tTime  0.144 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6227 (0.6202)\tD(fake) 0.2990 (0.3076)\tgrad(D) penalty 0.0114 (0.0099)\tRec loss 4267.9668 (4251.5464)\tnorm 1.0582 (1.0502)\n",
            "Epoch: [23][180/195]\tTime  0.148 ( 0.149)\tData  0.000 ( 0.001)\tD(real) 0.6232 (0.6204)\tD(fake) 0.3113 (0.3075)\tgrad(D) penalty 0.0138 (0.0100)\tRec loss 4005.4009 (4248.1828)\tnorm 1.0618 (1.0506)\n",
            "Epoch: [23][190/195]\tTime  0.147 ( 0.149)\tData  0.000 ( 0.001)\tD(real) 0.6234 (0.6205)\tD(fake) 0.3183 (0.3079)\tgrad(D) penalty 0.0122 (0.0101)\tRec loss 4285.5391 (4250.4845)\tnorm 1.0557 (1.0505)\n",
            "Epoch: [24][  0/195]\tTime  0.404 ( 0.404)\tData  0.209 ( 0.209)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4068.2820 (4068.2820)\tnorm 1.0606 (1.0606)\n",
            "Epoch: [24][ 10/195]\tTime  0.146 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.6236 (0.6237)\tD(fake) 0.2908 (0.2852)\tgrad(D) penalty 0.0113 (0.0123)\tRec loss 4176.3706 (4218.0057)\tnorm 1.0573 (1.0547)\n",
            "Epoch: [24][ 20/195]\tTime  0.145 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.6243 (0.6240)\tD(fake) 0.3148 (0.2996)\tgrad(D) penalty 0.0116 (0.0123)\tRec loss 4070.4517 (4202.5896)\tnorm 1.0459 (1.0514)\n",
            "Epoch: [24][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.6245 (0.6241)\tD(fake) 0.3055 (0.3019)\tgrad(D) penalty 0.0111 (0.0120)\tRec loss 4020.3599 (4205.8070)\tnorm 1.0403 (1.0481)\n",
            "Epoch: [24][ 40/195]\tTime  0.149 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.6249 (0.6243)\tD(fake) 0.2960 (0.3000)\tgrad(D) penalty 0.0143 (0.0124)\tRec loss 4151.6450 (4210.7253)\tnorm 1.0676 (1.0484)\n",
            "Epoch: [24][ 50/195]\tTime  0.166 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.6252 (0.6245)\tD(fake) 0.3038 (0.3004)\tgrad(D) penalty 0.0127 (0.0126)\tRec loss 4371.1392 (4208.3885)\tnorm 1.0309 (1.0462)\n",
            "Epoch: [24][ 60/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.6256 (0.6247)\tD(fake) 0.3048 (0.3010)\tgrad(D) penalty 0.0131 (0.0127)\tRec loss 4252.7886 (4210.3075)\tnorm 1.0489 (1.0442)\n",
            "Epoch: [24][ 70/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6258 (0.6248)\tD(fake) 0.3095 (0.3018)\tgrad(D) penalty 0.0131 (0.0128)\tRec loss 4432.1729 (4228.8834)\tnorm 1.0336 (1.0420)\n",
            "Epoch: [24][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6262 (0.6250)\tD(fake) 0.3051 (0.3022)\tgrad(D) penalty 0.0149 (0.0130)\tRec loss 4479.3223 (4228.4133)\tnorm 1.0446 (1.0420)\n",
            "Epoch: [24][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6264 (0.6251)\tD(fake) 0.3140 (0.3029)\tgrad(D) penalty 0.0161 (0.0132)\tRec loss 4153.4849 (4232.1316)\tnorm 1.0435 (1.0416)\n",
            "Epoch: [24][100/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6267 (0.6253)\tD(fake) 0.3115 (0.3042)\tgrad(D) penalty 0.0147 (0.0133)\tRec loss 4438.6113 (4234.0455)\tnorm 1.0421 (1.0412)\n",
            "Epoch: [24][110/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6270 (0.6254)\tD(fake) 0.3027 (0.3044)\tgrad(D) penalty 0.0146 (0.0134)\tRec loss 4163.7505 (4236.9802)\tnorm 1.0583 (1.0407)\n",
            "Epoch: [24][120/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6274 (0.6256)\tD(fake) 0.3060 (0.3044)\tgrad(D) penalty 0.0163 (0.0136)\tRec loss 4129.3594 (4241.3425)\tnorm 1.0399 (1.0415)\n",
            "Epoch: [24][130/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6276 (0.6257)\tD(fake) 0.3213 (0.3052)\tgrad(D) penalty 0.0138 (0.0136)\tRec loss 4359.8623 (4240.5443)\tnorm 1.0356 (1.0424)\n",
            "Epoch: [24][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6279 (0.6259)\tD(fake) 0.3032 (0.3058)\tgrad(D) penalty 0.0138 (0.0136)\tRec loss 4305.3379 (4242.8864)\tnorm 1.0453 (1.0426)\n",
            "Epoch: [24][150/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6281 (0.6260)\tD(fake) 0.2997 (0.3055)\tgrad(D) penalty 0.0126 (0.0136)\tRec loss 4314.4609 (4242.7208)\tnorm 1.0437 (1.0433)\n",
            "Epoch: [24][160/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6286 (0.6262)\tD(fake) 0.3186 (0.3061)\tgrad(D) penalty 0.0117 (0.0136)\tRec loss 4136.0054 (4244.0020)\tnorm 1.0318 (1.0433)\n",
            "Epoch: [24][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6288 (0.6263)\tD(fake) 0.3089 (0.3063)\tgrad(D) penalty 0.0120 (0.0135)\tRec loss 4393.3325 (4243.2119)\tnorm 1.0393 (1.0433)\n",
            "Epoch: [24][180/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6291 (0.6265)\tD(fake) 0.3029 (0.3062)\tgrad(D) penalty 0.0135 (0.0135)\tRec loss 3988.1143 (4243.0753)\tnorm 1.0454 (1.0433)\n",
            "Epoch: [24][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6294 (0.6266)\tD(fake) 0.3090 (0.3063)\tgrad(D) penalty 0.0110 (0.0134)\tRec loss 4463.4072 (4239.8501)\tnorm 1.0295 (1.0433)\n",
            "Epoch: [25][  0/195]\tTime  0.394 ( 0.394)\tData  0.199 ( 0.199)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4123.3730 (4123.3730)\tnorm 1.0432 (1.0432)\n",
            "Epoch: [25][ 10/195]\tTime  0.148 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.6299 (0.6299)\tD(fake) 0.3076 (0.3018)\tgrad(D) penalty 0.0108 (0.0116)\tRec loss 4232.9863 (4192.9235)\tnorm 1.0559 (1.0472)\n",
            "Epoch: [25][ 20/195]\tTime  0.148 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.6303 (0.6300)\tD(fake) 0.3121 (0.3090)\tgrad(D) penalty 0.0095 (0.0102)\tRec loss 4121.1787 (4222.8611)\tnorm 1.0484 (1.0473)\n",
            "Epoch: [25][ 30/195]\tTime  0.149 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.6305 (0.6302)\tD(fake) 0.2987 (0.3070)\tgrad(D) penalty 0.0093 (0.0099)\tRec loss 4337.8789 (4215.1870)\tnorm 1.0317 (1.0443)\n",
            "Epoch: [25][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.6309 (0.6303)\tD(fake) 0.2994 (0.3044)\tgrad(D) penalty 0.0073 (0.0094)\tRec loss 4417.1221 (4219.3992)\tnorm 1.0399 (1.0425)\n",
            "Epoch: [25][ 50/195]\tTime  0.169 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.6311 (0.6304)\tD(fake) 0.3042 (0.3044)\tgrad(D) penalty 0.0078 (0.0091)\tRec loss 4581.4209 (4226.9676)\tnorm 1.0389 (1.0402)\n",
            "Epoch: [25][ 60/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.6315 (0.6306)\tD(fake) 0.2952 (0.3031)\tgrad(D) penalty 0.0071 (0.0090)\tRec loss 4017.6531 (4240.8215)\tnorm 1.0375 (1.0397)\n",
            "Epoch: [25][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6317 (0.6307)\tD(fake) 0.2935 (0.3017)\tgrad(D) penalty 0.0078 (0.0089)\tRec loss 4296.2354 (4239.8105)\tnorm 1.0460 (1.0410)\n",
            "Epoch: [25][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6321 (0.6309)\tD(fake) 0.2930 (0.3002)\tgrad(D) penalty 0.0068 (0.0088)\tRec loss 4504.6543 (4244.1285)\tnorm 1.0493 (1.0420)\n",
            "Epoch: [25][ 90/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6324 (0.6310)\tD(fake) 0.2973 (0.2999)\tgrad(D) penalty 0.0061 (0.0087)\tRec loss 4275.0884 (4233.3447)\tnorm 1.0411 (1.0429)\n",
            "Epoch: [25][100/195]\tTime  0.161 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6326 (0.6312)\tD(fake) 0.2940 (0.2989)\tgrad(D) penalty 0.0069 (0.0086)\tRec loss 4236.4902 (4232.5234)\tnorm 1.0459 (1.0429)\n",
            "Epoch: [25][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6330 (0.6314)\tD(fake) 0.2930 (0.2985)\tgrad(D) penalty 0.0053 (0.0084)\tRec loss 4314.5259 (4229.2449)\tnorm 1.0472 (1.0428)\n",
            "Epoch: [25][120/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6333 (0.6315)\tD(fake) 0.2806 (0.2972)\tgrad(D) penalty 0.0101 (0.0083)\tRec loss 4520.1016 (4233.5835)\tnorm 1.0315 (1.0425)\n",
            "Epoch: [25][130/195]\tTime  0.156 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6336 (0.6317)\tD(fake) 0.2886 (0.2965)\tgrad(D) penalty 0.0058 (0.0082)\tRec loss 4378.6343 (4239.0197)\tnorm 1.0305 (1.0420)\n",
            "Epoch: [25][140/195]\tTime  0.156 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6340 (0.6319)\tD(fake) 0.2961 (0.2961)\tgrad(D) penalty 0.0059 (0.0081)\tRec loss 4590.2256 (4247.3275)\tnorm 1.0403 (1.0416)\n",
            "Epoch: [25][150/195]\tTime  0.168 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6344 (0.6320)\tD(fake) 0.3003 (0.2963)\tgrad(D) penalty 0.0086 (0.0081)\tRec loss 4457.6172 (4244.7384)\tnorm 1.0532 (1.0426)\n",
            "Epoch: [25][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6347 (0.6322)\tD(fake) 0.2830 (0.2955)\tgrad(D) penalty 0.0078 (0.0079)\tRec loss 4266.4336 (4244.9633)\tnorm 1.0516 (1.0431)\n",
            "Epoch: [25][170/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6350 (0.6323)\tD(fake) 0.2813 (0.2947)\tgrad(D) penalty 0.0073 (0.0079)\tRec loss 4191.7915 (4240.3521)\tnorm 1.0565 (1.0441)\n",
            "Epoch: [25][180/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6354 (0.6325)\tD(fake) 0.2896 (0.2944)\tgrad(D) penalty 0.0060 (0.0078)\tRec loss 4392.5000 (4240.7693)\tnorm 1.0394 (1.0445)\n",
            "Epoch: [25][190/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6356 (0.6326)\tD(fake) 0.2832 (0.2940)\tgrad(D) penalty 0.0056 (0.0077)\tRec loss 4348.7422 (4242.6208)\tnorm 1.0470 (1.0448)\n",
            "Epoch: [26][  0/195]\tTime  0.396 ( 0.396)\tData  0.197 ( 0.197)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4258.0859 (4258.0859)\tnorm 1.0464 (1.0464)\n",
            "Epoch: [26][ 10/195]\tTime  0.148 ( 0.173)\tData  0.000 ( 0.018)\tD(real) 0.6362 (0.6361)\tD(fake) 0.2724 (0.2667)\tgrad(D) penalty 0.0075 (0.0079)\tRec loss 4364.0869 (4189.4752)\tnorm 1.0334 (1.0490)\n",
            "Epoch: [26][ 20/195]\tTime  0.148 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.6366 (0.6364)\tD(fake) 0.2862 (0.2791)\tgrad(D) penalty 0.0044 (0.0064)\tRec loss 4411.7812 (4224.8931)\tnorm 1.0266 (1.0443)\n",
            "Epoch: [26][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.6369 (0.6365)\tD(fake) 0.2709 (0.2774)\tgrad(D) penalty 0.0066 (0.0062)\tRec loss 4001.5493 (4221.4665)\tnorm 1.0438 (1.0445)\n",
            "Epoch: [26][ 40/195]\tTime  0.149 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.6370 (0.6367)\tD(fake) 0.2850 (0.2776)\tgrad(D) penalty 0.0064 (0.0067)\tRec loss 4136.2549 (4223.0276)\tnorm 1.0270 (1.0424)\n",
            "Epoch: [26][ 50/195]\tTime  0.164 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.6375 (0.6368)\tD(fake) 0.2803 (0.2783)\tgrad(D) penalty 0.0050 (0.0065)\tRec loss 4024.5669 (4219.7002)\tnorm 1.0453 (1.0411)\n",
            "Epoch: [26][ 60/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.6380 (0.6370)\tD(fake) 0.2700 (0.2768)\tgrad(D) penalty 0.0067 (0.0066)\tRec loss 4390.6450 (4224.6577)\tnorm 1.0457 (1.0414)\n",
            "Epoch: [26][ 70/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6383 (0.6372)\tD(fake) 0.2714 (0.2762)\tgrad(D) penalty 0.0059 (0.0065)\tRec loss 4220.9561 (4229.9813)\tnorm 1.0310 (1.0406)\n",
            "Epoch: [26][ 80/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6386 (0.6374)\tD(fake) 0.2760 (0.2760)\tgrad(D) penalty 0.0065 (0.0065)\tRec loss 4429.6353 (4221.3289)\tnorm 1.0238 (1.0405)\n",
            "Epoch: [26][ 90/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6389 (0.6375)\tD(fake) 0.2643 (0.2753)\tgrad(D) penalty 0.0081 (0.0066)\tRec loss 4261.1631 (4223.7363)\tnorm 1.0249 (1.0403)\n",
            "Epoch: [26][100/195]\tTime  0.169 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6394 (0.6377)\tD(fake) 0.2774 (0.2752)\tgrad(D) penalty 0.0058 (0.0066)\tRec loss 4198.4834 (4229.4381)\tnorm 1.0457 (1.0394)\n",
            "Epoch: [26][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6397 (0.6378)\tD(fake) 0.2739 (0.2753)\tgrad(D) penalty 0.0068 (0.0066)\tRec loss 3931.7773 (4220.1600)\tnorm 1.0322 (1.0390)\n",
            "Epoch: [26][120/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6401 (0.6380)\tD(fake) 0.2740 (0.2750)\tgrad(D) penalty 0.0069 (0.0066)\tRec loss 4159.2710 (4223.6721)\tnorm 1.0143 (1.0382)\n",
            "Epoch: [26][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6403 (0.6382)\tD(fake) 0.2698 (0.2747)\tgrad(D) penalty 0.0075 (0.0066)\tRec loss 4091.3423 (4229.0688)\tnorm 1.0333 (1.0373)\n",
            "Epoch: [26][140/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6407 (0.6384)\tD(fake) 0.2653 (0.2738)\tgrad(D) penalty 0.0049 (0.0066)\tRec loss 4005.9592 (4218.7442)\tnorm 1.0367 (1.0365)\n",
            "Epoch: [26][150/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6411 (0.6385)\tD(fake) 0.2670 (0.2734)\tgrad(D) penalty 0.0061 (0.0066)\tRec loss 4092.6006 (4212.9115)\tnorm 1.0305 (1.0360)\n",
            "Epoch: [26][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6415 (0.6387)\tD(fake) 0.2665 (0.2730)\tgrad(D) penalty 0.0052 (0.0066)\tRec loss 4301.7305 (4217.1484)\tnorm 1.0043 (1.0354)\n",
            "Epoch: [26][170/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6417 (0.6389)\tD(fake) 0.2678 (0.2728)\tgrad(D) penalty 0.0059 (0.0065)\tRec loss 4394.3164 (4219.3973)\tnorm 1.0315 (1.0350)\n",
            "Epoch: [26][180/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6422 (0.6391)\tD(fake) 0.2745 (0.2723)\tgrad(D) penalty 0.0080 (0.0065)\tRec loss 4077.1831 (4219.1991)\tnorm 1.0238 (1.0344)\n",
            "Epoch: [26][190/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6425 (0.6392)\tD(fake) 0.2722 (0.2724)\tgrad(D) penalty 0.0055 (0.0065)\tRec loss 4245.6709 (4226.8150)\tnorm 1.0428 (1.0340)\n",
            "Epoch: [27][  0/195]\tTime  0.414 ( 0.414)\tData  0.218 ( 0.218)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4047.5596 (4047.5596)\tnorm 1.0228 (1.0228)\n",
            "Epoch: [27][ 10/195]\tTime  0.151 ( 0.175)\tData  0.000 ( 0.020)\tD(real) 0.6431 (0.6430)\tD(fake) 0.2581 (0.2511)\tgrad(D) penalty 0.0078 (0.0099)\tRec loss 4498.6777 (4231.5669)\tnorm 1.0322 (1.0236)\n",
            "Epoch: [27][ 20/195]\tTime  0.147 ( 0.163)\tData  0.000 ( 0.011)\tD(real) 0.6435 (0.6432)\tD(fake) 0.2745 (0.2673)\tgrad(D) penalty 0.0046 (0.0067)\tRec loss 4043.1099 (4223.7236)\tnorm 1.0330 (1.0246)\n",
            "Epoch: [27][ 30/195]\tTime  0.148 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.6437 (0.6433)\tD(fake) 0.2562 (0.2651)\tgrad(D) penalty 0.0060 (0.0063)\tRec loss 4180.8721 (4219.4785)\tnorm 1.0451 (1.0286)\n",
            "Epoch: [27][ 40/195]\tTime  0.149 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.6442 (0.6436)\tD(fake) 0.2628 (0.2626)\tgrad(D) penalty 0.0072 (0.0066)\tRec loss 4011.1133 (4213.4496)\tnorm 1.0285 (1.0286)\n",
            "Epoch: [27][ 50/195]\tTime  0.161 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.6445 (0.6437)\tD(fake) 0.2752 (0.2643)\tgrad(D) penalty 0.0056 (0.0065)\tRec loss 4268.3438 (4206.7705)\tnorm 1.0217 (1.0297)\n",
            "Epoch: [27][ 60/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.6448 (0.6439)\tD(fake) 0.2597 (0.2642)\tgrad(D) penalty 0.0070 (0.0064)\tRec loss 4198.8701 (4194.7027)\tnorm 1.0109 (1.0272)\n",
            "Epoch: [27][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6450 (0.6440)\tD(fake) 0.2566 (0.2632)\tgrad(D) penalty 0.0068 (0.0064)\tRec loss 4200.4419 (4205.4584)\tnorm 1.0162 (1.0249)\n",
            "Epoch: [27][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6455 (0.6442)\tD(fake) 0.2606 (0.2629)\tgrad(D) penalty 0.0069 (0.0064)\tRec loss 4308.6514 (4205.2458)\tnorm 1.0213 (1.0249)\n",
            "Epoch: [27][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6458 (0.6443)\tD(fake) 0.2612 (0.2627)\tgrad(D) penalty 0.0067 (0.0064)\tRec loss 4165.3447 (4205.4433)\tnorm 1.0247 (1.0249)\n",
            "Epoch: [27][100/195]\tTime  0.168 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6463 (0.6446)\tD(fake) 0.2623 (0.2628)\tgrad(D) penalty 0.0060 (0.0064)\tRec loss 3945.5508 (4211.0060)\tnorm 1.0256 (1.0249)\n",
            "Epoch: [27][110/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6465 (0.6447)\tD(fake) 0.2518 (0.2621)\tgrad(D) penalty 0.0068 (0.0064)\tRec loss 4054.4419 (4214.0731)\tnorm 1.0351 (1.0247)\n",
            "Epoch: [27][120/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6468 (0.6449)\tD(fake) 0.2614 (0.2618)\tgrad(D) penalty 0.0072 (0.0065)\tRec loss 4211.0303 (4208.9536)\tnorm 1.0112 (1.0239)\n",
            "Epoch: [27][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6472 (0.6450)\tD(fake) 0.2547 (0.2615)\tgrad(D) penalty 0.0061 (0.0065)\tRec loss 4392.4673 (4213.8067)\tnorm 1.0133 (1.0236)\n",
            "Epoch: [27][140/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6477 (0.6453)\tD(fake) 0.2546 (0.2608)\tgrad(D) penalty 0.0061 (0.0065)\tRec loss 3853.9958 (4212.2789)\tnorm 1.0065 (1.0230)\n",
            "Epoch: [27][150/195]\tTime  0.167 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6478 (0.6454)\tD(fake) 0.2566 (0.2605)\tgrad(D) penalty 0.0059 (0.0064)\tRec loss 4282.1675 (4212.6971)\tnorm 1.0205 (1.0225)\n",
            "Epoch: [27][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6484 (0.6456)\tD(fake) 0.2594 (0.2603)\tgrad(D) penalty 0.0077 (0.0065)\tRec loss 4208.2300 (4220.5458)\tnorm 1.0070 (1.0217)\n",
            "Epoch: [27][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6486 (0.6457)\tD(fake) 0.2653 (0.2605)\tgrad(D) penalty 0.0056 (0.0064)\tRec loss 4199.0703 (4222.7803)\tnorm 1.0182 (1.0217)\n",
            "Epoch: [27][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6490 (0.6459)\tD(fake) 0.2497 (0.2600)\tgrad(D) penalty 0.0070 (0.0064)\tRec loss 4490.1064 (4227.0208)\tnorm 1.0225 (1.0216)\n",
            "Epoch: [27][190/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6493 (0.6461)\tD(fake) 0.2534 (0.2597)\tgrad(D) penalty 0.0079 (0.0065)\tRec loss 4056.1802 (4227.7902)\tnorm 1.0557 (1.0225)\n",
            "Epoch: [28][  0/195]\tTime  0.402 ( 0.402)\tData  0.208 ( 0.208)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4105.3945 (4105.3945)\tnorm 1.0423 (1.0423)\n",
            "Epoch: [28][ 10/195]\tTime  0.150 ( 0.174)\tData  0.000 ( 0.019)\tD(real) 0.6499 (0.6498)\tD(fake) 0.2588 (0.2520)\tgrad(D) penalty 0.0062 (0.0066)\tRec loss 4218.2510 (4212.5012)\tnorm 1.0357 (1.0381)\n",
            "Epoch: [28][ 20/195]\tTime  0.149 ( 0.162)\tData  0.000 ( 0.010)\tD(real) 0.6503 (0.6500)\tD(fake) 0.2611 (0.2582)\tgrad(D) penalty 0.0067 (0.0062)\tRec loss 4101.8628 (4185.2068)\tnorm 1.0473 (1.0368)\n",
            "Epoch: [28][ 30/195]\tTime  0.151 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.6505 (0.6501)\tD(fake) 0.2538 (0.2571)\tgrad(D) penalty 0.0064 (0.0065)\tRec loss 4559.7705 (4205.2119)\tnorm 1.0125 (1.0336)\n",
            "Epoch: [28][ 40/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.6509 (0.6503)\tD(fake) 0.2656 (0.2580)\tgrad(D) penalty 0.0057 (0.0065)\tRec loss 4126.5244 (4223.9610)\tnorm 1.0171 (1.0303)\n",
            "Epoch: [28][ 50/195]\tTime  0.165 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.6513 (0.6505)\tD(fake) 0.2600 (0.2587)\tgrad(D) penalty 0.0060 (0.0064)\tRec loss 4297.1445 (4213.7540)\tnorm 1.0376 (1.0300)\n",
            "Epoch: [28][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.6516 (0.6507)\tD(fake) 0.2513 (0.2575)\tgrad(D) penalty 0.0082 (0.0064)\tRec loss 4271.4512 (4217.4877)\tnorm 1.0211 (1.0291)\n",
            "Epoch: [28][ 70/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6520 (0.6508)\tD(fake) 0.2650 (0.2581)\tgrad(D) penalty 0.0052 (0.0063)\tRec loss 4064.2654 (4225.6846)\tnorm 1.0381 (1.0280)\n",
            "Epoch: [28][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6523 (0.6510)\tD(fake) 0.2547 (0.2586)\tgrad(D) penalty 0.0076 (0.0063)\tRec loss 4355.5728 (4220.8655)\tnorm 1.0141 (1.0271)\n",
            "Epoch: [28][ 90/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6526 (0.6512)\tD(fake) 0.2620 (0.2587)\tgrad(D) penalty 0.0063 (0.0064)\tRec loss 4628.2393 (4235.1855)\tnorm 1.0224 (1.0270)\n",
            "Epoch: [28][100/195]\tTime  0.162 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6529 (0.6514)\tD(fake) 0.2609 (0.2592)\tgrad(D) penalty 0.0061 (0.0063)\tRec loss 4276.0918 (4228.0233)\tnorm 1.0375 (1.0267)\n",
            "Epoch: [28][110/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6530 (0.6515)\tD(fake) 0.2557 (0.2591)\tgrad(D) penalty 0.0052 (0.0063)\tRec loss 4582.5015 (4229.7069)\tnorm 1.0010 (1.0255)\n",
            "Epoch: [28][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6535 (0.6517)\tD(fake) 0.2521 (0.2584)\tgrad(D) penalty 0.0072 (0.0063)\tRec loss 4341.0156 (4230.3883)\tnorm 1.0044 (1.0243)\n",
            "Epoch: [28][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6539 (0.6518)\tD(fake) 0.2577 (0.2583)\tgrad(D) penalty 0.0050 (0.0063)\tRec loss 4234.4893 (4230.3243)\tnorm 1.0142 (1.0236)\n",
            "Epoch: [28][140/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6541 (0.6520)\tD(fake) 0.2552 (0.2584)\tgrad(D) penalty 0.0061 (0.0062)\tRec loss 4444.8081 (4228.9389)\tnorm 1.0188 (1.0233)\n",
            "Epoch: [28][150/195]\tTime  0.165 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6546 (0.6521)\tD(fake) 0.2560 (0.2582)\tgrad(D) penalty 0.0049 (0.0062)\tRec loss 4085.9546 (4225.7886)\tnorm 1.0235 (1.0233)\n",
            "Epoch: [28][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6549 (0.6523)\tD(fake) 0.2501 (0.2577)\tgrad(D) penalty 0.0067 (0.0062)\tRec loss 4086.3921 (4224.7736)\tnorm 1.0351 (1.0233)\n",
            "Epoch: [28][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6552 (0.6525)\tD(fake) 0.2609 (0.2577)\tgrad(D) penalty 0.0054 (0.0062)\tRec loss 3954.4922 (4223.5113)\tnorm 1.0269 (1.0233)\n",
            "Epoch: [28][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6555 (0.6527)\tD(fake) 0.2657 (0.2582)\tgrad(D) penalty 0.0058 (0.0062)\tRec loss 4344.0801 (4226.2411)\tnorm 1.0298 (1.0236)\n",
            "Epoch: [28][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6558 (0.6528)\tD(fake) 0.2535 (0.2581)\tgrad(D) penalty 0.0059 (0.0062)\tRec loss 4189.5571 (4224.9856)\tnorm 1.0284 (1.0240)\n",
            "Epoch: [29][  0/195]\tTime  0.367 ( 0.367)\tData  0.191 ( 0.191)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4051.1504 (4051.1504)\tnorm 1.0214 (1.0214)\n",
            "Epoch: [29][ 10/195]\tTime  0.149 ( 0.167)\tData  0.000 ( 0.018)\tD(real) 0.6564 (0.6563)\tD(fake) 0.2690 (0.2595)\tgrad(D) penalty 0.0058 (0.0069)\tRec loss 4051.3706 (4083.9086)\tnorm 1.0221 (1.0287)\n",
            "Epoch: [29][ 20/195]\tTime  0.145 ( 0.157)\tData  0.000 ( 0.009)\tD(real) 0.6567 (0.6565)\tD(fake) 0.2643 (0.2662)\tgrad(D) penalty 0.0053 (0.0061)\tRec loss 4389.4033 (4173.2960)\tnorm 1.0147 (1.0240)\n",
            "Epoch: [29][ 30/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.006)\tD(real) 0.6570 (0.6566)\tD(fake) 0.2559 (0.2635)\tgrad(D) penalty 0.0080 (0.0065)\tRec loss 4469.2129 (4179.4038)\tnorm 1.0185 (1.0214)\n",
            "Epoch: [29][ 40/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.005)\tD(real) 0.6574 (0.6568)\tD(fake) 0.2596 (0.2622)\tgrad(D) penalty 0.0083 (0.0066)\tRec loss 4263.8682 (4194.3676)\tnorm 0.9921 (1.0167)\n",
            "Epoch: [29][ 50/195]\tTime  0.162 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.6576 (0.6569)\tD(fake) 0.2656 (0.2627)\tgrad(D) penalty 0.0061 (0.0066)\tRec loss 3951.7766 (4201.6899)\tnorm 1.0175 (1.0141)\n",
            "Epoch: [29][ 60/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.6581 (0.6571)\tD(fake) 0.2584 (0.2622)\tgrad(D) penalty 0.0083 (0.0068)\tRec loss 4374.0557 (4201.7360)\tnorm 1.0093 (1.0125)\n",
            "Epoch: [29][ 70/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.6582 (0.6572)\tD(fake) 0.2645 (0.2623)\tgrad(D) penalty 0.0064 (0.0068)\tRec loss 4194.2402 (4198.9381)\tnorm 1.0077 (1.0121)\n",
            "Epoch: [29][ 80/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.6586 (0.6574)\tD(fake) 0.2579 (0.2627)\tgrad(D) penalty 0.0075 (0.0067)\tRec loss 4116.8262 (4200.5903)\tnorm 1.0091 (1.0121)\n",
            "Epoch: [29][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6589 (0.6576)\tD(fake) 0.2550 (0.2619)\tgrad(D) penalty 0.0081 (0.0069)\tRec loss 4596.8994 (4215.1316)\tnorm 1.0000 (1.0125)\n",
            "Epoch: [29][100/195]\tTime  0.166 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6592 (0.6577)\tD(fake) 0.2652 (0.2624)\tgrad(D) penalty 0.0060 (0.0069)\tRec loss 3972.7041 (4210.5197)\tnorm 1.0270 (1.0125)\n",
            "Epoch: [29][110/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6594 (0.6579)\tD(fake) 0.2582 (0.2623)\tgrad(D) penalty 0.0071 (0.0069)\tRec loss 4168.5210 (4207.7640)\tnorm 0.9911 (1.0113)\n",
            "Epoch: [29][120/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6598 (0.6580)\tD(fake) 0.2559 (0.2620)\tgrad(D) penalty 0.0079 (0.0070)\tRec loss 4641.0850 (4213.1096)\tnorm 1.0068 (1.0111)\n",
            "Epoch: [29][130/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6601 (0.6582)\tD(fake) 0.2702 (0.2623)\tgrad(D) penalty 0.0073 (0.0071)\tRec loss 4615.4668 (4214.6321)\tnorm 1.0261 (1.0113)\n",
            "Epoch: [29][140/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6605 (0.6583)\tD(fake) 0.2677 (0.2629)\tgrad(D) penalty 0.0090 (0.0071)\tRec loss 4374.0679 (4215.7473)\tnorm 1.0075 (1.0112)\n",
            "Epoch: [29][150/195]\tTime  0.166 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6606 (0.6585)\tD(fake) 0.2724 (0.2634)\tgrad(D) penalty 0.0086 (0.0072)\tRec loss 4582.8115 (4217.5005)\tnorm 1.0171 (1.0118)\n",
            "Epoch: [29][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6610 (0.6586)\tD(fake) 0.2683 (0.2640)\tgrad(D) penalty 0.0077 (0.0072)\tRec loss 3938.5312 (4213.3646)\tnorm 1.0222 (1.0118)\n",
            "Epoch: [29][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6612 (0.6588)\tD(fake) 0.2633 (0.2640)\tgrad(D) penalty 0.0094 (0.0073)\tRec loss 3989.4927 (4215.1447)\tnorm 0.9989 (1.0118)\n",
            "Epoch: [29][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6616 (0.6589)\tD(fake) 0.2748 (0.2644)\tgrad(D) penalty 0.0089 (0.0075)\tRec loss 4228.7349 (4217.8487)\tnorm 1.0267 (1.0123)\n",
            "Epoch: [29][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6617 (0.6591)\tD(fake) 0.2710 (0.2647)\tgrad(D) penalty 0.0090 (0.0076)\tRec loss 4159.5464 (4218.1632)\tnorm 1.0247 (1.0124)\n",
            "Epoch: [30][  0/195]\tTime  0.389 ( 0.389)\tData  0.197 ( 0.197)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3975.9189 (3975.9189)\tnorm 1.0177 (1.0177)\n",
            "Epoch: [30][ 10/195]\tTime  0.149 ( 0.170)\tData  0.000 ( 0.018)\tD(real) 0.6621 (0.6621)\tD(fake) 0.2662 (0.2572)\tgrad(D) penalty 0.0102 (0.0115)\tRec loss 4098.9370 (4122.7142)\tnorm 1.0082 (1.0144)\n",
            "Epoch: [30][ 20/195]\tTime  0.146 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.6626 (0.6623)\tD(fake) 0.2897 (0.2765)\tgrad(D) penalty 0.0111 (0.0116)\tRec loss 4106.4854 (4171.8602)\tnorm 1.0083 (1.0127)\n",
            "Epoch: [30][ 30/195]\tTime  0.146 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.6628 (0.6624)\tD(fake) 0.2718 (0.2764)\tgrad(D) penalty 0.0162 (0.0127)\tRec loss 4052.2146 (4201.8760)\tnorm 1.0097 (1.0121)\n",
            "Epoch: [30][ 40/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.6630 (0.6626)\tD(fake) 0.2753 (0.2753)\tgrad(D) penalty 0.0193 (0.0142)\tRec loss 4115.5771 (4208.5638)\tnorm 1.0114 (1.0126)\n",
            "Epoch: [30][ 50/195]\tTime  0.161 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.6633 (0.6627)\tD(fake) 0.2965 (0.2777)\tgrad(D) penalty 0.0207 (0.0153)\tRec loss 4438.1807 (4221.8160)\tnorm 1.0095 (1.0128)\n",
            "Epoch: [30][ 60/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6635 (0.6628)\tD(fake) 0.2936 (0.2812)\tgrad(D) penalty 0.0282 (0.0171)\tRec loss 4082.7764 (4212.5586)\tnorm 1.0139 (1.0130)\n",
            "Epoch: [30][ 70/195]\tTime  0.156 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6636 (0.6629)\tD(fake) 0.2898 (0.2824)\tgrad(D) penalty 0.0271 (0.0184)\tRec loss 4411.7798 (4209.4271)\tnorm 0.9873 (1.0112)\n",
            "Epoch: [30][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6639 (0.6631)\tD(fake) 0.2979 (0.2840)\tgrad(D) penalty 0.0333 (0.0208)\tRec loss 3998.8843 (4209.5247)\tnorm 1.0050 (1.0100)\n",
            "Epoch: [30][ 90/195]\tTime  0.154 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6642 (0.6632)\tD(fake) 0.3107 (0.2861)\tgrad(D) penalty 0.0375 (0.0222)\tRec loss 4260.0435 (4218.6264)\tnorm 1.0091 (1.0098)\n",
            "Epoch: [30][100/195]\tTime  0.169 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6643 (0.6633)\tD(fake) 0.3004 (0.2884)\tgrad(D) penalty 0.0387 (0.0241)\tRec loss 4313.0400 (4225.5537)\tnorm 1.0062 (1.0091)\n",
            "Epoch: [30][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6645 (0.6634)\tD(fake) 0.3014 (0.2892)\tgrad(D) penalty 0.0433 (0.0253)\tRec loss 4183.1133 (4220.5414)\tnorm 1.0086 (1.0088)\n",
            "Epoch: [30][120/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6646 (0.6635)\tD(fake) 0.3087 (0.2909)\tgrad(D) penalty 0.0395 (0.0270)\tRec loss 4244.6650 (4221.2739)\tnorm 1.0004 (1.0083)\n",
            "Epoch: [30][130/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6648 (0.6636)\tD(fake) 0.3176 (0.2923)\tgrad(D) penalty 0.0409 (0.0279)\tRec loss 4307.9634 (4227.4129)\tnorm 1.0071 (1.0089)\n",
            "Epoch: [30][140/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6649 (0.6637)\tD(fake) 0.3141 (0.2945)\tgrad(D) penalty 0.0448 (0.0292)\tRec loss 4277.8965 (4224.4405)\tnorm 1.0087 (1.0094)\n",
            "Epoch: [30][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6650 (0.6638)\tD(fake) 0.3037 (0.2950)\tgrad(D) penalty 0.0412 (0.0299)\tRec loss 3811.6084 (4221.0761)\tnorm 1.0196 (1.0094)\n",
            "Epoch: [30][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6650 (0.6639)\tD(fake) 0.3215 (0.2962)\tgrad(D) penalty 0.0444 (0.0310)\tRec loss 4287.2085 (4217.6431)\tnorm 1.0145 (1.0098)\n",
            "Epoch: [30][170/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6653 (0.6639)\tD(fake) 0.3244 (0.2974)\tgrad(D) penalty 0.0434 (0.0315)\tRec loss 4193.6494 (4212.6193)\tnorm 1.0148 (1.0100)\n",
            "Epoch: [30][180/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6655 (0.6640)\tD(fake) 0.3140 (0.2990)\tgrad(D) penalty 0.0398 (0.0323)\tRec loss 4236.6929 (4214.1389)\tnorm 0.9990 (1.0096)\n",
            "Epoch: [30][190/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6654 (0.6641)\tD(fake) 0.3102 (0.2994)\tgrad(D) penalty 0.0428 (0.0327)\tRec loss 4305.9678 (4210.0845)\tnorm 1.0028 (1.0095)\n",
            "Epoch: [31][  0/195]\tTime  0.390 ( 0.390)\tData  0.204 ( 0.204)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4132.0347 (4132.0347)\tnorm 1.0106 (1.0106)\n",
            "Epoch: [31][ 10/195]\tTime  0.149 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.6658 (0.6657)\tD(fake) 0.3301 (0.3199)\tgrad(D) penalty 0.0381 (0.0393)\tRec loss 4295.4648 (4234.0116)\tnorm 0.9927 (1.0033)\n",
            "Epoch: [31][ 20/195]\tTime  0.150 ( 0.162)\tData  0.000 ( 0.010)\tD(real) 0.6659 (0.6658)\tD(fake) 0.3214 (0.3277)\tgrad(D) penalty 0.0362 (0.0369)\tRec loss 4068.1606 (4223.4448)\tnorm 0.9942 (1.0016)\n",
            "Epoch: [31][ 30/195]\tTime  0.149 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.6660 (0.6658)\tD(fake) 0.3057 (0.3220)\tgrad(D) penalty 0.0368 (0.0367)\tRec loss 4211.8237 (4219.1581)\tnorm 1.0014 (1.0011)\n",
            "Epoch: [31][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.6661 (0.6659)\tD(fake) 0.3177 (0.3191)\tgrad(D) penalty 0.0361 (0.0368)\tRec loss 4339.5474 (4251.8538)\tnorm 0.9968 (1.0004)\n",
            "Epoch: [31][ 50/195]\tTime  0.165 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.6661 (0.6659)\tD(fake) 0.3235 (0.3199)\tgrad(D) penalty 0.0396 (0.0368)\tRec loss 4262.3633 (4262.9994)\tnorm 1.0164 (1.0022)\n",
            "Epoch: [31][ 60/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.6659 (0.6660)\tD(fake) 0.3248 (0.3210)\tgrad(D) penalty 0.0349 (0.0365)\tRec loss 3936.8975 (4255.7738)\tnorm 1.0107 (1.0040)\n",
            "Epoch: [31][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6663 (0.6660)\tD(fake) 0.3126 (0.3205)\tgrad(D) penalty 0.0323 (0.0361)\tRec loss 4259.4380 (4245.5745)\tnorm 1.0106 (1.0045)\n",
            "Epoch: [31][ 80/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6666 (0.6661)\tD(fake) 0.3156 (0.3198)\tgrad(D) penalty 0.0339 (0.0359)\tRec loss 4302.2842 (4238.8054)\tnorm 1.0040 (1.0052)\n",
            "Epoch: [31][ 90/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6666 (0.6662)\tD(fake) 0.3287 (0.3203)\tgrad(D) penalty 0.0308 (0.0356)\tRec loss 4179.1445 (4232.7201)\tnorm 1.0050 (1.0051)\n",
            "Epoch: [31][100/195]\tTime  0.164 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6668 (0.6662)\tD(fake) 0.3060 (0.3196)\tgrad(D) penalty 0.0340 (0.0352)\tRec loss 3961.7993 (4227.7826)\tnorm 1.0018 (1.0043)\n",
            "Epoch: [31][110/195]\tTime  0.153 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6668 (0.6663)\tD(fake) 0.3120 (0.3187)\tgrad(D) penalty 0.0318 (0.0350)\tRec loss 4189.9102 (4217.1562)\tnorm 1.0127 (1.0039)\n",
            "Epoch: [31][120/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6669 (0.6663)\tD(fake) 0.3341 (0.3196)\tgrad(D) penalty 0.0297 (0.0347)\tRec loss 4193.4644 (4211.2758)\tnorm 0.9928 (1.0031)\n",
            "Epoch: [31][130/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6670 (0.6664)\tD(fake) 0.3191 (0.3198)\tgrad(D) penalty 0.0290 (0.0344)\tRec loss 4248.9473 (4216.8119)\tnorm 0.9804 (1.0021)\n",
            "Epoch: [31][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6671 (0.6664)\tD(fake) 0.3081 (0.3189)\tgrad(D) penalty 0.0307 (0.0341)\tRec loss 3972.2935 (4209.5163)\tnorm 0.9968 (1.0015)\n",
            "Epoch: [31][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6672 (0.6665)\tD(fake) 0.3198 (0.3186)\tgrad(D) penalty 0.0301 (0.0339)\tRec loss 4162.1064 (4211.3623)\tnorm 0.9999 (1.0011)\n",
            "Epoch: [31][160/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6674 (0.6665)\tD(fake) 0.3210 (0.3191)\tgrad(D) penalty 0.0291 (0.0337)\tRec loss 4691.9346 (4213.2386)\tnorm 0.9883 (1.0014)\n",
            "Epoch: [31][170/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6674 (0.6666)\tD(fake) 0.3194 (0.3192)\tgrad(D) penalty 0.0298 (0.0335)\tRec loss 3896.0662 (4210.9184)\tnorm 1.0035 (1.0013)\n",
            "Epoch: [31][180/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6676 (0.6666)\tD(fake) 0.3207 (0.3192)\tgrad(D) penalty 0.0300 (0.0333)\tRec loss 3840.4946 (4204.9933)\tnorm 1.0207 (1.0017)\n",
            "Epoch: [31][190/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6677 (0.6667)\tD(fake) 0.3147 (0.3191)\tgrad(D) penalty 0.0287 (0.0331)\tRec loss 4158.3262 (4205.0828)\tnorm 1.0030 (1.0018)\n",
            "Epoch: [32][  0/195]\tTime  0.401 ( 0.401)\tData  0.197 ( 0.197)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3947.4185 (3947.4185)\tnorm 0.9989 (0.9989)\n",
            "Epoch: [32][ 10/195]\tTime  0.147 ( 0.172)\tData  0.000 ( 0.018)\tD(real) 0.6677 (0.6677)\tD(fake) 0.3061 (0.3020)\tgrad(D) penalty 0.0237 (0.0259)\tRec loss 4168.8506 (4122.5079)\tnorm 0.9859 (1.0034)\n",
            "Epoch: [32][ 20/195]\tTime  0.149 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.6679 (0.6678)\tD(fake) 0.3141 (0.3116)\tgrad(D) penalty 0.0226 (0.0248)\tRec loss 4035.3823 (4158.6908)\tnorm 0.9956 (0.9994)\n",
            "Epoch: [32][ 30/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.6681 (0.6679)\tD(fake) 0.3066 (0.3107)\tgrad(D) penalty 0.0248 (0.0250)\tRec loss 4362.4214 (4165.6443)\tnorm 1.0096 (0.9970)\n",
            "Epoch: [32][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.6681 (0.6680)\tD(fake) 0.3139 (0.3093)\tgrad(D) penalty 0.0242 (0.0243)\tRec loss 4099.3994 (4164.8967)\tnorm 0.9981 (0.9960)\n",
            "Epoch: [32][ 50/195]\tTime  0.164 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.6683 (0.6680)\tD(fake) 0.3196 (0.3109)\tgrad(D) penalty 0.0206 (0.0239)\tRec loss 4061.3491 (4177.1989)\tnorm 0.9832 (0.9954)\n",
            "Epoch: [32][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6685 (0.6681)\tD(fake) 0.3116 (0.3120)\tgrad(D) penalty 0.0193 (0.0236)\tRec loss 4204.7090 (4167.6617)\tnorm 0.9814 (0.9944)\n",
            "Epoch: [32][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6686 (0.6681)\tD(fake) 0.3041 (0.3113)\tgrad(D) penalty 0.0229 (0.0235)\tRec loss 4125.4082 (4170.0631)\tnorm 1.0050 (0.9939)\n",
            "Epoch: [32][ 80/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.6686 (0.6682)\tD(fake) 0.3162 (0.3108)\tgrad(D) penalty 0.0229 (0.0234)\tRec loss 4537.9102 (4181.2898)\tnorm 0.9971 (0.9936)\n",
            "Epoch: [32][ 90/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6688 (0.6682)\tD(fake) 0.3211 (0.3117)\tgrad(D) penalty 0.0185 (0.0231)\tRec loss 4169.6904 (4190.6310)\tnorm 0.9724 (0.9935)\n",
            "Epoch: [32][100/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6689 (0.6683)\tD(fake) 0.3003 (0.3116)\tgrad(D) penalty 0.0218 (0.0229)\tRec loss 4067.3992 (4190.4798)\tnorm 1.0058 (0.9940)\n",
            "Epoch: [32][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6691 (0.6684)\tD(fake) 0.2969 (0.3104)\tgrad(D) penalty 0.0236 (0.0230)\tRec loss 4339.3716 (4194.4917)\tnorm 0.9845 (0.9937)\n",
            "Epoch: [32][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6693 (0.6685)\tD(fake) 0.3124 (0.3104)\tgrad(D) penalty 0.0160 (0.0225)\tRec loss 4449.2236 (4198.4994)\tnorm 0.9988 (0.9932)\n",
            "Epoch: [32][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6695 (0.6685)\tD(fake) 0.3144 (0.3105)\tgrad(D) penalty 0.0203 (0.0224)\tRec loss 4233.4985 (4204.9425)\tnorm 1.0111 (0.9930)\n",
            "Epoch: [32][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6695 (0.6686)\tD(fake) 0.3054 (0.3099)\tgrad(D) penalty 0.0168 (0.0220)\tRec loss 4082.1455 (4202.3727)\tnorm 0.9955 (0.9937)\n",
            "Epoch: [32][150/195]\tTime  0.169 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6698 (0.6687)\tD(fake) 0.3022 (0.3096)\tgrad(D) penalty 0.0141 (0.0216)\tRec loss 3982.2622 (4203.9808)\tnorm 0.9974 (0.9940)\n",
            "Epoch: [32][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6699 (0.6688)\tD(fake) 0.3062 (0.3092)\tgrad(D) penalty 0.0150 (0.0212)\tRec loss 4079.1729 (4203.5350)\tnorm 0.9926 (0.9937)\n",
            "Epoch: [32][170/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6700 (0.6688)\tD(fake) 0.3090 (0.3091)\tgrad(D) penalty 0.0162 (0.0210)\tRec loss 4103.3574 (4201.1933)\tnorm 0.9929 (0.9937)\n",
            "Epoch: [32][180/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6702 (0.6689)\tD(fake) 0.3022 (0.3086)\tgrad(D) penalty 0.0163 (0.0208)\tRec loss 4261.3452 (4199.1884)\tnorm 1.0045 (0.9938)\n",
            "Epoch: [32][190/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6704 (0.6690)\tD(fake) 0.2925 (0.3082)\tgrad(D) penalty 0.0175 (0.0205)\tRec loss 4403.6367 (4196.5989)\tnorm 0.9897 (0.9937)\n",
            "Epoch: [33][  0/195]\tTime  0.422 ( 0.422)\tData  0.218 ( 0.218)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4634.9541 (4634.9541)\tnorm 0.9848 (0.9848)\n",
            "Epoch: [33][ 10/195]\tTime  0.148 ( 0.176)\tData  0.000 ( 0.020)\tD(real) 0.6707 (0.6707)\tD(fake) 0.2939 (0.2855)\tgrad(D) penalty 0.0171 (0.0187)\tRec loss 4053.4863 (4154.2692)\tnorm 0.9892 (0.9922)\n",
            "Epoch: [33][ 20/195]\tTime  0.147 ( 0.163)\tData  0.000 ( 0.011)\tD(real) 0.6710 (0.6708)\tD(fake) 0.3073 (0.2971)\tgrad(D) penalty 0.0156 (0.0162)\tRec loss 4440.4092 (4159.8678)\tnorm 0.9847 (0.9881)\n",
            "Epoch: [33][ 30/195]\tTime  0.147 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.6710 (0.6708)\tD(fake) 0.2932 (0.2973)\tgrad(D) penalty 0.0131 (0.0153)\tRec loss 3944.2197 (4155.8916)\tnorm 0.9840 (0.9864)\n",
            "Epoch: [33][ 40/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.6712 (0.6709)\tD(fake) 0.2876 (0.2941)\tgrad(D) penalty 0.0142 (0.0152)\tRec loss 4029.5730 (4135.4479)\tnorm 0.9965 (0.9866)\n",
            "Epoch: [33][ 50/195]\tTime  0.165 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.6715 (0.6710)\tD(fake) 0.2930 (0.2938)\tgrad(D) penalty 0.0124 (0.0149)\tRec loss 4573.2974 (4144.1817)\tnorm 0.9946 (0.9868)\n",
            "Epoch: [33][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.6718 (0.6712)\tD(fake) 0.2954 (0.2937)\tgrad(D) penalty 0.0135 (0.0147)\tRec loss 3938.8691 (4152.5789)\tnorm 0.9984 (0.9894)\n",
            "Epoch: [33][ 70/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6719 (0.6713)\tD(fake) 0.2925 (0.2936)\tgrad(D) penalty 0.0125 (0.0144)\tRec loss 4210.5337 (4159.0029)\tnorm 0.9997 (0.9905)\n",
            "Epoch: [33][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6722 (0.6714)\tD(fake) 0.2931 (0.2936)\tgrad(D) penalty 0.0149 (0.0144)\tRec loss 3965.5161 (4168.1108)\tnorm 1.0046 (0.9908)\n",
            "Epoch: [33][ 90/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6723 (0.6715)\tD(fake) 0.2974 (0.2937)\tgrad(D) penalty 0.0112 (0.0142)\tRec loss 4386.5464 (4174.4628)\tnorm 0.9935 (0.9919)\n",
            "Epoch: [33][100/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6726 (0.6716)\tD(fake) 0.2908 (0.2936)\tgrad(D) penalty 0.0138 (0.0141)\tRec loss 4636.0576 (4193.0351)\tnorm 0.9993 (0.9922)\n",
            "Epoch: [33][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6728 (0.6717)\tD(fake) 0.2907 (0.2935)\tgrad(D) penalty 0.0147 (0.0142)\tRec loss 4103.1973 (4193.6510)\tnorm 1.0018 (0.9929)\n",
            "Epoch: [33][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6731 (0.6718)\tD(fake) 0.2889 (0.2933)\tgrad(D) penalty 0.0128 (0.0141)\tRec loss 4056.9419 (4191.3923)\tnorm 0.9803 (0.9928)\n",
            "Epoch: [33][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6732 (0.6719)\tD(fake) 0.2908 (0.2932)\tgrad(D) penalty 0.0121 (0.0140)\tRec loss 4432.5142 (4193.0114)\tnorm 0.9946 (0.9931)\n",
            "Epoch: [33][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6734 (0.6720)\tD(fake) 0.2912 (0.2931)\tgrad(D) penalty 0.0133 (0.0139)\tRec loss 4321.9585 (4191.6064)\tnorm 1.0125 (0.9940)\n",
            "Epoch: [33][150/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6736 (0.6721)\tD(fake) 0.2893 (0.2929)\tgrad(D) penalty 0.0147 (0.0139)\tRec loss 4269.1450 (4185.0395)\tnorm 0.9868 (0.9947)\n",
            "Epoch: [33][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6738 (0.6722)\tD(fake) 0.2884 (0.2928)\tgrad(D) penalty 0.0113 (0.0137)\tRec loss 4217.5103 (4192.1407)\tnorm 0.9878 (0.9945)\n",
            "Epoch: [33][170/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6741 (0.6723)\tD(fake) 0.2816 (0.2924)\tgrad(D) penalty 0.0137 (0.0137)\tRec loss 3803.4561 (4195.0388)\tnorm 1.0006 (0.9947)\n",
            "Epoch: [33][180/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6743 (0.6724)\tD(fake) 0.2860 (0.2918)\tgrad(D) penalty 0.0125 (0.0136)\tRec loss 4017.8140 (4195.2549)\tnorm 0.9851 (0.9941)\n",
            "Epoch: [33][190/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6744 (0.6725)\tD(fake) 0.2976 (0.2918)\tgrad(D) penalty 0.0103 (0.0135)\tRec loss 3982.8723 (4195.9057)\tnorm 0.9793 (0.9935)\n",
            "Epoch: [34][  0/195]\tTime  0.405 ( 0.405)\tData  0.213 ( 0.213)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4156.7559 (4156.7559)\tnorm 0.9919 (0.9919)\n",
            "Epoch: [34][ 10/195]\tTime  0.148 ( 0.172)\tData  0.000 ( 0.020)\tD(real) 0.6749 (0.6749)\tD(fake) 0.2670 (0.2611)\tgrad(D) penalty 0.0158 (0.0147)\tRec loss 3898.9929 (4204.9416)\tnorm 0.9922 (0.9867)\n",
            "Epoch: [34][ 20/195]\tTime  0.146 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.6752 (0.6750)\tD(fake) 0.3047 (0.2832)\tgrad(D) penalty 0.0099 (0.0128)\tRec loss 4328.8149 (4196.3282)\tnorm 0.9997 (0.9904)\n",
            "Epoch: [34][ 30/195]\tTime  0.151 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.6754 (0.6751)\tD(fake) 0.2885 (0.2867)\tgrad(D) penalty 0.0085 (0.0116)\tRec loss 4389.6436 (4205.0300)\tnorm 0.9901 (0.9920)\n",
            "Epoch: [34][ 40/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.6755 (0.6752)\tD(fake) 0.2661 (0.2799)\tgrad(D) penalty 0.0119 (0.0118)\tRec loss 4268.5078 (4226.2501)\tnorm 0.9973 (0.9922)\n",
            "Epoch: [34][ 50/195]\tTime  0.166 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.6759 (0.6753)\tD(fake) 0.2766 (0.2792)\tgrad(D) penalty 0.0112 (0.0118)\tRec loss 4378.4951 (4231.4574)\tnorm 0.9898 (0.9914)\n",
            "Epoch: [34][ 60/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.6761 (0.6755)\tD(fake) 0.2861 (0.2802)\tgrad(D) penalty 0.0110 (0.0116)\tRec loss 3770.3511 (4226.2724)\tnorm 0.9862 (0.9917)\n",
            "Epoch: [34][ 70/195]\tTime  0.153 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6764 (0.6756)\tD(fake) 0.2877 (0.2814)\tgrad(D) penalty 0.0107 (0.0115)\tRec loss 4298.1284 (4223.3530)\tnorm 0.9886 (0.9917)\n",
            "Epoch: [34][ 80/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6766 (0.6757)\tD(fake) 0.2661 (0.2803)\tgrad(D) penalty 0.0105 (0.0113)\tRec loss 4049.8679 (4223.6549)\tnorm 0.9908 (0.9911)\n",
            "Epoch: [34][ 90/195]\tTime  0.153 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6769 (0.6758)\tD(fake) 0.2723 (0.2792)\tgrad(D) penalty 0.0117 (0.0113)\tRec loss 4266.6309 (4228.3188)\tnorm 0.9902 (0.9911)\n",
            "Epoch: [34][100/195]\tTime  0.168 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6770 (0.6759)\tD(fake) 0.2787 (0.2794)\tgrad(D) penalty 0.0098 (0.0112)\tRec loss 4272.8154 (4223.5637)\tnorm 1.0004 (0.9912)\n",
            "Epoch: [34][110/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6773 (0.6760)\tD(fake) 0.2689 (0.2786)\tgrad(D) penalty 0.0103 (0.0112)\tRec loss 4243.9844 (4219.2717)\tnorm 0.9922 (0.9911)\n",
            "Epoch: [34][120/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6777 (0.6762)\tD(fake) 0.2748 (0.2784)\tgrad(D) penalty 0.0089 (0.0111)\tRec loss 4217.6226 (4217.6624)\tnorm 0.9915 (0.9913)\n",
            "Epoch: [34][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6777 (0.6763)\tD(fake) 0.2773 (0.2784)\tgrad(D) penalty 0.0107 (0.0110)\tRec loss 4197.6270 (4215.9316)\tnorm 0.9881 (0.9912)\n",
            "Epoch: [34][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6781 (0.6764)\tD(fake) 0.2726 (0.2780)\tgrad(D) penalty 0.0091 (0.0109)\tRec loss 4349.9106 (4213.6173)\tnorm 1.0006 (0.9915)\n",
            "Epoch: [34][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6783 (0.6765)\tD(fake) 0.2689 (0.2776)\tgrad(D) penalty 0.0137 (0.0110)\tRec loss 4068.8013 (4204.4546)\tnorm 0.9996 (0.9916)\n",
            "Epoch: [34][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6786 (0.6767)\tD(fake) 0.2781 (0.2772)\tgrad(D) penalty 0.0106 (0.0110)\tRec loss 4090.6147 (4200.8943)\tnorm 1.0071 (0.9922)\n",
            "Epoch: [34][170/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6789 (0.6768)\tD(fake) 0.2759 (0.2771)\tgrad(D) penalty 0.0097 (0.0110)\tRec loss 4268.7466 (4201.8578)\tnorm 1.0045 (0.9926)\n",
            "Epoch: [34][180/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6789 (0.6769)\tD(fake) 0.2723 (0.2771)\tgrad(D) penalty 0.0098 (0.0109)\tRec loss 3812.9187 (4198.6201)\tnorm 1.0039 (0.9930)\n",
            "Epoch: [34][190/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6792 (0.6770)\tD(fake) 0.2628 (0.2767)\tgrad(D) penalty 0.0123 (0.0110)\tRec loss 4218.0967 (4198.0751)\tnorm 0.9889 (0.9927)\n",
            "Epoch: [35][  0/195]\tTime  0.382 ( 0.382)\tData  0.203 ( 0.203)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4422.6401 (4422.6401)\tnorm 0.9987 (0.9987)\n",
            "Epoch: [35][ 10/195]\tTime  0.149 ( 0.171)\tData  0.000 ( 0.019)\tD(real) 0.6798 (0.6797)\tD(fake) 0.2797 (0.2733)\tgrad(D) penalty 0.0089 (0.0090)\tRec loss 4355.8818 (4251.1410)\tnorm 0.9867 (0.9886)\n",
            "Epoch: [35][ 20/195]\tTime  0.149 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.6801 (0.6799)\tD(fake) 0.2763 (0.2773)\tgrad(D) penalty 0.0104 (0.0096)\tRec loss 4230.6621 (4201.7837)\tnorm 0.9860 (0.9905)\n",
            "Epoch: [35][ 30/195]\tTime  0.153 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.6802 (0.6799)\tD(fake) 0.2676 (0.2752)\tgrad(D) penalty 0.0091 (0.0095)\tRec loss 4060.7234 (4207.1504)\tnorm 0.9814 (0.9913)\n",
            "Epoch: [35][ 40/195]\tTime  0.149 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.6805 (0.6801)\tD(fake) 0.2742 (0.2740)\tgrad(D) penalty 0.0090 (0.0096)\tRec loss 4166.2402 (4191.3461)\tnorm 0.9752 (0.9897)\n",
            "Epoch: [35][ 50/195]\tTime  0.165 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.6807 (0.6802)\tD(fake) 0.2738 (0.2740)\tgrad(D) penalty 0.0079 (0.0096)\tRec loss 4106.8970 (4188.5003)\tnorm 1.0004 (0.9877)\n",
            "Epoch: [35][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.6809 (0.6803)\tD(fake) 0.2617 (0.2727)\tgrad(D) penalty 0.0105 (0.0098)\tRec loss 4016.6890 (4189.9940)\tnorm 0.9996 (0.9881)\n",
            "Epoch: [35][ 70/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6812 (0.6804)\tD(fake) 0.2646 (0.2716)\tgrad(D) penalty 0.0108 (0.0098)\tRec loss 4343.4233 (4190.3623)\tnorm 0.9819 (0.9882)\n",
            "Epoch: [35][ 80/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6814 (0.6805)\tD(fake) 0.2747 (0.2717)\tgrad(D) penalty 0.0101 (0.0098)\tRec loss 4405.9150 (4196.2801)\tnorm 0.9899 (0.9887)\n",
            "Epoch: [35][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6815 (0.6806)\tD(fake) 0.2600 (0.2711)\tgrad(D) penalty 0.0091 (0.0098)\tRec loss 4007.4033 (4209.2903)\tnorm 1.0089 (0.9899)\n",
            "Epoch: [35][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6818 (0.6808)\tD(fake) 0.2641 (0.2696)\tgrad(D) penalty 0.0097 (0.0099)\tRec loss 4090.1836 (4203.6460)\tnorm 0.9941 (0.9905)\n",
            "Epoch: [35][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6821 (0.6809)\tD(fake) 0.2725 (0.2699)\tgrad(D) penalty 0.0100 (0.0099)\tRec loss 4148.7998 (4205.4205)\tnorm 0.9941 (0.9915)\n",
            "Epoch: [35][120/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6823 (0.6810)\tD(fake) 0.2583 (0.2693)\tgrad(D) penalty 0.0093 (0.0099)\tRec loss 3936.6062 (4207.4405)\tnorm 1.0102 (0.9920)\n",
            "Epoch: [35][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6826 (0.6811)\tD(fake) 0.2638 (0.2688)\tgrad(D) penalty 0.0119 (0.0100)\tRec loss 4171.9854 (4211.2624)\tnorm 0.9929 (0.9925)\n",
            "Epoch: [35][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6829 (0.6812)\tD(fake) 0.2728 (0.2694)\tgrad(D) penalty 0.0084 (0.0098)\tRec loss 4552.5444 (4215.5437)\tnorm 1.0217 (0.9929)\n",
            "Epoch: [35][150/195]\tTime  0.171 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6832 (0.6813)\tD(fake) 0.2594 (0.2691)\tgrad(D) penalty 0.0074 (0.0097)\tRec loss 4160.0317 (4205.8893)\tnorm 0.9857 (0.9927)\n",
            "Epoch: [35][160/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6834 (0.6815)\tD(fake) 0.2657 (0.2686)\tgrad(D) penalty 0.0085 (0.0098)\tRec loss 4205.4043 (4203.0504)\tnorm 0.9749 (0.9924)\n",
            "Epoch: [35][170/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6837 (0.6816)\tD(fake) 0.2646 (0.2684)\tgrad(D) penalty 0.0080 (0.0098)\tRec loss 4322.6631 (4202.6546)\tnorm 0.9754 (0.9921)\n",
            "Epoch: [35][180/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6840 (0.6817)\tD(fake) 0.2630 (0.2680)\tgrad(D) penalty 0.0065 (0.0096)\tRec loss 4137.6660 (4201.8805)\tnorm 0.9769 (0.9916)\n",
            "Epoch: [35][190/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.6842 (0.6818)\tD(fake) 0.2617 (0.2677)\tgrad(D) penalty 0.0094 (0.0096)\tRec loss 4034.1255 (4193.9568)\tnorm 0.9738 (0.9912)\n",
            "Epoch: [36][  0/195]\tTime  0.382 ( 0.382)\tData  0.196 ( 0.196)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4339.0840 (4339.0840)\tnorm 0.9893 (0.9893)\n",
            "Epoch: [36][ 10/195]\tTime  0.149 ( 0.170)\tData  0.000 ( 0.018)\tD(real) 0.6845 (0.6844)\tD(fake) 0.2671 (0.2593)\tgrad(D) penalty 0.0091 (0.0092)\tRec loss 4261.7480 (4166.8836)\tnorm 0.9835 (0.9794)\n",
            "Epoch: [36][ 20/195]\tTime  0.154 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.6849 (0.6847)\tD(fake) 0.2775 (0.2715)\tgrad(D) penalty 0.0090 (0.0084)\tRec loss 3815.1987 (4162.5374)\tnorm 0.9758 (0.9769)\n",
            "Epoch: [36][ 30/195]\tTime  0.149 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.6851 (0.6848)\tD(fake) 0.2535 (0.2683)\tgrad(D) penalty 0.0085 (0.0083)\tRec loss 4163.0845 (4175.3974)\tnorm 0.9778 (0.9773)\n",
            "Epoch: [36][ 40/195]\tTime  0.150 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.6853 (0.6850)\tD(fake) 0.2552 (0.2633)\tgrad(D) penalty 0.0108 (0.0086)\tRec loss 4249.7842 (4192.6594)\tnorm 0.9696 (0.9777)\n",
            "Epoch: [36][ 50/195]\tTime  0.168 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.6856 (0.6850)\tD(fake) 0.2734 (0.2643)\tgrad(D) penalty 0.0083 (0.0086)\tRec loss 4502.0293 (4190.2175)\tnorm 0.9760 (0.9778)\n",
            "Epoch: [36][ 60/195]\tTime  0.153 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.6859 (0.6852)\tD(fake) 0.2558 (0.2648)\tgrad(D) penalty 0.0077 (0.0084)\tRec loss 3832.2976 (4179.7265)\tnorm 0.9922 (0.9777)\n",
            "Epoch: [36][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6860 (0.6853)\tD(fake) 0.2434 (0.2624)\tgrad(D) penalty 0.0117 (0.0085)\tRec loss 4031.5688 (4181.2261)\tnorm 0.9833 (0.9787)\n",
            "Epoch: [36][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6864 (0.6854)\tD(fake) 0.2774 (0.2632)\tgrad(D) penalty 0.0074 (0.0087)\tRec loss 4220.5547 (4190.5437)\tnorm 0.9680 (0.9796)\n",
            "Epoch: [36][ 90/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6867 (0.6855)\tD(fake) 0.2651 (0.2638)\tgrad(D) penalty 0.0106 (0.0087)\tRec loss 3938.0137 (4190.0572)\tnorm 0.9714 (0.9802)\n",
            "Epoch: [36][100/195]\tTime  0.163 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6869 (0.6857)\tD(fake) 0.2553 (0.2631)\tgrad(D) penalty 0.0101 (0.0087)\tRec loss 4250.3018 (4189.1109)\tnorm 0.9861 (0.9800)\n",
            "Epoch: [36][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6871 (0.6858)\tD(fake) 0.2605 (0.2628)\tgrad(D) penalty 0.0104 (0.0088)\tRec loss 4100.2358 (4182.1444)\tnorm 0.9642 (0.9803)\n",
            "Epoch: [36][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6874 (0.6859)\tD(fake) 0.2566 (0.2622)\tgrad(D) penalty 0.0083 (0.0088)\tRec loss 4228.2588 (4177.7515)\tnorm 0.9778 (0.9801)\n",
            "Epoch: [36][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6876 (0.6860)\tD(fake) 0.2550 (0.2617)\tgrad(D) penalty 0.0089 (0.0089)\tRec loss 4250.6221 (4179.8898)\tnorm 0.9833 (0.9803)\n",
            "Epoch: [36][140/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6879 (0.6862)\tD(fake) 0.2621 (0.2617)\tgrad(D) penalty 0.0078 (0.0088)\tRec loss 4553.4600 (4186.2471)\tnorm 0.9684 (0.9800)\n",
            "Epoch: [36][150/195]\tTime  0.166 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6880 (0.6863)\tD(fake) 0.2489 (0.2612)\tgrad(D) penalty 0.0079 (0.0087)\tRec loss 4462.9971 (4183.6333)\tnorm 0.9782 (0.9796)\n",
            "Epoch: [36][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6883 (0.6864)\tD(fake) 0.2635 (0.2608)\tgrad(D) penalty 0.0073 (0.0088)\tRec loss 3960.3721 (4181.1573)\tnorm 0.9616 (0.9788)\n",
            "Epoch: [36][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6886 (0.6865)\tD(fake) 0.2602 (0.2608)\tgrad(D) penalty 0.0112 (0.0089)\tRec loss 4187.2314 (4182.7402)\tnorm 0.9769 (0.9785)\n",
            "Epoch: [36][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6888 (0.6867)\tD(fake) 0.2570 (0.2606)\tgrad(D) penalty 0.0083 (0.0089)\tRec loss 4378.5186 (4184.6256)\tnorm 0.9686 (0.9779)\n",
            "Epoch: [36][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6890 (0.6868)\tD(fake) 0.2610 (0.2605)\tgrad(D) penalty 0.0088 (0.0089)\tRec loss 4149.5107 (4181.8169)\tnorm 0.9966 (0.9777)\n",
            "Epoch: [37][  0/195]\tTime  0.417 ( 0.417)\tData  0.222 ( 0.222)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4215.1953 (4215.1953)\tnorm 0.9758 (0.9758)\n",
            "Epoch: [37][ 10/195]\tTime  0.149 ( 0.173)\tData  0.000 ( 0.020)\tD(real) 0.6894 (0.6894)\tD(fake) 0.2539 (0.2480)\tgrad(D) penalty 0.0089 (0.0096)\tRec loss 4187.0879 (4191.6493)\tnorm 0.9862 (0.9821)\n",
            "Epoch: [37][ 20/195]\tTime  0.147 ( 0.163)\tData  0.000 ( 0.011)\tD(real) 0.6898 (0.6896)\tD(fake) 0.2760 (0.2635)\tgrad(D) penalty 0.0066 (0.0080)\tRec loss 4197.7324 (4220.8314)\tnorm 0.9762 (0.9787)\n",
            "Epoch: [37][ 30/195]\tTime  0.148 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.6900 (0.6897)\tD(fake) 0.2481 (0.2615)\tgrad(D) penalty 0.0075 (0.0078)\tRec loss 4258.8262 (4217.4435)\tnorm 0.9768 (0.9795)\n",
            "Epoch: [37][ 40/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.6902 (0.6898)\tD(fake) 0.2558 (0.2569)\tgrad(D) penalty 0.0087 (0.0087)\tRec loss 4079.0454 (4181.3198)\tnorm 0.9542 (0.9759)\n",
            "Epoch: [37][ 50/195]\tTime  0.169 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.6905 (0.6899)\tD(fake) 0.2719 (0.2592)\tgrad(D) penalty 0.0064 (0.0084)\tRec loss 4065.7686 (4172.8666)\tnorm 0.9746 (0.9738)\n",
            "Epoch: [37][ 60/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.6907 (0.6901)\tD(fake) 0.2526 (0.2593)\tgrad(D) penalty 0.0087 (0.0083)\tRec loss 4322.2827 (4160.2145)\tnorm 0.9795 (0.9737)\n",
            "Epoch: [37][ 70/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6910 (0.6902)\tD(fake) 0.2552 (0.2584)\tgrad(D) penalty 0.0078 (0.0083)\tRec loss 4095.7373 (4163.8980)\tnorm 0.9794 (0.9741)\n",
            "Epoch: [37][ 80/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.6912 (0.6903)\tD(fake) 0.2563 (0.2580)\tgrad(D) penalty 0.0103 (0.0083)\tRec loss 4337.7280 (4168.3775)\tnorm 0.9714 (0.9736)\n",
            "Epoch: [37][ 90/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6914 (0.6904)\tD(fake) 0.2635 (0.2584)\tgrad(D) penalty 0.0069 (0.0082)\tRec loss 3924.6958 (4161.0165)\tnorm 0.9650 (0.9734)\n",
            "Epoch: [37][100/195]\tTime  0.168 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.6917 (0.6906)\tD(fake) 0.2582 (0.2585)\tgrad(D) penalty 0.0088 (0.0083)\tRec loss 4317.7334 (4162.5198)\tnorm 0.9722 (0.9736)\n",
            "Epoch: [37][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6920 (0.6907)\tD(fake) 0.2497 (0.2580)\tgrad(D) penalty 0.0109 (0.0084)\tRec loss 4330.3735 (4165.0876)\tnorm 0.9635 (0.9733)\n",
            "Epoch: [37][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6922 (0.6908)\tD(fake) 0.2575 (0.2579)\tgrad(D) penalty 0.0082 (0.0084)\tRec loss 4222.6621 (4165.9862)\tnorm 0.9868 (0.9739)\n",
            "Epoch: [37][130/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6924 (0.6909)\tD(fake) 0.2521 (0.2576)\tgrad(D) penalty 0.0081 (0.0083)\tRec loss 4441.3037 (4167.4843)\tnorm 0.9778 (0.9743)\n",
            "Epoch: [37][140/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6927 (0.6910)\tD(fake) 0.2533 (0.2571)\tgrad(D) penalty 0.0083 (0.0084)\tRec loss 4154.3647 (4168.2782)\tnorm 0.9942 (0.9749)\n",
            "Epoch: [37][150/195]\tTime  0.168 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6926 (0.6911)\tD(fake) 0.2557 (0.2571)\tgrad(D) penalty 0.0091 (0.0085)\tRec loss 4309.6313 (4174.7339)\tnorm 0.9725 (0.9748)\n",
            "Epoch: [37][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6931 (0.6913)\tD(fake) 0.2538 (0.2566)\tgrad(D) penalty 0.0098 (0.0085)\tRec loss 4015.3572 (4174.7208)\tnorm 0.9732 (0.9750)\n",
            "Epoch: [37][170/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6929 (0.6913)\tD(fake) 0.2607 (0.2568)\tgrad(D) penalty 0.0092 (0.0085)\tRec loss 4325.9087 (4177.7780)\tnorm 0.9795 (0.9750)\n",
            "Epoch: [37][180/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6936 (0.6915)\tD(fake) 0.2493 (0.2566)\tgrad(D) penalty 0.0093 (0.0085)\tRec loss 4064.1011 (4179.7605)\tnorm 0.9826 (0.9750)\n",
            "Epoch: [37][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6935 (0.6916)\tD(fake) 0.2509 (0.2562)\tgrad(D) penalty 0.0118 (0.0085)\tRec loss 4328.3701 (4183.4369)\tnorm 0.9810 (0.9754)\n",
            "Epoch: [38][  0/195]\tTime  0.403 ( 0.403)\tData  0.205 ( 0.205)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4107.0332 (4107.0332)\tnorm 0.9936 (0.9936)\n",
            "Epoch: [38][ 10/195]\tTime  0.150 ( 0.171)\tData  0.000 ( 0.019)\tD(real) 0.6940 (0.6940)\tD(fake) 0.2591 (0.2508)\tgrad(D) penalty 0.0092 (0.0100)\tRec loss 4014.6089 (4114.6369)\tnorm 0.9802 (0.9828)\n",
            "Epoch: [38][ 20/195]\tTime  0.147 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.6943 (0.6941)\tD(fake) 0.2543 (0.2581)\tgrad(D) penalty 0.0071 (0.0080)\tRec loss 4248.1167 (4160.0894)\tnorm 0.9787 (0.9833)\n",
            "Epoch: [38][ 30/195]\tTime  0.149 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.6946 (0.6943)\tD(fake) 0.2340 (0.2524)\tgrad(D) penalty 0.0092 (0.0080)\tRec loss 4098.0352 (4163.4751)\tnorm 0.9746 (0.9819)\n",
            "Epoch: [38][ 40/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.6949 (0.6944)\tD(fake) 0.2607 (0.2527)\tgrad(D) penalty 0.0078 (0.0083)\tRec loss 3973.3062 (4154.6398)\tnorm 0.9725 (0.9807)\n",
            "Epoch: [38][ 50/195]\tTime  0.164 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.6951 (0.6945)\tD(fake) 0.2622 (0.2548)\tgrad(D) penalty 0.0068 (0.0081)\tRec loss 4108.5693 (4148.9114)\tnorm 0.9791 (0.9812)\n",
            "Epoch: [38][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.6950 (0.6946)\tD(fake) 0.2439 (0.2534)\tgrad(D) penalty 0.0093 (0.0083)\tRec loss 4326.3379 (4147.8673)\tnorm 0.9803 (0.9798)\n",
            "Epoch: [38][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6955 (0.6947)\tD(fake) 0.2529 (0.2532)\tgrad(D) penalty 0.0075 (0.0084)\tRec loss 3992.0498 (4140.1969)\tnorm 0.9872 (0.9794)\n",
            "Epoch: [38][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6957 (0.6949)\tD(fake) 0.2598 (0.2544)\tgrad(D) penalty 0.0073 (0.0082)\tRec loss 4186.7910 (4156.2145)\tnorm 0.9818 (0.9787)\n",
            "Epoch: [38][ 90/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.6959 (0.6950)\tD(fake) 0.2430 (0.2538)\tgrad(D) penalty 0.0100 (0.0083)\tRec loss 4199.7065 (4159.9800)\tnorm 0.9830 (0.9784)\n",
            "Epoch: [38][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6962 (0.6951)\tD(fake) 0.2539 (0.2531)\tgrad(D) penalty 0.0070 (0.0082)\tRec loss 3979.7595 (4171.3047)\tnorm 0.9892 (0.9783)\n",
            "Epoch: [38][110/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6964 (0.6952)\tD(fake) 0.2539 (0.2532)\tgrad(D) penalty 0.0066 (0.0081)\tRec loss 3844.5146 (4172.0899)\tnorm 0.9911 (0.9782)\n",
            "Epoch: [38][120/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6966 (0.6953)\tD(fake) 0.2532 (0.2529)\tgrad(D) penalty 0.0091 (0.0081)\tRec loss 4151.8418 (4171.1139)\tnorm 0.9854 (0.9789)\n",
            "Epoch: [38][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6968 (0.6954)\tD(fake) 0.2631 (0.2534)\tgrad(D) penalty 0.0073 (0.0080)\tRec loss 4596.5815 (4169.9644)\tnorm 0.9889 (0.9797)\n",
            "Epoch: [38][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6971 (0.6956)\tD(fake) 0.2538 (0.2538)\tgrad(D) penalty 0.0088 (0.0080)\tRec loss 4159.4814 (4169.7138)\tnorm 0.9652 (0.9795)\n",
            "Epoch: [38][150/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6973 (0.6956)\tD(fake) 0.2504 (0.2535)\tgrad(D) penalty 0.0082 (0.0080)\tRec loss 4335.4590 (4170.9918)\tnorm 0.9950 (0.9795)\n",
            "Epoch: [38][160/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.6975 (0.6958)\tD(fake) 0.2628 (0.2539)\tgrad(D) penalty 0.0072 (0.0080)\tRec loss 4182.8887 (4175.1373)\tnorm 0.9917 (0.9799)\n",
            "Epoch: [38][170/195]\tTime  0.153 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6976 (0.6959)\tD(fake) 0.2556 (0.2541)\tgrad(D) penalty 0.0083 (0.0080)\tRec loss 4334.8711 (4177.6157)\tnorm 0.9970 (0.9809)\n",
            "Epoch: [38][180/195]\tTime  0.154 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6979 (0.6960)\tD(fake) 0.2483 (0.2535)\tgrad(D) penalty 0.0075 (0.0080)\tRec loss 4232.8477 (4180.6646)\tnorm 0.9885 (0.9816)\n",
            "Epoch: [38][190/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.6980 (0.6961)\tD(fake) 0.2535 (0.2535)\tgrad(D) penalty 0.0068 (0.0080)\tRec loss 4222.8447 (4181.1117)\tnorm 0.9761 (0.9818)\n",
            "Epoch: [39][  0/195]\tTime  0.400 ( 0.400)\tData  0.201 ( 0.201)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3949.5830 (3949.5830)\tnorm 0.9832 (0.9832)\n",
            "Epoch: [39][ 10/195]\tTime  0.149 ( 0.173)\tData  0.000 ( 0.019)\tD(real) 0.6985 (0.6984)\tD(fake) 0.2521 (0.2459)\tgrad(D) penalty 0.0103 (0.0104)\tRec loss 4217.4521 (4117.3610)\tnorm 0.9933 (0.9921)\n",
            "Epoch: [39][ 20/195]\tTime  0.146 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.6988 (0.6985)\tD(fake) 0.2574 (0.2549)\tgrad(D) penalty 0.0075 (0.0084)\tRec loss 3950.5540 (4127.8214)\tnorm 0.9893 (0.9926)\n",
            "Epoch: [39][ 30/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.6989 (0.6986)\tD(fake) 0.2489 (0.2540)\tgrad(D) penalty 0.0079 (0.0081)\tRec loss 3998.1172 (4158.5412)\tnorm 1.0020 (0.9890)\n",
            "Epoch: [39][ 40/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.6991 (0.6987)\tD(fake) 0.2536 (0.2521)\tgrad(D) penalty 0.0093 (0.0082)\tRec loss 4670.3486 (4162.6766)\tnorm 0.9687 (0.9872)\n",
            "Epoch: [39][ 50/195]\tTime  0.167 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.6993 (0.6988)\tD(fake) 0.2618 (0.2537)\tgrad(D) penalty 0.0085 (0.0082)\tRec loss 4122.8569 (4154.3225)\tnorm 0.9823 (0.9870)\n",
            "Epoch: [39][ 60/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.6996 (0.6990)\tD(fake) 0.2457 (0.2536)\tgrad(D) penalty 0.0088 (0.0080)\tRec loss 4140.0195 (4160.6099)\tnorm 0.9763 (0.9859)\n",
            "Epoch: [39][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.6997 (0.6990)\tD(fake) 0.2499 (0.2528)\tgrad(D) penalty 0.0087 (0.0080)\tRec loss 4279.3799 (4158.2010)\tnorm 0.9846 (0.9853)\n",
            "Epoch: [39][ 80/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.6999 (0.6992)\tD(fake) 0.2554 (0.2533)\tgrad(D) penalty 0.0080 (0.0080)\tRec loss 4116.2598 (4154.5848)\tnorm 0.9830 (0.9842)\n",
            "Epoch: [39][ 90/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.6998 (0.6992)\tD(fake) 0.2526 (0.2532)\tgrad(D) penalty 0.0075 (0.0080)\tRec loss 4356.2197 (4145.5354)\tnorm 0.9573 (0.9823)\n",
            "Epoch: [39][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7000 (0.6993)\tD(fake) 0.2526 (0.2531)\tgrad(D) penalty 0.0083 (0.0079)\tRec loss 3869.2988 (4154.4225)\tnorm 0.9688 (0.9811)\n",
            "Epoch: [39][110/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7004 (0.6994)\tD(fake) 0.2520 (0.2531)\tgrad(D) penalty 0.0081 (0.0079)\tRec loss 4092.0566 (4153.5641)\tnorm 0.9596 (0.9801)\n",
            "Epoch: [39][120/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7008 (0.6996)\tD(fake) 0.2520 (0.2533)\tgrad(D) penalty 0.0098 (0.0080)\tRec loss 4331.4971 (4153.2405)\tnorm 0.9591 (0.9787)\n",
            "Epoch: [39][130/195]\tTime  0.144 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7010 (0.6996)\tD(fake) 0.2491 (0.2532)\tgrad(D) penalty 0.0068 (0.0080)\tRec loss 3973.1660 (4157.3351)\tnorm 0.9657 (0.9778)\n",
            "Epoch: [39][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7012 (0.6998)\tD(fake) 0.2513 (0.2528)\tgrad(D) penalty 0.0058 (0.0079)\tRec loss 3993.3770 (4161.3005)\tnorm 0.9777 (0.9770)\n",
            "Epoch: [39][150/195]\tTime  0.164 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7012 (0.6998)\tD(fake) 0.2457 (0.2525)\tgrad(D) penalty 0.0084 (0.0079)\tRec loss 4033.9280 (4162.4490)\tnorm 0.9735 (0.9764)\n",
            "Epoch: [39][160/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7013 (0.7000)\tD(fake) 0.2668 (0.2530)\tgrad(D) penalty 0.0057 (0.0079)\tRec loss 4453.8926 (4161.7223)\tnorm 0.9624 (0.9750)\n",
            "Epoch: [39][170/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7018 (0.7000)\tD(fake) 0.2517 (0.2531)\tgrad(D) penalty 0.0077 (0.0079)\tRec loss 4197.7002 (4165.4161)\tnorm 0.9715 (0.9743)\n",
            "Epoch: [39][180/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7020 (0.7002)\tD(fake) 0.2441 (0.2523)\tgrad(D) penalty 0.0073 (0.0079)\tRec loss 4150.8232 (4166.8607)\tnorm 0.9672 (0.9738)\n",
            "Epoch: [39][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7020 (0.7002)\tD(fake) 0.2580 (0.2525)\tgrad(D) penalty 0.0079 (0.0079)\tRec loss 4195.2871 (4171.1178)\tnorm 0.9550 (0.9735)\n",
            "Epoch: [40][  0/195]\tTime  0.391 ( 0.391)\tData  0.199 ( 0.199)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3927.2214 (3927.2214)\tnorm 0.9648 (0.9648)\n",
            "Epoch: [40][ 10/195]\tTime  0.151 ( 0.171)\tData  0.000 ( 0.018)\tD(real) 0.7023 (0.7023)\tD(fake) 0.2546 (0.2445)\tgrad(D) penalty 0.0090 (0.0109)\tRec loss 4480.8525 (4145.3054)\tnorm 0.9780 (0.9756)\n",
            "Epoch: [40][ 20/195]\tTime  0.149 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7027 (0.7025)\tD(fake) 0.2568 (0.2565)\tgrad(D) penalty 0.0081 (0.0083)\tRec loss 3956.1729 (4143.4724)\tnorm 0.9739 (0.9778)\n",
            "Epoch: [40][ 30/195]\tTime  0.150 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7028 (0.7026)\tD(fake) 0.2453 (0.2537)\tgrad(D) penalty 0.0111 (0.0085)\tRec loss 4129.1572 (4163.2113)\tnorm 0.9757 (0.9798)\n",
            "Epoch: [40][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7031 (0.7027)\tD(fake) 0.2488 (0.2523)\tgrad(D) penalty 0.0076 (0.0084)\tRec loss 4406.6826 (4173.3512)\tnorm 0.9790 (0.9801)\n",
            "Epoch: [40][ 50/195]\tTime  0.168 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7034 (0.7028)\tD(fake) 0.2510 (0.2523)\tgrad(D) penalty 0.0066 (0.0080)\tRec loss 4330.2666 (4168.0393)\tnorm 0.9866 (0.9805)\n",
            "Epoch: [40][ 60/195]\tTime  0.154 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7035 (0.7029)\tD(fake) 0.2466 (0.2516)\tgrad(D) penalty 0.0101 (0.0082)\tRec loss 4119.9238 (4167.3488)\tnorm 0.9749 (0.9801)\n",
            "Epoch: [40][ 70/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7037 (0.7030)\tD(fake) 0.2534 (0.2517)\tgrad(D) penalty 0.0084 (0.0081)\tRec loss 3801.2981 (4158.3784)\tnorm 0.9953 (0.9814)\n",
            "Epoch: [40][ 80/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7040 (0.7031)\tD(fake) 0.2503 (0.2519)\tgrad(D) penalty 0.0071 (0.0079)\tRec loss 4230.8135 (4166.3058)\tnorm 0.9914 (0.9818)\n",
            "Epoch: [40][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7040 (0.7032)\tD(fake) 0.2474 (0.2514)\tgrad(D) penalty 0.0068 (0.0079)\tRec loss 4043.1309 (4166.1794)\tnorm 0.9782 (0.9814)\n",
            "Epoch: [40][100/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7041 (0.7033)\tD(fake) 0.2525 (0.2511)\tgrad(D) penalty 0.0076 (0.0079)\tRec loss 4643.8457 (4170.8228)\tnorm 0.9748 (0.9803)\n",
            "Epoch: [40][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7045 (0.7034)\tD(fake) 0.2487 (0.2510)\tgrad(D) penalty 0.0080 (0.0078)\tRec loss 4168.6221 (4171.9223)\tnorm 0.9664 (0.9796)\n",
            "Epoch: [40][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7047 (0.7035)\tD(fake) 0.2520 (0.2510)\tgrad(D) penalty 0.0071 (0.0079)\tRec loss 4161.3003 (4172.1304)\tnorm 0.9610 (0.9789)\n",
            "Epoch: [40][130/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7048 (0.7036)\tD(fake) 0.2494 (0.2509)\tgrad(D) penalty 0.0082 (0.0079)\tRec loss 4175.9409 (4174.6809)\tnorm 0.9669 (0.9785)\n",
            "Epoch: [40][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7051 (0.7037)\tD(fake) 0.2509 (0.2507)\tgrad(D) penalty 0.0079 (0.0080)\tRec loss 4246.6084 (4168.2029)\tnorm 0.9630 (0.9775)\n",
            "Epoch: [40][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7053 (0.7038)\tD(fake) 0.2498 (0.2507)\tgrad(D) penalty 0.0102 (0.0080)\tRec loss 3814.1392 (4169.1350)\tnorm 0.9691 (0.9767)\n",
            "Epoch: [40][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7054 (0.7039)\tD(fake) 0.2578 (0.2512)\tgrad(D) penalty 0.0071 (0.0080)\tRec loss 4221.2378 (4169.0294)\tnorm 0.9633 (0.9760)\n",
            "Epoch: [40][170/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7055 (0.7040)\tD(fake) 0.2560 (0.2515)\tgrad(D) penalty 0.0082 (0.0080)\tRec loss 4195.7324 (4171.9934)\tnorm 0.9551 (0.9751)\n",
            "Epoch: [40][180/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7059 (0.7041)\tD(fake) 0.2480 (0.2513)\tgrad(D) penalty 0.0061 (0.0080)\tRec loss 4406.0776 (4171.7983)\tnorm 0.9541 (0.9741)\n",
            "Epoch: [40][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7060 (0.7042)\tD(fake) 0.2513 (0.2512)\tgrad(D) penalty 0.0080 (0.0080)\tRec loss 4129.1230 (4169.4661)\tnorm 0.9482 (0.9732)\n",
            "Epoch: [41][  0/195]\tTime  0.409 ( 0.409)\tData  0.227 ( 0.227)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4001.4224 (4001.4224)\tnorm 0.9499 (0.9499)\n",
            "Epoch: [41][ 10/195]\tTime  0.146 ( 0.172)\tData  0.000 ( 0.021)\tD(real) 0.7063 (0.7063)\tD(fake) 0.2577 (0.2492)\tgrad(D) penalty 0.0070 (0.0084)\tRec loss 3921.0205 (3996.1288)\tnorm 0.9613 (0.9524)\n",
            "Epoch: [41][ 20/195]\tTime  0.147 ( 0.160)\tData  0.000 ( 0.011)\tD(real) 0.7065 (0.7064)\tD(fake) 0.2598 (0.2585)\tgrad(D) penalty 0.0074 (0.0079)\tRec loss 3804.9233 (4071.9362)\tnorm 0.9746 (0.9544)\n",
            "Epoch: [41][ 30/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.008)\tD(real) 0.7066 (0.7064)\tD(fake) 0.2474 (0.2562)\tgrad(D) penalty 0.0083 (0.0080)\tRec loss 3817.2886 (4085.8699)\tnorm 0.9638 (0.9563)\n",
            "Epoch: [41][ 40/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7070 (0.7066)\tD(fake) 0.2511 (0.2533)\tgrad(D) penalty 0.0084 (0.0083)\tRec loss 3825.8315 (4091.4070)\tnorm 0.9554 (0.9559)\n",
            "Epoch: [41][ 50/195]\tTime  0.166 ( 0.153)\tData  0.000 ( 0.005)\tD(real) 0.7071 (0.7066)\tD(fake) 0.2533 (0.2535)\tgrad(D) penalty 0.0069 (0.0081)\tRec loss 4546.7134 (4113.5135)\tnorm 0.9558 (0.9558)\n",
            "Epoch: [41][ 60/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7073 (0.7067)\tD(fake) 0.2394 (0.2512)\tgrad(D) penalty 0.0095 (0.0082)\tRec loss 4369.7725 (4125.7546)\tnorm 0.9809 (0.9584)\n",
            "Epoch: [41][ 70/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7075 (0.7068)\tD(fake) 0.2478 (0.2505)\tgrad(D) penalty 0.0079 (0.0083)\tRec loss 4593.2808 (4137.3500)\tnorm 0.9715 (0.9602)\n",
            "Epoch: [41][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7077 (0.7069)\tD(fake) 0.2613 (0.2516)\tgrad(D) penalty 0.0072 (0.0083)\tRec loss 3977.2571 (4141.2374)\tnorm 0.9607 (0.9607)\n",
            "Epoch: [41][ 90/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7078 (0.7070)\tD(fake) 0.2554 (0.2521)\tgrad(D) penalty 0.0072 (0.0082)\tRec loss 4452.6230 (4135.9856)\tnorm 0.9688 (0.9620)\n",
            "Epoch: [41][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7081 (0.7071)\tD(fake) 0.2486 (0.2513)\tgrad(D) penalty 0.0089 (0.0082)\tRec loss 4346.0479 (4145.1100)\tnorm 0.9644 (0.9624)\n",
            "Epoch: [41][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7082 (0.7072)\tD(fake) 0.2511 (0.2513)\tgrad(D) penalty 0.0077 (0.0081)\tRec loss 4241.2119 (4156.6575)\tnorm 0.9724 (0.9634)\n",
            "Epoch: [41][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7084 (0.7073)\tD(fake) 0.2520 (0.2512)\tgrad(D) penalty 0.0101 (0.0082)\tRec loss 4442.4541 (4159.7792)\tnorm 0.9794 (0.9652)\n",
            "Epoch: [41][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7086 (0.7074)\tD(fake) 0.2498 (0.2512)\tgrad(D) penalty 0.0087 (0.0082)\tRec loss 4299.3213 (4167.9132)\tnorm 0.9844 (0.9664)\n",
            "Epoch: [41][140/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7088 (0.7075)\tD(fake) 0.2432 (0.2507)\tgrad(D) penalty 0.0082 (0.0082)\tRec loss 4066.4373 (4164.3622)\tnorm 0.9733 (0.9673)\n",
            "Epoch: [41][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7090 (0.7076)\tD(fake) 0.2458 (0.2503)\tgrad(D) penalty 0.0091 (0.0082)\tRec loss 4318.4619 (4161.3962)\tnorm 0.9655 (0.9679)\n",
            "Epoch: [41][160/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7089 (0.7077)\tD(fake) 0.2483 (0.2503)\tgrad(D) penalty 0.0091 (0.0082)\tRec loss 4348.2334 (4163.6741)\tnorm 0.9791 (0.9680)\n",
            "Epoch: [41][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7092 (0.7078)\tD(fake) 0.2370 (0.2499)\tgrad(D) penalty 0.0083 (0.0082)\tRec loss 4194.1299 (4164.9662)\tnorm 0.9674 (0.9681)\n",
            "Epoch: [41][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7094 (0.7079)\tD(fake) 0.2389 (0.2491)\tgrad(D) penalty 0.0076 (0.0082)\tRec loss 4269.8384 (4164.2641)\tnorm 0.9766 (0.9687)\n",
            "Epoch: [41][190/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7096 (0.7080)\tD(fake) 0.2466 (0.2489)\tgrad(D) penalty 0.0086 (0.0083)\tRec loss 4109.3872 (4166.6448)\tnorm 0.9513 (0.9685)\n",
            "Epoch: [42][  0/195]\tTime  0.417 ( 0.417)\tData  0.221 ( 0.221)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4130.9590 (4130.9590)\tnorm 0.9613 (0.9613)\n",
            "Epoch: [42][ 10/195]\tTime  0.151 ( 0.175)\tData  0.000 ( 0.020)\tD(real) 0.7101 (0.7100)\tD(fake) 0.2434 (0.2348)\tgrad(D) penalty 0.0137 (0.0124)\tRec loss 4112.8799 (4137.9633)\tnorm 0.9594 (0.9555)\n",
            "Epoch: [42][ 20/195]\tTime  0.153 ( 0.163)\tData  0.000 ( 0.011)\tD(real) 0.7103 (0.7101)\tD(fake) 0.2628 (0.2523)\tgrad(D) penalty 0.0070 (0.0096)\tRec loss 4155.8271 (4136.1916)\tnorm 0.9715 (0.9606)\n",
            "Epoch: [42][ 30/195]\tTime  0.146 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.7105 (0.7102)\tD(fake) 0.2367 (0.2496)\tgrad(D) penalty 0.0084 (0.0095)\tRec loss 4206.7666 (4152.5644)\tnorm 0.9542 (0.9629)\n",
            "Epoch: [42][ 40/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7107 (0.7103)\tD(fake) 0.2421 (0.2467)\tgrad(D) penalty 0.0094 (0.0092)\tRec loss 4340.8555 (4183.2640)\tnorm 0.9503 (0.9619)\n",
            "Epoch: [42][ 50/195]\tTime  0.162 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7107 (0.7104)\tD(fake) 0.2432 (0.2460)\tgrad(D) penalty 0.0113 (0.0095)\tRec loss 4192.8540 (4189.4000)\tnorm 0.9702 (0.9642)\n",
            "Epoch: [42][ 60/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7111 (0.7105)\tD(fake) 0.2391 (0.2447)\tgrad(D) penalty 0.0119 (0.0097)\tRec loss 4612.8633 (4185.1503)\tnorm 0.9598 (0.9647)\n",
            "Epoch: [42][ 70/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7111 (0.7106)\tD(fake) 0.2491 (0.2454)\tgrad(D) penalty 0.0085 (0.0096)\tRec loss 4074.4731 (4176.4540)\tnorm 0.9708 (0.9649)\n",
            "Epoch: [42][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7115 (0.7107)\tD(fake) 0.2366 (0.2442)\tgrad(D) penalty 0.0124 (0.0097)\tRec loss 3840.4526 (4177.3012)\tnorm 0.9733 (0.9661)\n",
            "Epoch: [42][ 90/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7116 (0.7108)\tD(fake) 0.2456 (0.2442)\tgrad(D) penalty 0.0116 (0.0098)\tRec loss 4164.1494 (4180.4317)\tnorm 0.9551 (0.9661)\n",
            "Epoch: [42][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7118 (0.7109)\tD(fake) 0.2448 (0.2448)\tgrad(D) penalty 0.0099 (0.0098)\tRec loss 4023.5967 (4179.4326)\tnorm 0.9933 (0.9670)\n",
            "Epoch: [42][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7119 (0.7110)\tD(fake) 0.2399 (0.2446)\tgrad(D) penalty 0.0112 (0.0098)\tRec loss 4133.0962 (4178.1460)\tnorm 0.9615 (0.9674)\n",
            "Epoch: [42][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7122 (0.7111)\tD(fake) 0.2517 (0.2448)\tgrad(D) penalty 0.0099 (0.0099)\tRec loss 4282.0298 (4177.5525)\tnorm 0.9686 (0.9675)\n",
            "Epoch: [42][130/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7124 (0.7112)\tD(fake) 0.2484 (0.2450)\tgrad(D) penalty 0.0105 (0.0098)\tRec loss 4116.4946 (4177.4672)\tnorm 0.9727 (0.9679)\n",
            "Epoch: [42][140/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7126 (0.7113)\tD(fake) 0.2352 (0.2445)\tgrad(D) penalty 0.0110 (0.0099)\tRec loss 4374.7305 (4178.2207)\tnorm 0.9596 (0.9679)\n",
            "Epoch: [42][150/195]\tTime  0.165 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7129 (0.7114)\tD(fake) 0.2416 (0.2443)\tgrad(D) penalty 0.0091 (0.0099)\tRec loss 3943.2754 (4171.0296)\tnorm 0.9840 (0.9684)\n",
            "Epoch: [42][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7130 (0.7115)\tD(fake) 0.2455 (0.2441)\tgrad(D) penalty 0.0107 (0.0101)\tRec loss 3686.2549 (4162.3078)\tnorm 0.9610 (0.9682)\n",
            "Epoch: [42][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7132 (0.7116)\tD(fake) 0.2453 (0.2442)\tgrad(D) penalty 0.0080 (0.0100)\tRec loss 4199.3872 (4163.6121)\tnorm 0.9692 (0.9681)\n",
            "Epoch: [42][180/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7134 (0.7117)\tD(fake) 0.2418 (0.2439)\tgrad(D) penalty 0.0090 (0.0101)\tRec loss 4106.3550 (4160.3212)\tnorm 0.9466 (0.9679)\n",
            "Epoch: [42][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7134 (0.7118)\tD(fake) 0.2376 (0.2436)\tgrad(D) penalty 0.0109 (0.0101)\tRec loss 4141.7793 (4160.2735)\tnorm 0.9773 (0.9673)\n",
            "Epoch: [43][  0/195]\tTime  0.421 ( 0.421)\tData  0.220 ( 0.220)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4175.9038 (4175.9038)\tnorm 0.9747 (0.9747)\n",
            "Epoch: [43][ 10/195]\tTime  0.146 ( 0.173)\tData  0.000 ( 0.020)\tD(real) 0.7139 (0.7139)\tD(fake) 0.2466 (0.2361)\tgrad(D) penalty 0.0117 (0.0151)\tRec loss 4026.1421 (4161.7691)\tnorm 0.9596 (0.9592)\n",
            "Epoch: [43][ 20/195]\tTime  0.150 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7141 (0.7140)\tD(fake) 0.2446 (0.2458)\tgrad(D) penalty 0.0101 (0.0121)\tRec loss 4112.3320 (4127.3115)\tnorm 0.9552 (0.9591)\n",
            "Epoch: [43][ 30/195]\tTime  0.147 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.7143 (0.7140)\tD(fake) 0.2391 (0.2440)\tgrad(D) penalty 0.0097 (0.0117)\tRec loss 4300.8633 (4145.2608)\tnorm 0.9579 (0.9587)\n",
            "Epoch: [43][ 40/195]\tTime  0.146 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7145 (0.7142)\tD(fake) 0.2464 (0.2433)\tgrad(D) penalty 0.0119 (0.0117)\tRec loss 4071.1265 (4129.6710)\tnorm 0.9625 (0.9595)\n",
            "Epoch: [43][ 50/195]\tTime  0.167 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7148 (0.7142)\tD(fake) 0.2509 (0.2449)\tgrad(D) penalty 0.0106 (0.0116)\tRec loss 4097.6943 (4140.0252)\tnorm 0.9500 (0.9589)\n",
            "Epoch: [43][ 60/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7147 (0.7143)\tD(fake) 0.2384 (0.2448)\tgrad(D) penalty 0.0112 (0.0115)\tRec loss 4433.8164 (4148.8708)\tnorm 0.9415 (0.9593)\n",
            "Epoch: [43][ 70/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7149 (0.7144)\tD(fake) 0.2352 (0.2438)\tgrad(D) penalty 0.0133 (0.0115)\tRec loss 4436.4780 (4159.2235)\tnorm 0.9572 (0.9590)\n",
            "Epoch: [43][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7152 (0.7145)\tD(fake) 0.2478 (0.2440)\tgrad(D) penalty 0.0115 (0.0116)\tRec loss 4144.5967 (4154.3333)\tnorm 0.9678 (0.9602)\n",
            "Epoch: [43][ 90/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7154 (0.7146)\tD(fake) 0.2496 (0.2443)\tgrad(D) penalty 0.0141 (0.0118)\tRec loss 4172.7051 (4160.9107)\tnorm 0.9653 (0.9603)\n",
            "Epoch: [43][100/195]\tTime  0.170 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7154 (0.7147)\tD(fake) 0.2423 (0.2446)\tgrad(D) penalty 0.0114 (0.0117)\tRec loss 4232.1777 (4157.6235)\tnorm 0.9506 (0.9603)\n",
            "Epoch: [43][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7156 (0.7148)\tD(fake) 0.2451 (0.2445)\tgrad(D) penalty 0.0125 (0.0117)\tRec loss 4190.3984 (4153.5846)\tnorm 0.9628 (0.9603)\n",
            "Epoch: [43][120/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7161 (0.7149)\tD(fake) 0.2462 (0.2447)\tgrad(D) penalty 0.0127 (0.0118)\tRec loss 4010.1646 (4146.3426)\tnorm 0.9572 (0.9599)\n",
            "Epoch: [43][130/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7161 (0.7150)\tD(fake) 0.2422 (0.2447)\tgrad(D) penalty 0.0127 (0.0119)\tRec loss 4156.2095 (4144.7660)\tnorm 0.9436 (0.9593)\n",
            "Epoch: [43][140/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7164 (0.7151)\tD(fake) 0.2362 (0.2440)\tgrad(D) penalty 0.0127 (0.0120)\tRec loss 4023.1201 (4146.2362)\tnorm 0.9513 (0.9588)\n",
            "Epoch: [43][150/195]\tTime  0.170 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7165 (0.7152)\tD(fake) 0.2537 (0.2444)\tgrad(D) penalty 0.0143 (0.0122)\tRec loss 4335.2207 (4150.1041)\tnorm 0.9697 (0.9588)\n",
            "Epoch: [43][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7167 (0.7153)\tD(fake) 0.2413 (0.2448)\tgrad(D) penalty 0.0139 (0.0123)\tRec loss 4297.9033 (4154.9485)\tnorm 0.9552 (0.9591)\n",
            "Epoch: [43][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7168 (0.7153)\tD(fake) 0.2490 (0.2446)\tgrad(D) penalty 0.0154 (0.0125)\tRec loss 4299.3652 (4155.0515)\tnorm 0.9491 (0.9588)\n",
            "Epoch: [43][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7170 (0.7154)\tD(fake) 0.2453 (0.2452)\tgrad(D) penalty 0.0166 (0.0127)\tRec loss 4233.5479 (4157.0253)\tnorm 0.9641 (0.9586)\n",
            "Epoch: [43][190/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7172 (0.7155)\tD(fake) 0.2461 (0.2453)\tgrad(D) penalty 0.0181 (0.0129)\tRec loss 4174.5107 (4155.4380)\tnorm 0.9467 (0.9583)\n",
            "Epoch: [44][  0/195]\tTime  0.380 ( 0.380)\tData  0.203 ( 0.203)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4051.4771 (4051.4771)\tnorm 0.9558 (0.9558)\n",
            "Epoch: [44][ 10/195]\tTime  0.150 ( 0.170)\tData  0.000 ( 0.019)\tD(real) 0.7176 (0.7175)\tD(fake) 0.2622 (0.2507)\tgrad(D) penalty 0.0190 (0.0183)\tRec loss 4020.0247 (4108.7622)\tnorm 0.9564 (0.9579)\n",
            "Epoch: [44][ 20/195]\tTime  0.145 ( 0.159)\tData  0.000 ( 0.010)\tD(real) 0.7177 (0.7176)\tD(fake) 0.2424 (0.2553)\tgrad(D) penalty 0.0135 (0.0156)\tRec loss 4170.9946 (4157.9099)\tnorm 0.9458 (0.9554)\n",
            "Epoch: [44][ 30/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7179 (0.7177)\tD(fake) 0.2359 (0.2484)\tgrad(D) penalty 0.0165 (0.0155)\tRec loss 4067.6633 (4156.8037)\tnorm 0.9567 (0.9549)\n",
            "Epoch: [44][ 40/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7182 (0.7177)\tD(fake) 0.2543 (0.2494)\tgrad(D) penalty 0.0153 (0.0149)\tRec loss 4034.5864 (4160.2374)\tnorm 0.9545 (0.9543)\n",
            "Epoch: [44][ 50/195]\tTime  0.163 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7183 (0.7178)\tD(fake) 0.2537 (0.2504)\tgrad(D) penalty 0.0129 (0.0150)\tRec loss 4167.0605 (4139.8781)\tnorm 0.9512 (0.9544)\n",
            "Epoch: [44][ 60/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7184 (0.7179)\tD(fake) 0.2350 (0.2490)\tgrad(D) penalty 0.0132 (0.0147)\tRec loss 4066.8242 (4130.6860)\tnorm 0.9431 (0.9533)\n",
            "Epoch: [44][ 70/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7185 (0.7180)\tD(fake) 0.2490 (0.2481)\tgrad(D) penalty 0.0175 (0.0152)\tRec loss 4202.4487 (4121.7057)\tnorm 0.9276 (0.9522)\n",
            "Epoch: [44][ 80/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7188 (0.7181)\tD(fake) 0.2434 (0.2490)\tgrad(D) penalty 0.0139 (0.0149)\tRec loss 4191.5151 (4128.9227)\tnorm 0.9396 (0.9523)\n",
            "Epoch: [44][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7188 (0.7182)\tD(fake) 0.2452 (0.2484)\tgrad(D) penalty 0.0159 (0.0151)\tRec loss 4179.3716 (4133.7103)\tnorm 0.9582 (0.9534)\n",
            "Epoch: [44][100/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7191 (0.7183)\tD(fake) 0.2467 (0.2488)\tgrad(D) penalty 0.0158 (0.0151)\tRec loss 4215.3853 (4137.6971)\tnorm 0.9496 (0.9527)\n",
            "Epoch: [44][110/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7192 (0.7183)\tD(fake) 0.2420 (0.2484)\tgrad(D) penalty 0.0170 (0.0152)\tRec loss 4323.7891 (4137.0286)\tnorm 0.9525 (0.9524)\n",
            "Epoch: [44][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7195 (0.7184)\tD(fake) 0.2537 (0.2485)\tgrad(D) penalty 0.0123 (0.0150)\tRec loss 3972.1914 (4139.0757)\tnorm 0.9432 (0.9518)\n",
            "Epoch: [44][130/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7197 (0.7185)\tD(fake) 0.2307 (0.2478)\tgrad(D) penalty 0.0152 (0.0150)\tRec loss 4177.0889 (4134.7497)\tnorm 0.9563 (0.9516)\n",
            "Epoch: [44][140/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7199 (0.7186)\tD(fake) 0.2597 (0.2477)\tgrad(D) penalty 0.0135 (0.0150)\tRec loss 4091.4543 (4138.5988)\tnorm 0.9442 (0.9512)\n",
            "Epoch: [44][150/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7198 (0.7187)\tD(fake) 0.2412 (0.2475)\tgrad(D) penalty 0.0144 (0.0150)\tRec loss 4295.9922 (4134.2196)\tnorm 0.9465 (0.9514)\n",
            "Epoch: [44][160/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7201 (0.7188)\tD(fake) 0.2476 (0.2470)\tgrad(D) penalty 0.0119 (0.0149)\tRec loss 4154.3008 (4133.5156)\tnorm 0.9472 (0.9510)\n",
            "Epoch: [44][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7203 (0.7189)\tD(fake) 0.2652 (0.2477)\tgrad(D) penalty 0.0133 (0.0148)\tRec loss 4391.8467 (4138.5922)\tnorm 0.9476 (0.9507)\n",
            "Epoch: [44][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7205 (0.7190)\tD(fake) 0.2408 (0.2475)\tgrad(D) penalty 0.0125 (0.0147)\tRec loss 4371.2754 (4147.0039)\tnorm 0.9546 (0.9505)\n",
            "Epoch: [44][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7206 (0.7190)\tD(fake) 0.2535 (0.2476)\tgrad(D) penalty 0.0129 (0.0147)\tRec loss 4296.8286 (4147.2910)\tnorm 0.9512 (0.9511)\n",
            "Epoch: [45][  0/195]\tTime  0.368 ( 0.368)\tData  0.193 ( 0.193)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4102.2231 (4102.2231)\tnorm 0.9597 (0.9597)\n",
            "Epoch: [45][ 10/195]\tTime  0.152 ( 0.170)\tData  0.000 ( 0.018)\tD(real) 0.7209 (0.7208)\tD(fake) 0.2685 (0.2576)\tgrad(D) penalty 0.0104 (0.0108)\tRec loss 4204.8579 (4200.3924)\tnorm 0.9430 (0.9479)\n",
            "Epoch: [45][ 20/195]\tTime  0.147 ( 0.159)\tData  0.000 ( 0.009)\tD(real) 0.7211 (0.7209)\tD(fake) 0.2407 (0.2561)\tgrad(D) penalty 0.0116 (0.0110)\tRec loss 4160.4976 (4181.1180)\tnorm 0.9444 (0.9514)\n",
            "Epoch: [45][ 30/195]\tTime  0.152 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7212 (0.7210)\tD(fake) 0.2315 (0.2487)\tgrad(D) penalty 0.0119 (0.0115)\tRec loss 3959.7712 (4185.9141)\tnorm 0.9547 (0.9507)\n",
            "Epoch: [45][ 40/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7215 (0.7211)\tD(fake) 0.2580 (0.2495)\tgrad(D) penalty 0.0113 (0.0118)\tRec loss 4235.0352 (4175.8050)\tnorm 0.9592 (0.9535)\n",
            "Epoch: [45][ 50/195]\tTime  0.165 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7215 (0.7212)\tD(fake) 0.2443 (0.2491)\tgrad(D) penalty 0.0167 (0.0122)\tRec loss 4512.7129 (4167.3361)\tnorm 0.9707 (0.9551)\n",
            "Epoch: [45][ 60/195]\tTime  0.153 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7218 (0.7213)\tD(fake) 0.2534 (0.2500)\tgrad(D) penalty 0.0134 (0.0123)\tRec loss 4183.1748 (4155.4565)\tnorm 0.9733 (0.9558)\n",
            "Epoch: [45][ 70/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7219 (0.7214)\tD(fake) 0.2410 (0.2494)\tgrad(D) penalty 0.0112 (0.0122)\tRec loss 3920.0171 (4138.1690)\tnorm 0.9551 (0.9557)\n",
            "Epoch: [45][ 80/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7222 (0.7215)\tD(fake) 0.2538 (0.2487)\tgrad(D) penalty 0.0130 (0.0125)\tRec loss 3895.9395 (4141.3634)\tnorm 0.9538 (0.9558)\n",
            "Epoch: [45][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7223 (0.7215)\tD(fake) 0.2505 (0.2494)\tgrad(D) penalty 0.0126 (0.0125)\tRec loss 4147.5332 (4149.2634)\tnorm 0.9569 (0.9572)\n",
            "Epoch: [45][100/195]\tTime  0.162 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7225 (0.7216)\tD(fake) 0.2447 (0.2484)\tgrad(D) penalty 0.0123 (0.0127)\tRec loss 4167.3931 (4150.4587)\tnorm 0.9477 (0.9574)\n",
            "Epoch: [45][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7226 (0.7217)\tD(fake) 0.2516 (0.2487)\tgrad(D) penalty 0.0155 (0.0127)\tRec loss 4035.7490 (4147.1693)\tnorm 0.9498 (0.9569)\n",
            "Epoch: [45][120/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7225 (0.7218)\tD(fake) 0.2388 (0.2483)\tgrad(D) penalty 0.0136 (0.0128)\tRec loss 3820.1313 (4146.8875)\tnorm 0.9352 (0.9563)\n",
            "Epoch: [45][130/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7228 (0.7219)\tD(fake) 0.2613 (0.2486)\tgrad(D) penalty 0.0135 (0.0128)\tRec loss 4079.1589 (4138.6944)\tnorm 0.9493 (0.9555)\n",
            "Epoch: [45][140/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7232 (0.7220)\tD(fake) 0.2447 (0.2490)\tgrad(D) penalty 0.0137 (0.0128)\tRec loss 3963.1055 (4138.2790)\tnorm 0.9451 (0.9547)\n",
            "Epoch: [45][150/195]\tTime  0.164 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7231 (0.7220)\tD(fake) 0.2385 (0.2486)\tgrad(D) penalty 0.0133 (0.0129)\tRec loss 4034.6299 (4139.4945)\tnorm 0.9463 (0.9542)\n",
            "Epoch: [45][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7232 (0.7221)\tD(fake) 0.2598 (0.2489)\tgrad(D) penalty 0.0118 (0.0128)\tRec loss 4341.2314 (4144.6393)\tnorm 0.9334 (0.9533)\n",
            "Epoch: [45][170/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7235 (0.7222)\tD(fake) 0.2620 (0.2496)\tgrad(D) penalty 0.0113 (0.0128)\tRec loss 4186.2109 (4136.3595)\tnorm 0.9470 (0.9530)\n",
            "Epoch: [45][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7236 (0.7223)\tD(fake) 0.2308 (0.2489)\tgrad(D) penalty 0.0118 (0.0127)\tRec loss 4234.7676 (4143.8967)\tnorm 0.9416 (0.9523)\n",
            "Epoch: [45][190/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7238 (0.7223)\tD(fake) 0.2527 (0.2488)\tgrad(D) penalty 0.0106 (0.0126)\tRec loss 4367.9507 (4146.3990)\tnorm 0.9585 (0.9524)\n",
            "Epoch: [46][  0/195]\tTime  0.384 ( 0.384)\tData  0.189 ( 0.189)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4084.2290 (4084.2290)\tnorm 0.9435 (0.9435)\n",
            "Epoch: [46][ 10/195]\tTime  0.151 ( 0.171)\tData  0.000 ( 0.018)\tD(real) 0.7242 (0.7240)\tD(fake) 0.2547 (0.2501)\tgrad(D) penalty 0.0109 (0.0107)\tRec loss 4136.2319 (4097.0771)\tnorm 0.9406 (0.9408)\n",
            "Epoch: [46][ 20/195]\tTime  0.147 ( 0.159)\tData  0.000 ( 0.009)\tD(real) 0.7242 (0.7241)\tD(fake) 0.2493 (0.2475)\tgrad(D) penalty 0.0113 (0.0115)\tRec loss 4042.6211 (4113.5973)\tnorm 0.9374 (0.9409)\n",
            "Epoch: [46][ 30/195]\tTime  0.150 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7244 (0.7242)\tD(fake) 0.2510 (0.2483)\tgrad(D) penalty 0.0113 (0.0118)\tRec loss 4040.8208 (4092.6909)\tnorm 0.9341 (0.9412)\n",
            "Epoch: [46][ 40/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7246 (0.7243)\tD(fake) 0.2392 (0.2478)\tgrad(D) penalty 0.0127 (0.0119)\tRec loss 4198.1553 (4091.4606)\tnorm 0.9642 (0.9406)\n",
            "Epoch: [46][ 50/195]\tTime  0.162 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7245 (0.7243)\tD(fake) 0.2536 (0.2480)\tgrad(D) penalty 0.0159 (0.0126)\tRec loss 4299.2314 (4101.4636)\tnorm 0.9363 (0.9414)\n",
            "Epoch: [46][ 60/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7249 (0.7244)\tD(fake) 0.2494 (0.2497)\tgrad(D) penalty 0.0103 (0.0121)\tRec loss 3975.8457 (4121.1644)\tnorm 0.9559 (0.9419)\n",
            "Epoch: [46][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7249 (0.7245)\tD(fake) 0.2370 (0.2488)\tgrad(D) penalty 0.0130 (0.0121)\tRec loss 4584.2568 (4127.6795)\tnorm 0.9426 (0.9427)\n",
            "Epoch: [46][ 80/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7250 (0.7246)\tD(fake) 0.2563 (0.2491)\tgrad(D) penalty 0.0104 (0.0120)\tRec loss 3956.7588 (4125.8262)\tnorm 0.9468 (0.9433)\n",
            "Epoch: [46][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7253 (0.7246)\tD(fake) 0.2401 (0.2490)\tgrad(D) penalty 0.0123 (0.0118)\tRec loss 4056.1145 (4124.9394)\tnorm 0.9276 (0.9431)\n",
            "Epoch: [46][100/195]\tTime  0.171 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7255 (0.7247)\tD(fake) 0.2519 (0.2482)\tgrad(D) penalty 0.0111 (0.0118)\tRec loss 3964.0720 (4124.7758)\tnorm 0.9511 (0.9430)\n",
            "Epoch: [46][110/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7256 (0.7248)\tD(fake) 0.2619 (0.2491)\tgrad(D) penalty 0.0111 (0.0118)\tRec loss 4162.7939 (4121.7646)\tnorm 0.9249 (0.9428)\n",
            "Epoch: [46][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7257 (0.7249)\tD(fake) 0.2325 (0.2489)\tgrad(D) penalty 0.0134 (0.0118)\tRec loss 4546.0088 (4130.1752)\tnorm 0.9348 (0.9424)\n",
            "Epoch: [46][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7257 (0.7249)\tD(fake) 0.2426 (0.2483)\tgrad(D) penalty 0.0112 (0.0118)\tRec loss 3987.0542 (4130.4793)\tnorm 0.9458 (0.9424)\n",
            "Epoch: [46][140/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7259 (0.7250)\tD(fake) 0.2388 (0.2483)\tgrad(D) penalty 0.0105 (0.0117)\tRec loss 4286.9932 (4134.7172)\tnorm 0.9426 (0.9421)\n",
            "Epoch: [46][150/195]\tTime  0.165 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7260 (0.7251)\tD(fake) 0.2402 (0.2478)\tgrad(D) penalty 0.0105 (0.0117)\tRec loss 3944.6392 (4137.2269)\tnorm 0.9384 (0.9420)\n",
            "Epoch: [46][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7263 (0.7251)\tD(fake) 0.2544 (0.2480)\tgrad(D) penalty 0.0097 (0.0118)\tRec loss 4304.6699 (4136.5999)\tnorm 0.9403 (0.9420)\n",
            "Epoch: [46][170/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7264 (0.7252)\tD(fake) 0.2305 (0.2476)\tgrad(D) penalty 0.0130 (0.0118)\tRec loss 4205.8311 (4137.1828)\tnorm 0.9273 (0.9421)\n",
            "Epoch: [46][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7267 (0.7253)\tD(fake) 0.2611 (0.2474)\tgrad(D) penalty 0.0107 (0.0119)\tRec loss 4356.4766 (4136.9391)\tnorm 0.9372 (0.9422)\n",
            "Epoch: [46][190/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7267 (0.7254)\tD(fake) 0.2447 (0.2476)\tgrad(D) penalty 0.0118 (0.0119)\tRec loss 4320.4146 (4138.2618)\tnorm 0.9521 (0.9424)\n",
            "Epoch: [47][  0/195]\tTime  0.397 ( 0.397)\tData  0.201 ( 0.201)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4045.1069 (4045.1069)\tnorm 0.9559 (0.9559)\n",
            "Epoch: [47][ 10/195]\tTime  0.147 ( 0.173)\tData  0.000 ( 0.019)\tD(real) 0.7270 (0.7270)\tD(fake) 0.2813 (0.2664)\tgrad(D) penalty 0.0114 (0.0128)\tRec loss 3943.3577 (4135.3909)\tnorm 0.9558 (0.9477)\n",
            "Epoch: [47][ 20/195]\tTime  0.149 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7272 (0.7270)\tD(fake) 0.2295 (0.2625)\tgrad(D) penalty 0.0127 (0.0116)\tRec loss 4057.5327 (4109.5562)\tnorm 0.9337 (0.9463)\n",
            "Epoch: [47][ 30/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.7273 (0.7271)\tD(fake) 0.2378 (0.2528)\tgrad(D) penalty 0.0134 (0.0124)\tRec loss 3920.5771 (4115.7004)\tnorm 0.9200 (0.9443)\n",
            "Epoch: [47][ 40/195]\tTime  0.154 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7275 (0.7272)\tD(fake) 0.2541 (0.2538)\tgrad(D) penalty 0.0113 (0.0119)\tRec loss 4102.0254 (4123.9789)\tnorm 0.9352 (0.9432)\n",
            "Epoch: [47][ 50/195]\tTime  0.162 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7276 (0.7273)\tD(fake) 0.2385 (0.2519)\tgrad(D) penalty 0.0106 (0.0116)\tRec loss 4131.3750 (4140.4770)\tnorm 0.9375 (0.9430)\n",
            "Epoch: [47][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7277 (0.7274)\tD(fake) 0.2517 (0.2511)\tgrad(D) penalty 0.0094 (0.0112)\tRec loss 4056.9966 (4146.2626)\tnorm 0.9307 (0.9414)\n",
            "Epoch: [47][ 70/195]\tTime  0.153 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7279 (0.7274)\tD(fake) 0.2304 (0.2494)\tgrad(D) penalty 0.0110 (0.0110)\tRec loss 4260.4355 (4137.9451)\tnorm 0.9351 (0.9413)\n",
            "Epoch: [47][ 80/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7281 (0.7275)\tD(fake) 0.2543 (0.2487)\tgrad(D) penalty 0.0123 (0.0113)\tRec loss 4158.1729 (4144.7439)\tnorm 0.9287 (0.9400)\n",
            "Epoch: [47][ 90/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7282 (0.7276)\tD(fake) 0.2347 (0.2479)\tgrad(D) penalty 0.0106 (0.0113)\tRec loss 3929.8457 (4144.6241)\tnorm 0.9401 (0.9397)\n",
            "Epoch: [47][100/195]\tTime  0.171 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7283 (0.7277)\tD(fake) 0.2408 (0.2463)\tgrad(D) penalty 0.0111 (0.0113)\tRec loss 4075.2422 (4142.1419)\tnorm 0.9286 (0.9386)\n",
            "Epoch: [47][110/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7284 (0.7277)\tD(fake) 0.2421 (0.2460)\tgrad(D) penalty 0.0121 (0.0113)\tRec loss 3960.7251 (4137.5860)\tnorm 0.9344 (0.9383)\n",
            "Epoch: [47][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7287 (0.7278)\tD(fake) 0.2449 (0.2460)\tgrad(D) penalty 0.0120 (0.0113)\tRec loss 4009.9236 (4138.7971)\tnorm 0.9269 (0.9376)\n",
            "Epoch: [47][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7287 (0.7279)\tD(fake) 0.2373 (0.2456)\tgrad(D) penalty 0.0128 (0.0114)\tRec loss 4255.5776 (4140.6761)\tnorm 0.9437 (0.9374)\n",
            "Epoch: [47][140/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7289 (0.7279)\tD(fake) 0.2610 (0.2462)\tgrad(D) penalty 0.0112 (0.0114)\tRec loss 4288.5488 (4149.6452)\tnorm 0.9423 (0.9378)\n",
            "Epoch: [47][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7290 (0.7280)\tD(fake) 0.2369 (0.2461)\tgrad(D) penalty 0.0096 (0.0114)\tRec loss 3815.4941 (4145.8758)\tnorm 0.9421 (0.9383)\n",
            "Epoch: [47][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7293 (0.7281)\tD(fake) 0.2342 (0.2449)\tgrad(D) penalty 0.0134 (0.0114)\tRec loss 4070.6133 (4143.8667)\tnorm 0.9530 (0.9389)\n",
            "Epoch: [47][170/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7294 (0.7282)\tD(fake) 0.2353 (0.2445)\tgrad(D) penalty 0.0098 (0.0113)\tRec loss 3962.9353 (4140.6207)\tnorm 0.9323 (0.9389)\n",
            "Epoch: [47][180/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7295 (0.7282)\tD(fake) 0.2350 (0.2440)\tgrad(D) penalty 0.0125 (0.0114)\tRec loss 3950.6040 (4138.4715)\tnorm 0.9350 (0.9390)\n",
            "Epoch: [47][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7297 (0.7283)\tD(fake) 0.2508 (0.2442)\tgrad(D) penalty 0.0074 (0.0113)\tRec loss 4060.4526 (4136.0442)\tnorm 0.9410 (0.9391)\n",
            "Epoch: [48][  0/195]\tTime  0.391 ( 0.391)\tData  0.197 ( 0.197)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4173.1992 (4173.1992)\tnorm 0.9451 (0.9451)\n",
            "Epoch: [48][ 10/195]\tTime  0.146 ( 0.169)\tData  0.000 ( 0.018)\tD(real) 0.7297 (0.7298)\tD(fake) 0.2511 (0.2411)\tgrad(D) penalty 0.0120 (0.0114)\tRec loss 4136.7500 (4182.6781)\tnorm 0.9359 (0.9383)\n",
            "Epoch: [48][ 20/195]\tTime  0.147 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.7301 (0.7299)\tD(fake) 0.2426 (0.2469)\tgrad(D) penalty 0.0110 (0.0107)\tRec loss 4138.5293 (4133.3367)\tnorm 0.9364 (0.9401)\n",
            "Epoch: [48][ 30/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.007)\tD(real) 0.7302 (0.7300)\tD(fake) 0.2347 (0.2430)\tgrad(D) penalty 0.0119 (0.0109)\tRec loss 3807.6885 (4112.7503)\tnorm 0.9340 (0.9379)\n",
            "Epoch: [48][ 40/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.005)\tD(real) 0.7304 (0.7301)\tD(fake) 0.2502 (0.2436)\tgrad(D) penalty 0.0121 (0.0111)\tRec loss 3805.4648 (4102.1961)\tnorm 0.9336 (0.9375)\n",
            "Epoch: [48][ 50/195]\tTime  0.168 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7305 (0.7301)\tD(fake) 0.2446 (0.2439)\tgrad(D) penalty 0.0118 (0.0112)\tRec loss 4007.0459 (4096.2915)\tnorm 0.9461 (0.9374)\n",
            "Epoch: [48][ 60/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7307 (0.7302)\tD(fake) 0.2513 (0.2447)\tgrad(D) penalty 0.0110 (0.0114)\tRec loss 3806.1387 (4104.4228)\tnorm 0.9290 (0.9371)\n",
            "Epoch: [48][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7308 (0.7303)\tD(fake) 0.2361 (0.2441)\tgrad(D) penalty 0.0123 (0.0116)\tRec loss 4103.3018 (4105.1806)\tnorm 0.9383 (0.9365)\n",
            "Epoch: [48][ 80/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7309 (0.7304)\tD(fake) 0.2524 (0.2443)\tgrad(D) penalty 0.0098 (0.0115)\tRec loss 4020.9995 (4113.3354)\tnorm 0.9371 (0.9365)\n",
            "Epoch: [48][ 90/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7309 (0.7304)\tD(fake) 0.2346 (0.2439)\tgrad(D) penalty 0.0104 (0.0113)\tRec loss 4170.7847 (4115.8006)\tnorm 0.9369 (0.9367)\n",
            "Epoch: [48][100/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7313 (0.7305)\tD(fake) 0.2367 (0.2427)\tgrad(D) penalty 0.0119 (0.0114)\tRec loss 4190.7920 (4121.5891)\tnorm 0.9472 (0.9370)\n",
            "Epoch: [48][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7314 (0.7306)\tD(fake) 0.2516 (0.2430)\tgrad(D) penalty 0.0124 (0.0115)\tRec loss 4235.4248 (4124.3662)\tnorm 0.9385 (0.9377)\n",
            "Epoch: [48][120/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7316 (0.7307)\tD(fake) 0.2200 (0.2419)\tgrad(D) penalty 0.0119 (0.0114)\tRec loss 4080.9336 (4129.8902)\tnorm 0.9351 (0.9379)\n",
            "Epoch: [48][130/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7316 (0.7307)\tD(fake) 0.2414 (0.2413)\tgrad(D) penalty 0.0129 (0.0115)\tRec loss 4071.1460 (4125.6720)\tnorm 0.9391 (0.9384)\n",
            "Epoch: [48][140/195]\tTime  0.153 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7316 (0.7308)\tD(fake) 0.2445 (0.2418)\tgrad(D) penalty 0.0141 (0.0114)\tRec loss 4292.6265 (4129.7112)\tnorm 0.9551 (0.9385)\n",
            "Epoch: [48][150/195]\tTime  0.161 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7318 (0.7309)\tD(fake) 0.2445 (0.2419)\tgrad(D) penalty 0.0107 (0.0114)\tRec loss 3663.8237 (4132.1541)\tnorm 0.9503 (0.9393)\n",
            "Epoch: [48][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7321 (0.7309)\tD(fake) 0.2379 (0.2414)\tgrad(D) penalty 0.0140 (0.0116)\tRec loss 4151.4189 (4132.3122)\tnorm 0.9339 (0.9392)\n",
            "Epoch: [48][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7321 (0.7310)\tD(fake) 0.2473 (0.2416)\tgrad(D) penalty 0.0111 (0.0115)\tRec loss 4330.5669 (4135.3504)\tnorm 0.9437 (0.9392)\n",
            "Epoch: [48][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7323 (0.7311)\tD(fake) 0.2483 (0.2417)\tgrad(D) penalty 0.0104 (0.0115)\tRec loss 4138.9297 (4131.5364)\tnorm 0.9448 (0.9394)\n",
            "Epoch: [48][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7324 (0.7311)\tD(fake) 0.2291 (0.2414)\tgrad(D) penalty 0.0126 (0.0115)\tRec loss 4293.2773 (4134.4848)\tnorm 0.9321 (0.9391)\n",
            "Epoch: [49][  0/195]\tTime  0.393 ( 0.393)\tData  0.201 ( 0.201)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3859.9170 (3859.9170)\tnorm 0.9307 (0.9307)\n",
            "Epoch: [49][ 10/195]\tTime  0.148 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.7326 (0.7326)\tD(fake) 0.2398 (0.2397)\tgrad(D) penalty 0.0130 (0.0120)\tRec loss 4347.4463 (4084.7668)\tnorm 0.9531 (0.9361)\n",
            "Epoch: [49][ 20/195]\tTime  0.150 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7328 (0.7327)\tD(fake) 0.2414 (0.2407)\tgrad(D) penalty 0.0117 (0.0122)\tRec loss 4081.0227 (4090.9365)\tnorm 0.9298 (0.9372)\n",
            "Epoch: [49][ 30/195]\tTime  0.149 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7330 (0.7327)\tD(fake) 0.2390 (0.2403)\tgrad(D) penalty 0.0129 (0.0122)\tRec loss 4142.3140 (4122.9966)\tnorm 0.9355 (0.9367)\n",
            "Epoch: [49][ 40/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7331 (0.7328)\tD(fake) 0.2432 (0.2397)\tgrad(D) penalty 0.0136 (0.0125)\tRec loss 4013.1104 (4143.2720)\tnorm 0.9396 (0.9364)\n",
            "Epoch: [49][ 50/195]\tTime  0.164 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7333 (0.7329)\tD(fake) 0.2418 (0.2407)\tgrad(D) penalty 0.0134 (0.0123)\tRec loss 3705.5635 (4140.5858)\tnorm 0.9442 (0.9370)\n",
            "Epoch: [49][ 60/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7333 (0.7330)\tD(fake) 0.2482 (0.2428)\tgrad(D) penalty 0.0119 (0.0124)\tRec loss 4444.9370 (4153.1420)\tnorm 0.9326 (0.9373)\n",
            "Epoch: [49][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7335 (0.7330)\tD(fake) 0.2300 (0.2420)\tgrad(D) penalty 0.0112 (0.0123)\tRec loss 3812.0686 (4143.0605)\tnorm 0.9482 (0.9374)\n",
            "Epoch: [49][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7332 (0.7331)\tD(fake) 0.2559 (0.2416)\tgrad(D) penalty 0.0105 (0.0124)\tRec loss 4022.9106 (4139.7510)\tnorm 0.9348 (0.9374)\n",
            "Epoch: [49][ 90/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7337 (0.7331)\tD(fake) 0.2339 (0.2416)\tgrad(D) penalty 0.0116 (0.0123)\tRec loss 4119.6914 (4144.4942)\tnorm 0.9362 (0.9376)\n",
            "Epoch: [49][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7338 (0.7332)\tD(fake) 0.2532 (0.2416)\tgrad(D) penalty 0.0136 (0.0124)\tRec loss 4148.5083 (4139.0523)\tnorm 0.9300 (0.9386)\n",
            "Epoch: [49][110/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7339 (0.7333)\tD(fake) 0.2376 (0.2419)\tgrad(D) penalty 0.0122 (0.0124)\tRec loss 3968.9028 (4147.2389)\tnorm 0.9515 (0.9388)\n",
            "Epoch: [49][120/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7341 (0.7333)\tD(fake) 0.2361 (0.2411)\tgrad(D) penalty 0.0117 (0.0125)\tRec loss 4177.9990 (4141.9311)\tnorm 0.9256 (0.9389)\n",
            "Epoch: [49][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7342 (0.7334)\tD(fake) 0.2314 (0.2405)\tgrad(D) penalty 0.0128 (0.0124)\tRec loss 4108.9336 (4143.9130)\tnorm 0.9419 (0.9385)\n",
            "Epoch: [49][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7341 (0.7335)\tD(fake) 0.2354 (0.2402)\tgrad(D) penalty 0.0121 (0.0124)\tRec loss 4432.6650 (4144.6755)\tnorm 0.9294 (0.9390)\n",
            "Epoch: [49][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7345 (0.7335)\tD(fake) 0.2503 (0.2406)\tgrad(D) penalty 0.0102 (0.0124)\tRec loss 4094.9016 (4140.8117)\tnorm 0.9295 (0.9390)\n",
            "Epoch: [49][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7347 (0.7336)\tD(fake) 0.2302 (0.2403)\tgrad(D) penalty 0.0131 (0.0124)\tRec loss 3977.8914 (4137.3698)\tnorm 0.9254 (0.9384)\n",
            "Epoch: [49][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7347 (0.7337)\tD(fake) 0.2421 (0.2402)\tgrad(D) penalty 0.0156 (0.0125)\tRec loss 4126.3691 (4134.0886)\tnorm 0.9567 (0.9383)\n",
            "Epoch: [49][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7350 (0.7337)\tD(fake) 0.2382 (0.2407)\tgrad(D) penalty 0.0112 (0.0124)\tRec loss 4135.2642 (4133.5112)\tnorm 0.9334 (0.9379)\n",
            "Epoch: [49][190/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7350 (0.7338)\tD(fake) 0.2414 (0.2405)\tgrad(D) penalty 0.0120 (0.0124)\tRec loss 4018.9561 (4132.5143)\tnorm 0.9375 (0.9378)\n",
            "Epoch: [50][  0/195]\tTime  0.405 ( 0.405)\tData  0.205 ( 0.205)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4645.8462 (4645.8462)\tnorm 0.9170 (0.9170)\n",
            "Epoch: [50][ 10/195]\tTime  0.149 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.7352 (0.7351)\tD(fake) 0.2368 (0.2302)\tgrad(D) penalty 0.0116 (0.0126)\tRec loss 4003.1938 (4147.3184)\tnorm 0.9145 (0.9323)\n",
            "Epoch: [50][ 20/195]\tTime  0.148 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7354 (0.7352)\tD(fake) 0.2309 (0.2359)\tgrad(D) penalty 0.0130 (0.0124)\tRec loss 4114.3970 (4126.4507)\tnorm 0.9348 (0.9312)\n",
            "Epoch: [50][ 30/195]\tTime  0.150 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7353 (0.7352)\tD(fake) 0.2308 (0.2341)\tgrad(D) penalty 0.0138 (0.0126)\tRec loss 4126.7446 (4125.8735)\tnorm 0.9295 (0.9297)\n",
            "Epoch: [50][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7355 (0.7353)\tD(fake) 0.2569 (0.2377)\tgrad(D) penalty 0.0113 (0.0126)\tRec loss 4199.0215 (4138.8305)\tnorm 0.9380 (0.9297)\n",
            "Epoch: [50][ 50/195]\tTime  0.166 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7356 (0.7354)\tD(fake) 0.2329 (0.2383)\tgrad(D) penalty 0.0123 (0.0125)\tRec loss 4231.7485 (4143.9928)\tnorm 0.9300 (0.9295)\n",
            "Epoch: [50][ 60/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7359 (0.7354)\tD(fake) 0.2541 (0.2383)\tgrad(D) penalty 0.0130 (0.0127)\tRec loss 4104.0566 (4142.7628)\tnorm 0.9505 (0.9296)\n",
            "Epoch: [50][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7359 (0.7355)\tD(fake) 0.2409 (0.2396)\tgrad(D) penalty 0.0097 (0.0124)\tRec loss 3913.4534 (4143.7229)\tnorm 0.9436 (0.9295)\n",
            "Epoch: [50][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7362 (0.7356)\tD(fake) 0.2298 (0.2375)\tgrad(D) penalty 0.0136 (0.0127)\tRec loss 4231.4814 (4150.7468)\tnorm 0.9143 (0.9292)\n",
            "Epoch: [50][ 90/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7362 (0.7356)\tD(fake) 0.2466 (0.2380)\tgrad(D) penalty 0.0145 (0.0128)\tRec loss 4429.5127 (4151.7162)\tnorm 0.9384 (0.9294)\n",
            "Epoch: [50][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7361 (0.7357)\tD(fake) 0.2364 (0.2391)\tgrad(D) penalty 0.0129 (0.0126)\tRec loss 3629.1553 (4140.8490)\tnorm 0.9542 (0.9307)\n",
            "Epoch: [50][110/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7363 (0.7357)\tD(fake) 0.2396 (0.2389)\tgrad(D) penalty 0.0121 (0.0127)\tRec loss 4260.6836 (4143.4407)\tnorm 0.9333 (0.9310)\n",
            "Epoch: [50][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7365 (0.7358)\tD(fake) 0.2236 (0.2384)\tgrad(D) penalty 0.0154 (0.0127)\tRec loss 4296.3315 (4143.6672)\tnorm 0.9478 (0.9324)\n",
            "Epoch: [50][130/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7367 (0.7359)\tD(fake) 0.2394 (0.2383)\tgrad(D) penalty 0.0104 (0.0127)\tRec loss 3975.4790 (4141.8231)\tnorm 0.9493 (0.9333)\n",
            "Epoch: [50][140/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7368 (0.7360)\tD(fake) 0.2380 (0.2388)\tgrad(D) penalty 0.0136 (0.0128)\tRec loss 3870.1274 (4138.0452)\tnorm 0.9415 (0.9342)\n",
            "Epoch: [50][150/195]\tTime  0.164 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7370 (0.7360)\tD(fake) 0.2384 (0.2387)\tgrad(D) penalty 0.0145 (0.0129)\tRec loss 3883.9585 (4136.6400)\tnorm 0.9336 (0.9344)\n",
            "Epoch: [50][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7371 (0.7361)\tD(fake) 0.2469 (0.2387)\tgrad(D) penalty 0.0123 (0.0129)\tRec loss 4247.1699 (4135.5325)\tnorm 0.9266 (0.9344)\n",
            "Epoch: [50][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7371 (0.7361)\tD(fake) 0.2422 (0.2390)\tgrad(D) penalty 0.0131 (0.0129)\tRec loss 4212.8394 (4134.7710)\tnorm 0.9430 (0.9345)\n",
            "Epoch: [50][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7373 (0.7362)\tD(fake) 0.2343 (0.2384)\tgrad(D) penalty 0.0138 (0.0130)\tRec loss 4055.8813 (4133.1994)\tnorm 0.9298 (0.9346)\n",
            "Epoch: [50][190/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7375 (0.7363)\tD(fake) 0.2317 (0.2382)\tgrad(D) penalty 0.0130 (0.0130)\tRec loss 4186.0322 (4131.2698)\tnorm 0.9233 (0.9345)\n",
            "Epoch: [51][  0/195]\tTime  0.395 ( 0.395)\tData  0.212 ( 0.212)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4289.8789 (4289.8789)\tnorm 0.9392 (0.9392)\n",
            "Epoch: [51][ 10/195]\tTime  0.150 ( 0.172)\tData  0.000 ( 0.020)\tD(real) 0.7377 (0.7376)\tD(fake) 0.2539 (0.2449)\tgrad(D) penalty 0.0120 (0.0133)\tRec loss 4221.6270 (4104.2739)\tnorm 0.9089 (0.9374)\n",
            "Epoch: [51][ 20/195]\tTime  0.148 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.7378 (0.7377)\tD(fake) 0.2271 (0.2392)\tgrad(D) penalty 0.0144 (0.0131)\tRec loss 3895.0767 (4112.9244)\tnorm 0.9474 (0.9399)\n",
            "Epoch: [51][ 30/195]\tTime  0.145 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7380 (0.7377)\tD(fake) 0.2538 (0.2415)\tgrad(D) penalty 0.0114 (0.0127)\tRec loss 4323.9131 (4080.0147)\tnorm 0.9313 (0.9400)\n",
            "Epoch: [51][ 40/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7381 (0.7378)\tD(fake) 0.2244 (0.2411)\tgrad(D) penalty 0.0121 (0.0125)\tRec loss 4021.9761 (4071.9858)\tnorm 0.9330 (0.9383)\n",
            "Epoch: [51][ 50/195]\tTime  0.163 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7379 (0.7379)\tD(fake) 0.2292 (0.2383)\tgrad(D) penalty 0.0138 (0.0130)\tRec loss 4115.9180 (4084.2472)\tnorm 0.9232 (0.9374)\n",
            "Epoch: [51][ 60/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7381 (0.7379)\tD(fake) 0.2476 (0.2401)\tgrad(D) penalty 0.0127 (0.0130)\tRec loss 4079.2998 (4082.0810)\tnorm 0.9257 (0.9375)\n",
            "Epoch: [51][ 70/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7384 (0.7380)\tD(fake) 0.2474 (0.2405)\tgrad(D) penalty 0.0142 (0.0131)\tRec loss 4278.6929 (4100.8442)\tnorm 0.9304 (0.9371)\n",
            "Epoch: [51][ 80/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7385 (0.7381)\tD(fake) 0.2388 (0.2405)\tgrad(D) penalty 0.0158 (0.0134)\tRec loss 4057.1338 (4104.7667)\tnorm 0.9373 (0.9369)\n",
            "Epoch: [51][ 90/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7386 (0.7381)\tD(fake) 0.2574 (0.2414)\tgrad(D) penalty 0.0109 (0.0133)\tRec loss 4168.7739 (4106.2530)\tnorm 0.9282 (0.9363)\n",
            "Epoch: [51][100/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7387 (0.7382)\tD(fake) 0.2379 (0.2420)\tgrad(D) penalty 0.0126 (0.0132)\tRec loss 4313.3223 (4109.6229)\tnorm 0.9268 (0.9360)\n",
            "Epoch: [51][110/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7386 (0.7382)\tD(fake) 0.2291 (0.2411)\tgrad(D) penalty 0.0151 (0.0133)\tRec loss 4109.4492 (4120.9540)\tnorm 0.9129 (0.9354)\n",
            "Epoch: [51][120/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7389 (0.7383)\tD(fake) 0.2448 (0.2404)\tgrad(D) penalty 0.0142 (0.0133)\tRec loss 4158.2979 (4122.0109)\tnorm 0.9352 (0.9354)\n",
            "Epoch: [51][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7389 (0.7383)\tD(fake) 0.2364 (0.2402)\tgrad(D) penalty 0.0129 (0.0133)\tRec loss 4326.8320 (4124.8089)\tnorm 0.9346 (0.9356)\n",
            "Epoch: [51][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7391 (0.7384)\tD(fake) 0.2371 (0.2402)\tgrad(D) penalty 0.0112 (0.0133)\tRec loss 3707.3066 (4118.5597)\tnorm 0.9502 (0.9362)\n",
            "Epoch: [51][150/195]\tTime  0.172 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7393 (0.7384)\tD(fake) 0.2264 (0.2394)\tgrad(D) penalty 0.0153 (0.0133)\tRec loss 4237.4844 (4119.6249)\tnorm 0.9346 (0.9367)\n",
            "Epoch: [51][160/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7394 (0.7385)\tD(fake) 0.2520 (0.2397)\tgrad(D) penalty 0.0110 (0.0133)\tRec loss 3952.7065 (4117.6938)\tnorm 0.9310 (0.9366)\n",
            "Epoch: [51][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7394 (0.7385)\tD(fake) 0.2197 (0.2392)\tgrad(D) penalty 0.0140 (0.0133)\tRec loss 4050.5034 (4120.2572)\tnorm 0.9372 (0.9364)\n",
            "Epoch: [51][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7396 (0.7386)\tD(fake) 0.2543 (0.2396)\tgrad(D) penalty 0.0135 (0.0133)\tRec loss 4343.5635 (4123.3554)\tnorm 0.9258 (0.9360)\n",
            "Epoch: [51][190/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7397 (0.7386)\tD(fake) 0.2208 (0.2392)\tgrad(D) penalty 0.0127 (0.0133)\tRec loss 4146.4614 (4124.9661)\tnorm 0.9399 (0.9361)\n",
            "Epoch: [52][  0/195]\tTime  0.387 ( 0.387)\tData  0.208 ( 0.208)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3922.9956 (3922.9956)\tnorm 0.9334 (0.9334)\n",
            "Epoch: [52][ 10/195]\tTime  0.148 ( 0.170)\tData  0.000 ( 0.019)\tD(real) 0.7399 (0.7399)\tD(fake) 0.2478 (0.2495)\tgrad(D) penalty 0.0134 (0.0122)\tRec loss 3824.5322 (4018.5926)\tnorm 0.9425 (0.9430)\n",
            "Epoch: [52][ 20/195]\tTime  0.148 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.7399 (0.7399)\tD(fake) 0.2309 (0.2387)\tgrad(D) penalty 0.0147 (0.0137)\tRec loss 3824.2417 (4041.9631)\tnorm 0.9350 (0.9374)\n",
            "Epoch: [52][ 30/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7401 (0.7399)\tD(fake) 0.2345 (0.2369)\tgrad(D) penalty 0.0141 (0.0135)\tRec loss 4425.4829 (4060.3278)\tnorm 0.9402 (0.9368)\n",
            "Epoch: [52][ 40/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7401 (0.7400)\tD(fake) 0.2421 (0.2387)\tgrad(D) penalty 0.0128 (0.0135)\tRec loss 4088.3201 (4086.7978)\tnorm 0.9436 (0.9369)\n",
            "Epoch: [52][ 50/195]\tTime  0.163 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7403 (0.7400)\tD(fake) 0.2290 (0.2380)\tgrad(D) penalty 0.0129 (0.0133)\tRec loss 3976.3555 (4088.9114)\tnorm 0.9190 (0.9359)\n",
            "Epoch: [52][ 60/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7402 (0.7401)\tD(fake) 0.2465 (0.2370)\tgrad(D) penalty 0.0114 (0.0136)\tRec loss 4120.3193 (4100.0858)\tnorm 0.9241 (0.9338)\n",
            "Epoch: [52][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7404 (0.7401)\tD(fake) 0.2393 (0.2382)\tgrad(D) penalty 0.0118 (0.0134)\tRec loss 4210.5298 (4101.3558)\tnorm 0.9429 (0.9346)\n",
            "Epoch: [52][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7406 (0.7402)\tD(fake) 0.2573 (0.2380)\tgrad(D) penalty 0.0141 (0.0136)\tRec loss 3918.6885 (4096.5672)\tnorm 0.9361 (0.9343)\n",
            "Epoch: [52][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7406 (0.7402)\tD(fake) 0.2480 (0.2396)\tgrad(D) penalty 0.0110 (0.0135)\tRec loss 4169.0884 (4103.2308)\tnorm 0.9250 (0.9341)\n",
            "Epoch: [52][100/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7403 (0.7403)\tD(fake) 0.2442 (0.2376)\tgrad(D) penalty 0.0130 (0.0140)\tRec loss 3918.4502 (4109.6740)\tnorm 0.9265 (0.9333)\n",
            "Epoch: [52][110/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7409 (0.7403)\tD(fake) 0.2397 (0.2380)\tgrad(D) penalty 0.0128 (0.0139)\tRec loss 4107.3188 (4117.0301)\tnorm 0.9439 (0.9335)\n",
            "Epoch: [52][120/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7409 (0.7404)\tD(fake) 0.2403 (0.2371)\tgrad(D) penalty 0.0141 (0.0139)\tRec loss 3755.2212 (4121.0642)\tnorm 0.9422 (0.9333)\n",
            "Epoch: [52][130/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7410 (0.7404)\tD(fake) 0.2482 (0.2380)\tgrad(D) penalty 0.0124 (0.0138)\tRec loss 4093.2576 (4115.9109)\tnorm 0.9397 (0.9338)\n",
            "Epoch: [52][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7411 (0.7405)\tD(fake) 0.2387 (0.2378)\tgrad(D) penalty 0.0131 (0.0139)\tRec loss 4225.2686 (4116.9561)\tnorm 0.9211 (0.9338)\n",
            "Epoch: [52][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7413 (0.7405)\tD(fake) 0.2399 (0.2382)\tgrad(D) penalty 0.0142 (0.0139)\tRec loss 3948.0876 (4120.0041)\tnorm 0.9281 (0.9338)\n",
            "Epoch: [52][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7412 (0.7405)\tD(fake) 0.2302 (0.2377)\tgrad(D) penalty 0.0148 (0.0140)\tRec loss 4089.3350 (4120.0173)\tnorm 0.9442 (0.9345)\n",
            "Epoch: [52][170/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7414 (0.7406)\tD(fake) 0.2302 (0.2372)\tgrad(D) penalty 0.0144 (0.0140)\tRec loss 3965.0737 (4118.4706)\tnorm 0.9312 (0.9346)\n",
            "Epoch: [52][180/195]\tTime  0.153 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7415 (0.7406)\tD(fake) 0.2308 (0.2375)\tgrad(D) penalty 0.0154 (0.0140)\tRec loss 4177.9551 (4118.0037)\tnorm 0.9414 (0.9347)\n",
            "Epoch: [52][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7416 (0.7407)\tD(fake) 0.2308 (0.2371)\tgrad(D) penalty 0.0118 (0.0140)\tRec loss 3831.0327 (4118.8114)\tnorm 0.9359 (0.9346)\n",
            "Epoch: [53][  0/195]\tTime  0.380 ( 0.380)\tData  0.202 ( 0.202)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3914.8914 (3914.8914)\tnorm 0.9440 (0.9440)\n",
            "Epoch: [53][ 10/195]\tTime  0.149 ( 0.178)\tData  0.000 ( 0.019)\tD(real) 0.7418 (0.7418)\tD(fake) 0.2405 (0.2379)\tgrad(D) penalty 0.0110 (0.0110)\tRec loss 4069.0776 (4136.8255)\tnorm 0.9297 (0.9347)\n",
            "Epoch: [53][ 20/195]\tTime  0.148 ( 0.164)\tData  0.000 ( 0.010)\tD(real) 0.7420 (0.7419)\tD(fake) 0.2484 (0.2395)\tgrad(D) penalty 0.0119 (0.0122)\tRec loss 4135.4292 (4133.4732)\tnorm 0.9273 (0.9354)\n",
            "Epoch: [53][ 30/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.7420 (0.7419)\tD(fake) 0.2294 (0.2380)\tgrad(D) penalty 0.0142 (0.0127)\tRec loss 4443.9795 (4140.4486)\tnorm 0.9388 (0.9347)\n",
            "Epoch: [53][ 40/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7421 (0.7420)\tD(fake) 0.2407 (0.2381)\tgrad(D) penalty 0.0135 (0.0131)\tRec loss 3905.8828 (4126.7252)\tnorm 0.9376 (0.9350)\n",
            "Epoch: [53][ 50/195]\tTime  0.173 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7422 (0.7420)\tD(fake) 0.2384 (0.2383)\tgrad(D) penalty 0.0158 (0.0134)\tRec loss 4359.9531 (4132.1668)\tnorm 0.9272 (0.9345)\n",
            "Epoch: [53][ 60/195]\tTime  0.153 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7422 (0.7420)\tD(fake) 0.2323 (0.2385)\tgrad(D) penalty 0.0115 (0.0134)\tRec loss 4092.3628 (4124.4030)\tnorm 0.9353 (0.9343)\n",
            "Epoch: [53][ 70/195]\tTime  0.154 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7424 (0.7421)\tD(fake) 0.2226 (0.2368)\tgrad(D) penalty 0.0135 (0.0134)\tRec loss 4000.5347 (4140.5548)\tnorm 0.9239 (0.9333)\n",
            "Epoch: [53][ 80/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7423 (0.7421)\tD(fake) 0.2480 (0.2388)\tgrad(D) penalty 0.0115 (0.0133)\tRec loss 4320.0518 (4129.8326)\tnorm 0.9333 (0.9337)\n",
            "Epoch: [53][ 90/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7426 (0.7422)\tD(fake) 0.2256 (0.2378)\tgrad(D) penalty 0.0159 (0.0134)\tRec loss 4194.0977 (4134.8634)\tnorm 0.9426 (0.9340)\n",
            "Epoch: [53][100/195]\tTime  0.164 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7428 (0.7422)\tD(fake) 0.2455 (0.2387)\tgrad(D) penalty 0.0141 (0.0137)\tRec loss 3912.8127 (4131.4763)\tnorm 0.9442 (0.9343)\n",
            "Epoch: [53][110/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7428 (0.7422)\tD(fake) 0.2279 (0.2382)\tgrad(D) penalty 0.0156 (0.0138)\tRec loss 3959.3491 (4130.9257)\tnorm 0.9244 (0.9340)\n",
            "Epoch: [53][120/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7429 (0.7423)\tD(fake) 0.2424 (0.2379)\tgrad(D) penalty 0.0138 (0.0139)\tRec loss 3942.0928 (4128.2447)\tnorm 0.9452 (0.9341)\n",
            "Epoch: [53][130/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7430 (0.7423)\tD(fake) 0.2330 (0.2379)\tgrad(D) penalty 0.0143 (0.0139)\tRec loss 4111.5820 (4124.8108)\tnorm 0.9264 (0.9339)\n",
            "Epoch: [53][140/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7431 (0.7424)\tD(fake) 0.2456 (0.2380)\tgrad(D) penalty 0.0125 (0.0139)\tRec loss 4260.0054 (4131.6285)\tnorm 0.9395 (0.9337)\n",
            "Epoch: [53][150/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7430 (0.7424)\tD(fake) 0.2353 (0.2380)\tgrad(D) penalty 0.0158 (0.0140)\tRec loss 4081.7273 (4128.5097)\tnorm 0.9408 (0.9338)\n",
            "Epoch: [53][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7431 (0.7425)\tD(fake) 0.2411 (0.2378)\tgrad(D) penalty 0.0142 (0.0141)\tRec loss 4228.8760 (4123.8980)\tnorm 0.9162 (0.9336)\n",
            "Epoch: [53][170/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7433 (0.7425)\tD(fake) 0.2363 (0.2378)\tgrad(D) penalty 0.0131 (0.0141)\tRec loss 4078.4790 (4121.2869)\tnorm 0.9305 (0.9336)\n",
            "Epoch: [53][180/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7433 (0.7426)\tD(fake) 0.2353 (0.2376)\tgrad(D) penalty 0.0138 (0.0141)\tRec loss 4201.1689 (4118.6466)\tnorm 0.9270 (0.9334)\n",
            "Epoch: [53][190/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7435 (0.7426)\tD(fake) 0.2376 (0.2375)\tgrad(D) penalty 0.0123 (0.0140)\tRec loss 4236.5571 (4119.2258)\tnorm 0.9377 (0.9332)\n",
            "Epoch: [54][  0/195]\tTime  0.399 ( 0.399)\tData  0.202 ( 0.202)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4221.0762 (4221.0762)\tnorm 0.9304 (0.9304)\n",
            "Epoch: [54][ 10/195]\tTime  0.152 ( 0.173)\tData  0.000 ( 0.019)\tD(real) 0.7437 (0.7436)\tD(fake) 0.2439 (0.2341)\tgrad(D) penalty 0.0135 (0.0138)\tRec loss 4157.4771 (4115.6736)\tnorm 0.9446 (0.9373)\n",
            "Epoch: [54][ 20/195]\tTime  0.151 ( 0.162)\tData  0.000 ( 0.010)\tD(real) 0.7438 (0.7437)\tD(fake) 0.2404 (0.2410)\tgrad(D) penalty 0.0128 (0.0138)\tRec loss 4174.7153 (4100.1515)\tnorm 0.9360 (0.9326)\n",
            "Epoch: [54][ 30/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.7438 (0.7437)\tD(fake) 0.2321 (0.2401)\tgrad(D) penalty 0.0136 (0.0134)\tRec loss 4140.1504 (4078.2353)\tnorm 0.9285 (0.9322)\n",
            "Epoch: [54][ 40/195]\tTime  0.150 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7438 (0.7438)\tD(fake) 0.2346 (0.2386)\tgrad(D) penalty 0.0128 (0.0135)\tRec loss 4049.1570 (4077.0999)\tnorm 0.9286 (0.9302)\n",
            "Epoch: [54][ 50/195]\tTime  0.168 ( 0.156)\tData  0.000 ( 0.004)\tD(real) 0.7440 (0.7438)\tD(fake) 0.2436 (0.2388)\tgrad(D) penalty 0.0140 (0.0134)\tRec loss 3933.2463 (4090.9648)\tnorm 0.9318 (0.9288)\n",
            "Epoch: [54][ 60/195]\tTime  0.150 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7443 (0.7439)\tD(fake) 0.2354 (0.2378)\tgrad(D) penalty 0.0119 (0.0132)\tRec loss 4255.8979 (4104.2612)\tnorm 0.9219 (0.9283)\n",
            "Epoch: [54][ 70/195]\tTime  0.154 ( 0.155)\tData  0.000 ( 0.003)\tD(real) 0.7443 (0.7439)\tD(fake) 0.2374 (0.2374)\tgrad(D) penalty 0.0144 (0.0133)\tRec loss 4049.2056 (4096.8727)\tnorm 0.9271 (0.9272)\n",
            "Epoch: [54][ 80/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7443 (0.7440)\tD(fake) 0.2438 (0.2393)\tgrad(D) penalty 0.0145 (0.0135)\tRec loss 3932.5674 (4092.0823)\tnorm 0.9060 (0.9265)\n",
            "Epoch: [54][ 90/195]\tTime  0.155 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7445 (0.7440)\tD(fake) 0.2299 (0.2386)\tgrad(D) penalty 0.0164 (0.0137)\tRec loss 4141.4160 (4089.0180)\tnorm 0.9156 (0.9261)\n",
            "Epoch: [54][100/195]\tTime  0.172 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7444 (0.7441)\tD(fake) 0.2488 (0.2398)\tgrad(D) penalty 0.0157 (0.0138)\tRec loss 4300.3379 (4092.6127)\tnorm 0.9383 (0.9258)\n",
            "Epoch: [54][110/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7445 (0.7441)\tD(fake) 0.2273 (0.2392)\tgrad(D) penalty 0.0139 (0.0137)\tRec loss 3979.1135 (4081.8828)\tnorm 0.9428 (0.9262)\n",
            "Epoch: [54][120/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7446 (0.7442)\tD(fake) 0.2384 (0.2384)\tgrad(D) penalty 0.0139 (0.0138)\tRec loss 4049.1472 (4084.6017)\tnorm 0.9226 (0.9264)\n",
            "Epoch: [54][130/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7448 (0.7442)\tD(fake) 0.2483 (0.2388)\tgrad(D) penalty 0.0132 (0.0138)\tRec loss 4372.3096 (4090.0301)\tnorm 0.9332 (0.9265)\n",
            "Epoch: [54][140/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7448 (0.7443)\tD(fake) 0.2251 (0.2382)\tgrad(D) penalty 0.0144 (0.0137)\tRec loss 4077.3762 (4092.9176)\tnorm 0.9291 (0.9267)\n",
            "Epoch: [54][150/195]\tTime  0.174 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7449 (0.7443)\tD(fake) 0.2348 (0.2378)\tgrad(D) penalty 0.0131 (0.0137)\tRec loss 3979.6914 (4099.1597)\tnorm 0.9358 (0.9271)\n",
            "Epoch: [54][160/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7451 (0.7443)\tD(fake) 0.2226 (0.2371)\tgrad(D) penalty 0.0193 (0.0138)\tRec loss 4101.8975 (4104.4129)\tnorm 0.9373 (0.9276)\n",
            "Epoch: [54][170/195]\tTime  0.154 ( 0.152)\tData  0.000 ( 0.001)\tD(real) 0.7452 (0.7444)\tD(fake) 0.2544 (0.2377)\tgrad(D) penalty 0.0149 (0.0138)\tRec loss 4237.4307 (4105.9640)\tnorm 0.9493 (0.9286)\n",
            "Epoch: [54][180/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.001)\tD(real) 0.7453 (0.7444)\tD(fake) 0.2313 (0.2379)\tgrad(D) penalty 0.0154 (0.0139)\tRec loss 4045.6111 (4108.6629)\tnorm 0.9355 (0.9292)\n",
            "Epoch: [54][190/195]\tTime  0.154 ( 0.152)\tData  0.000 ( 0.001)\tD(real) 0.7452 (0.7445)\tD(fake) 0.2311 (0.2376)\tgrad(D) penalty 0.0127 (0.0138)\tRec loss 4254.0127 (4110.2024)\tnorm 0.9428 (0.9299)\n",
            "Epoch: [55][  0/195]\tTime  0.426 ( 0.426)\tData  0.227 ( 0.227)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4054.2737 (4054.2737)\tnorm 0.9156 (0.9156)\n",
            "Epoch: [55][ 10/195]\tTime  0.150 ( 0.176)\tData  0.000 ( 0.021)\tD(real) 0.7455 (0.7454)\tD(fake) 0.2641 (0.2447)\tgrad(D) penalty 0.0112 (0.0142)\tRec loss 4098.1406 (4125.6624)\tnorm 0.9290 (0.9295)\n",
            "Epoch: [55][ 20/195]\tTime  0.149 ( 0.164)\tData  0.000 ( 0.011)\tD(real) 0.7456 (0.7455)\tD(fake) 0.2201 (0.2456)\tgrad(D) penalty 0.0151 (0.0134)\tRec loss 4232.3267 (4106.2787)\tnorm 0.9263 (0.9288)\n",
            "Epoch: [55][ 30/195]\tTime  0.149 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7457 (0.7455)\tD(fake) 0.2329 (0.2391)\tgrad(D) penalty 0.0142 (0.0140)\tRec loss 3892.6157 (4112.2619)\tnorm 0.9350 (0.9282)\n",
            "Epoch: [55][ 40/195]\tTime  0.151 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7458 (0.7456)\tD(fake) 0.2421 (0.2409)\tgrad(D) penalty 0.0117 (0.0137)\tRec loss 3857.4548 (4105.0922)\tnorm 0.9278 (0.9283)\n",
            "Epoch: [55][ 50/195]\tTime  0.168 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7458 (0.7456)\tD(fake) 0.2357 (0.2398)\tgrad(D) penalty 0.0116 (0.0137)\tRec loss 4051.3140 (4120.1808)\tnorm 0.9174 (0.9269)\n",
            "Epoch: [55][ 60/195]\tTime  0.149 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7459 (0.7457)\tD(fake) 0.2228 (0.2370)\tgrad(D) penalty 0.0143 (0.0139)\tRec loss 4039.3059 (4117.8685)\tnorm 0.9278 (0.9280)\n",
            "Epoch: [55][ 70/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7461 (0.7457)\tD(fake) 0.2488 (0.2377)\tgrad(D) penalty 0.0128 (0.0141)\tRec loss 4070.3398 (4116.2565)\tnorm 0.9234 (0.9278)\n",
            "Epoch: [55][ 80/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7460 (0.7458)\tD(fake) 0.2363 (0.2386)\tgrad(D) penalty 0.0155 (0.0141)\tRec loss 3967.6772 (4113.0477)\tnorm 0.9177 (0.9279)\n",
            "Epoch: [55][ 90/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7462 (0.7458)\tD(fake) 0.2329 (0.2378)\tgrad(D) penalty 0.0134 (0.0141)\tRec loss 3902.7744 (4114.2862)\tnorm 0.9433 (0.9282)\n",
            "Epoch: [55][100/195]\tTime  0.166 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7462 (0.7458)\tD(fake) 0.2456 (0.2390)\tgrad(D) penalty 0.0141 (0.0142)\tRec loss 4383.3457 (4121.9220)\tnorm 0.9285 (0.9287)\n",
            "Epoch: [55][110/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7461 (0.7459)\tD(fake) 0.2290 (0.2385)\tgrad(D) penalty 0.0185 (0.0144)\tRec loss 4161.7056 (4118.8598)\tnorm 0.9379 (0.9291)\n",
            "Epoch: [55][120/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7463 (0.7459)\tD(fake) 0.2531 (0.2392)\tgrad(D) penalty 0.0114 (0.0144)\tRec loss 4719.8853 (4116.6371)\tnorm 0.9263 (0.9296)\n",
            "Epoch: [55][130/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7463 (0.7459)\tD(fake) 0.2329 (0.2389)\tgrad(D) penalty 0.0123 (0.0143)\tRec loss 4030.1802 (4116.2639)\tnorm 0.9375 (0.9300)\n",
            "Epoch: [55][140/195]\tTime  0.153 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7465 (0.7460)\tD(fake) 0.2393 (0.2389)\tgrad(D) penalty 0.0139 (0.0143)\tRec loss 4205.3901 (4111.7284)\tnorm 0.9323 (0.9307)\n",
            "Epoch: [55][150/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7465 (0.7460)\tD(fake) 0.2311 (0.2388)\tgrad(D) penalty 0.0147 (0.0142)\tRec loss 4094.8315 (4110.4026)\tnorm 0.9381 (0.9311)\n",
            "Epoch: [55][160/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7465 (0.7461)\tD(fake) 0.2529 (0.2386)\tgrad(D) penalty 0.0133 (0.0143)\tRec loss 4109.9468 (4108.0989)\tnorm 0.9300 (0.9312)\n",
            "Epoch: [55][170/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7466 (0.7461)\tD(fake) 0.2393 (0.2388)\tgrad(D) penalty 0.0139 (0.0142)\tRec loss 4112.8799 (4109.1778)\tnorm 0.9300 (0.9311)\n",
            "Epoch: [55][180/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7467 (0.7461)\tD(fake) 0.2408 (0.2387)\tgrad(D) penalty 0.0142 (0.0143)\tRec loss 4343.2881 (4107.8905)\tnorm 0.9264 (0.9309)\n",
            "Epoch: [55][190/195]\tTime  0.155 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7468 (0.7462)\tD(fake) 0.2209 (0.2382)\tgrad(D) penalty 0.0139 (0.0143)\tRec loss 4362.8989 (4109.2740)\tnorm 0.9336 (0.9308)\n",
            "Epoch: [56][  0/195]\tTime  0.435 ( 0.435)\tData  0.236 ( 0.236)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3998.8579 (3998.8579)\tnorm 0.9353 (0.9353)\n",
            "Epoch: [56][ 10/195]\tTime  0.150 ( 0.178)\tData  0.000 ( 0.022)\tD(real) 0.7470 (0.7470)\tD(fake) 0.2493 (0.2448)\tgrad(D) penalty 0.0163 (0.0170)\tRec loss 4529.4336 (4131.7408)\tnorm 0.9198 (0.9279)\n",
            "Epoch: [56][ 20/195]\tTime  0.151 ( 0.165)\tData  0.000 ( 0.012)\tD(real) 0.7469 (0.7470)\tD(fake) 0.2418 (0.2415)\tgrad(D) penalty 0.0146 (0.0160)\tRec loss 4326.0972 (4098.1866)\tnorm 0.9134 (0.9224)\n",
            "Epoch: [56][ 30/195]\tTime  0.153 ( 0.161)\tData  0.000 ( 0.008)\tD(real) 0.7472 (0.7471)\tD(fake) 0.2355 (0.2413)\tgrad(D) penalty 0.0139 (0.0157)\tRec loss 3961.3840 (4095.4774)\tnorm 0.9006 (0.9226)\n",
            "Epoch: [56][ 40/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.006)\tD(real) 0.7471 (0.7471)\tD(fake) 0.2480 (0.2410)\tgrad(D) penalty 0.0135 (0.0157)\tRec loss 4156.6123 (4080.5269)\tnorm 0.9327 (0.9236)\n",
            "Epoch: [56][ 50/195]\tTime  0.168 ( 0.157)\tData  0.000 ( 0.005)\tD(real) 0.7472 (0.7471)\tD(fake) 0.2259 (0.2401)\tgrad(D) penalty 0.0154 (0.0155)\tRec loss 4258.4443 (4092.0786)\tnorm 0.9365 (0.9234)\n",
            "Epoch: [56][ 60/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.004)\tD(real) 0.7473 (0.7472)\tD(fake) 0.2381 (0.2398)\tgrad(D) penalty 0.0143 (0.0154)\tRec loss 4064.8232 (4106.0623)\tnorm 0.9174 (0.9241)\n",
            "Epoch: [56][ 70/195]\tTime  0.151 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7471 (0.7472)\tD(fake) 0.2241 (0.2385)\tgrad(D) penalty 0.0139 (0.0155)\tRec loss 3972.2349 (4095.0044)\tnorm 0.9348 (0.9240)\n",
            "Epoch: [56][ 80/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7475 (0.7472)\tD(fake) 0.2319 (0.2378)\tgrad(D) penalty 0.0166 (0.0155)\tRec loss 4329.7969 (4096.6928)\tnorm 0.9149 (0.9243)\n",
            "Epoch: [56][ 90/195]\tTime  0.156 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7475 (0.7472)\tD(fake) 0.2430 (0.2381)\tgrad(D) penalty 0.0134 (0.0153)\tRec loss 4165.6606 (4096.5499)\tnorm 0.9164 (0.9240)\n",
            "Epoch: [56][100/195]\tTime  0.166 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7476 (0.7473)\tD(fake) 0.2337 (0.2389)\tgrad(D) penalty 0.0118 (0.0151)\tRec loss 4218.7041 (4101.4031)\tnorm 0.9245 (0.9234)\n",
            "Epoch: [56][110/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7476 (0.7473)\tD(fake) 0.2364 (0.2384)\tgrad(D) penalty 0.0158 (0.0152)\tRec loss 4289.2197 (4104.2995)\tnorm 0.9228 (0.9232)\n",
            "Epoch: [56][120/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7478 (0.7473)\tD(fake) 0.2502 (0.2398)\tgrad(D) penalty 0.0165 (0.0152)\tRec loss 4033.2107 (4104.6702)\tnorm 0.9307 (0.9235)\n",
            "Epoch: [56][130/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7478 (0.7474)\tD(fake) 0.2363 (0.2395)\tgrad(D) penalty 0.0145 (0.0151)\tRec loss 4225.5552 (4109.4951)\tnorm 0.9339 (0.9242)\n",
            "Epoch: [56][140/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7479 (0.7474)\tD(fake) 0.2320 (0.2395)\tgrad(D) penalty 0.0143 (0.0152)\tRec loss 3932.9805 (4104.5554)\tnorm 0.9469 (0.9249)\n",
            "Epoch: [56][150/195]\tTime  0.168 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7479 (0.7474)\tD(fake) 0.2457 (0.2396)\tgrad(D) penalty 0.0132 (0.0152)\tRec loss 3965.0002 (4104.2570)\tnorm 0.9286 (0.9253)\n",
            "Epoch: [56][160/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7479 (0.7475)\tD(fake) 0.2320 (0.2392)\tgrad(D) penalty 0.0205 (0.0153)\tRec loss 3885.9395 (4103.6483)\tnorm 0.9305 (0.9258)\n",
            "Epoch: [56][170/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7481 (0.7475)\tD(fake) 0.2442 (0.2395)\tgrad(D) penalty 0.0153 (0.0152)\tRec loss 3917.2876 (4101.4790)\tnorm 0.9422 (0.9264)\n",
            "Epoch: [56][180/195]\tTime  0.153 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7480 (0.7475)\tD(fake) 0.2295 (0.2390)\tgrad(D) penalty 0.0147 (0.0152)\tRec loss 3884.5078 (4100.4538)\tnorm 0.9347 (0.9265)\n",
            "Epoch: [56][190/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7481 (0.7476)\tD(fake) 0.2558 (0.2393)\tgrad(D) penalty 0.0159 (0.0152)\tRec loss 4060.4663 (4103.2563)\tnorm 0.9408 (0.9271)\n",
            "Epoch: [57][  0/195]\tTime  0.425 ( 0.425)\tData  0.228 ( 0.228)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4191.2104 (4191.2104)\tnorm 0.9342 (0.9342)\n",
            "Epoch: [57][ 10/195]\tTime  0.154 ( 0.178)\tData  0.000 ( 0.021)\tD(real) 0.7483 (0.7483)\tD(fake) 0.2398 (0.2334)\tgrad(D) penalty 0.0142 (0.0165)\tRec loss 3995.6558 (4169.3131)\tnorm 0.9346 (0.9267)\n",
            "Epoch: [57][ 20/195]\tTime  0.149 ( 0.165)\tData  0.000 ( 0.011)\tD(real) 0.7483 (0.7483)\tD(fake) 0.2346 (0.2397)\tgrad(D) penalty 0.0157 (0.0150)\tRec loss 4300.1826 (4125.9519)\tnorm 0.9304 (0.9321)\n",
            "Epoch: [57][ 30/195]\tTime  0.150 ( 0.160)\tData  0.000 ( 0.008)\tD(real) 0.7484 (0.7483)\tD(fake) 0.2436 (0.2383)\tgrad(D) penalty 0.0152 (0.0152)\tRec loss 3891.8242 (4106.2785)\tnorm 0.9451 (0.9342)\n",
            "Epoch: [57][ 40/195]\tTime  0.149 ( 0.158)\tData  0.000 ( 0.006)\tD(real) 0.7484 (0.7483)\tD(fake) 0.2328 (0.2400)\tgrad(D) penalty 0.0166 (0.0151)\tRec loss 4071.7239 (4121.6006)\tnorm 0.9223 (0.9331)\n",
            "Epoch: [57][ 50/195]\tTime  0.169 ( 0.157)\tData  0.000 ( 0.005)\tD(real) 0.7485 (0.7484)\tD(fake) 0.2272 (0.2373)\tgrad(D) penalty 0.0166 (0.0154)\tRec loss 4044.2456 (4100.4979)\tnorm 0.9346 (0.9331)\n",
            "Epoch: [57][ 60/195]\tTime  0.151 ( 0.156)\tData  0.000 ( 0.004)\tD(real) 0.7487 (0.7484)\tD(fake) 0.2586 (0.2406)\tgrad(D) penalty 0.0135 (0.0153)\tRec loss 4197.1494 (4105.8695)\tnorm 0.9296 (0.9329)\n",
            "Epoch: [57][ 70/195]\tTime  0.154 ( 0.156)\tData  0.000 ( 0.003)\tD(real) 0.7486 (0.7484)\tD(fake) 0.2345 (0.2408)\tgrad(D) penalty 0.0157 (0.0153)\tRec loss 4071.5623 (4102.8195)\tnorm 0.9381 (0.9334)\n",
            "Epoch: [57][ 80/195]\tTime  0.152 ( 0.155)\tData  0.000 ( 0.003)\tD(real) 0.7487 (0.7484)\tD(fake) 0.2435 (0.2401)\tgrad(D) penalty 0.0152 (0.0154)\tRec loss 4281.5249 (4106.5915)\tnorm 0.9289 (0.9334)\n",
            "Epoch: [57][ 90/195]\tTime  0.154 ( 0.155)\tData  0.000 ( 0.003)\tD(real) 0.7486 (0.7484)\tD(fake) 0.2352 (0.2399)\tgrad(D) penalty 0.0152 (0.0153)\tRec loss 4181.3745 (4107.9288)\tnorm 0.9357 (0.9336)\n",
            "Epoch: [57][100/195]\tTime  0.166 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7487 (0.7485)\tD(fake) 0.2563 (0.2403)\tgrad(D) penalty 0.0185 (0.0157)\tRec loss 4242.1445 (4105.1830)\tnorm 0.9388 (0.9343)\n",
            "Epoch: [57][110/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.002)\tD(real) 0.7488 (0.7485)\tD(fake) 0.2355 (0.2408)\tgrad(D) penalty 0.0159 (0.0156)\tRec loss 4173.6162 (4109.0990)\tnorm 0.9233 (0.9333)\n",
            "Epoch: [57][120/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.002)\tD(real) 0.7489 (0.7485)\tD(fake) 0.2457 (0.2401)\tgrad(D) penalty 0.0185 (0.0158)\tRec loss 3899.3418 (4103.2973)\tnorm 0.9322 (0.9330)\n",
            "Epoch: [57][130/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7489 (0.7486)\tD(fake) 0.2440 (0.2408)\tgrad(D) penalty 0.0167 (0.0158)\tRec loss 3992.2393 (4101.4960)\tnorm 0.9345 (0.9323)\n",
            "Epoch: [57][140/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7490 (0.7486)\tD(fake) 0.2345 (0.2402)\tgrad(D) penalty 0.0166 (0.0161)\tRec loss 4113.9238 (4103.6250)\tnorm 0.9095 (0.9313)\n",
            "Epoch: [57][150/195]\tTime  0.168 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7488 (0.7486)\tD(fake) 0.2412 (0.2400)\tgrad(D) penalty 0.0155 (0.0161)\tRec loss 4156.4805 (4103.6928)\tnorm 0.9198 (0.9302)\n",
            "Epoch: [57][160/195]\tTime  0.154 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7489 (0.7486)\tD(fake) 0.2476 (0.2403)\tgrad(D) penalty 0.0185 (0.0162)\tRec loss 4053.8662 (4104.1954)\tnorm 0.9350 (0.9299)\n",
            "Epoch: [57][170/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7490 (0.7486)\tD(fake) 0.2334 (0.2403)\tgrad(D) penalty 0.0151 (0.0162)\tRec loss 3786.2617 (4098.5185)\tnorm 0.9269 (0.9295)\n",
            "Epoch: [57][180/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7491 (0.7487)\tD(fake) 0.2429 (0.2400)\tgrad(D) penalty 0.0148 (0.0163)\tRec loss 4321.4268 (4101.4026)\tnorm 0.9164 (0.9288)\n",
            "Epoch: [57][190/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.001)\tD(real) 0.7490 (0.7487)\tD(fake) 0.2312 (0.2399)\tgrad(D) penalty 0.0188 (0.0163)\tRec loss 4138.0342 (4102.4850)\tnorm 0.9212 (0.9286)\n",
            "Epoch: [58][  0/195]\tTime  0.406 ( 0.406)\tData  0.203 ( 0.203)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4115.8115 (4115.8115)\tnorm 0.9449 (0.9449)\n",
            "Epoch: [58][ 10/195]\tTime  0.157 ( 0.176)\tData  0.000 ( 0.019)\tD(real) 0.7492 (0.7491)\tD(fake) 0.2558 (0.2447)\tgrad(D) penalty 0.0172 (0.0173)\tRec loss 4103.3491 (4098.3865)\tnorm 0.9362 (0.9299)\n",
            "Epoch: [58][ 20/195]\tTime  0.151 ( 0.164)\tData  0.000 ( 0.010)\tD(real) 0.7492 (0.7492)\tD(fake) 0.2369 (0.2470)\tgrad(D) penalty 0.0181 (0.0169)\tRec loss 4198.2803 (4103.6162)\tnorm 0.9186 (0.9281)\n",
            "Epoch: [58][ 30/195]\tTime  0.151 ( 0.160)\tData  0.000 ( 0.007)\tD(real) 0.7491 (0.7492)\tD(fake) 0.2357 (0.2432)\tgrad(D) penalty 0.0204 (0.0175)\tRec loss 4069.7656 (4081.6927)\tnorm 0.9169 (0.9268)\n",
            "Epoch: [58][ 40/195]\tTime  0.151 ( 0.157)\tData  0.000 ( 0.005)\tD(real) 0.7493 (0.7492)\tD(fake) 0.2583 (0.2476)\tgrad(D) penalty 0.0163 (0.0175)\tRec loss 4248.9287 (4075.3934)\tnorm 0.9203 (0.9248)\n",
            "Epoch: [58][ 50/195]\tTime  0.168 ( 0.156)\tData  0.000 ( 0.004)\tD(real) 0.7492 (0.7492)\tD(fake) 0.2421 (0.2467)\tgrad(D) penalty 0.0158 (0.0173)\tRec loss 3993.7080 (4075.2324)\tnorm 0.9143 (0.9241)\n",
            "Epoch: [58][ 60/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7493 (0.7492)\tD(fake) 0.2373 (0.2435)\tgrad(D) penalty 0.0182 (0.0173)\tRec loss 3984.7642 (4073.2856)\tnorm 0.9135 (0.9229)\n",
            "Epoch: [58][ 70/195]\tTime  0.150 ( 0.155)\tData  0.000 ( 0.003)\tD(real) 0.7493 (0.7492)\tD(fake) 0.2540 (0.2447)\tgrad(D) penalty 0.0170 (0.0173)\tRec loss 4113.1230 (4072.7341)\tnorm 0.9235 (0.9236)\n",
            "Epoch: [58][ 80/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7493 (0.7493)\tD(fake) 0.2375 (0.2450)\tgrad(D) penalty 0.0176 (0.0173)\tRec loss 4104.3638 (4075.8744)\tnorm 0.9117 (0.9236)\n",
            "Epoch: [58][ 90/195]\tTime  0.154 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7493 (0.7493)\tD(fake) 0.2422 (0.2443)\tgrad(D) penalty 0.0211 (0.0175)\tRec loss 4296.1084 (4083.9080)\tnorm 0.9358 (0.9243)\n",
            "Epoch: [58][100/195]\tTime  0.166 ( 0.154)\tData  0.000 ( 0.002)\tD(real) 0.7494 (0.7493)\tD(fake) 0.2526 (0.2447)\tgrad(D) penalty 0.0196 (0.0175)\tRec loss 4294.3398 (4087.4933)\tnorm 0.9217 (0.9246)\n",
            "Epoch: [58][110/195]\tTime  0.153 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7493 (0.7493)\tD(fake) 0.2345 (0.2445)\tgrad(D) penalty 0.0182 (0.0176)\tRec loss 4384.4229 (4090.9338)\tnorm 0.9107 (0.9241)\n",
            "Epoch: [58][120/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7495 (0.7493)\tD(fake) 0.2424 (0.2440)\tgrad(D) penalty 0.0177 (0.0176)\tRec loss 4231.0962 (4099.1406)\tnorm 0.9266 (0.9239)\n",
            "Epoch: [58][130/195]\tTime  0.154 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7493 (0.7493)\tD(fake) 0.2342 (0.2436)\tgrad(D) penalty 0.0173 (0.0175)\tRec loss 4214.1436 (4099.3300)\tnorm 0.9326 (0.9239)\n",
            "Epoch: [58][140/195]\tTime  0.157 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7495 (0.7493)\tD(fake) 0.2368 (0.2435)\tgrad(D) penalty 0.0168 (0.0176)\tRec loss 3952.3467 (4095.7620)\tnorm 0.9299 (0.9240)\n",
            "Epoch: [58][150/195]\tTime  0.166 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7495 (0.7493)\tD(fake) 0.2335 (0.2429)\tgrad(D) penalty 0.0162 (0.0177)\tRec loss 4117.0610 (4099.5388)\tnorm 0.9102 (0.9238)\n",
            "Epoch: [58][160/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7494 (0.7493)\tD(fake) 0.2228 (0.2421)\tgrad(D) penalty 0.0168 (0.0177)\tRec loss 3954.7366 (4097.3853)\tnorm 0.9182 (0.9236)\n",
            "Epoch: [58][170/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.001)\tD(real) 0.7495 (0.7493)\tD(fake) 0.2536 (0.2422)\tgrad(D) penalty 0.0196 (0.0177)\tRec loss 3990.5044 (4096.8859)\tnorm 0.9319 (0.9240)\n",
            "Epoch: [58][180/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.001)\tD(real) 0.7496 (0.7494)\tD(fake) 0.2403 (0.2426)\tgrad(D) penalty 0.0178 (0.0177)\tRec loss 4198.4619 (4101.4693)\tnorm 0.9411 (0.9243)\n",
            "Epoch: [58][190/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.001)\tD(real) 0.7496 (0.7494)\tD(fake) 0.2240 (0.2419)\tgrad(D) penalty 0.0188 (0.0178)\tRec loss 4032.6780 (4096.4002)\tnorm 0.9349 (0.9246)\n",
            "Epoch: [59][  0/195]\tTime  0.420 ( 0.420)\tData  0.213 ( 0.213)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4174.0264 (4174.0264)\tnorm 0.9334 (0.9334)\n",
            "Epoch: [59][ 10/195]\tTime  0.149 ( 0.173)\tData  0.000 ( 0.020)\tD(real) 0.7497 (0.7497)\tD(fake) 0.2555 (0.2516)\tgrad(D) penalty 0.0184 (0.0178)\tRec loss 4616.2451 (4088.6420)\tnorm 0.9352 (0.9293)\n",
            "Epoch: [59][ 20/195]\tTime  0.155 ( 0.163)\tData  0.000 ( 0.010)\tD(real) 0.7498 (0.7497)\tD(fake) 0.2321 (0.2438)\tgrad(D) penalty 0.0178 (0.0173)\tRec loss 3949.7437 (4033.0138)\tnorm 0.9299 (0.9282)\n",
            "Epoch: [59][ 30/195]\tTime  0.146 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.7497 (0.7497)\tD(fake) 0.2337 (0.2405)\tgrad(D) penalty 0.0213 (0.0178)\tRec loss 4262.0068 (4069.3126)\tnorm 0.9269 (0.9270)\n",
            "Epoch: [59][ 40/195]\tTime  0.150 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7499 (0.7497)\tD(fake) 0.2551 (0.2447)\tgrad(D) penalty 0.0174 (0.0183)\tRec loss 3974.0708 (4071.3652)\tnorm 0.9238 (0.9260)\n",
            "Epoch: [59][ 50/195]\tTime  0.169 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7497 (0.7497)\tD(fake) 0.2425 (0.2448)\tgrad(D) penalty 0.0224 (0.0188)\tRec loss 4421.7427 (4079.7443)\tnorm 0.9280 (0.9258)\n",
            "Epoch: [59][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7499 (0.7498)\tD(fake) 0.2317 (0.2427)\tgrad(D) penalty 0.0193 (0.0189)\tRec loss 3785.9663 (4069.3613)\tnorm 0.9146 (0.9257)\n",
            "Epoch: [59][ 70/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7499 (0.7498)\tD(fake) 0.2386 (0.2416)\tgrad(D) penalty 0.0229 (0.0192)\tRec loss 4440.2524 (4056.1570)\tnorm 0.9019 (0.9242)\n",
            "Epoch: [59][ 80/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7500 (0.7498)\tD(fake) 0.2407 (0.2422)\tgrad(D) penalty 0.0162 (0.0192)\tRec loss 4002.5991 (4067.6452)\tnorm 0.9270 (0.9234)\n",
            "Epoch: [59][ 90/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7499 (0.7498)\tD(fake) 0.2389 (0.2419)\tgrad(D) penalty 0.0208 (0.0193)\tRec loss 4162.7935 (4080.8095)\tnorm 0.9217 (0.9236)\n",
            "Epoch: [59][100/195]\tTime  0.166 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7500 (0.7498)\tD(fake) 0.2285 (0.2413)\tgrad(D) penalty 0.0196 (0.0192)\tRec loss 3959.8159 (4085.6770)\tnorm 0.9379 (0.9246)\n",
            "Epoch: [59][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7500 (0.7498)\tD(fake) 0.2550 (0.2417)\tgrad(D) penalty 0.0176 (0.0192)\tRec loss 4141.0303 (4087.5330)\tnorm 0.9233 (0.9240)\n",
            "Epoch: [59][120/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7501 (0.7498)\tD(fake) 0.2342 (0.2419)\tgrad(D) penalty 0.0191 (0.0191)\tRec loss 4138.4600 (4089.9435)\tnorm 0.9237 (0.9245)\n",
            "Epoch: [59][130/195]\tTime  0.144 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7501 (0.7499)\tD(fake) 0.2384 (0.2416)\tgrad(D) penalty 0.0163 (0.0191)\tRec loss 4061.2290 (4098.7048)\tnorm 0.9285 (0.9244)\n",
            "Epoch: [59][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7499 (0.7499)\tD(fake) 0.2538 (0.2422)\tgrad(D) penalty 0.0171 (0.0190)\tRec loss 3999.4038 (4101.0288)\tnorm 0.9264 (0.9247)\n",
            "Epoch: [59][150/195]\tTime  0.161 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7500 (0.7499)\tD(fake) 0.2271 (0.2417)\tgrad(D) penalty 0.0194 (0.0190)\tRec loss 3939.2188 (4099.6941)\tnorm 0.9218 (0.9250)\n",
            "Epoch: [59][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7501 (0.7499)\tD(fake) 0.2540 (0.2418)\tgrad(D) penalty 0.0189 (0.0190)\tRec loss 4040.6050 (4095.3560)\tnorm 0.9148 (0.9249)\n",
            "Epoch: [59][170/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7500 (0.7499)\tD(fake) 0.2483 (0.2422)\tgrad(D) penalty 0.0186 (0.0191)\tRec loss 4078.5791 (4091.9835)\tnorm 0.9283 (0.9248)\n",
            "Epoch: [59][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7502 (0.7499)\tD(fake) 0.2488 (0.2425)\tgrad(D) penalty 0.0199 (0.0191)\tRec loss 4154.0850 (4093.2040)\tnorm 0.9112 (0.9244)\n",
            "Epoch: [59][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7502 (0.7499)\tD(fake) 0.2472 (0.2427)\tgrad(D) penalty 0.0211 (0.0192)\tRec loss 4099.7627 (4092.7684)\tnorm 0.9128 (0.9242)\n",
            "Epoch: [60][  0/195]\tTime  0.399 ( 0.399)\tData  0.206 ( 0.206)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3906.2188 (3906.2188)\tnorm 0.9345 (0.9345)\n",
            "Epoch: [60][ 10/195]\tTime  0.149 ( 0.173)\tData  0.000 ( 0.019)\tD(real) 0.7501 (0.7501)\tD(fake) 0.2481 (0.2464)\tgrad(D) penalty 0.0174 (0.0178)\tRec loss 3678.7305 (4018.7178)\tnorm 0.8961 (0.9181)\n",
            "Epoch: [60][ 20/195]\tTime  0.148 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7503 (0.7502)\tD(fake) 0.2445 (0.2435)\tgrad(D) penalty 0.0236 (0.0201)\tRec loss 3711.5515 (4046.5462)\tnorm 0.9153 (0.9167)\n",
            "Epoch: [60][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7501 (0.7501)\tD(fake) 0.2389 (0.2433)\tgrad(D) penalty 0.0175 (0.0194)\tRec loss 4020.5352 (4066.2198)\tnorm 0.9272 (0.9166)\n",
            "Epoch: [60][ 40/195]\tTime  0.150 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7502 (0.7501)\tD(fake) 0.2425 (0.2422)\tgrad(D) penalty 0.0197 (0.0191)\tRec loss 3954.0454 (4070.6128)\tnorm 0.9157 (0.9168)\n",
            "Epoch: [60][ 50/195]\tTime  0.164 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7502 (0.7502)\tD(fake) 0.2356 (0.2410)\tgrad(D) penalty 0.0209 (0.0194)\tRec loss 4206.0859 (4088.0500)\tnorm 0.9246 (0.9185)\n",
            "Epoch: [60][ 60/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7503 (0.7502)\tD(fake) 0.2491 (0.2424)\tgrad(D) penalty 0.0185 (0.0195)\tRec loss 3910.6008 (4086.5009)\tnorm 0.9162 (0.9192)\n",
            "Epoch: [60][ 70/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7502 (0.7502)\tD(fake) 0.2194 (0.2406)\tgrad(D) penalty 0.0189 (0.0195)\tRec loss 4195.3149 (4088.3291)\tnorm 0.9236 (0.9197)\n",
            "Epoch: [60][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7503 (0.7502)\tD(fake) 0.2514 (0.2400)\tgrad(D) penalty 0.0185 (0.0198)\tRec loss 4017.0337 (4104.4090)\tnorm 0.9296 (0.9207)\n",
            "Epoch: [60][ 90/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7503 (0.7502)\tD(fake) 0.2237 (0.2396)\tgrad(D) penalty 0.0172 (0.0195)\tRec loss 4069.4219 (4097.3526)\tnorm 0.8988 (0.9215)\n",
            "Epoch: [60][100/195]\tTime  0.173 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7504 (0.7502)\tD(fake) 0.2534 (0.2392)\tgrad(D) penalty 0.0199 (0.0196)\tRec loss 4067.1909 (4091.9309)\tnorm 0.9164 (0.9213)\n",
            "Epoch: [60][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7503 (0.7502)\tD(fake) 0.2484 (0.2405)\tgrad(D) penalty 0.0177 (0.0194)\tRec loss 4167.6172 (4094.2103)\tnorm 0.9155 (0.9211)\n",
            "Epoch: [60][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7504 (0.7502)\tD(fake) 0.2480 (0.2403)\tgrad(D) penalty 0.0222 (0.0198)\tRec loss 4091.2397 (4094.3605)\tnorm 0.9321 (0.9212)\n",
            "Epoch: [60][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7503 (0.7503)\tD(fake) 0.2563 (0.2414)\tgrad(D) penalty 0.0191 (0.0197)\tRec loss 4111.3184 (4090.0963)\tnorm 0.9293 (0.9209)\n",
            "Epoch: [60][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7503 (0.7503)\tD(fake) 0.2282 (0.2403)\tgrad(D) penalty 0.0183 (0.0197)\tRec loss 4111.5020 (4096.1792)\tnorm 0.9058 (0.9203)\n",
            "Epoch: [60][150/195]\tTime  0.163 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7505 (0.7503)\tD(fake) 0.2692 (0.2413)\tgrad(D) penalty 0.0183 (0.0197)\tRec loss 3959.9756 (4095.7129)\tnorm 0.9334 (0.9204)\n",
            "Epoch: [60][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7504 (0.7503)\tD(fake) 0.2306 (0.2419)\tgrad(D) penalty 0.0239 (0.0198)\tRec loss 4394.4277 (4097.5631)\tnorm 0.9114 (0.9201)\n",
            "Epoch: [60][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7504 (0.7503)\tD(fake) 0.2385 (0.2415)\tgrad(D) penalty 0.0177 (0.0198)\tRec loss 4175.0732 (4095.5790)\tnorm 0.9163 (0.9199)\n",
            "Epoch: [60][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7504 (0.7503)\tD(fake) 0.2397 (0.2418)\tgrad(D) penalty 0.0177 (0.0197)\tRec loss 4052.1199 (4093.0540)\tnorm 0.8927 (0.9196)\n",
            "Epoch: [60][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7505 (0.7503)\tD(fake) 0.2209 (0.2411)\tgrad(D) penalty 0.0201 (0.0196)\tRec loss 4478.6577 (4090.1451)\tnorm 0.9219 (0.9192)\n",
            "Epoch: [61][  0/195]\tTime  0.415 ( 0.415)\tData  0.210 ( 0.210)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4119.8672 (4119.8672)\tnorm 0.9295 (0.9295)\n",
            "Epoch: [61][ 10/195]\tTime  0.147 ( 0.174)\tData  0.000 ( 0.019)\tD(real) 0.7506 (0.7505)\tD(fake) 0.2537 (0.2426)\tgrad(D) penalty 0.0177 (0.0190)\tRec loss 3961.6299 (4060.5977)\tnorm 0.9286 (0.9222)\n",
            "Epoch: [61][ 20/195]\tTime  0.146 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7505 (0.7505)\tD(fake) 0.2328 (0.2436)\tgrad(D) penalty 0.0174 (0.0187)\tRec loss 3821.0903 (4075.0526)\tnorm 0.9215 (0.9200)\n",
            "Epoch: [61][ 30/195]\tTime  0.155 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7504 (0.7505)\tD(fake) 0.2284 (0.2369)\tgrad(D) penalty 0.0205 (0.0191)\tRec loss 4024.8892 (4076.2632)\tnorm 0.9195 (0.9207)\n",
            "Epoch: [61][ 40/195]\tTime  0.146 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7507 (0.7506)\tD(fake) 0.2408 (0.2401)\tgrad(D) penalty 0.0186 (0.0190)\tRec loss 4066.9958 (4063.7160)\tnorm 0.9145 (0.9198)\n",
            "Epoch: [61][ 50/195]\tTime  0.165 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7508 (0.7506)\tD(fake) 0.2350 (0.2388)\tgrad(D) penalty 0.0180 (0.0189)\tRec loss 4114.5503 (4073.5818)\tnorm 0.9144 (0.9183)\n",
            "Epoch: [61][ 60/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7508 (0.7507)\tD(fake) 0.2381 (0.2390)\tgrad(D) penalty 0.0205 (0.0190)\tRec loss 3781.1602 (4071.7676)\tnorm 0.9154 (0.9185)\n",
            "Epoch: [61][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7509 (0.7507)\tD(fake) 0.2460 (0.2393)\tgrad(D) penalty 0.0208 (0.0192)\tRec loss 4032.8784 (4080.0932)\tnorm 0.9285 (0.9190)\n",
            "Epoch: [61][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7510 (0.7507)\tD(fake) 0.2339 (0.2387)\tgrad(D) penalty 0.0179 (0.0192)\tRec loss 4324.9546 (4104.5368)\tnorm 0.9160 (0.9189)\n",
            "Epoch: [61][ 90/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7508 (0.7507)\tD(fake) 0.2328 (0.2382)\tgrad(D) penalty 0.0196 (0.0192)\tRec loss 3776.4148 (4104.1068)\tnorm 0.9345 (0.9200)\n",
            "Epoch: [61][100/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7510 (0.7508)\tD(fake) 0.2400 (0.2384)\tgrad(D) penalty 0.0199 (0.0190)\tRec loss 3941.3267 (4099.1647)\tnorm 0.9168 (0.9202)\n",
            "Epoch: [61][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7511 (0.7508)\tD(fake) 0.2425 (0.2386)\tgrad(D) penalty 0.0213 (0.0191)\tRec loss 3917.8149 (4093.1071)\tnorm 0.9190 (0.9193)\n",
            "Epoch: [61][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7511 (0.7508)\tD(fake) 0.2442 (0.2384)\tgrad(D) penalty 0.0161 (0.0190)\tRec loss 4251.2822 (4087.2504)\tnorm 0.9277 (0.9192)\n",
            "Epoch: [61][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7512 (0.7508)\tD(fake) 0.2311 (0.2382)\tgrad(D) penalty 0.0220 (0.0190)\tRec loss 3981.0698 (4084.3100)\tnorm 0.9023 (0.9186)\n",
            "Epoch: [61][140/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7510 (0.7509)\tD(fake) 0.2533 (0.2390)\tgrad(D) penalty 0.0176 (0.0190)\tRec loss 4140.8730 (4089.7239)\tnorm 0.9160 (0.9179)\n",
            "Epoch: [61][150/195]\tTime  0.168 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7513 (0.7509)\tD(fake) 0.2256 (0.2385)\tgrad(D) penalty 0.0207 (0.0190)\tRec loss 4172.7412 (4088.5733)\tnorm 0.9162 (0.9180)\n",
            "Epoch: [61][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7512 (0.7509)\tD(fake) 0.2516 (0.2380)\tgrad(D) penalty 0.0195 (0.0191)\tRec loss 4129.9224 (4092.8021)\tnorm 0.9300 (0.9184)\n",
            "Epoch: [61][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7513 (0.7509)\tD(fake) 0.2461 (0.2388)\tgrad(D) penalty 0.0173 (0.0190)\tRec loss 3973.0835 (4092.3674)\tnorm 0.9205 (0.9190)\n",
            "Epoch: [61][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7514 (0.7510)\tD(fake) 0.2511 (0.2386)\tgrad(D) penalty 0.0195 (0.0190)\tRec loss 4240.1470 (4091.4623)\tnorm 0.9247 (0.9193)\n",
            "Epoch: [61][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7514 (0.7510)\tD(fake) 0.2434 (0.2392)\tgrad(D) penalty 0.0174 (0.0189)\tRec loss 3970.2114 (4090.6060)\tnorm 0.9173 (0.9194)\n",
            "Epoch: [62][  0/195]\tTime  0.406 ( 0.406)\tData  0.209 ( 0.209)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3940.4697 (3940.4697)\tnorm 0.9260 (0.9260)\n",
            "Epoch: [62][ 10/195]\tTime  0.146 ( 0.171)\tData  0.000 ( 0.019)\tD(real) 0.7514 (0.7515)\tD(fake) 0.2596 (0.2484)\tgrad(D) penalty 0.0185 (0.0194)\tRec loss 3917.4680 (4044.2129)\tnorm 0.9229 (0.9129)\n",
            "Epoch: [62][ 20/195]\tTime  0.149 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.7515 (0.7515)\tD(fake) 0.2246 (0.2465)\tgrad(D) penalty 0.0199 (0.0183)\tRec loss 4190.8286 (4050.9764)\tnorm 0.9271 (0.9146)\n",
            "Epoch: [62][ 30/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7514 (0.7515)\tD(fake) 0.2342 (0.2421)\tgrad(D) penalty 0.0164 (0.0183)\tRec loss 4249.3066 (4050.8864)\tnorm 0.9143 (0.9132)\n",
            "Epoch: [62][ 40/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7515 (0.7515)\tD(fake) 0.2336 (0.2405)\tgrad(D) penalty 0.0164 (0.0182)\tRec loss 4189.3408 (4043.8396)\tnorm 0.8993 (0.9140)\n",
            "Epoch: [62][ 50/195]\tTime  0.163 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7515 (0.7515)\tD(fake) 0.2273 (0.2387)\tgrad(D) penalty 0.0182 (0.0180)\tRec loss 4236.3560 (4057.3058)\tnorm 0.9298 (0.9148)\n",
            "Epoch: [62][ 60/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7516 (0.7515)\tD(fake) 0.2498 (0.2384)\tgrad(D) penalty 0.0191 (0.0184)\tRec loss 4320.4692 (4054.2398)\tnorm 0.9345 (0.9162)\n",
            "Epoch: [62][ 70/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7516 (0.7515)\tD(fake) 0.2258 (0.2377)\tgrad(D) penalty 0.0177 (0.0183)\tRec loss 4067.1194 (4054.1164)\tnorm 0.9306 (0.9175)\n",
            "Epoch: [62][ 80/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7517 (0.7515)\tD(fake) 0.2404 (0.2369)\tgrad(D) penalty 0.0165 (0.0182)\tRec loss 3814.4141 (4058.8780)\tnorm 0.9274 (0.9189)\n",
            "Epoch: [62][ 90/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7517 (0.7516)\tD(fake) 0.2345 (0.2366)\tgrad(D) penalty 0.0188 (0.0183)\tRec loss 4515.1406 (4062.2240)\tnorm 0.9169 (0.9197)\n",
            "Epoch: [62][100/195]\tTime  0.162 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7517 (0.7516)\tD(fake) 0.2463 (0.2371)\tgrad(D) penalty 0.0171 (0.0184)\tRec loss 4020.0613 (4068.3727)\tnorm 0.9327 (0.9202)\n",
            "Epoch: [62][110/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7518 (0.7516)\tD(fake) 0.2248 (0.2367)\tgrad(D) penalty 0.0166 (0.0183)\tRec loss 4500.1787 (4074.8532)\tnorm 0.9173 (0.9206)\n",
            "Epoch: [62][120/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7518 (0.7516)\tD(fake) 0.2344 (0.2357)\tgrad(D) penalty 0.0166 (0.0183)\tRec loss 4007.2627 (4071.9591)\tnorm 0.9138 (0.9211)\n",
            "Epoch: [62][130/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7519 (0.7516)\tD(fake) 0.2346 (0.2357)\tgrad(D) penalty 0.0149 (0.0182)\tRec loss 4565.0981 (4076.7478)\tnorm 0.9346 (0.9214)\n",
            "Epoch: [62][140/195]\tTime  0.145 ( 0.149)\tData  0.000 ( 0.002)\tD(real) 0.7519 (0.7517)\tD(fake) 0.2316 (0.2353)\tgrad(D) penalty 0.0192 (0.0183)\tRec loss 3817.2148 (4076.5175)\tnorm 0.9274 (0.9213)\n",
            "Epoch: [62][150/195]\tTime  0.166 ( 0.149)\tData  0.000 ( 0.002)\tD(real) 0.7520 (0.7517)\tD(fake) 0.2364 (0.2354)\tgrad(D) penalty 0.0182 (0.0182)\tRec loss 4123.8135 (4075.6609)\tnorm 0.9105 (0.9213)\n",
            "Epoch: [62][160/195]\tTime  0.148 ( 0.149)\tData  0.000 ( 0.002)\tD(real) 0.7522 (0.7517)\tD(fake) 0.2431 (0.2357)\tgrad(D) penalty 0.0170 (0.0181)\tRec loss 4217.5806 (4082.5453)\tnorm 0.9065 (0.9212)\n",
            "Epoch: [62][170/195]\tTime  0.149 ( 0.149)\tData  0.000 ( 0.001)\tD(real) 0.7521 (0.7517)\tD(fake) 0.2478 (0.2362)\tgrad(D) penalty 0.0182 (0.0181)\tRec loss 3817.1729 (4080.3419)\tnorm 0.9233 (0.9212)\n",
            "Epoch: [62][180/195]\tTime  0.146 ( 0.149)\tData  0.000 ( 0.001)\tD(real) 0.7518 (0.7518)\tD(fake) 0.2343 (0.2362)\tgrad(D) penalty 0.0201 (0.0182)\tRec loss 4165.6572 (4082.7501)\tnorm 0.9239 (0.9211)\n",
            "Epoch: [62][190/195]\tTime  0.146 ( 0.149)\tData  0.000 ( 0.001)\tD(real) 0.7523 (0.7518)\tD(fake) 0.2438 (0.2365)\tgrad(D) penalty 0.0180 (0.0181)\tRec loss 4026.3625 (4085.9012)\tnorm 0.9312 (0.9213)\n",
            "Epoch: [63][  0/195]\tTime  0.382 ( 0.382)\tData  0.201 ( 0.201)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4122.4512 (4122.4512)\tnorm 0.9267 (0.9267)\n",
            "Epoch: [63][ 10/195]\tTime  0.148 ( 0.171)\tData  0.000 ( 0.019)\tD(real) 0.7523 (0.7523)\tD(fake) 0.2488 (0.2304)\tgrad(D) penalty 0.0174 (0.0190)\tRec loss 3913.3901 (4038.6430)\tnorm 0.9244 (0.9244)\n",
            "Epoch: [63][ 20/195]\tTime  0.148 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.7523 (0.7523)\tD(fake) 0.2262 (0.2387)\tgrad(D) penalty 0.0155 (0.0187)\tRec loss 4171.6562 (4057.9772)\tnorm 0.9189 (0.9211)\n",
            "Epoch: [63][ 30/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7524 (0.7523)\tD(fake) 0.2260 (0.2332)\tgrad(D) penalty 0.0197 (0.0193)\tRec loss 4286.2705 (4056.4169)\tnorm 0.9146 (0.9203)\n",
            "Epoch: [63][ 40/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7525 (0.7524)\tD(fake) 0.2367 (0.2360)\tgrad(D) penalty 0.0167 (0.0186)\tRec loss 3700.0273 (4056.4566)\tnorm 0.9189 (0.9213)\n",
            "Epoch: [63][ 50/195]\tTime  0.164 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7525 (0.7524)\tD(fake) 0.2250 (0.2342)\tgrad(D) penalty 0.0181 (0.0184)\tRec loss 4048.9075 (4074.9492)\tnorm 0.9329 (0.9227)\n",
            "Epoch: [63][ 60/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7526 (0.7524)\tD(fake) 0.2341 (0.2332)\tgrad(D) penalty 0.0146 (0.0177)\tRec loss 4025.4387 (4086.9017)\tnorm 0.9184 (0.9233)\n",
            "Epoch: [63][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7527 (0.7525)\tD(fake) 0.2442 (0.2347)\tgrad(D) penalty 0.0182 (0.0178)\tRec loss 3932.3340 (4086.2020)\tnorm 0.9078 (0.9222)\n",
            "Epoch: [63][ 80/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7526 (0.7525)\tD(fake) 0.2252 (0.2338)\tgrad(D) penalty 0.0170 (0.0176)\tRec loss 4052.4395 (4088.4588)\tnorm 0.9153 (0.9216)\n",
            "Epoch: [63][ 90/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7527 (0.7525)\tD(fake) 0.2587 (0.2352)\tgrad(D) penalty 0.0170 (0.0175)\tRec loss 4282.6670 (4089.2088)\tnorm 0.9120 (0.9212)\n",
            "Epoch: [63][100/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7526 (0.7525)\tD(fake) 0.2184 (0.2343)\tgrad(D) penalty 0.0182 (0.0174)\tRec loss 4121.0127 (4084.0592)\tnorm 0.9092 (0.9203)\n",
            "Epoch: [63][110/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7529 (0.7525)\tD(fake) 0.2325 (0.2340)\tgrad(D) penalty 0.0129 (0.0171)\tRec loss 3979.2993 (4084.7515)\tnorm 0.9158 (0.9201)\n",
            "Epoch: [63][120/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7529 (0.7526)\tD(fake) 0.2330 (0.2333)\tgrad(D) penalty 0.0179 (0.0172)\tRec loss 4088.2520 (4084.0604)\tnorm 0.9152 (0.9198)\n",
            "Epoch: [63][130/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7529 (0.7526)\tD(fake) 0.2322 (0.2334)\tgrad(D) penalty 0.0149 (0.0170)\tRec loss 4200.7876 (4084.8892)\tnorm 0.9207 (0.9197)\n",
            "Epoch: [63][140/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7529 (0.7526)\tD(fake) 0.2389 (0.2337)\tgrad(D) penalty 0.0187 (0.0170)\tRec loss 4413.8047 (4086.0228)\tnorm 0.9206 (0.9198)\n",
            "Epoch: [63][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7531 (0.7526)\tD(fake) 0.2269 (0.2336)\tgrad(D) penalty 0.0157 (0.0169)\tRec loss 4153.7900 (4082.6560)\tnorm 0.9094 (0.9197)\n",
            "Epoch: [63][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7531 (0.7527)\tD(fake) 0.2397 (0.2335)\tgrad(D) penalty 0.0146 (0.0169)\tRec loss 4029.1494 (4084.3320)\tnorm 0.9195 (0.9194)\n",
            "Epoch: [63][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7531 (0.7527)\tD(fake) 0.2390 (0.2337)\tgrad(D) penalty 0.0152 (0.0169)\tRec loss 4243.6035 (4085.4147)\tnorm 0.8999 (0.9192)\n",
            "Epoch: [63][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7532 (0.7527)\tD(fake) 0.2310 (0.2334)\tgrad(D) penalty 0.0155 (0.0168)\tRec loss 4126.1431 (4085.3653)\tnorm 0.9114 (0.9191)\n",
            "Epoch: [63][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7532 (0.7528)\tD(fake) 0.2424 (0.2337)\tgrad(D) penalty 0.0146 (0.0168)\tRec loss 3727.1177 (4080.5058)\tnorm 0.9165 (0.9190)\n",
            "Epoch: [64][  0/195]\tTime  0.421 ( 0.421)\tData  0.220 ( 0.220)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4209.1240 (4209.1240)\tnorm 0.9122 (0.9122)\n",
            "Epoch: [64][ 10/195]\tTime  0.148 ( 0.175)\tData  0.000 ( 0.020)\tD(real) 0.7533 (0.7533)\tD(fake) 0.2340 (0.2307)\tgrad(D) penalty 0.0144 (0.0162)\tRec loss 4077.8789 (4068.8981)\tnorm 0.9154 (0.9161)\n",
            "Epoch: [64][ 20/195]\tTime  0.150 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7533 (0.7533)\tD(fake) 0.2263 (0.2321)\tgrad(D) penalty 0.0151 (0.0156)\tRec loss 4191.3906 (4051.4575)\tnorm 0.9356 (0.9198)\n",
            "Epoch: [64][ 30/195]\tTime  0.145 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7532 (0.7533)\tD(fake) 0.2135 (0.2280)\tgrad(D) penalty 0.0196 (0.0161)\tRec loss 4076.1628 (4086.6758)\tnorm 0.9459 (0.9207)\n",
            "Epoch: [64][ 40/195]\tTime  0.153 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7535 (0.7534)\tD(fake) 0.2411 (0.2314)\tgrad(D) penalty 0.0171 (0.0163)\tRec loss 4460.4492 (4081.6244)\tnorm 0.9226 (0.9215)\n",
            "Epoch: [64][ 50/195]\tTime  0.169 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7535 (0.7534)\tD(fake) 0.2145 (0.2295)\tgrad(D) penalty 0.0219 (0.0169)\tRec loss 4256.0762 (4079.8217)\tnorm 0.9320 (0.9219)\n",
            "Epoch: [64][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7536 (0.7534)\tD(fake) 0.2455 (0.2312)\tgrad(D) penalty 0.0151 (0.0166)\tRec loss 4036.5457 (4077.0592)\tnorm 0.9230 (0.9212)\n",
            "Epoch: [64][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7537 (0.7534)\tD(fake) 0.2291 (0.2313)\tgrad(D) penalty 0.0212 (0.0170)\tRec loss 4018.7441 (4070.0106)\tnorm 0.9159 (0.9204)\n",
            "Epoch: [64][ 80/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7537 (0.7535)\tD(fake) 0.2400 (0.2320)\tgrad(D) penalty 0.0211 (0.0169)\tRec loss 4093.5571 (4067.8696)\tnorm 0.9267 (0.9203)\n",
            "Epoch: [64][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7538 (0.7535)\tD(fake) 0.2455 (0.2335)\tgrad(D) penalty 0.0137 (0.0168)\tRec loss 3997.3413 (4066.0338)\tnorm 0.9369 (0.9210)\n",
            "Epoch: [64][100/195]\tTime  0.164 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7538 (0.7535)\tD(fake) 0.2211 (0.2323)\tgrad(D) penalty 0.0180 (0.0169)\tRec loss 4038.2712 (4065.1612)\tnorm 0.9316 (0.9210)\n",
            "Epoch: [64][110/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7536 (0.7535)\tD(fake) 0.2451 (0.2330)\tgrad(D) penalty 0.0138 (0.0168)\tRec loss 3892.1250 (4076.9093)\tnorm 0.9000 (0.9204)\n",
            "Epoch: [64][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7539 (0.7536)\tD(fake) 0.2411 (0.2332)\tgrad(D) penalty 0.0168 (0.0167)\tRec loss 4317.9116 (4075.7888)\tnorm 0.9118 (0.9205)\n",
            "Epoch: [64][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7539 (0.7536)\tD(fake) 0.2286 (0.2333)\tgrad(D) penalty 0.0163 (0.0167)\tRec loss 3916.0757 (4076.9674)\tnorm 0.9085 (0.9196)\n",
            "Epoch: [64][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7539 (0.7536)\tD(fake) 0.2322 (0.2332)\tgrad(D) penalty 0.0164 (0.0167)\tRec loss 3980.7295 (4083.9207)\tnorm 0.9243 (0.9196)\n",
            "Epoch: [64][150/195]\tTime  0.160 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7541 (0.7537)\tD(fake) 0.2259 (0.2328)\tgrad(D) penalty 0.0177 (0.0167)\tRec loss 4150.2354 (4081.4909)\tnorm 0.9240 (0.9199)\n",
            "Epoch: [64][160/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7541 (0.7537)\tD(fake) 0.2197 (0.2320)\tgrad(D) penalty 0.0197 (0.0168)\tRec loss 4367.2969 (4083.4636)\tnorm 0.9082 (0.9196)\n",
            "Epoch: [64][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7541 (0.7537)\tD(fake) 0.2476 (0.2326)\tgrad(D) penalty 0.0162 (0.0167)\tRec loss 4229.3423 (4080.0483)\tnorm 0.9182 (0.9195)\n",
            "Epoch: [64][180/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7542 (0.7537)\tD(fake) 0.1994 (0.2314)\tgrad(D) penalty 0.0215 (0.0167)\tRec loss 4037.3384 (4080.0305)\tnorm 0.9153 (0.9190)\n",
            "Epoch: [64][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7543 (0.7538)\tD(fake) 0.2546 (0.2318)\tgrad(D) penalty 0.0156 (0.0167)\tRec loss 4106.0142 (4079.8246)\tnorm 0.9308 (0.9190)\n",
            "Epoch: [65][  0/195]\tTime  0.385 ( 0.385)\tData  0.208 ( 0.208)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4061.5557 (4061.5557)\tnorm 0.9118 (0.9118)\n",
            "Epoch: [65][ 10/195]\tTime  0.148 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.7542 (0.7542)\tD(fake) 0.2370 (0.2239)\tgrad(D) penalty 0.0163 (0.0178)\tRec loss 4019.0879 (4184.0604)\tnorm 0.9158 (0.9116)\n",
            "Epoch: [65][ 20/195]\tTime  0.148 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7543 (0.7543)\tD(fake) 0.2249 (0.2338)\tgrad(D) penalty 0.0151 (0.0158)\tRec loss 3964.8750 (4126.0543)\tnorm 0.9084 (0.9131)\n",
            "Epoch: [65][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7545 (0.7543)\tD(fake) 0.2315 (0.2320)\tgrad(D) penalty 0.0174 (0.0167)\tRec loss 4375.9463 (4092.3241)\tnorm 0.9068 (0.9121)\n",
            "Epoch: [65][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7546 (0.7544)\tD(fake) 0.2387 (0.2346)\tgrad(D) penalty 0.0158 (0.0164)\tRec loss 3970.3677 (4061.1826)\tnorm 0.9054 (0.9121)\n",
            "Epoch: [65][ 50/195]\tTime  0.170 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7541 (0.7544)\tD(fake) 0.2379 (0.2352)\tgrad(D) penalty 0.0186 (0.0166)\tRec loss 4022.5771 (4056.0733)\tnorm 0.9071 (0.9126)\n",
            "Epoch: [65][ 60/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7546 (0.7544)\tD(fake) 0.2295 (0.2341)\tgrad(D) penalty 0.0146 (0.0164)\tRec loss 4073.9380 (4064.1479)\tnorm 0.9150 (0.9139)\n",
            "Epoch: [65][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7547 (0.7544)\tD(fake) 0.2313 (0.2334)\tgrad(D) penalty 0.0180 (0.0164)\tRec loss 3931.2073 (4076.8920)\tnorm 0.9101 (0.9149)\n",
            "Epoch: [65][ 80/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7547 (0.7545)\tD(fake) 0.2287 (0.2338)\tgrad(D) penalty 0.0144 (0.0162)\tRec loss 3932.3667 (4073.7661)\tnorm 0.9115 (0.9156)\n",
            "Epoch: [65][ 90/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7547 (0.7545)\tD(fake) 0.2144 (0.2323)\tgrad(D) penalty 0.0157 (0.0162)\tRec loss 4233.3984 (4075.0160)\tnorm 0.9151 (0.9152)\n",
            "Epoch: [65][100/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7549 (0.7545)\tD(fake) 0.2410 (0.2321)\tgrad(D) penalty 0.0148 (0.0162)\tRec loss 4357.4619 (4082.8222)\tnorm 0.9204 (0.9153)\n",
            "Epoch: [65][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7549 (0.7546)\tD(fake) 0.2262 (0.2319)\tgrad(D) penalty 0.0162 (0.0162)\tRec loss 3995.3379 (4079.5820)\tnorm 0.9218 (0.9157)\n",
            "Epoch: [65][120/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7550 (0.7546)\tD(fake) 0.2333 (0.2319)\tgrad(D) penalty 0.0149 (0.0160)\tRec loss 4154.6221 (4081.1656)\tnorm 0.9129 (0.9156)\n",
            "Epoch: [65][130/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7549 (0.7546)\tD(fake) 0.2323 (0.2319)\tgrad(D) penalty 0.0159 (0.0161)\tRec loss 4056.5144 (4075.2319)\tnorm 0.9186 (0.9157)\n",
            "Epoch: [65][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7551 (0.7547)\tD(fake) 0.2213 (0.2312)\tgrad(D) penalty 0.0159 (0.0160)\tRec loss 3741.4360 (4072.3039)\tnorm 0.9136 (0.9155)\n",
            "Epoch: [65][150/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7552 (0.7547)\tD(fake) 0.2403 (0.2315)\tgrad(D) penalty 0.0153 (0.0160)\tRec loss 4171.1216 (4073.0106)\tnorm 0.9151 (0.9152)\n",
            "Epoch: [65][160/195]\tTime  0.144 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7553 (0.7547)\tD(fake) 0.2235 (0.2317)\tgrad(D) penalty 0.0151 (0.0158)\tRec loss 3837.3276 (4067.8965)\tnorm 0.9075 (0.9152)\n",
            "Epoch: [65][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7552 (0.7548)\tD(fake) 0.2404 (0.2316)\tgrad(D) penalty 0.0148 (0.0159)\tRec loss 3896.2378 (4070.5892)\tnorm 0.9143 (0.9149)\n",
            "Epoch: [65][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7552 (0.7548)\tD(fake) 0.2396 (0.2326)\tgrad(D) penalty 0.0169 (0.0159)\tRec loss 4009.9675 (4073.8162)\tnorm 0.9152 (0.9150)\n",
            "Epoch: [65][190/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7554 (0.7548)\tD(fake) 0.2397 (0.2328)\tgrad(D) penalty 0.0179 (0.0159)\tRec loss 4132.5181 (4076.3297)\tnorm 0.9169 (0.9150)\n",
            "Epoch: [66][  0/195]\tTime  0.390 ( 0.390)\tData  0.196 ( 0.196)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3880.6506 (3880.6506)\tnorm 0.9152 (0.9152)\n",
            "Epoch: [66][ 10/195]\tTime  0.146 ( 0.171)\tData  0.000 ( 0.018)\tD(real) 0.7554 (0.7554)\tD(fake) 0.2445 (0.2397)\tgrad(D) penalty 0.0164 (0.0151)\tRec loss 4275.0342 (4089.6247)\tnorm 0.9375 (0.9236)\n",
            "Epoch: [66][ 20/195]\tTime  0.145 ( 0.159)\tData  0.000 ( 0.010)\tD(real) 0.7553 (0.7554)\tD(fake) 0.2158 (0.2321)\tgrad(D) penalty 0.0140 (0.0148)\tRec loss 4301.9712 (4099.2733)\tnorm 0.9131 (0.9195)\n",
            "Epoch: [66][ 30/195]\tTime  0.146 ( 0.155)\tData  0.000 ( 0.007)\tD(real) 0.7554 (0.7554)\tD(fake) 0.2325 (0.2304)\tgrad(D) penalty 0.0154 (0.0152)\tRec loss 4145.3735 (4077.9427)\tnorm 0.9259 (0.9189)\n",
            "Epoch: [66][ 40/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7552 (0.7554)\tD(fake) 0.2119 (0.2293)\tgrad(D) penalty 0.0152 (0.0148)\tRec loss 4010.0876 (4067.1467)\tnorm 0.9078 (0.9184)\n",
            "Epoch: [66][ 50/195]\tTime  0.163 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7556 (0.7554)\tD(fake) 0.2464 (0.2304)\tgrad(D) penalty 0.0139 (0.0149)\tRec loss 3947.0605 (4078.2563)\tnorm 0.9223 (0.9172)\n",
            "Epoch: [66][ 60/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7556 (0.7555)\tD(fake) 0.2170 (0.2320)\tgrad(D) penalty 0.0159 (0.0148)\tRec loss 4261.0815 (4089.0802)\tnorm 0.9046 (0.9178)\n",
            "Epoch: [66][ 70/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7557 (0.7555)\tD(fake) 0.2259 (0.2303)\tgrad(D) penalty 0.0160 (0.0152)\tRec loss 3978.8677 (4078.3010)\tnorm 0.9153 (0.9178)\n",
            "Epoch: [66][ 80/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7558 (0.7555)\tD(fake) 0.2290 (0.2315)\tgrad(D) penalty 0.0150 (0.0152)\tRec loss 3835.0720 (4070.8632)\tnorm 0.9062 (0.9168)\n",
            "Epoch: [66][ 90/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7558 (0.7556)\tD(fake) 0.2164 (0.2306)\tgrad(D) penalty 0.0163 (0.0152)\tRec loss 4192.4209 (4071.2147)\tnorm 0.9061 (0.9154)\n",
            "Epoch: [66][100/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7558 (0.7556)\tD(fake) 0.2426 (0.2307)\tgrad(D) penalty 0.0175 (0.0154)\tRec loss 3993.2271 (4068.9450)\tnorm 0.9173 (0.9155)\n",
            "Epoch: [66][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7559 (0.7556)\tD(fake) 0.2451 (0.2318)\tgrad(D) penalty 0.0172 (0.0155)\tRec loss 4176.9004 (4069.4368)\tnorm 0.9085 (0.9153)\n",
            "Epoch: [66][120/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7560 (0.7556)\tD(fake) 0.2337 (0.2324)\tgrad(D) penalty 0.0141 (0.0155)\tRec loss 3946.1946 (4060.6720)\tnorm 0.9134 (0.9154)\n",
            "Epoch: [66][130/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7559 (0.7556)\tD(fake) 0.2215 (0.2315)\tgrad(D) penalty 0.0162 (0.0155)\tRec loss 4089.2380 (4060.0149)\tnorm 0.8981 (0.9145)\n",
            "Epoch: [66][140/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7560 (0.7557)\tD(fake) 0.2513 (0.2327)\tgrad(D) penalty 0.0171 (0.0155)\tRec loss 4191.2910 (4058.5103)\tnorm 0.9044 (0.9141)\n",
            "Epoch: [66][150/195]\tTime  0.163 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7557 (0.7557)\tD(fake) 0.2133 (0.2322)\tgrad(D) penalty 0.0154 (0.0154)\tRec loss 4209.7471 (4062.0860)\tnorm 0.9042 (0.9135)\n",
            "Epoch: [66][160/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7561 (0.7557)\tD(fake) 0.2522 (0.2326)\tgrad(D) penalty 0.0133 (0.0155)\tRec loss 4016.8162 (4062.0570)\tnorm 0.9146 (0.9129)\n",
            "Epoch: [66][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7562 (0.7557)\tD(fake) 0.2221 (0.2325)\tgrad(D) penalty 0.0158 (0.0155)\tRec loss 4388.8618 (4065.5653)\tnorm 0.9053 (0.9128)\n",
            "Epoch: [66][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7561 (0.7558)\tD(fake) 0.2497 (0.2325)\tgrad(D) penalty 0.0156 (0.0156)\tRec loss 3775.3840 (4062.3794)\tnorm 0.9210 (0.9127)\n",
            "Epoch: [66][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7560 (0.7558)\tD(fake) 0.2399 (0.2330)\tgrad(D) penalty 0.0131 (0.0155)\tRec loss 4272.1250 (4068.0519)\tnorm 0.9030 (0.9125)\n",
            "Epoch: [67][  0/195]\tTime  0.394 ( 0.394)\tData  0.201 ( 0.201)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3927.3965 (3927.3965)\tnorm 0.9169 (0.9169)\n",
            "Epoch: [67][ 10/195]\tTime  0.151 ( 0.171)\tData  0.000 ( 0.019)\tD(real) 0.7560 (0.7560)\tD(fake) 0.2601 (0.2435)\tgrad(D) penalty 0.0151 (0.0163)\tRec loss 4228.2690 (4161.9409)\tnorm 0.8994 (0.9146)\n",
            "Epoch: [67][ 20/195]\tTime  0.147 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.7563 (0.7562)\tD(fake) 0.2102 (0.2381)\tgrad(D) penalty 0.0176 (0.0153)\tRec loss 4322.8271 (4116.8902)\tnorm 0.9205 (0.9163)\n",
            "Epoch: [67][ 30/195]\tTime  0.150 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7562 (0.7562)\tD(fake) 0.2446 (0.2370)\tgrad(D) penalty 0.0135 (0.0154)\tRec loss 4075.7383 (4089.7493)\tnorm 0.9015 (0.9151)\n",
            "Epoch: [67][ 40/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7561 (0.7562)\tD(fake) 0.2352 (0.2377)\tgrad(D) penalty 0.0149 (0.0152)\tRec loss 4103.3013 (4106.0074)\tnorm 0.9179 (0.9138)\n",
            "Epoch: [67][ 50/195]\tTime  0.166 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7564 (0.7562)\tD(fake) 0.2255 (0.2364)\tgrad(D) penalty 0.0171 (0.0155)\tRec loss 3947.0107 (4069.9153)\tnorm 0.9179 (0.9133)\n",
            "Epoch: [67][ 60/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7565 (0.7563)\tD(fake) 0.2385 (0.2360)\tgrad(D) penalty 0.0168 (0.0153)\tRec loss 3851.4275 (4064.7273)\tnorm 0.9103 (0.9120)\n",
            "Epoch: [67][ 70/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7565 (0.7563)\tD(fake) 0.2386 (0.2369)\tgrad(D) penalty 0.0149 (0.0153)\tRec loss 4262.4482 (4069.8551)\tnorm 0.9155 (0.9112)\n",
            "Epoch: [67][ 80/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7565 (0.7563)\tD(fake) 0.2332 (0.2350)\tgrad(D) penalty 0.0202 (0.0158)\tRec loss 4052.7339 (4059.3060)\tnorm 0.9109 (0.9110)\n",
            "Epoch: [67][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7566 (0.7563)\tD(fake) 0.2465 (0.2361)\tgrad(D) penalty 0.0157 (0.0158)\tRec loss 3963.2388 (4053.7057)\tnorm 0.9097 (0.9111)\n",
            "Epoch: [67][100/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7566 (0.7563)\tD(fake) 0.2261 (0.2353)\tgrad(D) penalty 0.0116 (0.0157)\tRec loss 3968.3389 (4051.3736)\tnorm 0.9095 (0.9105)\n",
            "Epoch: [67][110/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7565 (0.7564)\tD(fake) 0.2287 (0.2347)\tgrad(D) penalty 0.0139 (0.0157)\tRec loss 4345.5078 (4054.6252)\tnorm 0.9193 (0.9108)\n",
            "Epoch: [67][120/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7567 (0.7564)\tD(fake) 0.2433 (0.2349)\tgrad(D) penalty 0.0149 (0.0156)\tRec loss 4101.6309 (4045.3029)\tnorm 0.8980 (0.9106)\n",
            "Epoch: [67][130/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7567 (0.7564)\tD(fake) 0.2209 (0.2346)\tgrad(D) penalty 0.0174 (0.0157)\tRec loss 4287.5068 (4057.6973)\tnorm 0.9030 (0.9103)\n",
            "Epoch: [67][140/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7568 (0.7564)\tD(fake) 0.2469 (0.2345)\tgrad(D) penalty 0.0179 (0.0157)\tRec loss 4194.4258 (4060.7513)\tnorm 0.9183 (0.9106)\n",
            "Epoch: [67][150/195]\tTime  0.165 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7569 (0.7564)\tD(fake) 0.2235 (0.2345)\tgrad(D) penalty 0.0145 (0.0156)\tRec loss 4106.2471 (4057.7325)\tnorm 0.9101 (0.9111)\n",
            "Epoch: [67][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7569 (0.7565)\tD(fake) 0.2405 (0.2335)\tgrad(D) penalty 0.0135 (0.0156)\tRec loss 4055.3625 (4058.0095)\tnorm 0.9060 (0.9112)\n",
            "Epoch: [67][170/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7568 (0.7565)\tD(fake) 0.2383 (0.2340)\tgrad(D) penalty 0.0145 (0.0156)\tRec loss 4284.8628 (4060.9213)\tnorm 0.9130 (0.9108)\n",
            "Epoch: [67][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7567 (0.7565)\tD(fake) 0.2227 (0.2331)\tgrad(D) penalty 0.0135 (0.0155)\tRec loss 3920.0640 (4060.1753)\tnorm 0.9004 (0.9108)\n",
            "Epoch: [67][190/195]\tTime  0.145 ( 0.149)\tData  0.000 ( 0.001)\tD(real) 0.7570 (0.7565)\tD(fake) 0.2372 (0.2330)\tgrad(D) penalty 0.0176 (0.0156)\tRec loss 4019.3552 (4064.6388)\tnorm 0.9081 (0.9108)\n",
            "Epoch: [68][  0/195]\tTime  0.406 ( 0.406)\tData  0.213 ( 0.213)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3957.2498 (3957.2498)\tnorm 0.9013 (0.9013)\n",
            "Epoch: [68][ 10/195]\tTime  0.150 ( 0.173)\tData  0.000 ( 0.020)\tD(real) 0.7570 (0.7571)\tD(fake) 0.2357 (0.2247)\tgrad(D) penalty 0.0165 (0.0158)\tRec loss 4169.9883 (4107.6893)\tnorm 0.9015 (0.9035)\n",
            "Epoch: [68][ 20/195]\tTime  0.146 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7570 (0.7571)\tD(fake) 0.2175 (0.2252)\tgrad(D) penalty 0.0168 (0.0150)\tRec loss 4295.3447 (4092.7350)\tnorm 0.9103 (0.9052)\n",
            "Epoch: [68][ 30/195]\tTime  0.146 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7571 (0.7571)\tD(fake) 0.2496 (0.2297)\tgrad(D) penalty 0.0131 (0.0147)\tRec loss 3883.6450 (4079.4671)\tnorm 0.9159 (0.9064)\n",
            "Epoch: [68][ 40/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7571 (0.7571)\tD(fake) 0.2221 (0.2297)\tgrad(D) penalty 0.0180 (0.0153)\tRec loss 4364.6152 (4078.8502)\tnorm 0.9053 (0.9069)\n",
            "Epoch: [68][ 50/195]\tTime  0.162 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7572 (0.7571)\tD(fake) 0.2344 (0.2291)\tgrad(D) penalty 0.0155 (0.0155)\tRec loss 4513.5430 (4094.3284)\tnorm 0.9046 (0.9078)\n",
            "Epoch: [68][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7569 (0.7571)\tD(fake) 0.2402 (0.2323)\tgrad(D) penalty 0.0159 (0.0155)\tRec loss 4243.3203 (4102.9286)\tnorm 0.9098 (0.9072)\n",
            "Epoch: [68][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7572 (0.7571)\tD(fake) 0.2292 (0.2315)\tgrad(D) penalty 0.0165 (0.0157)\tRec loss 4207.5674 (4095.0800)\tnorm 0.9076 (0.9079)\n",
            "Epoch: [68][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7572 (0.7571)\tD(fake) 0.2393 (0.2337)\tgrad(D) penalty 0.0152 (0.0158)\tRec loss 3834.4941 (4081.7773)\tnorm 0.9235 (0.9081)\n",
            "Epoch: [68][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7572 (0.7571)\tD(fake) 0.2251 (0.2328)\tgrad(D) penalty 0.0159 (0.0158)\tRec loss 3878.7146 (4076.7545)\tnorm 0.8939 (0.9080)\n",
            "Epoch: [68][100/195]\tTime  0.169 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7573 (0.7572)\tD(fake) 0.2369 (0.2338)\tgrad(D) penalty 0.0170 (0.0159)\tRec loss 4017.9529 (4069.3128)\tnorm 0.9093 (0.9079)\n",
            "Epoch: [68][110/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7573 (0.7572)\tD(fake) 0.2352 (0.2337)\tgrad(D) penalty 0.0147 (0.0159)\tRec loss 3886.3657 (4067.1406)\tnorm 0.9018 (0.9076)\n",
            "Epoch: [68][120/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7573 (0.7572)\tD(fake) 0.2454 (0.2341)\tgrad(D) penalty 0.0153 (0.0159)\tRec loss 4220.2324 (4066.8639)\tnorm 0.9153 (0.9074)\n",
            "Epoch: [68][130/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7573 (0.7572)\tD(fake) 0.2332 (0.2342)\tgrad(D) penalty 0.0160 (0.0159)\tRec loss 4072.0713 (4072.8467)\tnorm 0.9183 (0.9074)\n",
            "Epoch: [68][140/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7574 (0.7572)\tD(fake) 0.2364 (0.2343)\tgrad(D) penalty 0.0161 (0.0159)\tRec loss 4089.8193 (4070.4565)\tnorm 0.9180 (0.9078)\n",
            "Epoch: [68][150/195]\tTime  0.164 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7575 (0.7572)\tD(fake) 0.2377 (0.2344)\tgrad(D) penalty 0.0135 (0.0159)\tRec loss 3812.8545 (4066.2266)\tnorm 0.9227 (0.9087)\n",
            "Epoch: [68][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7575 (0.7573)\tD(fake) 0.2262 (0.2334)\tgrad(D) penalty 0.0177 (0.0158)\tRec loss 4210.1934 (4063.4346)\tnorm 0.9121 (0.9089)\n",
            "Epoch: [68][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7576 (0.7573)\tD(fake) 0.2483 (0.2340)\tgrad(D) penalty 0.0169 (0.0158)\tRec loss 3982.2180 (4065.3715)\tnorm 0.9040 (0.9089)\n",
            "Epoch: [68][180/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7574 (0.7573)\tD(fake) 0.2293 (0.2335)\tgrad(D) penalty 0.0139 (0.0158)\tRec loss 4073.9800 (4061.9670)\tnorm 0.9300 (0.9094)\n",
            "Epoch: [68][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7573 (0.7573)\tD(fake) 0.2345 (0.2335)\tgrad(D) penalty 0.0153 (0.0158)\tRec loss 3817.8953 (4061.9264)\tnorm 0.9077 (0.9094)\n",
            "Epoch: [69][  0/195]\tTime  0.407 ( 0.407)\tData  0.206 ( 0.206)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4036.3779 (4036.3779)\tnorm 0.9036 (0.9036)\n",
            "Epoch: [69][ 10/195]\tTime  0.148 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.7577 (0.7577)\tD(fake) 0.2420 (0.2275)\tgrad(D) penalty 0.0164 (0.0164)\tRec loss 4323.2695 (4026.3541)\tnorm 0.9206 (0.9119)\n",
            "Epoch: [69][ 20/195]\tTime  0.146 ( 0.159)\tData  0.000 ( 0.010)\tD(real) 0.7577 (0.7577)\tD(fake) 0.2220 (0.2355)\tgrad(D) penalty 0.0156 (0.0157)\tRec loss 4001.2185 (4014.9410)\tnorm 0.9027 (0.9100)\n",
            "Epoch: [69][ 30/195]\tTime  0.146 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7578 (0.7577)\tD(fake) 0.2390 (0.2322)\tgrad(D) penalty 0.0186 (0.0168)\tRec loss 4324.6011 (4029.6205)\tnorm 0.8945 (0.9080)\n",
            "Epoch: [69][ 40/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7574 (0.7577)\tD(fake) 0.2361 (0.2401)\tgrad(D) penalty 0.0151 (0.0165)\tRec loss 3895.1982 (4044.0669)\tnorm 0.9098 (0.9086)\n",
            "Epoch: [69][ 50/195]\tTime  0.167 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7578 (0.7577)\tD(fake) 0.2303 (0.2370)\tgrad(D) penalty 0.0156 (0.0165)\tRec loss 4000.3042 (4047.5064)\tnorm 0.9060 (0.9084)\n",
            "Epoch: [69][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7579 (0.7577)\tD(fake) 0.2364 (0.2377)\tgrad(D) penalty 0.0162 (0.0164)\tRec loss 3918.1807 (4044.2544)\tnorm 0.9080 (0.9084)\n",
            "Epoch: [69][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7578 (0.7577)\tD(fake) 0.2313 (0.2367)\tgrad(D) penalty 0.0181 (0.0164)\tRec loss 4119.2373 (4045.9322)\tnorm 0.8965 (0.9083)\n",
            "Epoch: [69][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7579 (0.7577)\tD(fake) 0.2267 (0.2362)\tgrad(D) penalty 0.0147 (0.0162)\tRec loss 4029.0730 (4056.9185)\tnorm 0.9102 (0.9083)\n",
            "Epoch: [69][ 90/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7579 (0.7578)\tD(fake) 0.2347 (0.2358)\tgrad(D) penalty 0.0156 (0.0163)\tRec loss 4022.8757 (4059.6402)\tnorm 0.9288 (0.9093)\n",
            "Epoch: [69][100/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7578 (0.7578)\tD(fake) 0.2350 (0.2359)\tgrad(D) penalty 0.0157 (0.0163)\tRec loss 4003.4604 (4066.6484)\tnorm 0.9175 (0.9102)\n",
            "Epoch: [69][110/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7580 (0.7578)\tD(fake) 0.2208 (0.2348)\tgrad(D) penalty 0.0162 (0.0163)\tRec loss 3984.5239 (4069.5059)\tnorm 0.9046 (0.9105)\n",
            "Epoch: [69][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7581 (0.7578)\tD(fake) 0.2526 (0.2353)\tgrad(D) penalty 0.0140 (0.0163)\tRec loss 3969.1997 (4071.0520)\tnorm 0.9150 (0.9115)\n",
            "Epoch: [69][130/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7580 (0.7578)\tD(fake) 0.2263 (0.2351)\tgrad(D) penalty 0.0169 (0.0163)\tRec loss 4188.3208 (4065.8847)\tnorm 0.9171 (0.9122)\n",
            "Epoch: [69][140/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7582 (0.7578)\tD(fake) 0.2294 (0.2343)\tgrad(D) penalty 0.0170 (0.0162)\tRec loss 3877.7683 (4060.1289)\tnorm 0.9166 (0.9126)\n",
            "Epoch: [69][150/195]\tTime  0.168 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7580 (0.7579)\tD(fake) 0.2552 (0.2350)\tgrad(D) penalty 0.0151 (0.0161)\tRec loss 4026.5825 (4058.0054)\tnorm 0.9225 (0.9134)\n",
            "Epoch: [69][160/195]\tTime  0.153 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7581 (0.7579)\tD(fake) 0.2281 (0.2353)\tgrad(D) penalty 0.0163 (0.0160)\tRec loss 3921.2671 (4053.9582)\tnorm 0.9298 (0.9136)\n",
            "Epoch: [69][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7583 (0.7579)\tD(fake) 0.2413 (0.2354)\tgrad(D) penalty 0.0161 (0.0160)\tRec loss 4129.2944 (4055.9362)\tnorm 0.8898 (0.9128)\n",
            "Epoch: [69][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7583 (0.7579)\tD(fake) 0.2365 (0.2358)\tgrad(D) penalty 0.0179 (0.0161)\tRec loss 4312.8545 (4065.0878)\tnorm 0.9130 (0.9126)\n",
            "Epoch: [69][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7582 (0.7579)\tD(fake) 0.2222 (0.2353)\tgrad(D) penalty 0.0148 (0.0160)\tRec loss 4153.6562 (4063.2432)\tnorm 0.9218 (0.9128)\n",
            "Epoch: [70][  0/195]\tTime  0.399 ( 0.399)\tData  0.202 ( 0.202)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3857.4680 (3857.4680)\tnorm 0.9214 (0.9214)\n",
            "Epoch: [70][ 10/195]\tTime  0.145 ( 0.169)\tData  0.000 ( 0.019)\tD(real) 0.7583 (0.7583)\tD(fake) 0.2349 (0.2364)\tgrad(D) penalty 0.0147 (0.0139)\tRec loss 4021.8140 (4012.6180)\tnorm 0.9153 (0.9192)\n",
            "Epoch: [70][ 20/195]\tTime  0.147 ( 0.159)\tData  0.000 ( 0.010)\tD(real) 0.7585 (0.7583)\tD(fake) 0.2363 (0.2317)\tgrad(D) penalty 0.0136 (0.0148)\tRec loss 4115.4321 (4026.1796)\tnorm 0.9088 (0.9164)\n",
            "Epoch: [70][ 30/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.007)\tD(real) 0.7584 (0.7583)\tD(fake) 0.2153 (0.2294)\tgrad(D) penalty 0.0136 (0.0144)\tRec loss 4058.7642 (4034.4519)\tnorm 0.9044 (0.9149)\n",
            "Epoch: [70][ 40/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7583 (0.7584)\tD(fake) 0.2531 (0.2320)\tgrad(D) penalty 0.0151 (0.0147)\tRec loss 3914.3447 (4011.3543)\tnorm 0.8974 (0.9124)\n",
            "Epoch: [70][ 50/195]\tTime  0.163 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7586 (0.7584)\tD(fake) 0.2234 (0.2321)\tgrad(D) penalty 0.0150 (0.0147)\tRec loss 4350.0137 (4023.0376)\tnorm 0.8981 (0.9106)\n",
            "Epoch: [70][ 60/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7586 (0.7584)\tD(fake) 0.2318 (0.2307)\tgrad(D) penalty 0.0144 (0.0148)\tRec loss 3983.7319 (4030.9358)\tnorm 0.8989 (0.9096)\n",
            "Epoch: [70][ 70/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7587 (0.7584)\tD(fake) 0.2371 (0.2310)\tgrad(D) penalty 0.0176 (0.0150)\tRec loss 3819.0425 (4024.6524)\tnorm 0.8997 (0.9083)\n",
            "Epoch: [70][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7584 (0.7585)\tD(fake) 0.2345 (0.2311)\tgrad(D) penalty 0.0185 (0.0151)\tRec loss 4027.3064 (4027.0229)\tnorm 0.8958 (0.9065)\n",
            "Epoch: [70][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7587 (0.7585)\tD(fake) 0.2468 (0.2327)\tgrad(D) penalty 0.0160 (0.0151)\tRec loss 3960.4990 (4040.5446)\tnorm 0.8956 (0.9063)\n",
            "Epoch: [70][100/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7584 (0.7585)\tD(fake) 0.2221 (0.2324)\tgrad(D) penalty 0.0155 (0.0153)\tRec loss 4105.0815 (4039.4040)\tnorm 0.8922 (0.9060)\n",
            "Epoch: [70][110/195]\tTime  0.144 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7586 (0.7585)\tD(fake) 0.2440 (0.2328)\tgrad(D) penalty 0.0174 (0.0155)\tRec loss 3940.9487 (4035.1877)\tnorm 0.8899 (0.9061)\n",
            "Epoch: [70][120/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7587 (0.7585)\tD(fake) 0.2180 (0.2327)\tgrad(D) penalty 0.0142 (0.0155)\tRec loss 4150.8008 (4042.1120)\tnorm 0.9052 (0.9064)\n",
            "Epoch: [70][130/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7587 (0.7585)\tD(fake) 0.2386 (0.2327)\tgrad(D) penalty 0.0148 (0.0155)\tRec loss 4324.8745 (4050.2164)\tnorm 0.8957 (0.9063)\n",
            "Epoch: [70][140/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7588 (0.7585)\tD(fake) 0.2310 (0.2325)\tgrad(D) penalty 0.0195 (0.0157)\tRec loss 4241.9414 (4049.0836)\tnorm 0.9092 (0.9069)\n",
            "Epoch: [70][150/195]\tTime  0.163 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7588 (0.7586)\tD(fake) 0.2484 (0.2333)\tgrad(D) penalty 0.0163 (0.0157)\tRec loss 3916.2537 (4053.3076)\tnorm 0.9025 (0.9072)\n",
            "Epoch: [70][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7585 (0.7586)\tD(fake) 0.2305 (0.2332)\tgrad(D) penalty 0.0183 (0.0159)\tRec loss 3887.3496 (4050.9620)\tnorm 0.9047 (0.9074)\n",
            "Epoch: [70][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7587 (0.7586)\tD(fake) 0.2530 (0.2340)\tgrad(D) penalty 0.0149 (0.0158)\tRec loss 4021.6060 (4049.5821)\tnorm 0.9269 (0.9079)\n",
            "Epoch: [70][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7587 (0.7586)\tD(fake) 0.2296 (0.2341)\tgrad(D) penalty 0.0158 (0.0159)\tRec loss 3868.8943 (4055.0333)\tnorm 0.9086 (0.9081)\n",
            "Epoch: [70][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7589 (0.7586)\tD(fake) 0.2391 (0.2340)\tgrad(D) penalty 0.0145 (0.0159)\tRec loss 4043.3599 (4057.1719)\tnorm 0.9137 (0.9083)\n",
            "Epoch: [71][  0/195]\tTime  0.411 ( 0.411)\tData  0.212 ( 0.212)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4192.7002 (4192.7002)\tnorm 0.9184 (0.9184)\n",
            "Epoch: [71][ 10/195]\tTime  0.146 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.7588 (0.7588)\tD(fake) 0.2429 (0.2372)\tgrad(D) penalty 0.0145 (0.0155)\tRec loss 4124.1719 (4124.6804)\tnorm 0.9061 (0.9128)\n",
            "Epoch: [71][ 20/195]\tTime  0.151 ( 0.163)\tData  0.000 ( 0.010)\tD(real) 0.7589 (0.7588)\tD(fake) 0.2213 (0.2311)\tgrad(D) penalty 0.0209 (0.0169)\tRec loss 4074.3005 (4075.4400)\tnorm 0.9043 (0.9112)\n",
            "Epoch: [71][ 30/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.7589 (0.7588)\tD(fake) 0.2447 (0.2332)\tgrad(D) penalty 0.0174 (0.0173)\tRec loss 4033.3777 (4078.7882)\tnorm 0.9249 (0.9116)\n",
            "Epoch: [71][ 40/195]\tTime  0.154 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7589 (0.7589)\tD(fake) 0.2234 (0.2326)\tgrad(D) penalty 0.0186 (0.0168)\tRec loss 4165.3896 (4089.8744)\tnorm 0.9335 (0.9128)\n",
            "Epoch: [71][ 50/195]\tTime  0.166 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7588 (0.7589)\tD(fake) 0.2356 (0.2324)\tgrad(D) penalty 0.0180 (0.0168)\tRec loss 4196.5059 (4092.3050)\tnorm 0.9127 (0.9131)\n",
            "Epoch: [71][ 60/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7586 (0.7588)\tD(fake) 0.2335 (0.2328)\tgrad(D) penalty 0.0167 (0.0165)\tRec loss 3891.1108 (4089.9485)\tnorm 0.9140 (0.9125)\n",
            "Epoch: [71][ 70/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7591 (0.7588)\tD(fake) 0.2403 (0.2331)\tgrad(D) penalty 0.0142 (0.0164)\tRec loss 3975.6938 (4086.3038)\tnorm 0.9061 (0.9131)\n",
            "Epoch: [71][ 80/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7590 (0.7589)\tD(fake) 0.2340 (0.2341)\tgrad(D) penalty 0.0148 (0.0163)\tRec loss 4493.9990 (4084.8894)\tnorm 0.9108 (0.9134)\n",
            "Epoch: [71][ 90/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7589 (0.7589)\tD(fake) 0.2223 (0.2334)\tgrad(D) penalty 0.0142 (0.0161)\tRec loss 3904.1509 (4083.6103)\tnorm 0.9055 (0.9129)\n",
            "Epoch: [71][100/195]\tTime  0.162 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7591 (0.7589)\tD(fake) 0.2427 (0.2337)\tgrad(D) penalty 0.0158 (0.0162)\tRec loss 3983.8367 (4074.2562)\tnorm 0.9114 (0.9124)\n",
            "Epoch: [71][110/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7590 (0.7589)\tD(fake) 0.2275 (0.2332)\tgrad(D) penalty 0.0170 (0.0163)\tRec loss 3916.4041 (4056.6487)\tnorm 0.9075 (0.9125)\n",
            "Epoch: [71][120/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7591 (0.7589)\tD(fake) 0.2324 (0.2326)\tgrad(D) penalty 0.0157 (0.0163)\tRec loss 3964.5471 (4049.3464)\tnorm 0.9086 (0.9119)\n",
            "Epoch: [71][130/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7591 (0.7589)\tD(fake) 0.2264 (0.2324)\tgrad(D) penalty 0.0176 (0.0163)\tRec loss 3906.4976 (4049.2929)\tnorm 0.9058 (0.9117)\n",
            "Epoch: [71][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7592 (0.7589)\tD(fake) 0.2414 (0.2326)\tgrad(D) penalty 0.0169 (0.0164)\tRec loss 3850.7505 (4047.3962)\tnorm 0.8974 (0.9112)\n",
            "Epoch: [71][150/195]\tTime  0.170 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7590 (0.7590)\tD(fake) 0.2281 (0.2328)\tgrad(D) penalty 0.0183 (0.0164)\tRec loss 4279.2324 (4047.7157)\tnorm 0.9020 (0.9104)\n",
            "Epoch: [71][160/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7591 (0.7590)\tD(fake) 0.2280 (0.2323)\tgrad(D) penalty 0.0142 (0.0163)\tRec loss 4039.4541 (4047.1954)\tnorm 0.9183 (0.9100)\n",
            "Epoch: [71][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7592 (0.7590)\tD(fake) 0.2310 (0.2323)\tgrad(D) penalty 0.0136 (0.0163)\tRec loss 3927.7832 (4047.8478)\tnorm 0.8974 (0.9098)\n",
            "Epoch: [71][180/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7592 (0.7590)\tD(fake) 0.2392 (0.2322)\tgrad(D) penalty 0.0162 (0.0163)\tRec loss 4142.4658 (4051.8248)\tnorm 0.9094 (0.9098)\n",
            "Epoch: [71][190/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7592 (0.7590)\tD(fake) 0.2350 (0.2323)\tgrad(D) penalty 0.0189 (0.0163)\tRec loss 4215.5254 (4053.6770)\tnorm 0.9007 (0.9097)\n",
            "Epoch: [72][  0/195]\tTime  0.401 ( 0.401)\tData  0.211 ( 0.211)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4022.4341 (4022.4341)\tnorm 0.9017 (0.9017)\n",
            "Epoch: [72][ 10/195]\tTime  0.146 ( 0.171)\tData  0.000 ( 0.019)\tD(real) 0.7592 (0.7593)\tD(fake) 0.2559 (0.2456)\tgrad(D) penalty 0.0152 (0.0161)\tRec loss 4195.4209 (4067.0139)\tnorm 0.9082 (0.9065)\n",
            "Epoch: [72][ 20/195]\tTime  0.150 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7592 (0.7593)\tD(fake) 0.2148 (0.2398)\tgrad(D) penalty 0.0175 (0.0156)\tRec loss 3987.8491 (4069.6142)\tnorm 0.9058 (0.9052)\n",
            "Epoch: [72][ 30/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7593 (0.7593)\tD(fake) 0.2451 (0.2390)\tgrad(D) penalty 0.0150 (0.0159)\tRec loss 4262.9595 (4068.1458)\tnorm 0.8954 (0.9069)\n",
            "Epoch: [72][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7594 (0.7593)\tD(fake) 0.2394 (0.2422)\tgrad(D) penalty 0.0181 (0.0165)\tRec loss 4026.0107 (4079.2320)\tnorm 0.9122 (0.9063)\n",
            "Epoch: [72][ 50/195]\tTime  0.161 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7594 (0.7593)\tD(fake) 0.2382 (0.2402)\tgrad(D) penalty 0.0175 (0.0169)\tRec loss 4111.1362 (4067.8216)\tnorm 0.9161 (0.9077)\n",
            "Epoch: [72][ 60/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7593 (0.7593)\tD(fake) 0.2297 (0.2393)\tgrad(D) penalty 0.0171 (0.0167)\tRec loss 3956.1123 (4069.6166)\tnorm 0.9081 (0.9084)\n",
            "Epoch: [72][ 70/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7594 (0.7593)\tD(fake) 0.2274 (0.2386)\tgrad(D) penalty 0.0141 (0.0164)\tRec loss 3890.6099 (4057.8343)\tnorm 0.9203 (0.9091)\n",
            "Epoch: [72][ 80/195]\tTime  0.154 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7593 (0.7593)\tD(fake) 0.2397 (0.2373)\tgrad(D) penalty 0.0127 (0.0164)\tRec loss 4072.7993 (4044.1950)\tnorm 0.8933 (0.9088)\n",
            "Epoch: [72][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7593 (0.7593)\tD(fake) 0.2274 (0.2372)\tgrad(D) penalty 0.0167 (0.0164)\tRec loss 4053.6130 (4036.2055)\tnorm 0.8909 (0.9075)\n",
            "Epoch: [72][100/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7594 (0.7593)\tD(fake) 0.2376 (0.2366)\tgrad(D) penalty 0.0162 (0.0163)\tRec loss 4134.7319 (4043.8970)\tnorm 0.9059 (0.9068)\n",
            "Epoch: [72][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7595 (0.7593)\tD(fake) 0.2411 (0.2370)\tgrad(D) penalty 0.0146 (0.0163)\tRec loss 4070.8103 (4039.2196)\tnorm 0.8935 (0.9070)\n",
            "Epoch: [72][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7594 (0.7594)\tD(fake) 0.2192 (0.2350)\tgrad(D) penalty 0.0178 (0.0163)\tRec loss 4034.2183 (4041.6388)\tnorm 0.9028 (0.9067)\n",
            "Epoch: [72][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7595 (0.7594)\tD(fake) 0.2575 (0.2360)\tgrad(D) penalty 0.0142 (0.0162)\tRec loss 3919.6128 (4045.6638)\tnorm 0.8931 (0.9064)\n",
            "Epoch: [72][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7593 (0.7594)\tD(fake) 0.2143 (0.2350)\tgrad(D) penalty 0.0157 (0.0162)\tRec loss 3906.8042 (4043.4887)\tnorm 0.9032 (0.9066)\n",
            "Epoch: [72][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7594 (0.7594)\tD(fake) 0.2429 (0.2352)\tgrad(D) penalty 0.0170 (0.0163)\tRec loss 3757.2773 (4044.6194)\tnorm 0.9030 (0.9064)\n",
            "Epoch: [72][160/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7596 (0.7594)\tD(fake) 0.2288 (0.2349)\tgrad(D) penalty 0.0138 (0.0162)\tRec loss 4145.2095 (4045.3474)\tnorm 0.8998 (0.9062)\n",
            "Epoch: [72][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7596 (0.7594)\tD(fake) 0.2227 (0.2343)\tgrad(D) penalty 0.0196 (0.0162)\tRec loss 3975.8936 (4045.6239)\tnorm 0.9228 (0.9061)\n",
            "Epoch: [72][180/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7597 (0.7594)\tD(fake) 0.2404 (0.2349)\tgrad(D) penalty 0.0161 (0.0162)\tRec loss 4039.7295 (4047.1095)\tnorm 0.9083 (0.9065)\n",
            "Epoch: [72][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7595 (0.7594)\tD(fake) 0.2233 (0.2344)\tgrad(D) penalty 0.0163 (0.0163)\tRec loss 3950.5115 (4048.7729)\tnorm 0.9088 (0.9068)\n",
            "Epoch: [73][  0/195]\tTime  0.394 ( 0.394)\tData  0.217 ( 0.217)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4166.2715 (4166.2715)\tnorm 0.9050 (0.9050)\n",
            "Epoch: [73][ 10/195]\tTime  0.147 ( 0.171)\tData  0.000 ( 0.020)\tD(real) 0.7596 (0.7596)\tD(fake) 0.2366 (0.2360)\tgrad(D) penalty 0.0147 (0.0162)\tRec loss 4119.6729 (4009.0301)\tnorm 0.8998 (0.9022)\n",
            "Epoch: [73][ 20/195]\tTime  0.148 ( 0.160)\tData  0.000 ( 0.011)\tD(real) 0.7598 (0.7597)\tD(fake) 0.2159 (0.2283)\tgrad(D) penalty 0.0170 (0.0163)\tRec loss 3788.3892 (4054.1035)\tnorm 0.9153 (0.9060)\n",
            "Epoch: [73][ 30/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7598 (0.7597)\tD(fake) 0.2425 (0.2319)\tgrad(D) penalty 0.0154 (0.0164)\tRec loss 3810.4177 (4052.4384)\tnorm 0.9061 (0.9080)\n",
            "Epoch: [73][ 40/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.006)\tD(real) 0.7598 (0.7597)\tD(fake) 0.2295 (0.2328)\tgrad(D) penalty 0.0178 (0.0161)\tRec loss 3722.5752 (4030.1117)\tnorm 0.9195 (0.9099)\n",
            "Epoch: [73][ 50/195]\tTime  0.162 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7599 (0.7597)\tD(fake) 0.2385 (0.2334)\tgrad(D) penalty 0.0177 (0.0161)\tRec loss 4001.3560 (4045.4698)\tnorm 0.9202 (0.9107)\n",
            "Epoch: [73][ 60/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7596 (0.7597)\tD(fake) 0.2276 (0.2320)\tgrad(D) penalty 0.0190 (0.0163)\tRec loss 4162.3154 (4051.9964)\tnorm 0.9167 (0.9103)\n",
            "Epoch: [73][ 70/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7599 (0.7597)\tD(fake) 0.2384 (0.2330)\tgrad(D) penalty 0.0146 (0.0161)\tRec loss 3826.1689 (4044.8973)\tnorm 0.9161 (0.9115)\n",
            "Epoch: [73][ 80/195]\tTime  0.155 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7598 (0.7598)\tD(fake) 0.2280 (0.2316)\tgrad(D) penalty 0.0190 (0.0163)\tRec loss 4211.3159 (4041.3065)\tnorm 0.9091 (0.9125)\n",
            "Epoch: [73][ 90/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7597 (0.7598)\tD(fake) 0.2574 (0.2338)\tgrad(D) penalty 0.0173 (0.0164)\tRec loss 4121.2578 (4039.7969)\tnorm 0.9119 (0.9135)\n",
            "Epoch: [73][100/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7599 (0.7598)\tD(fake) 0.2227 (0.2335)\tgrad(D) penalty 0.0178 (0.0164)\tRec loss 4051.5352 (4040.8383)\tnorm 0.9164 (0.9136)\n",
            "Epoch: [73][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7598 (0.7598)\tD(fake) 0.2295 (0.2329)\tgrad(D) penalty 0.0162 (0.0164)\tRec loss 4166.8838 (4050.1748)\tnorm 0.9193 (0.9142)\n",
            "Epoch: [73][120/195]\tTime  0.157 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7600 (0.7598)\tD(fake) 0.2315 (0.2333)\tgrad(D) penalty 0.0193 (0.0165)\tRec loss 4535.6748 (4051.8028)\tnorm 0.9238 (0.9148)\n",
            "Epoch: [73][130/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7599 (0.7598)\tD(fake) 0.2325 (0.2330)\tgrad(D) penalty 0.0153 (0.0165)\tRec loss 4310.9780 (4050.0522)\tnorm 0.9176 (0.9145)\n",
            "Epoch: [73][140/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7601 (0.7598)\tD(fake) 0.2291 (0.2326)\tgrad(D) penalty 0.0140 (0.0165)\tRec loss 4225.0562 (4059.3627)\tnorm 0.9252 (0.9141)\n",
            "Epoch: [73][150/195]\tTime  0.170 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7600 (0.7598)\tD(fake) 0.2340 (0.2326)\tgrad(D) penalty 0.0159 (0.0165)\tRec loss 3977.4009 (4058.1638)\tnorm 0.8989 (0.9143)\n",
            "Epoch: [73][160/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7599 (0.7598)\tD(fake) 0.2218 (0.2325)\tgrad(D) penalty 0.0178 (0.0165)\tRec loss 4077.9985 (4056.4960)\tnorm 0.9139 (0.9138)\n",
            "Epoch: [73][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7601 (0.7598)\tD(fake) 0.2435 (0.2325)\tgrad(D) penalty 0.0155 (0.0164)\tRec loss 3954.0396 (4057.6783)\tnorm 0.9154 (0.9135)\n",
            "Epoch: [73][180/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7601 (0.7599)\tD(fake) 0.2429 (0.2337)\tgrad(D) penalty 0.0145 (0.0164)\tRec loss 4066.8328 (4056.5126)\tnorm 0.9065 (0.9133)\n",
            "Epoch: [73][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7601 (0.7599)\tD(fake) 0.2109 (0.2329)\tgrad(D) penalty 0.0180 (0.0164)\tRec loss 4019.3762 (4052.9441)\tnorm 0.9052 (0.9130)\n",
            "Epoch: [74][  0/195]\tTime  0.404 ( 0.404)\tData  0.213 ( 0.213)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4213.6895 (4213.6895)\tnorm 0.9144 (0.9144)\n",
            "Epoch: [74][ 10/195]\tTime  0.145 ( 0.172)\tData  0.000 ( 0.020)\tD(real) 0.7602 (0.7602)\tD(fake) 0.2403 (0.2376)\tgrad(D) penalty 0.0134 (0.0162)\tRec loss 4430.7222 (4097.5079)\tnorm 0.9036 (0.9027)\n",
            "Epoch: [74][ 20/195]\tTime  0.149 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7602 (0.7602)\tD(fake) 0.2379 (0.2379)\tgrad(D) penalty 0.0167 (0.0166)\tRec loss 4172.5063 (4074.6669)\tnorm 0.9104 (0.9062)\n",
            "Epoch: [74][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7602 (0.7602)\tD(fake) 0.2175 (0.2334)\tgrad(D) penalty 0.0157 (0.0160)\tRec loss 4100.9209 (4071.2673)\tnorm 0.8999 (0.9057)\n",
            "Epoch: [74][ 40/195]\tTime  0.146 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7603 (0.7602)\tD(fake) 0.2472 (0.2347)\tgrad(D) penalty 0.0148 (0.0166)\tRec loss 4130.8975 (4056.0007)\tnorm 0.8947 (0.9053)\n",
            "Epoch: [74][ 50/195]\tTime  0.166 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7602 (0.7602)\tD(fake) 0.2388 (0.2354)\tgrad(D) penalty 0.0164 (0.0165)\tRec loss 4010.8535 (4057.1279)\tnorm 0.8995 (0.9038)\n",
            "Epoch: [74][ 60/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7602 (0.7602)\tD(fake) 0.2402 (0.2359)\tgrad(D) penalty 0.0149 (0.0164)\tRec loss 4029.4075 (4052.1723)\tnorm 0.9023 (0.9041)\n",
            "Epoch: [74][ 70/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7603 (0.7602)\tD(fake) 0.2231 (0.2349)\tgrad(D) penalty 0.0178 (0.0164)\tRec loss 4171.8560 (4045.2757)\tnorm 0.8982 (0.9042)\n",
            "Epoch: [74][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7603 (0.7602)\tD(fake) 0.2370 (0.2353)\tgrad(D) penalty 0.0168 (0.0165)\tRec loss 4425.5493 (4048.7221)\tnorm 0.8994 (0.9042)\n",
            "Epoch: [74][ 90/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7601 (0.7602)\tD(fake) 0.2364 (0.2348)\tgrad(D) penalty 0.0191 (0.0167)\tRec loss 4057.0623 (4038.6412)\tnorm 0.9043 (0.9034)\n",
            "Epoch: [74][100/195]\tTime  0.169 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7602 (0.7602)\tD(fake) 0.2207 (0.2350)\tgrad(D) penalty 0.0157 (0.0165)\tRec loss 4332.3794 (4044.1312)\tnorm 0.8967 (0.9027)\n",
            "Epoch: [74][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7602 (0.7602)\tD(fake) 0.2266 (0.2335)\tgrad(D) penalty 0.0217 (0.0167)\tRec loss 4221.7866 (4043.3532)\tnorm 0.9070 (0.9028)\n",
            "Epoch: [74][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7603 (0.7602)\tD(fake) 0.2425 (0.2352)\tgrad(D) penalty 0.0151 (0.0166)\tRec loss 4379.1792 (4046.4028)\tnorm 0.9013 (0.9028)\n",
            "Epoch: [74][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7599 (0.7602)\tD(fake) 0.2198 (0.2343)\tgrad(D) penalty 0.0236 (0.0168)\tRec loss 4082.6372 (4044.1898)\tnorm 0.9100 (0.9030)\n",
            "Epoch: [74][140/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7603 (0.7602)\tD(fake) 0.2451 (0.2354)\tgrad(D) penalty 0.0151 (0.0167)\tRec loss 3985.8774 (4042.9157)\tnorm 0.9050 (0.9033)\n",
            "Epoch: [74][150/195]\tTime  0.166 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7602 (0.7602)\tD(fake) 0.2052 (0.2340)\tgrad(D) penalty 0.0200 (0.0167)\tRec loss 4254.3560 (4044.2065)\tnorm 0.8993 (0.9031)\n",
            "Epoch: [74][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7603 (0.7602)\tD(fake) 0.2452 (0.2343)\tgrad(D) penalty 0.0160 (0.0167)\tRec loss 3879.3315 (4042.9210)\tnorm 0.9057 (0.9035)\n",
            "Epoch: [74][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7603 (0.7602)\tD(fake) 0.2174 (0.2336)\tgrad(D) penalty 0.0184 (0.0168)\tRec loss 3999.5598 (4044.0089)\tnorm 0.9100 (0.9036)\n",
            "Epoch: [74][180/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7604 (0.7602)\tD(fake) 0.2498 (0.2342)\tgrad(D) penalty 0.0148 (0.0167)\tRec loss 4285.0137 (4049.0143)\tnorm 0.9152 (0.9041)\n",
            "Epoch: [74][190/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7605 (0.7603)\tD(fake) 0.2302 (0.2343)\tgrad(D) penalty 0.0164 (0.0167)\tRec loss 4004.5396 (4042.1289)\tnorm 0.9123 (0.9045)\n",
            "Epoch: [75][  0/195]\tTime  0.403 ( 0.403)\tData  0.211 ( 0.211)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3841.3052 (3841.3052)\tnorm 0.9300 (0.9300)\n",
            "Epoch: [75][ 10/195]\tTime  0.152 ( 0.171)\tData  0.000 ( 0.019)\tD(real) 0.7606 (0.7605)\tD(fake) 0.2485 (0.2451)\tgrad(D) penalty 0.0134 (0.0158)\tRec loss 4163.8433 (3964.3224)\tnorm 0.9105 (0.9049)\n",
            "Epoch: [75][ 20/195]\tTime  0.148 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.7606 (0.7605)\tD(fake) 0.2249 (0.2339)\tgrad(D) penalty 0.0180 (0.0158)\tRec loss 3779.0356 (3986.9261)\tnorm 0.9181 (0.9088)\n",
            "Epoch: [75][ 30/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7606 (0.7605)\tD(fake) 0.2505 (0.2369)\tgrad(D) penalty 0.0153 (0.0158)\tRec loss 3914.0444 (4018.7631)\tnorm 0.9153 (0.9100)\n",
            "Epoch: [75][ 40/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7605 (0.7605)\tD(fake) 0.2241 (0.2364)\tgrad(D) penalty 0.0192 (0.0158)\tRec loss 3952.6724 (4003.3714)\tnorm 0.9002 (0.9099)\n",
            "Epoch: [75][ 50/195]\tTime  0.166 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7606 (0.7605)\tD(fake) 0.2371 (0.2353)\tgrad(D) penalty 0.0166 (0.0160)\tRec loss 4260.2563 (4009.3754)\tnorm 0.8974 (0.9101)\n",
            "Epoch: [75][ 60/195]\tTime  0.153 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7607 (0.7605)\tD(fake) 0.2298 (0.2366)\tgrad(D) penalty 0.0147 (0.0159)\tRec loss 4384.2700 (4024.0715)\tnorm 0.9144 (0.9104)\n",
            "Epoch: [75][ 70/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7604 (0.7605)\tD(fake) 0.2293 (0.2353)\tgrad(D) penalty 0.0167 (0.0159)\tRec loss 3935.4783 (4023.1151)\tnorm 0.9225 (0.9118)\n",
            "Epoch: [75][ 80/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7606 (0.7605)\tD(fake) 0.2335 (0.2353)\tgrad(D) penalty 0.0159 (0.0158)\tRec loss 4001.3193 (4023.9574)\tnorm 0.9162 (0.9129)\n",
            "Epoch: [75][ 90/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7607 (0.7606)\tD(fake) 0.2309 (0.2346)\tgrad(D) penalty 0.0170 (0.0159)\tRec loss 4302.6792 (4028.1964)\tnorm 0.8961 (0.9124)\n",
            "Epoch: [75][100/195]\tTime  0.170 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7607 (0.7606)\tD(fake) 0.2344 (0.2336)\tgrad(D) penalty 0.0164 (0.0159)\tRec loss 4065.9043 (4038.3389)\tnorm 0.9111 (0.9121)\n",
            "Epoch: [75][110/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7608 (0.7606)\tD(fake) 0.2445 (0.2349)\tgrad(D) penalty 0.0150 (0.0158)\tRec loss 3999.8550 (4041.1450)\tnorm 0.9132 (0.9118)\n",
            "Epoch: [75][120/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7608 (0.7606)\tD(fake) 0.2297 (0.2336)\tgrad(D) penalty 0.0168 (0.0158)\tRec loss 4098.1035 (4041.0711)\tnorm 0.9215 (0.9116)\n",
            "Epoch: [75][130/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7607 (0.7606)\tD(fake) 0.2616 (0.2349)\tgrad(D) penalty 0.0135 (0.0157)\tRec loss 3981.3208 (4042.8979)\tnorm 0.9179 (0.9118)\n",
            "Epoch: [75][140/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7609 (0.7606)\tD(fake) 0.2198 (0.2342)\tgrad(D) penalty 0.0205 (0.0159)\tRec loss 4356.9971 (4042.1639)\tnorm 0.8959 (0.9119)\n",
            "Epoch: [75][150/195]\tTime  0.165 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7608 (0.7606)\tD(fake) 0.2541 (0.2350)\tgrad(D) penalty 0.0140 (0.0159)\tRec loss 4521.3813 (4044.9578)\tnorm 0.9067 (0.9120)\n",
            "Epoch: [75][160/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7608 (0.7607)\tD(fake) 0.2157 (0.2342)\tgrad(D) penalty 0.0273 (0.0161)\tRec loss 3847.2061 (4039.2680)\tnorm 0.9215 (0.9120)\n",
            "Epoch: [75][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7608 (0.7607)\tD(fake) 0.2553 (0.2350)\tgrad(D) penalty 0.0173 (0.0161)\tRec loss 4195.4561 (4040.7054)\tnorm 0.9104 (0.9117)\n",
            "Epoch: [75][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7609 (0.7607)\tD(fake) 0.2248 (0.2349)\tgrad(D) penalty 0.0187 (0.0161)\tRec loss 4185.7056 (4044.2076)\tnorm 0.9114 (0.9114)\n",
            "Epoch: [75][190/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7609 (0.7607)\tD(fake) 0.2429 (0.2352)\tgrad(D) penalty 0.0151 (0.0161)\tRec loss 4149.0649 (4042.5394)\tnorm 0.9059 (0.9110)\n",
            "Epoch: [76][  0/195]\tTime  0.405 ( 0.405)\tData  0.210 ( 0.210)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3824.0540 (3824.0540)\tnorm 0.9053 (0.9053)\n",
            "Epoch: [76][ 10/195]\tTime  0.147 ( 0.173)\tData  0.000 ( 0.019)\tD(real) 0.7609 (0.7609)\tD(fake) 0.2462 (0.2353)\tgrad(D) penalty 0.0182 (0.0188)\tRec loss 3845.7449 (3951.8057)\tnorm 0.9065 (0.9058)\n",
            "Epoch: [76][ 20/195]\tTime  0.147 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7610 (0.7609)\tD(fake) 0.2177 (0.2328)\tgrad(D) penalty 0.0156 (0.0171)\tRec loss 4005.9329 (3979.9320)\tnorm 0.8942 (0.9030)\n",
            "Epoch: [76][ 30/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7610 (0.7610)\tD(fake) 0.2414 (0.2328)\tgrad(D) penalty 0.0136 (0.0165)\tRec loss 4111.3652 (3992.9047)\tnorm 0.9043 (0.9042)\n",
            "Epoch: [76][ 40/195]\tTime  0.145 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7610 (0.7610)\tD(fake) 0.2288 (0.2338)\tgrad(D) penalty 0.0149 (0.0160)\tRec loss 4146.4023 (4018.1001)\tnorm 0.9249 (0.9058)\n",
            "Epoch: [76][ 50/195]\tTime  0.167 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7610 (0.7610)\tD(fake) 0.2422 (0.2343)\tgrad(D) penalty 0.0130 (0.0159)\tRec loss 4135.3467 (4012.6489)\tnorm 0.9040 (0.9051)\n",
            "Epoch: [76][ 60/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7611 (0.7610)\tD(fake) 0.2312 (0.2342)\tgrad(D) penalty 0.0166 (0.0158)\tRec loss 4286.0195 (4018.3329)\tnorm 0.8927 (0.9047)\n",
            "Epoch: [76][ 70/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7610 (0.7610)\tD(fake) 0.2310 (0.2341)\tgrad(D) penalty 0.0169 (0.0160)\tRec loss 3828.4375 (4024.2604)\tnorm 0.9018 (0.9043)\n",
            "Epoch: [76][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7610 (0.7610)\tD(fake) 0.2434 (0.2350)\tgrad(D) penalty 0.0169 (0.0165)\tRec loss 3952.1450 (4032.0500)\tnorm 0.8940 (0.9037)\n",
            "Epoch: [76][ 90/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7611 (0.7610)\tD(fake) 0.2394 (0.2356)\tgrad(D) penalty 0.0209 (0.0167)\tRec loss 4280.4219 (4047.9747)\tnorm 0.8939 (0.9038)\n",
            "Epoch: [76][100/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7611 (0.7610)\tD(fake) 0.2215 (0.2344)\tgrad(D) penalty 0.0199 (0.0167)\tRec loss 4238.6235 (4054.7307)\tnorm 0.9035 (0.9039)\n",
            "Epoch: [76][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7611 (0.7610)\tD(fake) 0.2446 (0.2348)\tgrad(D) penalty 0.0138 (0.0166)\tRec loss 4029.6458 (4054.9261)\tnorm 0.9190 (0.9045)\n",
            "Epoch: [76][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7612 (0.7610)\tD(fake) 0.2243 (0.2344)\tgrad(D) penalty 0.0153 (0.0165)\tRec loss 4057.8738 (4051.5698)\tnorm 0.8997 (0.9051)\n",
            "Epoch: [76][130/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7610 (0.7610)\tD(fake) 0.2400 (0.2346)\tgrad(D) penalty 0.0158 (0.0165)\tRec loss 4059.6079 (4046.7678)\tnorm 0.9041 (0.9053)\n",
            "Epoch: [76][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7612 (0.7610)\tD(fake) 0.2295 (0.2351)\tgrad(D) penalty 0.0166 (0.0165)\tRec loss 4037.9705 (4044.2969)\tnorm 0.9074 (0.9054)\n",
            "Epoch: [76][150/195]\tTime  0.163 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7612 (0.7611)\tD(fake) 0.2393 (0.2352)\tgrad(D) penalty 0.0163 (0.0165)\tRec loss 3877.0854 (4043.3591)\tnorm 0.9020 (0.9053)\n",
            "Epoch: [76][160/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7610 (0.7611)\tD(fake) 0.2373 (0.2353)\tgrad(D) penalty 0.0184 (0.0166)\tRec loss 3933.0188 (4037.5860)\tnorm 0.9024 (0.9051)\n",
            "Epoch: [76][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7611 (0.7611)\tD(fake) 0.2236 (0.2352)\tgrad(D) penalty 0.0193 (0.0167)\tRec loss 3891.5632 (4032.3517)\tnorm 0.9038 (0.9051)\n",
            "Epoch: [76][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7612 (0.7611)\tD(fake) 0.2438 (0.2351)\tgrad(D) penalty 0.0148 (0.0167)\tRec loss 4066.4878 (4033.7949)\tnorm 0.9074 (0.9048)\n",
            "Epoch: [76][190/195]\tTime  0.148 ( 0.149)\tData  0.000 ( 0.001)\tD(real) 0.7612 (0.7611)\tD(fake) 0.2392 (0.2355)\tgrad(D) penalty 0.0150 (0.0167)\tRec loss 4309.6763 (4032.4319)\tnorm 0.9072 (0.9044)\n",
            "Epoch: [77][  0/195]\tTime  0.419 ( 0.419)\tData  0.222 ( 0.222)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4063.9204 (4063.9204)\tnorm 0.9006 (0.9006)\n",
            "Epoch: [77][ 10/195]\tTime  0.151 ( 0.176)\tData  0.000 ( 0.020)\tD(real) 0.7612 (0.7612)\tD(fake) 0.2525 (0.2374)\tgrad(D) penalty 0.0162 (0.0171)\tRec loss 4027.6460 (4023.6235)\tnorm 0.9013 (0.8984)\n",
            "Epoch: [77][ 20/195]\tTime  0.149 ( 0.163)\tData  0.000 ( 0.011)\tD(real) 0.7612 (0.7612)\tD(fake) 0.2201 (0.2338)\tgrad(D) penalty 0.0169 (0.0170)\tRec loss 3919.1609 (4050.2926)\tnorm 0.8941 (0.8984)\n",
            "Epoch: [77][ 30/195]\tTime  0.151 ( 0.159)\tData  0.000 ( 0.007)\tD(real) 0.7613 (0.7612)\tD(fake) 0.2434 (0.2337)\tgrad(D) penalty 0.0202 (0.0176)\tRec loss 3950.5320 (4019.2626)\tnorm 0.8956 (0.8979)\n",
            "Epoch: [77][ 40/195]\tTime  0.145 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7613 (0.7613)\tD(fake) 0.2259 (0.2341)\tgrad(D) penalty 0.0129 (0.0172)\tRec loss 4136.6182 (4006.4331)\tnorm 0.8984 (0.8973)\n",
            "Epoch: [77][ 50/195]\tTime  0.167 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7613 (0.7613)\tD(fake) 0.2244 (0.2323)\tgrad(D) penalty 0.0154 (0.0169)\tRec loss 3957.8354 (4031.3073)\tnorm 0.9146 (0.8981)\n",
            "Epoch: [77][ 60/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7613 (0.7613)\tD(fake) 0.2341 (0.2325)\tgrad(D) penalty 0.0145 (0.0166)\tRec loss 4196.0488 (4033.7088)\tnorm 0.9131 (0.8997)\n",
            "Epoch: [77][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7614 (0.7613)\tD(fake) 0.2297 (0.2321)\tgrad(D) penalty 0.0199 (0.0169)\tRec loss 4042.1541 (4034.5532)\tnorm 0.9061 (0.8999)\n",
            "Epoch: [77][ 80/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7613 (0.7613)\tD(fake) 0.2188 (0.2310)\tgrad(D) penalty 0.0181 (0.0168)\tRec loss 4167.4053 (4033.6456)\tnorm 0.9044 (0.9001)\n",
            "Epoch: [77][ 90/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7614 (0.7613)\tD(fake) 0.2310 (0.2306)\tgrad(D) penalty 0.0162 (0.0168)\tRec loss 3834.4883 (4026.9823)\tnorm 0.9061 (0.9007)\n",
            "Epoch: [77][100/195]\tTime  0.164 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7614 (0.7613)\tD(fake) 0.2347 (0.2316)\tgrad(D) penalty 0.0161 (0.0168)\tRec loss 4404.9492 (4043.9954)\tnorm 0.8837 (0.8994)\n",
            "Epoch: [77][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7612 (0.7613)\tD(fake) 0.2213 (0.2310)\tgrad(D) penalty 0.0204 (0.0169)\tRec loss 3813.2212 (4039.9493)\tnorm 0.8918 (0.8993)\n",
            "Epoch: [77][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7615 (0.7613)\tD(fake) 0.2401 (0.2314)\tgrad(D) penalty 0.0138 (0.0168)\tRec loss 4063.9282 (4040.0727)\tnorm 0.9027 (0.8994)\n",
            "Epoch: [77][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7612 (0.7613)\tD(fake) 0.2218 (0.2308)\tgrad(D) penalty 0.0209 (0.0170)\tRec loss 3800.1533 (4040.3469)\tnorm 0.9059 (0.8995)\n",
            "Epoch: [77][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7613 (0.7613)\tD(fake) 0.2276 (0.2309)\tgrad(D) penalty 0.0137 (0.0168)\tRec loss 4521.2852 (4041.3686)\tnorm 0.8912 (0.8993)\n",
            "Epoch: [77][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7614 (0.7613)\tD(fake) 0.2261 (0.2306)\tgrad(D) penalty 0.0169 (0.0168)\tRec loss 4066.5613 (4039.7331)\tnorm 0.9019 (0.8991)\n",
            "Epoch: [77][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7615 (0.7613)\tD(fake) 0.2408 (0.2311)\tgrad(D) penalty 0.0187 (0.0168)\tRec loss 3972.5645 (4037.1746)\tnorm 0.9163 (0.8995)\n",
            "Epoch: [77][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7617 (0.7614)\tD(fake) 0.2368 (0.2317)\tgrad(D) penalty 0.0163 (0.0168)\tRec loss 3985.6648 (4035.7352)\tnorm 0.9035 (0.8996)\n",
            "Epoch: [77][180/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7616 (0.7614)\tD(fake) 0.2130 (0.2305)\tgrad(D) penalty 0.0192 (0.0168)\tRec loss 3847.0737 (4033.1939)\tnorm 0.8939 (0.8996)\n",
            "Epoch: [77][190/195]\tTime  0.153 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7615 (0.7614)\tD(fake) 0.2428 (0.2308)\tgrad(D) penalty 0.0180 (0.0169)\tRec loss 4220.3408 (4034.2400)\tnorm 0.9044 (0.8996)\n",
            "Epoch: [78][  0/195]\tTime  0.419 ( 0.419)\tData  0.219 ( 0.219)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4085.4106 (4085.4106)\tnorm 0.9133 (0.9133)\n",
            "Epoch: [78][ 10/195]\tTime  0.146 ( 0.174)\tData  0.000 ( 0.020)\tD(real) 0.7615 (0.7616)\tD(fake) 0.2309 (0.2169)\tgrad(D) penalty 0.0178 (0.0183)\tRec loss 4108.6562 (4067.0345)\tnorm 0.8990 (0.9023)\n",
            "Epoch: [78][ 20/195]\tTime  0.147 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7617 (0.7616)\tD(fake) 0.2175 (0.2282)\tgrad(D) penalty 0.0149 (0.0167)\tRec loss 4216.1094 (4056.5637)\tnorm 0.8911 (0.9014)\n",
            "Epoch: [78][ 30/195]\tTime  0.150 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7616 (0.7616)\tD(fake) 0.2369 (0.2274)\tgrad(D) penalty 0.0142 (0.0164)\tRec loss 4045.9961 (4049.9595)\tnorm 0.9065 (0.9008)\n",
            "Epoch: [78][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7617 (0.7616)\tD(fake) 0.2459 (0.2316)\tgrad(D) penalty 0.0179 (0.0165)\tRec loss 4085.5332 (4037.7155)\tnorm 0.8879 (0.9009)\n",
            "Epoch: [78][ 50/195]\tTime  0.164 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7617 (0.7616)\tD(fake) 0.2283 (0.2318)\tgrad(D) penalty 0.0148 (0.0164)\tRec loss 3638.9468 (4010.4020)\tnorm 0.9194 (0.9027)\n",
            "Epoch: [78][ 60/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7617 (0.7616)\tD(fake) 0.2447 (0.2324)\tgrad(D) penalty 0.0163 (0.0165)\tRec loss 3937.7515 (4014.9529)\tnorm 0.9020 (0.9020)\n",
            "Epoch: [78][ 70/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7616 (0.7616)\tD(fake) 0.2263 (0.2328)\tgrad(D) penalty 0.0166 (0.0165)\tRec loss 3857.9048 (4022.6747)\tnorm 0.8867 (0.9000)\n",
            "Epoch: [78][ 80/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7617 (0.7616)\tD(fake) 0.2525 (0.2336)\tgrad(D) penalty 0.0158 (0.0166)\tRec loss 3819.6826 (4021.8832)\tnorm 0.8979 (0.8990)\n",
            "Epoch: [78][ 90/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7617 (0.7616)\tD(fake) 0.2402 (0.2346)\tgrad(D) penalty 0.0181 (0.0166)\tRec loss 4228.7285 (4026.8681)\tnorm 0.8894 (0.8983)\n",
            "Epoch: [78][100/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7614 (0.7616)\tD(fake) 0.2378 (0.2343)\tgrad(D) penalty 0.0139 (0.0165)\tRec loss 3869.0740 (4014.5546)\tnorm 0.9032 (0.8982)\n",
            "Epoch: [78][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7617 (0.7616)\tD(fake) 0.2385 (0.2348)\tgrad(D) penalty 0.0196 (0.0166)\tRec loss 3872.7288 (4018.5994)\tnorm 0.9023 (0.8985)\n",
            "Epoch: [78][120/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7615 (0.7616)\tD(fake) 0.2394 (0.2344)\tgrad(D) penalty 0.0172 (0.0167)\tRec loss 4350.4609 (4024.2319)\tnorm 0.9016 (0.8988)\n",
            "Epoch: [78][130/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7616 (0.7616)\tD(fake) 0.2351 (0.2348)\tgrad(D) penalty 0.0190 (0.0167)\tRec loss 3909.7815 (4025.1000)\tnorm 0.9137 (0.8991)\n",
            "Epoch: [78][140/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7616 (0.7616)\tD(fake) 0.2347 (0.2339)\tgrad(D) penalty 0.0167 (0.0167)\tRec loss 4077.0498 (4026.3885)\tnorm 0.9156 (0.8997)\n",
            "Epoch: [78][150/195]\tTime  0.164 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7616 (0.7616)\tD(fake) 0.2402 (0.2345)\tgrad(D) penalty 0.0141 (0.0167)\tRec loss 4097.3096 (4034.9368)\tnorm 0.9091 (0.9002)\n",
            "Epoch: [78][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7616 (0.7616)\tD(fake) 0.2182 (0.2333)\tgrad(D) penalty 0.0162 (0.0167)\tRec loss 4060.8037 (4038.9883)\tnorm 0.9193 (0.9011)\n",
            "Epoch: [78][170/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7616 (0.7616)\tD(fake) 0.2378 (0.2332)\tgrad(D) penalty 0.0179 (0.0168)\tRec loss 3810.1685 (4039.5873)\tnorm 0.9191 (0.9019)\n",
            "Epoch: [78][180/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7617 (0.7616)\tD(fake) 0.2439 (0.2338)\tgrad(D) penalty 0.0172 (0.0168)\tRec loss 3861.7183 (4038.5368)\tnorm 0.9166 (0.9023)\n",
            "Epoch: [78][190/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7614 (0.7616)\tD(fake) 0.2260 (0.2337)\tgrad(D) penalty 0.0198 (0.0169)\tRec loss 4035.1230 (4037.1412)\tnorm 0.9011 (0.9026)\n",
            "Epoch: [79][  0/195]\tTime  0.401 ( 0.401)\tData  0.210 ( 0.210)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3889.3875 (3889.3875)\tnorm 0.9146 (0.9146)\n",
            "Epoch: [79][ 10/195]\tTime  0.146 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.7618 (0.7618)\tD(fake) 0.2569 (0.2451)\tgrad(D) penalty 0.0169 (0.0184)\tRec loss 3994.9868 (4046.4169)\tnorm 0.9087 (0.9084)\n",
            "Epoch: [79][ 20/195]\tTime  0.148 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7617 (0.7617)\tD(fake) 0.2229 (0.2411)\tgrad(D) penalty 0.0166 (0.0171)\tRec loss 3949.1875 (4018.4299)\tnorm 0.9087 (0.9083)\n",
            "Epoch: [79][ 30/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7616 (0.7617)\tD(fake) 0.2284 (0.2365)\tgrad(D) penalty 0.0156 (0.0165)\tRec loss 4239.3003 (4021.8429)\tnorm 0.9113 (0.9063)\n",
            "Epoch: [79][ 40/195]\tTime  0.145 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7618 (0.7618)\tD(fake) 0.2336 (0.2359)\tgrad(D) penalty 0.0191 (0.0166)\tRec loss 3796.9695 (4018.0334)\tnorm 0.9039 (0.9056)\n",
            "Epoch: [79][ 50/195]\tTime  0.173 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7619 (0.7618)\tD(fake) 0.2410 (0.2358)\tgrad(D) penalty 0.0167 (0.0168)\tRec loss 3944.0559 (4018.9821)\tnorm 0.9081 (0.9059)\n",
            "Epoch: [79][ 60/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7617 (0.7618)\tD(fake) 0.2250 (0.2345)\tgrad(D) penalty 0.0184 (0.0173)\tRec loss 4121.0474 (4032.7647)\tnorm 0.9026 (0.9059)\n",
            "Epoch: [79][ 70/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7618 (0.7618)\tD(fake) 0.2296 (0.2344)\tgrad(D) penalty 0.0153 (0.0172)\tRec loss 3992.4082 (4019.9211)\tnorm 0.9037 (0.9053)\n",
            "Epoch: [79][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7616 (0.7618)\tD(fake) 0.2407 (0.2344)\tgrad(D) penalty 0.0173 (0.0172)\tRec loss 4010.8223 (4018.7059)\tnorm 0.8928 (0.9051)\n",
            "Epoch: [79][ 90/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7619 (0.7618)\tD(fake) 0.2353 (0.2346)\tgrad(D) penalty 0.0178 (0.0172)\tRec loss 3789.5964 (4020.3063)\tnorm 0.9008 (0.9046)\n",
            "Epoch: [79][100/195]\tTime  0.164 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7619 (0.7618)\tD(fake) 0.2439 (0.2344)\tgrad(D) penalty 0.0173 (0.0173)\tRec loss 3936.8340 (4022.4211)\tnorm 0.8922 (0.9041)\n",
            "Epoch: [79][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7618 (0.7618)\tD(fake) 0.2381 (0.2351)\tgrad(D) penalty 0.0159 (0.0173)\tRec loss 4066.7275 (4020.1938)\tnorm 0.8994 (0.9036)\n",
            "Epoch: [79][120/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7620 (0.7618)\tD(fake) 0.2364 (0.2344)\tgrad(D) penalty 0.0131 (0.0172)\tRec loss 3760.2336 (4017.8949)\tnorm 0.8914 (0.9035)\n",
            "Epoch: [79][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7618 (0.7618)\tD(fake) 0.2194 (0.2337)\tgrad(D) penalty 0.0188 (0.0172)\tRec loss 4301.5430 (4019.5929)\tnorm 0.8881 (0.9035)\n",
            "Epoch: [79][140/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7621 (0.7618)\tD(fake) 0.2386 (0.2337)\tgrad(D) penalty 0.0151 (0.0171)\tRec loss 4197.4990 (4022.9082)\tnorm 0.8990 (0.9030)\n",
            "Epoch: [79][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7621 (0.7619)\tD(fake) 0.2392 (0.2337)\tgrad(D) penalty 0.0175 (0.0172)\tRec loss 3905.6216 (4028.6820)\tnorm 0.8944 (0.9028)\n",
            "Epoch: [79][160/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7618 (0.7619)\tD(fake) 0.2437 (0.2348)\tgrad(D) penalty 0.0158 (0.0171)\tRec loss 3906.3008 (4025.6826)\tnorm 0.9037 (0.9036)\n",
            "Epoch: [79][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7621 (0.7619)\tD(fake) 0.2328 (0.2348)\tgrad(D) penalty 0.0160 (0.0171)\tRec loss 4042.1091 (4026.8464)\tnorm 0.8936 (0.9035)\n",
            "Epoch: [79][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7620 (0.7619)\tD(fake) 0.2282 (0.2346)\tgrad(D) penalty 0.0189 (0.0171)\tRec loss 3956.6538 (4026.0264)\tnorm 0.8945 (0.9038)\n",
            "Epoch: [79][190/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7620 (0.7619)\tD(fake) 0.2473 (0.2350)\tgrad(D) penalty 0.0151 (0.0171)\tRec loss 3796.4058 (4026.7890)\tnorm 0.8996 (0.9036)\n",
            "Epoch: [80][  0/195]\tTime  0.397 ( 0.397)\tData  0.218 ( 0.218)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4038.7527 (4038.7527)\tnorm 0.8974 (0.8974)\n",
            "Epoch: [80][ 10/195]\tTime  0.145 ( 0.171)\tData  0.000 ( 0.020)\tD(real) 0.7622 (0.7621)\tD(fake) 0.2476 (0.2380)\tgrad(D) penalty 0.0149 (0.0170)\tRec loss 3916.1162 (4003.6515)\tnorm 0.8975 (0.8973)\n",
            "Epoch: [80][ 20/195]\tTime  0.147 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7621 (0.7621)\tD(fake) 0.2218 (0.2353)\tgrad(D) penalty 0.0215 (0.0175)\tRec loss 4108.1357 (4001.7513)\tnorm 0.8819 (0.8953)\n",
            "Epoch: [80][ 30/195]\tTime  0.146 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7621 (0.7621)\tD(fake) 0.2498 (0.2382)\tgrad(D) penalty 0.0162 (0.0171)\tRec loss 4032.6562 (4024.1679)\tnorm 0.8991 (0.8961)\n",
            "Epoch: [80][ 40/195]\tTime  0.152 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7622 (0.7622)\tD(fake) 0.2244 (0.2350)\tgrad(D) penalty 0.0193 (0.0172)\tRec loss 3917.8564 (4024.7813)\tnorm 0.8979 (0.8969)\n",
            "Epoch: [80][ 50/195]\tTime  0.167 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7622 (0.7622)\tD(fake) 0.2536 (0.2374)\tgrad(D) penalty 0.0151 (0.0169)\tRec loss 3923.0564 (4009.6316)\tnorm 0.8861 (0.8966)\n",
            "Epoch: [80][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7621 (0.7622)\tD(fake) 0.2346 (0.2375)\tgrad(D) penalty 0.0165 (0.0168)\tRec loss 3853.9919 (4014.1244)\tnorm 0.8971 (0.8966)\n",
            "Epoch: [80][ 70/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7621 (0.7622)\tD(fake) 0.2324 (0.2371)\tgrad(D) penalty 0.0169 (0.0168)\tRec loss 3774.1265 (4011.2982)\tnorm 0.8977 (0.8959)\n",
            "Epoch: [80][ 80/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7623 (0.7622)\tD(fake) 0.2476 (0.2372)\tgrad(D) penalty 0.0145 (0.0168)\tRec loss 4468.1709 (4015.1063)\tnorm 0.8852 (0.8955)\n",
            "Epoch: [80][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7621 (0.7622)\tD(fake) 0.2283 (0.2368)\tgrad(D) penalty 0.0138 (0.0165)\tRec loss 3785.3828 (4009.6515)\tnorm 0.8934 (0.8954)\n",
            "Epoch: [80][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7623 (0.7622)\tD(fake) 0.2399 (0.2362)\tgrad(D) penalty 0.0188 (0.0165)\tRec loss 3670.8184 (4004.3841)\tnorm 0.8754 (0.8950)\n",
            "Epoch: [80][110/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7622 (0.7622)\tD(fake) 0.2293 (0.2360)\tgrad(D) penalty 0.0157 (0.0164)\tRec loss 3977.8281 (4007.3161)\tnorm 0.8848 (0.8946)\n",
            "Epoch: [80][120/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7619 (0.7622)\tD(fake) 0.2323 (0.2353)\tgrad(D) penalty 0.0165 (0.0165)\tRec loss 4219.4399 (4013.4658)\tnorm 0.9073 (0.8953)\n",
            "Epoch: [80][130/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7621 (0.7622)\tD(fake) 0.2286 (0.2350)\tgrad(D) penalty 0.0181 (0.0165)\tRec loss 4156.1865 (4016.4426)\tnorm 0.9048 (0.8961)\n",
            "Epoch: [80][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7624 (0.7622)\tD(fake) 0.2306 (0.2350)\tgrad(D) penalty 0.0168 (0.0165)\tRec loss 4262.7495 (4023.5628)\tnorm 0.9125 (0.8966)\n",
            "Epoch: [80][150/195]\tTime  0.163 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7623 (0.7622)\tD(fake) 0.2283 (0.2345)\tgrad(D) penalty 0.0141 (0.0164)\tRec loss 4236.6553 (4023.3594)\tnorm 0.8967 (0.8970)\n",
            "Epoch: [80][160/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7623 (0.7622)\tD(fake) 0.2348 (0.2347)\tgrad(D) penalty 0.0155 (0.0164)\tRec loss 4066.4033 (4025.6094)\tnorm 0.8987 (0.8973)\n",
            "Epoch: [80][170/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7623 (0.7622)\tD(fake) 0.2261 (0.2343)\tgrad(D) penalty 0.0176 (0.0164)\tRec loss 3824.7263 (4023.3373)\tnorm 0.8931 (0.8976)\n",
            "Epoch: [80][180/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7624 (0.7622)\tD(fake) 0.2338 (0.2339)\tgrad(D) penalty 0.0164 (0.0164)\tRec loss 3926.0493 (4022.3632)\tnorm 0.8981 (0.8977)\n",
            "Epoch: [80][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7624 (0.7622)\tD(fake) 0.2204 (0.2335)\tgrad(D) penalty 0.0158 (0.0163)\tRec loss 3970.1377 (4021.7377)\tnorm 0.8844 (0.8974)\n",
            "Epoch: [81][  0/195]\tTime  0.392 ( 0.392)\tData  0.215 ( 0.215)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3707.3118 (3707.3118)\tnorm 0.8972 (0.8972)\n",
            "Epoch: [81][ 10/195]\tTime  0.146 ( 0.170)\tData  0.000 ( 0.020)\tD(real) 0.7623 (0.7624)\tD(fake) 0.2453 (0.2436)\tgrad(D) penalty 0.0159 (0.0155)\tRec loss 3836.2217 (3948.2994)\tnorm 0.9050 (0.8980)\n",
            "Epoch: [81][ 20/195]\tTime  0.148 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.7623 (0.7624)\tD(fake) 0.2202 (0.2299)\tgrad(D) penalty 0.0200 (0.0169)\tRec loss 4116.9116 (3967.9817)\tnorm 0.8867 (0.8946)\n",
            "Epoch: [81][ 30/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7624 (0.7624)\tD(fake) 0.2476 (0.2346)\tgrad(D) penalty 0.0152 (0.0167)\tRec loss 4118.1553 (3985.0351)\tnorm 0.9033 (0.8925)\n",
            "Epoch: [81][ 40/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7623 (0.7624)\tD(fake) 0.2282 (0.2344)\tgrad(D) penalty 0.0178 (0.0172)\tRec loss 4087.2266 (3985.2115)\tnorm 0.9034 (0.8948)\n",
            "Epoch: [81][ 50/195]\tTime  0.165 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7624 (0.7624)\tD(fake) 0.2291 (0.2334)\tgrad(D) penalty 0.0218 (0.0174)\tRec loss 3998.1465 (3988.5309)\tnorm 0.9005 (0.8950)\n",
            "Epoch: [81][ 60/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7625 (0.7624)\tD(fake) 0.2261 (0.2343)\tgrad(D) penalty 0.0167 (0.0171)\tRec loss 3887.5146 (3993.6909)\tnorm 0.9158 (0.8953)\n",
            "Epoch: [81][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7621 (0.7624)\tD(fake) 0.2296 (0.2332)\tgrad(D) penalty 0.0159 (0.0170)\tRec loss 4017.7009 (3996.7703)\tnorm 0.9261 (0.8968)\n",
            "Epoch: [81][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7624 (0.7624)\tD(fake) 0.2273 (0.2333)\tgrad(D) penalty 0.0196 (0.0170)\tRec loss 3926.8562 (3993.4116)\tnorm 0.9134 (0.8969)\n",
            "Epoch: [81][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7624 (0.7624)\tD(fake) 0.2312 (0.2328)\tgrad(D) penalty 0.0159 (0.0170)\tRec loss 4220.6812 (4004.6061)\tnorm 0.8970 (0.8971)\n",
            "Epoch: [81][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7624 (0.7624)\tD(fake) 0.2327 (0.2336)\tgrad(D) penalty 0.0189 (0.0170)\tRec loss 4053.6089 (4007.1278)\tnorm 0.9024 (0.8977)\n",
            "Epoch: [81][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7626 (0.7624)\tD(fake) 0.2355 (0.2332)\tgrad(D) penalty 0.0186 (0.0171)\tRec loss 4340.6060 (4014.5858)\tnorm 0.9050 (0.8983)\n",
            "Epoch: [81][120/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7624 (0.7624)\tD(fake) 0.2322 (0.2332)\tgrad(D) penalty 0.0154 (0.0170)\tRec loss 3877.8433 (4016.9860)\tnorm 0.8941 (0.8985)\n",
            "Epoch: [81][130/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7625 (0.7624)\tD(fake) 0.2512 (0.2340)\tgrad(D) penalty 0.0167 (0.0170)\tRec loss 3717.3882 (4016.5254)\tnorm 0.9041 (0.8987)\n",
            "Epoch: [81][140/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7625 (0.7624)\tD(fake) 0.2250 (0.2345)\tgrad(D) penalty 0.0173 (0.0169)\tRec loss 4060.4175 (4018.0908)\tnorm 0.9012 (0.8990)\n",
            "Epoch: [81][150/195]\tTime  0.163 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7625 (0.7624)\tD(fake) 0.2295 (0.2339)\tgrad(D) penalty 0.0167 (0.0169)\tRec loss 4005.4209 (4024.4722)\tnorm 0.9182 (0.8996)\n",
            "Epoch: [81][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7620 (0.7624)\tD(fake) 0.2231 (0.2340)\tgrad(D) penalty 0.0174 (0.0169)\tRec loss 4381.1343 (4026.9524)\tnorm 0.9031 (0.9001)\n",
            "Epoch: [81][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7624 (0.7624)\tD(fake) 0.2299 (0.2335)\tgrad(D) penalty 0.0174 (0.0169)\tRec loss 4073.0183 (4025.8983)\tnorm 0.8989 (0.8998)\n",
            "Epoch: [81][180/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7624 (0.7624)\tD(fake) 0.2280 (0.2334)\tgrad(D) penalty 0.0146 (0.0168)\tRec loss 4042.1191 (4023.4187)\tnorm 0.9111 (0.9000)\n",
            "Epoch: [81][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7624 (0.7624)\tD(fake) 0.2321 (0.2333)\tgrad(D) penalty 0.0167 (0.0168)\tRec loss 3929.6147 (4021.5615)\tnorm 0.9001 (0.9000)\n",
            "Epoch: [82][  0/195]\tTime  0.416 ( 0.416)\tData  0.219 ( 0.219)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3905.4246 (3905.4246)\tnorm 0.8949 (0.8949)\n",
            "Epoch: [82][ 10/195]\tTime  0.145 ( 0.172)\tData  0.000 ( 0.020)\tD(real) 0.7624 (0.7624)\tD(fake) 0.2329 (0.2269)\tgrad(D) penalty 0.0182 (0.0166)\tRec loss 3967.3726 (3992.4779)\tnorm 0.8906 (0.8977)\n",
            "Epoch: [82][ 20/195]\tTime  0.148 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7624 (0.7625)\tD(fake) 0.2320 (0.2357)\tgrad(D) penalty 0.0138 (0.0157)\tRec loss 3829.7876 (3995.8646)\tnorm 0.8999 (0.8974)\n",
            "Epoch: [82][ 30/195]\tTime  0.146 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7626 (0.7625)\tD(fake) 0.2302 (0.2324)\tgrad(D) penalty 0.0168 (0.0162)\tRec loss 4011.5859 (4018.5787)\tnorm 0.9015 (0.8975)\n",
            "Epoch: [82][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7626 (0.7625)\tD(fake) 0.2329 (0.2338)\tgrad(D) penalty 0.0161 (0.0162)\tRec loss 4122.3975 (4029.6742)\tnorm 0.9109 (0.9003)\n",
            "Epoch: [82][ 50/195]\tTime  0.165 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7624 (0.7625)\tD(fake) 0.2232 (0.2322)\tgrad(D) penalty 0.0171 (0.0164)\tRec loss 3775.1470 (4027.9475)\tnorm 0.9091 (0.9020)\n",
            "Epoch: [82][ 60/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7625 (0.7625)\tD(fake) 0.2376 (0.2311)\tgrad(D) penalty 0.0145 (0.0163)\tRec loss 4328.7651 (4024.9397)\tnorm 0.8902 (0.9015)\n",
            "Epoch: [82][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7626 (0.7625)\tD(fake) 0.2390 (0.2320)\tgrad(D) penalty 0.0177 (0.0162)\tRec loss 3963.4487 (4015.0422)\tnorm 0.8996 (0.9009)\n",
            "Epoch: [82][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7626 (0.7625)\tD(fake) 0.2247 (0.2317)\tgrad(D) penalty 0.0152 (0.0161)\tRec loss 4359.1187 (4006.5793)\tnorm 0.9002 (0.9003)\n",
            "Epoch: [82][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7626 (0.7625)\tD(fake) 0.2371 (0.2318)\tgrad(D) penalty 0.0157 (0.0160)\tRec loss 4071.9819 (4010.0431)\tnorm 0.9119 (0.9009)\n",
            "Epoch: [82][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7625 (0.7625)\tD(fake) 0.2231 (0.2316)\tgrad(D) penalty 0.0158 (0.0159)\tRec loss 3962.2981 (4015.2458)\tnorm 0.9009 (0.9014)\n",
            "Epoch: [82][110/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7626 (0.7625)\tD(fake) 0.2295 (0.2314)\tgrad(D) penalty 0.0137 (0.0159)\tRec loss 3982.4856 (4011.7288)\tnorm 0.9074 (0.9021)\n",
            "Epoch: [82][120/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7625 (0.7625)\tD(fake) 0.2385 (0.2317)\tgrad(D) penalty 0.0161 (0.0159)\tRec loss 4124.3906 (4014.0367)\tnorm 0.8952 (0.9021)\n",
            "Epoch: [82][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7627 (0.7625)\tD(fake) 0.2226 (0.2313)\tgrad(D) penalty 0.0134 (0.0158)\tRec loss 4449.1650 (4015.6863)\tnorm 0.8861 (0.9017)\n",
            "Epoch: [82][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7625 (0.7625)\tD(fake) 0.2357 (0.2312)\tgrad(D) penalty 0.0162 (0.0159)\tRec loss 4145.7822 (4017.5296)\tnorm 0.9017 (0.9014)\n",
            "Epoch: [82][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7626 (0.7626)\tD(fake) 0.2318 (0.2314)\tgrad(D) penalty 0.0183 (0.0159)\tRec loss 3971.2571 (4018.5855)\tnorm 0.8985 (0.9015)\n",
            "Epoch: [82][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7627 (0.7626)\tD(fake) 0.2383 (0.2314)\tgrad(D) penalty 0.0142 (0.0159)\tRec loss 3908.9990 (4024.9542)\tnorm 0.9091 (0.9018)\n",
            "Epoch: [82][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7628 (0.7626)\tD(fake) 0.2320 (0.2314)\tgrad(D) penalty 0.0173 (0.0160)\tRec loss 3620.4312 (4022.4529)\tnorm 0.9077 (0.9019)\n",
            "Epoch: [82][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7628 (0.7626)\tD(fake) 0.2380 (0.2318)\tgrad(D) penalty 0.0158 (0.0160)\tRec loss 3849.0342 (4023.4571)\tnorm 0.8944 (0.9020)\n",
            "Epoch: [82][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7629 (0.7626)\tD(fake) 0.2260 (0.2317)\tgrad(D) penalty 0.0181 (0.0160)\tRec loss 3927.7808 (4020.4141)\tnorm 0.9045 (0.9021)\n",
            "Epoch: [83][  0/195]\tTime  0.394 ( 0.394)\tData  0.215 ( 0.215)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4202.5020 (4202.5020)\tnorm 0.8912 (0.8912)\n",
            "Epoch: [83][ 10/195]\tTime  0.148 ( 0.173)\tData  0.000 ( 0.020)\tD(real) 0.7629 (0.7629)\tD(fake) 0.2563 (0.2481)\tgrad(D) penalty 0.0144 (0.0136)\tRec loss 3918.1265 (4068.0637)\tnorm 0.9121 (0.9025)\n",
            "Epoch: [83][ 20/195]\tTime  0.148 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7629 (0.7629)\tD(fake) 0.2251 (0.2401)\tgrad(D) penalty 0.0160 (0.0145)\tRec loss 4152.6934 (4053.2887)\tnorm 0.8987 (0.9023)\n",
            "Epoch: [83][ 30/195]\tTime  0.150 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7629 (0.7629)\tD(fake) 0.2317 (0.2369)\tgrad(D) penalty 0.0158 (0.0151)\tRec loss 4206.2871 (4025.8672)\tnorm 0.9006 (0.9030)\n",
            "Epoch: [83][ 40/195]\tTime  0.153 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7629 (0.7629)\tD(fake) 0.2390 (0.2383)\tgrad(D) penalty 0.0158 (0.0154)\tRec loss 4136.4375 (4019.0482)\tnorm 0.9136 (0.9032)\n",
            "Epoch: [83][ 50/195]\tTime  0.165 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7630 (0.7629)\tD(fake) 0.2244 (0.2358)\tgrad(D) penalty 0.0150 (0.0156)\tRec loss 3784.2566 (4012.9464)\tnorm 0.9044 (0.9038)\n",
            "Epoch: [83][ 60/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7629 (0.7629)\tD(fake) 0.2379 (0.2358)\tgrad(D) penalty 0.0168 (0.0157)\tRec loss 4496.7671 (4030.5275)\tnorm 0.8982 (0.9040)\n",
            "Epoch: [83][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7630 (0.7629)\tD(fake) 0.2414 (0.2365)\tgrad(D) penalty 0.0147 (0.0157)\tRec loss 4204.5928 (4032.3794)\tnorm 0.9068 (0.9049)\n",
            "Epoch: [83][ 80/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7629 (0.7629)\tD(fake) 0.2266 (0.2352)\tgrad(D) penalty 0.0174 (0.0158)\tRec loss 3710.6006 (4020.8492)\tnorm 0.9112 (0.9056)\n",
            "Epoch: [83][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7628 (0.7629)\tD(fake) 0.2217 (0.2340)\tgrad(D) penalty 0.0144 (0.0159)\tRec loss 4309.6094 (4024.7415)\tnorm 0.9013 (0.9051)\n",
            "Epoch: [83][100/195]\tTime  0.162 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7629 (0.7629)\tD(fake) 0.2338 (0.2340)\tgrad(D) penalty 0.0157 (0.0159)\tRec loss 3938.2041 (4021.0398)\tnorm 0.9047 (0.9049)\n",
            "Epoch: [83][110/195]\tTime  0.155 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7630 (0.7629)\tD(fake) 0.2493 (0.2345)\tgrad(D) penalty 0.0153 (0.0159)\tRec loss 3891.7427 (4011.9154)\tnorm 0.8850 (0.9049)\n",
            "Epoch: [83][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7631 (0.7629)\tD(fake) 0.2202 (0.2348)\tgrad(D) penalty 0.0146 (0.0157)\tRec loss 3994.4980 (4002.8338)\tnorm 0.9137 (0.9042)\n",
            "Epoch: [83][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7631 (0.7630)\tD(fake) 0.2272 (0.2340)\tgrad(D) penalty 0.0160 (0.0156)\tRec loss 4003.9229 (4003.8467)\tnorm 0.8942 (0.9037)\n",
            "Epoch: [83][140/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7632 (0.7630)\tD(fake) 0.2431 (0.2344)\tgrad(D) penalty 0.0145 (0.0156)\tRec loss 3795.3396 (4000.5546)\tnorm 0.9107 (0.9036)\n",
            "Epoch: [83][150/195]\tTime  0.170 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7632 (0.7630)\tD(fake) 0.2168 (0.2338)\tgrad(D) penalty 0.0175 (0.0156)\tRec loss 4205.4644 (4011.0994)\tnorm 0.9020 (0.9029)\n",
            "Epoch: [83][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7630 (0.7630)\tD(fake) 0.2435 (0.2336)\tgrad(D) penalty 0.0144 (0.0156)\tRec loss 4194.8442 (4014.7220)\tnorm 0.9097 (0.9031)\n",
            "Epoch: [83][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7633 (0.7630)\tD(fake) 0.2271 (0.2336)\tgrad(D) penalty 0.0174 (0.0156)\tRec loss 3782.0059 (4015.7715)\tnorm 0.8927 (0.9031)\n",
            "Epoch: [83][180/195]\tTime  0.153 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7631 (0.7630)\tD(fake) 0.2394 (0.2336)\tgrad(D) penalty 0.0162 (0.0156)\tRec loss 3712.5549 (4012.5243)\tnorm 0.8973 (0.9028)\n",
            "Epoch: [83][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7633 (0.7630)\tD(fake) 0.2366 (0.2337)\tgrad(D) penalty 0.0207 (0.0157)\tRec loss 4230.7720 (4016.7736)\tnorm 0.8748 (0.9025)\n",
            "Epoch: [84][  0/195]\tTime  0.434 ( 0.434)\tData  0.235 ( 0.235)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4107.3193 (4107.3193)\tnorm 0.9042 (0.9042)\n",
            "Epoch: [84][ 10/195]\tTime  0.150 ( 0.177)\tData  0.000 ( 0.022)\tD(real) 0.7632 (0.7633)\tD(fake) 0.2326 (0.2291)\tgrad(D) penalty 0.0129 (0.0143)\tRec loss 4249.0303 (4028.3202)\tnorm 0.8967 (0.8997)\n",
            "Epoch: [84][ 20/195]\tTime  0.152 ( 0.164)\tData  0.000 ( 0.011)\tD(real) 0.7633 (0.7633)\tD(fake) 0.2196 (0.2258)\tgrad(D) penalty 0.0164 (0.0158)\tRec loss 3910.9351 (4013.5267)\tnorm 0.9078 (0.9004)\n",
            "Epoch: [84][ 30/195]\tTime  0.147 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7631 (0.7633)\tD(fake) 0.2419 (0.2294)\tgrad(D) penalty 0.0154 (0.0161)\tRec loss 4433.0234 (4025.6018)\tnorm 0.9117 (0.8991)\n",
            "Epoch: [84][ 40/195]\tTime  0.150 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7634 (0.7633)\tD(fake) 0.2178 (0.2284)\tgrad(D) penalty 0.0177 (0.0159)\tRec loss 3933.2778 (4028.4539)\tnorm 0.9057 (0.9002)\n",
            "Epoch: [84][ 50/195]\tTime  0.164 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7634 (0.7633)\tD(fake) 0.2234 (0.2279)\tgrad(D) penalty 0.0129 (0.0156)\tRec loss 3969.4683 (4019.4461)\tnorm 0.9017 (0.9006)\n",
            "Epoch: [84][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7634 (0.7633)\tD(fake) 0.2501 (0.2291)\tgrad(D) penalty 0.0146 (0.0156)\tRec loss 3922.8364 (4020.0407)\tnorm 0.8961 (0.9008)\n",
            "Epoch: [84][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7634 (0.7633)\tD(fake) 0.2361 (0.2310)\tgrad(D) penalty 0.0157 (0.0154)\tRec loss 4195.9146 (4011.4755)\tnorm 0.8897 (0.8999)\n",
            "Epoch: [84][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7635 (0.7634)\tD(fake) 0.2489 (0.2320)\tgrad(D) penalty 0.0159 (0.0155)\tRec loss 3729.1594 (4011.6368)\tnorm 0.8911 (0.8995)\n",
            "Epoch: [84][ 90/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7634 (0.7634)\tD(fake) 0.2198 (0.2315)\tgrad(D) penalty 0.0155 (0.0154)\tRec loss 3900.1956 (4012.6500)\tnorm 0.9078 (0.8988)\n",
            "Epoch: [84][100/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7632 (0.7634)\tD(fake) 0.2325 (0.2316)\tgrad(D) penalty 0.0159 (0.0155)\tRec loss 3673.2935 (4010.8344)\tnorm 0.9022 (0.8990)\n",
            "Epoch: [84][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7635 (0.7634)\tD(fake) 0.2435 (0.2323)\tgrad(D) penalty 0.0162 (0.0155)\tRec loss 3969.4351 (4011.9642)\tnorm 0.8971 (0.8992)\n",
            "Epoch: [84][120/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7635 (0.7634)\tD(fake) 0.2268 (0.2327)\tgrad(D) penalty 0.0158 (0.0156)\tRec loss 3992.0806 (4015.5216)\tnorm 0.8934 (0.8993)\n",
            "Epoch: [84][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7636 (0.7634)\tD(fake) 0.2312 (0.2319)\tgrad(D) penalty 0.0172 (0.0157)\tRec loss 3807.9370 (4011.1313)\tnorm 0.9036 (0.8994)\n",
            "Epoch: [84][140/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7636 (0.7634)\tD(fake) 0.2443 (0.2335)\tgrad(D) penalty 0.0148 (0.0157)\tRec loss 4027.3887 (4010.4280)\tnorm 0.8907 (0.8991)\n",
            "Epoch: [84][150/195]\tTime  0.162 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7636 (0.7634)\tD(fake) 0.2248 (0.2329)\tgrad(D) penalty 0.0179 (0.0158)\tRec loss 4239.0381 (4007.9384)\tnorm 0.8935 (0.8987)\n",
            "Epoch: [84][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7636 (0.7634)\tD(fake) 0.2333 (0.2332)\tgrad(D) penalty 0.0150 (0.0158)\tRec loss 3906.2183 (4009.0224)\tnorm 0.9116 (0.8986)\n",
            "Epoch: [84][170/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7636 (0.7634)\tD(fake) 0.2263 (0.2330)\tgrad(D) penalty 0.0173 (0.0159)\tRec loss 3976.8481 (4008.4995)\tnorm 0.8933 (0.8987)\n",
            "Epoch: [84][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7637 (0.7634)\tD(fake) 0.2490 (0.2332)\tgrad(D) penalty 0.0153 (0.0160)\tRec loss 4040.2183 (4010.4653)\tnorm 0.8909 (0.8986)\n",
            "Epoch: [84][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7637 (0.7635)\tD(fake) 0.2301 (0.2333)\tgrad(D) penalty 0.0156 (0.0160)\tRec loss 4196.2549 (4008.3958)\tnorm 0.8917 (0.8985)\n",
            "Epoch: [85][  0/195]\tTime  0.402 ( 0.402)\tData  0.210 ( 0.210)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4145.0938 (4145.0938)\tnorm 0.8999 (0.8999)\n",
            "Epoch: [85][ 10/195]\tTime  0.150 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.7638 (0.7637)\tD(fake) 0.2593 (0.2447)\tgrad(D) penalty 0.0142 (0.0149)\tRec loss 4074.0386 (4013.5561)\tnorm 0.8915 (0.8958)\n",
            "Epoch: [85][ 20/195]\tTime  0.147 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7636 (0.7636)\tD(fake) 0.2134 (0.2340)\tgrad(D) penalty 0.0168 (0.0156)\tRec loss 3760.7236 (3983.8754)\tnorm 0.9010 (0.8985)\n",
            "Epoch: [85][ 30/195]\tTime  0.154 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.7636 (0.7636)\tD(fake) 0.2462 (0.2354)\tgrad(D) penalty 0.0172 (0.0159)\tRec loss 3638.1938 (4008.9264)\tnorm 0.9095 (0.8979)\n",
            "Epoch: [85][ 40/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7638 (0.7636)\tD(fake) 0.2171 (0.2340)\tgrad(D) penalty 0.0203 (0.0161)\tRec loss 4257.2246 (4004.4847)\tnorm 0.9008 (0.8992)\n",
            "Epoch: [85][ 50/195]\tTime  0.170 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7638 (0.7637)\tD(fake) 0.2484 (0.2352)\tgrad(D) penalty 0.0140 (0.0159)\tRec loss 3961.0828 (4008.5044)\tnorm 0.9029 (0.8990)\n",
            "Epoch: [85][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7638 (0.7637)\tD(fake) 0.2114 (0.2337)\tgrad(D) penalty 0.0189 (0.0159)\tRec loss 4068.9985 (4020.5020)\tnorm 0.9040 (0.8980)\n",
            "Epoch: [85][ 70/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7638 (0.7637)\tD(fake) 0.2488 (0.2338)\tgrad(D) penalty 0.0157 (0.0159)\tRec loss 3798.1597 (4015.9068)\tnorm 0.9121 (0.8983)\n",
            "Epoch: [85][ 80/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7636 (0.7637)\tD(fake) 0.2120 (0.2340)\tgrad(D) penalty 0.0172 (0.0158)\tRec loss 3969.2163 (4017.5055)\tnorm 0.9004 (0.8989)\n",
            "Epoch: [85][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7638 (0.7637)\tD(fake) 0.2372 (0.2333)\tgrad(D) penalty 0.0182 (0.0161)\tRec loss 4044.9756 (4017.8546)\tnorm 0.9089 (0.8987)\n",
            "Epoch: [85][100/195]\tTime  0.164 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7638 (0.7637)\tD(fake) 0.2227 (0.2341)\tgrad(D) penalty 0.0147 (0.0161)\tRec loss 4026.2056 (4010.5086)\tnorm 0.8867 (0.8986)\n",
            "Epoch: [85][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7638 (0.7637)\tD(fake) 0.2294 (0.2334)\tgrad(D) penalty 0.0170 (0.0162)\tRec loss 3938.8718 (4014.1073)\tnorm 0.9036 (0.8984)\n",
            "Epoch: [85][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7636 (0.7637)\tD(fake) 0.2287 (0.2332)\tgrad(D) penalty 0.0171 (0.0161)\tRec loss 3917.4209 (4010.2886)\tnorm 0.9041 (0.8983)\n",
            "Epoch: [85][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7637 (0.7637)\tD(fake) 0.2583 (0.2343)\tgrad(D) penalty 0.0149 (0.0161)\tRec loss 4238.8066 (4009.0201)\tnorm 0.8758 (0.8978)\n",
            "Epoch: [85][140/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7638 (0.7637)\tD(fake) 0.2178 (0.2333)\tgrad(D) penalty 0.0187 (0.0161)\tRec loss 3898.2090 (4001.5770)\tnorm 0.8873 (0.8974)\n",
            "Epoch: [85][150/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7638 (0.7637)\tD(fake) 0.2503 (0.2343)\tgrad(D) penalty 0.0136 (0.0159)\tRec loss 4038.9648 (4003.5275)\tnorm 0.8951 (0.8969)\n",
            "Epoch: [85][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7639 (0.7637)\tD(fake) 0.2167 (0.2333)\tgrad(D) penalty 0.0175 (0.0160)\tRec loss 3787.7944 (4000.9787)\tnorm 0.9036 (0.8968)\n",
            "Epoch: [85][170/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7638 (0.7637)\tD(fake) 0.2427 (0.2336)\tgrad(D) penalty 0.0170 (0.0159)\tRec loss 3934.1646 (3996.9906)\tnorm 0.8904 (0.8964)\n",
            "Epoch: [85][180/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7639 (0.7637)\tD(fake) 0.2193 (0.2335)\tgrad(D) penalty 0.0175 (0.0159)\tRec loss 4010.8477 (3997.9612)\tnorm 0.8917 (0.8962)\n",
            "Epoch: [85][190/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7639 (0.7637)\tD(fake) 0.2225 (0.2328)\tgrad(D) penalty 0.0155 (0.0159)\tRec loss 4145.0283 (3999.8469)\tnorm 0.9074 (0.8966)\n",
            "Epoch: [86][  0/195]\tTime  0.419 ( 0.419)\tData  0.218 ( 0.218)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4225.8555 (4225.8555)\tnorm 0.8866 (0.8866)\n",
            "Epoch: [86][ 10/195]\tTime  0.149 ( 0.174)\tData  0.000 ( 0.020)\tD(real) 0.7639 (0.7639)\tD(fake) 0.2325 (0.2310)\tgrad(D) penalty 0.0139 (0.0165)\tRec loss 4086.2554 (4025.1612)\tnorm 0.9005 (0.8972)\n",
            "Epoch: [86][ 20/195]\tTime  0.147 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7640 (0.7639)\tD(fake) 0.2266 (0.2311)\tgrad(D) penalty 0.0145 (0.0155)\tRec loss 3857.9619 (3990.0738)\tnorm 0.9104 (0.8959)\n",
            "Epoch: [86][ 30/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7640 (0.7639)\tD(fake) 0.2257 (0.2290)\tgrad(D) penalty 0.0152 (0.0155)\tRec loss 4373.4121 (3974.5301)\tnorm 0.8909 (0.8963)\n",
            "Epoch: [86][ 40/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.006)\tD(real) 0.7641 (0.7639)\tD(fake) 0.2398 (0.2342)\tgrad(D) penalty 0.0137 (0.0150)\tRec loss 4189.0479 (3964.9178)\tnorm 0.8948 (0.8965)\n",
            "Epoch: [86][ 50/195]\tTime  0.163 ( 0.153)\tData  0.000 ( 0.005)\tD(real) 0.7637 (0.7639)\tD(fake) 0.2261 (0.2324)\tgrad(D) penalty 0.0160 (0.0152)\tRec loss 3708.7964 (3964.0126)\tnorm 0.9044 (0.8975)\n",
            "Epoch: [86][ 60/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7639 (0.7639)\tD(fake) 0.2244 (0.2319)\tgrad(D) penalty 0.0146 (0.0151)\tRec loss 4172.5298 (3972.5864)\tnorm 0.9020 (0.8979)\n",
            "Epoch: [86][ 70/195]\tTime  0.154 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7640 (0.7640)\tD(fake) 0.2348 (0.2320)\tgrad(D) penalty 0.0150 (0.0152)\tRec loss 4191.0063 (3964.4682)\tnorm 0.8870 (0.8981)\n",
            "Epoch: [86][ 80/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7639 (0.7640)\tD(fake) 0.2323 (0.2331)\tgrad(D) penalty 0.0134 (0.0154)\tRec loss 3884.5623 (3969.0092)\tnorm 0.8998 (0.8966)\n",
            "Epoch: [86][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7640 (0.7640)\tD(fake) 0.2225 (0.2316)\tgrad(D) penalty 0.0181 (0.0155)\tRec loss 3937.8149 (3964.9573)\tnorm 0.8970 (0.8972)\n",
            "Epoch: [86][100/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7641 (0.7640)\tD(fake) 0.2352 (0.2315)\tgrad(D) penalty 0.0149 (0.0155)\tRec loss 4052.0388 (3965.1623)\tnorm 0.8924 (0.8965)\n",
            "Epoch: [86][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7641 (0.7640)\tD(fake) 0.2259 (0.2311)\tgrad(D) penalty 0.0153 (0.0156)\tRec loss 4183.1396 (3972.6312)\tnorm 0.8960 (0.8959)\n",
            "Epoch: [86][120/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7641 (0.7640)\tD(fake) 0.2423 (0.2321)\tgrad(D) penalty 0.0157 (0.0155)\tRec loss 4037.7534 (3971.5983)\tnorm 0.8988 (0.8957)\n",
            "Epoch: [86][130/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7639 (0.7640)\tD(fake) 0.2177 (0.2315)\tgrad(D) penalty 0.0161 (0.0155)\tRec loss 4075.7424 (3984.3836)\tnorm 0.9164 (0.8960)\n",
            "Epoch: [86][140/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7639 (0.7640)\tD(fake) 0.2430 (0.2315)\tgrad(D) penalty 0.0164 (0.0155)\tRec loss 4179.6543 (3989.9869)\tnorm 0.8887 (0.8959)\n",
            "Epoch: [86][150/195]\tTime  0.164 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7642 (0.7640)\tD(fake) 0.2396 (0.2322)\tgrad(D) penalty 0.0140 (0.0155)\tRec loss 3912.4590 (3993.6712)\tnorm 0.8981 (0.8960)\n",
            "Epoch: [86][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7641 (0.7640)\tD(fake) 0.2252 (0.2313)\tgrad(D) penalty 0.0145 (0.0155)\tRec loss 4176.3174 (4000.4138)\tnorm 0.8864 (0.8963)\n",
            "Epoch: [86][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7642 (0.7640)\tD(fake) 0.2368 (0.2317)\tgrad(D) penalty 0.0144 (0.0154)\tRec loss 4095.1714 (4003.4586)\tnorm 0.8839 (0.8962)\n",
            "Epoch: [86][180/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7642 (0.7640)\tD(fake) 0.2187 (0.2308)\tgrad(D) penalty 0.0184 (0.0154)\tRec loss 4123.7446 (3999.0847)\tnorm 0.9016 (0.8962)\n",
            "Epoch: [86][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7642 (0.7640)\tD(fake) 0.2430 (0.2311)\tgrad(D) penalty 0.0181 (0.0154)\tRec loss 3657.0354 (3997.6226)\tnorm 0.8963 (0.8961)\n",
            "Epoch: [87][  0/195]\tTime  0.408 ( 0.408)\tData  0.209 ( 0.209)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3663.3950 (3663.3950)\tnorm 0.9018 (0.9018)\n",
            "Epoch: [87][ 10/195]\tTime  0.152 ( 0.172)\tData  0.000 ( 0.019)\tD(real) 0.7643 (0.7642)\tD(fake) 0.2219 (0.2208)\tgrad(D) penalty 0.0169 (0.0157)\tRec loss 3860.1597 (4033.6252)\tnorm 0.8721 (0.8935)\n",
            "Epoch: [87][ 20/195]\tTime  0.146 ( 0.161)\tData  0.000 ( 0.010)\tD(real) 0.7642 (0.7642)\tD(fake) 0.2541 (0.2364)\tgrad(D) penalty 0.0145 (0.0151)\tRec loss 4046.4155 (4005.0567)\tnorm 0.8939 (0.8928)\n",
            "Epoch: [87][ 30/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7644 (0.7642)\tD(fake) 0.2224 (0.2342)\tgrad(D) penalty 0.0195 (0.0159)\tRec loss 4162.4370 (4005.5538)\tnorm 0.9021 (0.8924)\n",
            "Epoch: [87][ 40/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7644 (0.7642)\tD(fake) 0.2324 (0.2333)\tgrad(D) penalty 0.0136 (0.0155)\tRec loss 3802.2261 (3995.2090)\tnorm 0.8849 (0.8924)\n",
            "Epoch: [87][ 50/195]\tTime  0.163 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7644 (0.7643)\tD(fake) 0.2321 (0.2331)\tgrad(D) penalty 0.0154 (0.0157)\tRec loss 3822.4534 (3998.1057)\tnorm 0.8974 (0.8921)\n",
            "Epoch: [87][ 60/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7642 (0.7643)\tD(fake) 0.2331 (0.2340)\tgrad(D) penalty 0.0144 (0.0158)\tRec loss 3896.0334 (3993.8182)\tnorm 0.8913 (0.8922)\n",
            "Epoch: [87][ 70/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7643 (0.7643)\tD(fake) 0.2229 (0.2326)\tgrad(D) penalty 0.0164 (0.0158)\tRec loss 3993.7305 (3987.8338)\tnorm 0.8902 (0.8924)\n",
            "Epoch: [87][ 80/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7644 (0.7643)\tD(fake) 0.2271 (0.2319)\tgrad(D) penalty 0.0165 (0.0158)\tRec loss 3907.6702 (3986.6418)\tnorm 0.9007 (0.8937)\n",
            "Epoch: [87][ 90/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7644 (0.7643)\tD(fake) 0.2373 (0.2319)\tgrad(D) penalty 0.0165 (0.0158)\tRec loss 3848.1836 (3986.2670)\tnorm 0.9089 (0.8951)\n",
            "Epoch: [87][100/195]\tTime  0.164 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7644 (0.7643)\tD(fake) 0.2132 (0.2318)\tgrad(D) penalty 0.0165 (0.0158)\tRec loss 4189.4302 (3988.2671)\tnorm 0.9080 (0.8956)\n",
            "Epoch: [87][110/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7644 (0.7643)\tD(fake) 0.2419 (0.2319)\tgrad(D) penalty 0.0184 (0.0159)\tRec loss 3960.5107 (3983.1710)\tnorm 0.8970 (0.8957)\n",
            "Epoch: [87][120/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7644 (0.7643)\tD(fake) 0.2185 (0.2314)\tgrad(D) penalty 0.0163 (0.0159)\tRec loss 3993.3777 (3979.1787)\tnorm 0.8896 (0.8954)\n",
            "Epoch: [87][130/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7642 (0.7643)\tD(fake) 0.2352 (0.2315)\tgrad(D) penalty 0.0201 (0.0162)\tRec loss 4155.4556 (3979.0772)\tnorm 0.8965 (0.8948)\n",
            "Epoch: [87][140/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7645 (0.7643)\tD(fake) 0.2287 (0.2314)\tgrad(D) penalty 0.0167 (0.0163)\tRec loss 4304.3926 (3987.2867)\tnorm 0.9027 (0.8944)\n",
            "Epoch: [87][150/195]\tTime  0.162 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7645 (0.7643)\tD(fake) 0.2310 (0.2314)\tgrad(D) penalty 0.0138 (0.0162)\tRec loss 3883.7979 (3991.8427)\tnorm 0.9016 (0.8943)\n",
            "Epoch: [87][160/195]\tTime  0.153 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7644 (0.7643)\tD(fake) 0.2314 (0.2314)\tgrad(D) penalty 0.0152 (0.0162)\tRec loss 3886.2314 (3992.6221)\tnorm 0.8933 (0.8946)\n",
            "Epoch: [87][170/195]\tTime  0.145 ( 0.149)\tData  0.000 ( 0.001)\tD(real) 0.7646 (0.7644)\tD(fake) 0.2338 (0.2318)\tgrad(D) penalty 0.0127 (0.0160)\tRec loss 4127.0337 (3993.7938)\tnorm 0.8825 (0.8943)\n",
            "Epoch: [87][180/195]\tTime  0.149 ( 0.149)\tData  0.000 ( 0.001)\tD(real) 0.7645 (0.7644)\tD(fake) 0.2483 (0.2320)\tgrad(D) penalty 0.0141 (0.0160)\tRec loss 3675.4065 (3997.8468)\tnorm 0.9049 (0.8946)\n",
            "Epoch: [87][190/195]\tTime  0.149 ( 0.149)\tData  0.000 ( 0.001)\tD(real) 0.7644 (0.7644)\tD(fake) 0.2301 (0.2324)\tgrad(D) penalty 0.0141 (0.0159)\tRec loss 4061.1003 (3999.0372)\tnorm 0.8990 (0.8948)\n",
            "Epoch: [88][  0/195]\tTime  0.411 ( 0.411)\tData  0.215 ( 0.215)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3910.0483 (3910.0483)\tnorm 0.9070 (0.9070)\n",
            "Epoch: [88][ 10/195]\tTime  0.152 ( 0.175)\tData  0.000 ( 0.020)\tD(real) 0.7646 (0.7645)\tD(fake) 0.2608 (0.2507)\tgrad(D) penalty 0.0145 (0.0140)\tRec loss 3915.9146 (4018.5279)\tnorm 0.8970 (0.9006)\n",
            "Epoch: [88][ 20/195]\tTime  0.145 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7646 (0.7646)\tD(fake) 0.2249 (0.2371)\tgrad(D) penalty 0.0183 (0.0161)\tRec loss 4090.5098 (3999.9035)\tnorm 0.8867 (0.8995)\n",
            "Epoch: [88][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7644 (0.7646)\tD(fake) 0.2434 (0.2390)\tgrad(D) penalty 0.0174 (0.0163)\tRec loss 4031.9868 (4013.6672)\tnorm 0.9069 (0.8999)\n",
            "Epoch: [88][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7645 (0.7646)\tD(fake) 0.2136 (0.2347)\tgrad(D) penalty 0.0180 (0.0165)\tRec loss 3738.1509 (3997.9243)\tnorm 0.9081 (0.9002)\n",
            "Epoch: [88][ 50/195]\tTime  0.167 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7646 (0.7646)\tD(fake) 0.2491 (0.2355)\tgrad(D) penalty 0.0145 (0.0165)\tRec loss 3956.3088 (3996.4286)\tnorm 0.9054 (0.8999)\n",
            "Epoch: [88][ 60/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7645 (0.7646)\tD(fake) 0.2173 (0.2331)\tgrad(D) penalty 0.0202 (0.0166)\tRec loss 3891.2776 (3974.5073)\tnorm 0.9122 (0.9012)\n",
            "Epoch: [88][ 70/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7646 (0.7646)\tD(fake) 0.2430 (0.2340)\tgrad(D) penalty 0.0135 (0.0164)\tRec loss 3955.4746 (3974.3214)\tnorm 0.8938 (0.9003)\n",
            "Epoch: [88][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7646 (0.7646)\tD(fake) 0.2174 (0.2334)\tgrad(D) penalty 0.0164 (0.0166)\tRec loss 4093.2363 (3972.0486)\tnorm 0.8827 (0.9001)\n",
            "Epoch: [88][ 90/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7646 (0.7646)\tD(fake) 0.2385 (0.2328)\tgrad(D) penalty 0.0158 (0.0166)\tRec loss 4184.8496 (3969.6788)\tnorm 0.8972 (0.8989)\n",
            "Epoch: [88][100/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7647 (0.7646)\tD(fake) 0.2059 (0.2328)\tgrad(D) penalty 0.0152 (0.0163)\tRec loss 3904.3770 (3969.0115)\tnorm 0.9032 (0.8993)\n",
            "Epoch: [88][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7647 (0.7646)\tD(fake) 0.2562 (0.2333)\tgrad(D) penalty 0.0162 (0.0163)\tRec loss 4009.3931 (3976.5242)\tnorm 0.8957 (0.8988)\n",
            "Epoch: [88][120/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7648 (0.7646)\tD(fake) 0.2129 (0.2335)\tgrad(D) penalty 0.0178 (0.0162)\tRec loss 4152.0215 (3983.3293)\tnorm 0.8997 (0.8986)\n",
            "Epoch: [88][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7647 (0.7646)\tD(fake) 0.2383 (0.2335)\tgrad(D) penalty 0.0131 (0.0161)\tRec loss 4125.3398 (3983.2385)\tnorm 0.8922 (0.8988)\n",
            "Epoch: [88][140/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7648 (0.7646)\tD(fake) 0.2274 (0.2329)\tgrad(D) penalty 0.0174 (0.0162)\tRec loss 4018.6836 (3990.2388)\tnorm 0.8913 (0.8987)\n",
            "Epoch: [88][150/195]\tTime  0.162 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7646 (0.7646)\tD(fake) 0.2472 (0.2337)\tgrad(D) penalty 0.0154 (0.0162)\tRec loss 4007.6904 (3990.6569)\tnorm 0.9000 (0.8988)\n",
            "Epoch: [88][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7648 (0.7646)\tD(fake) 0.2161 (0.2325)\tgrad(D) penalty 0.0170 (0.0162)\tRec loss 3701.6172 (3995.5177)\tnorm 0.9147 (0.8991)\n",
            "Epoch: [88][170/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7649 (0.7647)\tD(fake) 0.2426 (0.2330)\tgrad(D) penalty 0.0146 (0.0162)\tRec loss 4250.5029 (3996.2518)\tnorm 0.8912 (0.8993)\n",
            "Epoch: [88][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7648 (0.7647)\tD(fake) 0.2222 (0.2324)\tgrad(D) penalty 0.0153 (0.0161)\tRec loss 4061.3611 (3997.0463)\tnorm 0.9006 (0.8995)\n",
            "Epoch: [88][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7649 (0.7647)\tD(fake) 0.2321 (0.2323)\tgrad(D) penalty 0.0159 (0.0161)\tRec loss 4257.3369 (3998.3246)\tnorm 0.8937 (0.8993)\n",
            "Epoch: [89][  0/195]\tTime  0.406 ( 0.406)\tData  0.216 ( 0.216)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4573.6064 (4573.6064)\tnorm 0.8888 (0.8888)\n",
            "Epoch: [89][ 10/195]\tTime  0.152 ( 0.173)\tData  0.000 ( 0.020)\tD(real) 0.7648 (0.7648)\tD(fake) 0.2291 (0.2282)\tgrad(D) penalty 0.0152 (0.0140)\tRec loss 3976.2253 (4053.8795)\tnorm 0.8927 (0.9034)\n",
            "Epoch: [89][ 20/195]\tTime  0.150 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7648 (0.7648)\tD(fake) 0.2501 (0.2363)\tgrad(D) penalty 0.0143 (0.0149)\tRec loss 4155.0420 (4030.5222)\tnorm 0.8932 (0.9004)\n",
            "Epoch: [89][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7649 (0.7648)\tD(fake) 0.2284 (0.2354)\tgrad(D) penalty 0.0161 (0.0149)\tRec loss 4040.3931 (4024.1290)\tnorm 0.8940 (0.9005)\n",
            "Epoch: [89][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7648 (0.7648)\tD(fake) 0.2497 (0.2354)\tgrad(D) penalty 0.0144 (0.0153)\tRec loss 3869.0896 (4003.8437)\tnorm 0.9056 (0.9002)\n",
            "Epoch: [89][ 50/195]\tTime  0.167 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7649 (0.7648)\tD(fake) 0.2318 (0.2350)\tgrad(D) penalty 0.0142 (0.0152)\tRec loss 3863.7505 (3994.6939)\tnorm 0.8860 (0.8989)\n",
            "Epoch: [89][ 60/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7649 (0.7648)\tD(fake) 0.2461 (0.2349)\tgrad(D) penalty 0.0165 (0.0154)\tRec loss 4022.2461 (3981.5820)\tnorm 0.8993 (0.8975)\n",
            "Epoch: [89][ 70/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7649 (0.7648)\tD(fake) 0.2370 (0.2360)\tgrad(D) penalty 0.0157 (0.0155)\tRec loss 3949.9709 (3979.7723)\tnorm 0.8877 (0.8974)\n",
            "Epoch: [89][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7649 (0.7648)\tD(fake) 0.2351 (0.2360)\tgrad(D) penalty 0.0145 (0.0155)\tRec loss 4347.0063 (3989.2976)\tnorm 0.8851 (0.8965)\n",
            "Epoch: [89][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7649 (0.7648)\tD(fake) 0.2313 (0.2362)\tgrad(D) penalty 0.0157 (0.0155)\tRec loss 4128.5098 (3988.0403)\tnorm 0.8942 (0.8959)\n",
            "Epoch: [89][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7648 (0.7648)\tD(fake) 0.2467 (0.2363)\tgrad(D) penalty 0.0177 (0.0158)\tRec loss 3912.0649 (3987.5832)\tnorm 0.9097 (0.8961)\n",
            "Epoch: [89][110/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7648 (0.7648)\tD(fake) 0.2217 (0.2363)\tgrad(D) penalty 0.0147 (0.0157)\tRec loss 4064.3250 (3990.0117)\tnorm 0.8890 (0.8958)\n",
            "Epoch: [89][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7648 (0.7648)\tD(fake) 0.2373 (0.2351)\tgrad(D) penalty 0.0163 (0.0158)\tRec loss 3838.8999 (3983.5048)\tnorm 0.9029 (0.8962)\n",
            "Epoch: [89][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7650 (0.7649)\tD(fake) 0.2245 (0.2350)\tgrad(D) penalty 0.0158 (0.0159)\tRec loss 3710.4912 (3984.3154)\tnorm 0.8795 (0.8960)\n",
            "Epoch: [89][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7650 (0.7649)\tD(fake) 0.2095 (0.2331)\tgrad(D) penalty 0.0171 (0.0159)\tRec loss 4088.6941 (3989.6475)\tnorm 0.8864 (0.8953)\n",
            "Epoch: [89][150/195]\tTime  0.170 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7648 (0.7649)\tD(fake) 0.2493 (0.2337)\tgrad(D) penalty 0.0157 (0.0160)\tRec loss 4113.6782 (3989.8586)\tnorm 0.8871 (0.8949)\n",
            "Epoch: [89][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7648 (0.7649)\tD(fake) 0.2174 (0.2330)\tgrad(D) penalty 0.0172 (0.0161)\tRec loss 4146.2021 (3986.7328)\tnorm 0.8997 (0.8946)\n",
            "Epoch: [89][170/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7648 (0.7649)\tD(fake) 0.2619 (0.2340)\tgrad(D) penalty 0.0176 (0.0162)\tRec loss 4173.9712 (3987.4449)\tnorm 0.8840 (0.8942)\n",
            "Epoch: [89][180/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2176 (0.2338)\tgrad(D) penalty 0.0175 (0.0163)\tRec loss 3971.3760 (3988.2060)\tnorm 0.8989 (0.8944)\n",
            "Epoch: [89][190/195]\tTime  0.144 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2560 (0.2345)\tgrad(D) penalty 0.0171 (0.0163)\tRec loss 3843.1333 (3990.0961)\tnorm 0.8980 (0.8943)\n",
            "Epoch: [90][  0/195]\tTime  0.399 ( 0.399)\tData  0.213 ( 0.213)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4349.4458 (4349.4458)\tnorm 0.8903 (0.8903)\n",
            "Epoch: [90][ 10/195]\tTime  0.148 ( 0.171)\tData  0.000 ( 0.020)\tD(real) 0.7650 (0.7650)\tD(fake) 0.2797 (0.2545)\tgrad(D) penalty 0.0168 (0.0182)\tRec loss 4321.8145 (4067.3016)\tnorm 0.8891 (0.8991)\n",
            "Epoch: [90][ 20/195]\tTime  0.148 ( 0.160)\tData  0.000 ( 0.010)\tD(real) 0.7648 (0.7649)\tD(fake) 0.1988 (0.2406)\tgrad(D) penalty 0.0161 (0.0164)\tRec loss 3858.1948 (4031.8502)\tnorm 0.9074 (0.8974)\n",
            "Epoch: [90][ 30/195]\tTime  0.150 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7650 (0.7649)\tD(fake) 0.2704 (0.2431)\tgrad(D) penalty 0.0154 (0.0168)\tRec loss 4062.8408 (4008.8333)\tnorm 0.8966 (0.8984)\n",
            "Epoch: [90][ 40/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7650 (0.7649)\tD(fake) 0.2177 (0.2418)\tgrad(D) penalty 0.0208 (0.0166)\tRec loss 3978.3564 (4002.2785)\tnorm 0.8826 (0.8967)\n",
            "Epoch: [90][ 50/195]\tTime  0.163 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2401 (0.2414)\tgrad(D) penalty 0.0146 (0.0163)\tRec loss 3679.3120 (3994.7035)\tnorm 0.8916 (0.8960)\n",
            "Epoch: [90][ 60/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7648 (0.7649)\tD(fake) 0.2252 (0.2395)\tgrad(D) penalty 0.0154 (0.0161)\tRec loss 4056.7795 (4001.5990)\tnorm 0.8920 (0.8960)\n",
            "Epoch: [90][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7650 (0.7649)\tD(fake) 0.2299 (0.2377)\tgrad(D) penalty 0.0166 (0.0162)\tRec loss 4015.2358 (3995.6113)\tnorm 0.9033 (0.8965)\n",
            "Epoch: [90][ 80/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7647 (0.7649)\tD(fake) 0.2381 (0.2378)\tgrad(D) penalty 0.0148 (0.0159)\tRec loss 4027.7585 (3988.6302)\tnorm 0.8938 (0.8968)\n",
            "Epoch: [90][ 90/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7647 (0.7649)\tD(fake) 0.2376 (0.2370)\tgrad(D) penalty 0.0168 (0.0161)\tRec loss 3942.8955 (4003.0318)\tnorm 0.9040 (0.8965)\n",
            "Epoch: [90][100/195]\tTime  0.172 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7647 (0.7649)\tD(fake) 0.2161 (0.2380)\tgrad(D) penalty 0.0185 (0.0161)\tRec loss 3599.8647 (3990.6961)\tnorm 0.8892 (0.8965)\n",
            "Epoch: [90][110/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2324 (0.2372)\tgrad(D) penalty 0.0146 (0.0160)\tRec loss 3943.7065 (3983.9777)\tnorm 0.8688 (0.8960)\n",
            "Epoch: [90][120/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2344 (0.2368)\tgrad(D) penalty 0.0162 (0.0160)\tRec loss 4049.2144 (3989.1233)\tnorm 0.8894 (0.8960)\n",
            "Epoch: [90][130/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7650 (0.7649)\tD(fake) 0.2293 (0.2366)\tgrad(D) penalty 0.0158 (0.0160)\tRec loss 3893.8809 (3985.1271)\tnorm 0.8906 (0.8962)\n",
            "Epoch: [90][140/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2530 (0.2370)\tgrad(D) penalty 0.0144 (0.0160)\tRec loss 4097.5786 (3989.9911)\tnorm 0.8956 (0.8962)\n",
            "Epoch: [90][150/195]\tTime  0.166 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7646 (0.7649)\tD(fake) 0.2143 (0.2362)\tgrad(D) penalty 0.0141 (0.0159)\tRec loss 3742.2827 (3990.1079)\tnorm 0.8973 (0.8966)\n",
            "Epoch: [90][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2478 (0.2364)\tgrad(D) penalty 0.0126 (0.0159)\tRec loss 3769.4971 (3987.5846)\tnorm 0.8986 (0.8964)\n",
            "Epoch: [90][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2369 (0.2364)\tgrad(D) penalty 0.0133 (0.0158)\tRec loss 3870.2476 (3988.4333)\tnorm 0.8769 (0.8959)\n",
            "Epoch: [90][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2326 (0.2365)\tgrad(D) penalty 0.0202 (0.0160)\tRec loss 4252.0000 (3989.4853)\tnorm 0.8829 (0.8956)\n",
            "Epoch: [90][190/195]\tTime  0.148 ( 0.149)\tData  0.000 ( 0.001)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2467 (0.2367)\tgrad(D) penalty 0.0142 (0.0160)\tRec loss 3732.2600 (3988.0429)\tnorm 0.9026 (0.8954)\n",
            "Epoch: [91][  0/195]\tTime  0.424 ( 0.424)\tData  0.226 ( 0.226)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4160.3857 (4160.3857)\tnorm 0.8939 (0.8939)\n",
            "Epoch: [91][ 10/195]\tTime  0.156 ( 0.175)\tData  0.000 ( 0.021)\tD(real) 0.7649 (0.7648)\tD(fake) 0.2528 (0.2413)\tgrad(D) penalty 0.0145 (0.0152)\tRec loss 3955.7275 (4062.6758)\tnorm 0.9078 (0.8988)\n",
            "Epoch: [91][ 20/195]\tTime  0.150 ( 0.164)\tData  0.000 ( 0.011)\tD(real) 0.7649 (0.7648)\tD(fake) 0.2151 (0.2342)\tgrad(D) penalty 0.0151 (0.0160)\tRec loss 4033.7908 (4062.9145)\tnorm 0.8978 (0.9004)\n",
            "Epoch: [91][ 30/195]\tTime  0.150 ( 0.160)\tData  0.000 ( 0.008)\tD(real) 0.7649 (0.7648)\tD(fake) 0.2481 (0.2350)\tgrad(D) penalty 0.0140 (0.0159)\tRec loss 4028.1150 (4048.2659)\tnorm 0.9020 (0.9000)\n",
            "Epoch: [91][ 40/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2150 (0.2315)\tgrad(D) penalty 0.0187 (0.0160)\tRec loss 3720.4470 (4008.3062)\tnorm 0.8882 (0.8977)\n",
            "Epoch: [91][ 50/195]\tTime  0.166 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7649 (0.7648)\tD(fake) 0.2495 (0.2330)\tgrad(D) penalty 0.0157 (0.0163)\tRec loss 3963.0801 (3988.8510)\tnorm 0.8908 (0.8969)\n",
            "Epoch: [91][ 60/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7649 (0.7648)\tD(fake) 0.2134 (0.2321)\tgrad(D) penalty 0.0136 (0.0159)\tRec loss 3930.9819 (3978.6304)\tnorm 0.8899 (0.8964)\n",
            "Epoch: [91][ 70/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7649 (0.7648)\tD(fake) 0.2406 (0.2316)\tgrad(D) penalty 0.0188 (0.0162)\tRec loss 3836.5510 (3983.6588)\tnorm 0.8831 (0.8959)\n",
            "Epoch: [91][ 80/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2119 (0.2325)\tgrad(D) penalty 0.0171 (0.0160)\tRec loss 3764.9009 (3987.3589)\tnorm 0.9063 (0.8961)\n",
            "Epoch: [91][ 90/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7648 (0.7649)\tD(fake) 0.2447 (0.2326)\tgrad(D) penalty 0.0175 (0.0161)\tRec loss 3998.1055 (3993.8392)\tnorm 0.8934 (0.8957)\n",
            "Epoch: [91][100/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7648 (0.7648)\tD(fake) 0.2220 (0.2328)\tgrad(D) penalty 0.0181 (0.0161)\tRec loss 3820.1816 (3991.3917)\tnorm 0.8922 (0.8956)\n",
            "Epoch: [91][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7651 (0.7649)\tD(fake) 0.2332 (0.2331)\tgrad(D) penalty 0.0135 (0.0160)\tRec loss 4049.4695 (3993.0963)\tnorm 0.8961 (0.8958)\n",
            "Epoch: [91][120/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2381 (0.2334)\tgrad(D) penalty 0.0127 (0.0157)\tRec loss 3930.3384 (3989.7640)\tnorm 0.9074 (0.8964)\n",
            "Epoch: [91][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2351 (0.2335)\tgrad(D) penalty 0.0117 (0.0155)\tRec loss 3902.3848 (3995.6906)\tnorm 0.9036 (0.8968)\n",
            "Epoch: [91][140/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7650 (0.7649)\tD(fake) 0.2168 (0.2326)\tgrad(D) penalty 0.0180 (0.0155)\tRec loss 3940.3647 (3992.8540)\tnorm 0.9067 (0.8970)\n",
            "Epoch: [91][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2325 (0.2326)\tgrad(D) penalty 0.0162 (0.0155)\tRec loss 3898.9043 (3989.7143)\tnorm 0.8851 (0.8968)\n",
            "Epoch: [91][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7650 (0.7649)\tD(fake) 0.2298 (0.2323)\tgrad(D) penalty 0.0118 (0.0154)\tRec loss 4068.7495 (3987.2976)\tnorm 0.8890 (0.8963)\n",
            "Epoch: [91][170/195]\tTime  0.144 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7651 (0.7649)\tD(fake) 0.2421 (0.2324)\tgrad(D) penalty 0.0150 (0.0154)\tRec loss 4017.7749 (3987.5064)\tnorm 0.8835 (0.8958)\n",
            "Epoch: [91][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7649 (0.7649)\tD(fake) 0.2226 (0.2323)\tgrad(D) penalty 0.0170 (0.0153)\tRec loss 3501.8320 (3982.6006)\tnorm 0.8764 (0.8952)\n",
            "Epoch: [91][190/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7650 (0.7649)\tD(fake) 0.2412 (0.2322)\tgrad(D) penalty 0.0192 (0.0155)\tRec loss 3814.3564 (3983.0878)\tnorm 0.9017 (0.8948)\n",
            "Epoch: [92][  0/195]\tTime  0.403 ( 0.403)\tData  0.224 ( 0.224)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4133.4912 (4133.4912)\tnorm 0.8963 (0.8963)\n",
            "Epoch: [92][ 10/195]\tTime  0.145 ( 0.173)\tData  0.000 ( 0.021)\tD(real) 0.7649 (0.7650)\tD(fake) 0.2223 (0.2230)\tgrad(D) penalty 0.0156 (0.0166)\tRec loss 3833.1865 (3992.7618)\tnorm 0.8825 (0.8891)\n",
            "Epoch: [92][ 20/195]\tTime  0.146 ( 0.160)\tData  0.000 ( 0.011)\tD(real) 0.7650 (0.7650)\tD(fake) 0.2335 (0.2277)\tgrad(D) penalty 0.0164 (0.0173)\tRec loss 4065.8669 (3957.2361)\tnorm 0.8841 (0.8879)\n",
            "Epoch: [92][ 30/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7651 (0.7650)\tD(fake) 0.2341 (0.2293)\tgrad(D) penalty 0.0144 (0.0164)\tRec loss 3750.4111 (3974.0794)\tnorm 0.8954 (0.8866)\n",
            "Epoch: [92][ 40/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.006)\tD(real) 0.7651 (0.7650)\tD(fake) 0.2318 (0.2305)\tgrad(D) penalty 0.0164 (0.0159)\tRec loss 4110.2646 (3998.8874)\tnorm 0.9135 (0.8891)\n",
            "Epoch: [92][ 50/195]\tTime  0.163 ( 0.153)\tData  0.000 ( 0.005)\tD(real) 0.7651 (0.7650)\tD(fake) 0.2390 (0.2312)\tgrad(D) penalty 0.0132 (0.0157)\tRec loss 4052.0571 (4009.5736)\tnorm 0.8943 (0.8900)\n",
            "Epoch: [92][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7651 (0.7650)\tD(fake) 0.2268 (0.2319)\tgrad(D) penalty 0.0162 (0.0157)\tRec loss 4286.2456 (4008.6646)\tnorm 0.8918 (0.8907)\n",
            "Epoch: [92][ 70/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7650 (0.7650)\tD(fake) 0.2346 (0.2322)\tgrad(D) penalty 0.0147 (0.0156)\tRec loss 3751.6716 (4000.8608)\tnorm 0.9027 (0.8914)\n",
            "Epoch: [92][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7650 (0.7650)\tD(fake) 0.2256 (0.2321)\tgrad(D) penalty 0.0171 (0.0157)\tRec loss 3770.6377 (3986.1595)\tnorm 0.8940 (0.8912)\n",
            "Epoch: [92][ 90/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7650 (0.7650)\tD(fake) 0.2300 (0.2319)\tgrad(D) penalty 0.0153 (0.0157)\tRec loss 4335.0000 (3987.1915)\tnorm 0.9011 (0.8910)\n",
            "Epoch: [92][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7650 (0.7650)\tD(fake) 0.2323 (0.2322)\tgrad(D) penalty 0.0130 (0.0156)\tRec loss 4060.3628 (3986.3545)\tnorm 0.8902 (0.8907)\n",
            "Epoch: [92][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7649 (0.7650)\tD(fake) 0.2302 (0.2317)\tgrad(D) penalty 0.0139 (0.0154)\tRec loss 4077.1946 (3977.1608)\tnorm 0.8813 (0.8905)\n",
            "Epoch: [92][120/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7650 (0.7650)\tD(fake) 0.2263 (0.2324)\tgrad(D) penalty 0.0154 (0.0153)\tRec loss 3848.9897 (3975.8075)\tnorm 0.8867 (0.8901)\n",
            "Epoch: [92][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7651 (0.7650)\tD(fake) 0.2475 (0.2327)\tgrad(D) penalty 0.0138 (0.0153)\tRec loss 3705.6169 (3971.2967)\tnorm 0.8943 (0.8899)\n",
            "Epoch: [92][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7649 (0.7650)\tD(fake) 0.2192 (0.2318)\tgrad(D) penalty 0.0185 (0.0153)\tRec loss 3716.4702 (3976.0807)\tnorm 0.8917 (0.8899)\n",
            "Epoch: [92][150/195]\tTime  0.170 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7651 (0.7650)\tD(fake) 0.2393 (0.2323)\tgrad(D) penalty 0.0159 (0.0153)\tRec loss 3888.1196 (3975.0378)\tnorm 0.8869 (0.8899)\n",
            "Epoch: [92][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7649 (0.7650)\tD(fake) 0.2308 (0.2318)\tgrad(D) penalty 0.0151 (0.0153)\tRec loss 3849.4514 (3978.1490)\tnorm 0.8857 (0.8898)\n",
            "Epoch: [92][170/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7651 (0.7650)\tD(fake) 0.2382 (0.2321)\tgrad(D) penalty 0.0148 (0.0153)\tRec loss 4228.1450 (3982.8376)\tnorm 0.8743 (0.8898)\n",
            "Epoch: [92][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7651 (0.7650)\tD(fake) 0.2425 (0.2330)\tgrad(D) penalty 0.0134 (0.0152)\tRec loss 3936.2197 (3982.5839)\tnorm 0.9017 (0.8899)\n",
            "Epoch: [92][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7650 (0.7650)\tD(fake) 0.2236 (0.2326)\tgrad(D) penalty 0.0182 (0.0153)\tRec loss 4070.5737 (3981.1185)\tnorm 0.8932 (0.8899)\n",
            "Epoch: [93][  0/195]\tTime  0.417 ( 0.417)\tData  0.221 ( 0.221)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3869.5210 (3869.5210)\tnorm 0.8885 (0.8885)\n",
            "Epoch: [93][ 10/195]\tTime  0.147 ( 0.174)\tData  0.000 ( 0.020)\tD(real) 0.7651 (0.7649)\tD(fake) 0.2519 (0.2424)\tgrad(D) penalty 0.0158 (0.0160)\tRec loss 3982.3806 (3979.0720)\tnorm 0.8987 (0.8935)\n",
            "Epoch: [93][ 20/195]\tTime  0.151 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7651 (0.7651)\tD(fake) 0.2220 (0.2340)\tgrad(D) penalty 0.0172 (0.0154)\tRec loss 4058.8140 (3946.6278)\tnorm 0.8904 (0.8945)\n",
            "Epoch: [93][ 30/195]\tTime  0.145 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7651 (0.7651)\tD(fake) 0.2438 (0.2362)\tgrad(D) penalty 0.0159 (0.0157)\tRec loss 4379.9570 (3962.1229)\tnorm 0.8888 (0.8925)\n",
            "Epoch: [93][ 40/195]\tTime  0.146 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7651 (0.7651)\tD(fake) 0.2354 (0.2366)\tgrad(D) penalty 0.0169 (0.0162)\tRec loss 3810.1479 (3948.7215)\tnorm 0.8996 (0.8926)\n",
            "Epoch: [93][ 50/195]\tTime  0.164 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7651 (0.7651)\tD(fake) 0.2310 (0.2358)\tgrad(D) penalty 0.0172 (0.0163)\tRec loss 3924.6436 (3949.6306)\tnorm 0.9027 (0.8920)\n",
            "Epoch: [93][ 60/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7652 (0.7651)\tD(fake) 0.2248 (0.2349)\tgrad(D) penalty 0.0143 (0.0160)\tRec loss 4118.1455 (3962.9291)\tnorm 0.8819 (0.8916)\n",
            "Epoch: [93][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7651 (0.7651)\tD(fake) 0.2239 (0.2341)\tgrad(D) penalty 0.0136 (0.0159)\tRec loss 3826.2393 (3957.8672)\tnorm 0.8870 (0.8916)\n",
            "Epoch: [93][ 80/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7651 (0.7651)\tD(fake) 0.2357 (0.2339)\tgrad(D) penalty 0.0172 (0.0158)\tRec loss 3950.6584 (3963.9502)\tnorm 0.8786 (0.8912)\n",
            "Epoch: [93][ 90/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7651 (0.7651)\tD(fake) 0.2402 (0.2338)\tgrad(D) penalty 0.0153 (0.0157)\tRec loss 3875.7568 (3957.6363)\tnorm 0.8854 (0.8917)\n",
            "Epoch: [93][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7652 (0.7651)\tD(fake) 0.2250 (0.2341)\tgrad(D) penalty 0.0130 (0.0154)\tRec loss 3811.7571 (3953.7815)\tnorm 0.8836 (0.8916)\n",
            "Epoch: [93][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7653 (0.7651)\tD(fake) 0.2249 (0.2333)\tgrad(D) penalty 0.0141 (0.0153)\tRec loss 4084.5361 (3959.5099)\tnorm 0.8932 (0.8911)\n",
            "Epoch: [93][120/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7651 (0.7651)\tD(fake) 0.2362 (0.2333)\tgrad(D) penalty 0.0141 (0.0153)\tRec loss 3972.1091 (3965.6235)\tnorm 0.8809 (0.8907)\n",
            "Epoch: [93][130/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7651 (0.7651)\tD(fake) 0.2340 (0.2331)\tgrad(D) penalty 0.0163 (0.0154)\tRec loss 3809.2678 (3962.4627)\tnorm 0.8921 (0.8908)\n",
            "Epoch: [93][140/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7652 (0.7651)\tD(fake) 0.2335 (0.2333)\tgrad(D) penalty 0.0169 (0.0154)\tRec loss 4062.0710 (3967.3754)\tnorm 0.8840 (0.8910)\n",
            "Epoch: [93][150/195]\tTime  0.163 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7652 (0.7651)\tD(fake) 0.2311 (0.2334)\tgrad(D) penalty 0.0157 (0.0154)\tRec loss 3895.6956 (3966.6296)\tnorm 0.8963 (0.8910)\n",
            "Epoch: [93][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7652 (0.7651)\tD(fake) 0.2382 (0.2338)\tgrad(D) penalty 0.0153 (0.0155)\tRec loss 3996.4812 (3972.7670)\tnorm 0.8863 (0.8909)\n",
            "Epoch: [93][170/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7652 (0.7651)\tD(fake) 0.2290 (0.2333)\tgrad(D) penalty 0.0163 (0.0155)\tRec loss 3989.3989 (3973.1054)\tnorm 0.8833 (0.8908)\n",
            "Epoch: [93][180/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7651 (0.7651)\tD(fake) 0.2266 (0.2334)\tgrad(D) penalty 0.0152 (0.0155)\tRec loss 3850.7661 (3974.1301)\tnorm 0.9001 (0.8907)\n",
            "Epoch: [93][190/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7652 (0.7651)\tD(fake) 0.2417 (0.2335)\tgrad(D) penalty 0.0142 (0.0154)\tRec loss 4061.0273 (3977.0264)\tnorm 0.8888 (0.8908)\n",
            "Epoch: [94][  0/195]\tTime  0.402 ( 0.402)\tData  0.221 ( 0.221)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4000.9192 (4000.9192)\tnorm 0.9036 (0.9036)\n",
            "Epoch: [94][ 10/195]\tTime  0.153 ( 0.174)\tData  0.000 ( 0.020)\tD(real) 0.7652 (0.7652)\tD(fake) 0.2269 (0.2242)\tgrad(D) penalty 0.0159 (0.0151)\tRec loss 4011.7583 (4023.8151)\tnorm 0.8907 (0.8922)\n",
            "Epoch: [94][ 20/195]\tTime  0.149 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7652 (0.7652)\tD(fake) 0.2285 (0.2287)\tgrad(D) penalty 0.0138 (0.0147)\tRec loss 4245.5303 (3984.4022)\tnorm 0.8913 (0.8945)\n",
            "Epoch: [94][ 30/195]\tTime  0.153 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.7652 (0.7652)\tD(fake) 0.2214 (0.2279)\tgrad(D) penalty 0.0136 (0.0144)\tRec loss 3675.8660 (3929.1420)\tnorm 0.8991 (0.8920)\n",
            "Epoch: [94][ 40/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7651 (0.7652)\tD(fake) 0.2377 (0.2297)\tgrad(D) penalty 0.0134 (0.0144)\tRec loss 3927.3428 (3945.3840)\tnorm 0.9005 (0.8908)\n",
            "Epoch: [94][ 50/195]\tTime  0.164 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7653 (0.7652)\tD(fake) 0.2173 (0.2283)\tgrad(D) penalty 0.0172 (0.0147)\tRec loss 3823.4905 (3971.8235)\tnorm 0.8938 (0.8918)\n",
            "Epoch: [94][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7653 (0.7652)\tD(fake) 0.2535 (0.2310)\tgrad(D) penalty 0.0119 (0.0143)\tRec loss 4091.7200 (3973.5708)\tnorm 0.8894 (0.8923)\n",
            "Epoch: [94][ 70/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7653 (0.7652)\tD(fake) 0.2114 (0.2299)\tgrad(D) penalty 0.0160 (0.0143)\tRec loss 4138.6206 (3976.6626)\tnorm 0.8942 (0.8930)\n",
            "Epoch: [94][ 80/195]\tTime  0.154 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7653 (0.7652)\tD(fake) 0.2543 (0.2310)\tgrad(D) penalty 0.0161 (0.0147)\tRec loss 4291.0703 (3981.7613)\tnorm 0.8987 (0.8926)\n",
            "Epoch: [94][ 90/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7651 (0.7652)\tD(fake) 0.2191 (0.2312)\tgrad(D) penalty 0.0156 (0.0146)\tRec loss 3989.3926 (3985.8294)\tnorm 0.8963 (0.8924)\n",
            "Epoch: [94][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7652 (0.7652)\tD(fake) 0.2274 (0.2301)\tgrad(D) penalty 0.0145 (0.0147)\tRec loss 3847.5161 (3984.7646)\tnorm 0.9020 (0.8931)\n",
            "Epoch: [94][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7652 (0.7652)\tD(fake) 0.2277 (0.2300)\tgrad(D) penalty 0.0137 (0.0146)\tRec loss 4134.6250 (3981.7260)\tnorm 0.8980 (0.8929)\n",
            "Epoch: [94][120/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7653 (0.7652)\tD(fake) 0.2212 (0.2293)\tgrad(D) penalty 0.0177 (0.0148)\tRec loss 3724.7930 (3982.2884)\tnorm 0.8902 (0.8931)\n",
            "Epoch: [94][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7654 (0.7652)\tD(fake) 0.2485 (0.2304)\tgrad(D) penalty 0.0122 (0.0148)\tRec loss 3852.8225 (3971.3511)\tnorm 0.8814 (0.8928)\n",
            "Epoch: [94][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7653 (0.7652)\tD(fake) 0.2291 (0.2302)\tgrad(D) penalty 0.0132 (0.0147)\tRec loss 4306.2710 (3971.2393)\tnorm 0.9078 (0.8928)\n",
            "Epoch: [94][150/195]\tTime  0.171 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7654 (0.7652)\tD(fake) 0.2306 (0.2302)\tgrad(D) penalty 0.0122 (0.0146)\tRec loss 3825.4854 (3970.7837)\tnorm 0.8911 (0.8925)\n",
            "Epoch: [94][160/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7651 (0.7652)\tD(fake) 0.2390 (0.2307)\tgrad(D) penalty 0.0140 (0.0145)\tRec loss 3725.7998 (3972.8356)\tnorm 0.9029 (0.8925)\n",
            "Epoch: [94][170/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7654 (0.7653)\tD(fake) 0.2406 (0.2312)\tgrad(D) penalty 0.0158 (0.0146)\tRec loss 4154.8887 (3975.3398)\tnorm 0.8844 (0.8923)\n",
            "Epoch: [94][180/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7652 (0.7653)\tD(fake) 0.2391 (0.2316)\tgrad(D) penalty 0.0137 (0.0146)\tRec loss 4196.1826 (3977.3141)\tnorm 0.8860 (0.8927)\n",
            "Epoch: [94][190/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7654 (0.7653)\tD(fake) 0.2362 (0.2318)\tgrad(D) penalty 0.0128 (0.0146)\tRec loss 3803.2373 (3973.1074)\tnorm 0.8848 (0.8925)\n",
            "Epoch: [95][  0/195]\tTime  0.412 ( 0.412)\tData  0.217 ( 0.217)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4090.4680 (4090.4680)\tnorm 0.8872 (0.8872)\n",
            "Epoch: [95][ 10/195]\tTime  0.152 ( 0.174)\tData  0.000 ( 0.020)\tD(real) 0.7651 (0.7651)\tD(fake) 0.2334 (0.2303)\tgrad(D) penalty 0.0122 (0.0132)\tRec loss 3932.1528 (3948.7607)\tnorm 0.8896 (0.8885)\n",
            "Epoch: [95][ 20/195]\tTime  0.146 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7653 (0.7653)\tD(fake) 0.2223 (0.2280)\tgrad(D) penalty 0.0172 (0.0145)\tRec loss 3712.9263 (3940.6041)\tnorm 0.8819 (0.8878)\n",
            "Epoch: [95][ 30/195]\tTime  0.149 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7652 (0.7653)\tD(fake) 0.2407 (0.2296)\tgrad(D) penalty 0.0138 (0.0144)\tRec loss 3967.0908 (3940.8930)\tnorm 0.9012 (0.8873)\n",
            "Epoch: [95][ 40/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.006)\tD(real) 0.7655 (0.7653)\tD(fake) 0.2282 (0.2312)\tgrad(D) penalty 0.0146 (0.0145)\tRec loss 3904.0610 (3952.5216)\tnorm 0.8806 (0.8882)\n",
            "Epoch: [95][ 50/195]\tTime  0.165 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7653 (0.7653)\tD(fake) 0.2453 (0.2329)\tgrad(D) penalty 0.0146 (0.0145)\tRec loss 3860.5552 (3971.1802)\tnorm 0.8928 (0.8891)\n",
            "Epoch: [95][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7654 (0.7654)\tD(fake) 0.2343 (0.2344)\tgrad(D) penalty 0.0138 (0.0147)\tRec loss 3869.8586 (3947.8132)\tnorm 0.8944 (0.8906)\n",
            "Epoch: [95][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7653 (0.7654)\tD(fake) 0.2361 (0.2346)\tgrad(D) penalty 0.0122 (0.0146)\tRec loss 4021.7183 (3952.8465)\tnorm 0.8985 (0.8902)\n",
            "Epoch: [95][ 80/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7654 (0.7654)\tD(fake) 0.2439 (0.2347)\tgrad(D) penalty 0.0150 (0.0144)\tRec loss 3873.8877 (3953.0876)\tnorm 0.8923 (0.8905)\n",
            "Epoch: [95][ 90/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7653 (0.7654)\tD(fake) 0.2302 (0.2345)\tgrad(D) penalty 0.0140 (0.0142)\tRec loss 4053.6990 (3956.5408)\tnorm 0.8928 (0.8904)\n",
            "Epoch: [95][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7654 (0.7654)\tD(fake) 0.2399 (0.2350)\tgrad(D) penalty 0.0142 (0.0141)\tRec loss 3915.0952 (3961.4788)\tnorm 0.8927 (0.8905)\n",
            "Epoch: [95][110/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7651 (0.7654)\tD(fake) 0.2198 (0.2346)\tgrad(D) penalty 0.0167 (0.0142)\tRec loss 3791.6033 (3950.5345)\tnorm 0.8951 (0.8903)\n",
            "Epoch: [95][120/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7655 (0.7654)\tD(fake) 0.2348 (0.2341)\tgrad(D) penalty 0.0136 (0.0143)\tRec loss 3721.4001 (3952.3523)\tnorm 0.8923 (0.8903)\n",
            "Epoch: [95][130/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7655 (0.7654)\tD(fake) 0.2347 (0.2341)\tgrad(D) penalty 0.0136 (0.0144)\tRec loss 4093.7266 (3956.8817)\tnorm 0.8888 (0.8900)\n",
            "Epoch: [95][140/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7655 (0.7654)\tD(fake) 0.2227 (0.2329)\tgrad(D) penalty 0.0176 (0.0145)\tRec loss 4067.2776 (3966.8314)\tnorm 0.8823 (0.8897)\n",
            "Epoch: [95][150/195]\tTime  0.168 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7656 (0.7654)\tD(fake) 0.2468 (0.2334)\tgrad(D) penalty 0.0146 (0.0145)\tRec loss 3846.6543 (3966.1519)\tnorm 0.8858 (0.8898)\n",
            "Epoch: [95][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7655 (0.7654)\tD(fake) 0.2159 (0.2329)\tgrad(D) penalty 0.0161 (0.0145)\tRec loss 3919.3208 (3969.7616)\tnorm 0.8920 (0.8898)\n",
            "Epoch: [95][170/195]\tTime  0.154 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7657 (0.7654)\tD(fake) 0.2486 (0.2335)\tgrad(D) penalty 0.0134 (0.0146)\tRec loss 3998.8257 (3970.2436)\tnorm 0.8922 (0.8902)\n",
            "Epoch: [95][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7656 (0.7654)\tD(fake) 0.2228 (0.2326)\tgrad(D) penalty 0.0156 (0.0146)\tRec loss 4139.8198 (3972.9420)\tnorm 0.9105 (0.8909)\n",
            "Epoch: [95][190/195]\tTime  0.154 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7657 (0.7654)\tD(fake) 0.2493 (0.2331)\tgrad(D) penalty 0.0125 (0.0146)\tRec loss 3801.4946 (3971.1287)\tnorm 0.8838 (0.8908)\n",
            "Epoch: [96][  0/195]\tTime  0.438 ( 0.438)\tData  0.241 ( 0.241)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4086.0576 (4086.0576)\tnorm 0.8969 (0.8969)\n",
            "Epoch: [96][ 10/195]\tTime  0.147 ( 0.175)\tData  0.000 ( 0.022)\tD(real) 0.7657 (0.7657)\tD(fake) 0.2729 (0.2588)\tgrad(D) penalty 0.0122 (0.0142)\tRec loss 4149.0527 (3971.3715)\tnorm 0.8891 (0.9009)\n",
            "Epoch: [96][ 20/195]\tTime  0.147 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7654 (0.7656)\tD(fake) 0.2249 (0.2378)\tgrad(D) penalty 0.0176 (0.0153)\tRec loss 4017.1282 (3997.8334)\tnorm 0.8872 (0.8965)\n",
            "Epoch: [96][ 30/195]\tTime  0.151 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7657 (0.7656)\tD(fake) 0.2305 (0.2376)\tgrad(D) penalty 0.0155 (0.0149)\tRec loss 3873.9897 (3968.1912)\tnorm 0.8777 (0.8951)\n",
            "Epoch: [96][ 40/195]\tTime  0.150 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7656 (0.7656)\tD(fake) 0.2575 (0.2361)\tgrad(D) penalty 0.0146 (0.0150)\tRec loss 4104.3218 (3988.7489)\tnorm 0.9049 (0.8934)\n",
            "Epoch: [96][ 50/195]\tTime  0.165 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7656 (0.7656)\tD(fake) 0.2180 (0.2357)\tgrad(D) penalty 0.0191 (0.0150)\tRec loss 3951.3677 (3964.5121)\tnorm 0.8827 (0.8932)\n",
            "Epoch: [96][ 60/195]\tTime  0.152 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7656 (0.7656)\tD(fake) 0.2320 (0.2352)\tgrad(D) penalty 0.0152 (0.0151)\tRec loss 4006.7896 (3951.8376)\tnorm 0.9060 (0.8936)\n",
            "Epoch: [96][ 70/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7657 (0.7656)\tD(fake) 0.2222 (0.2336)\tgrad(D) penalty 0.0159 (0.0152)\tRec loss 3571.1021 (3952.4412)\tnorm 0.8838 (0.8929)\n",
            "Epoch: [96][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7657 (0.7657)\tD(fake) 0.2139 (0.2325)\tgrad(D) penalty 0.0150 (0.0150)\tRec loss 4011.2527 (3954.1274)\tnorm 0.8912 (0.8930)\n",
            "Epoch: [96][ 90/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7657 (0.7657)\tD(fake) 0.2433 (0.2327)\tgrad(D) penalty 0.0126 (0.0149)\tRec loss 4403.8345 (3958.7451)\tnorm 0.8888 (0.8933)\n",
            "Epoch: [96][100/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7657 (0.7657)\tD(fake) 0.2069 (0.2317)\tgrad(D) penalty 0.0155 (0.0149)\tRec loss 3875.3435 (3957.5706)\tnorm 0.8784 (0.8927)\n",
            "Epoch: [96][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7658 (0.7657)\tD(fake) 0.2387 (0.2313)\tgrad(D) penalty 0.0141 (0.0150)\tRec loss 3774.2705 (3956.5392)\tnorm 0.8913 (0.8929)\n",
            "Epoch: [96][120/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7657 (0.7657)\tD(fake) 0.2205 (0.2316)\tgrad(D) penalty 0.0187 (0.0151)\tRec loss 4018.2144 (3961.8994)\tnorm 0.9140 (0.8930)\n",
            "Epoch: [96][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7657 (0.7657)\tD(fake) 0.2275 (0.2315)\tgrad(D) penalty 0.0150 (0.0151)\tRec loss 4068.5298 (3962.9909)\tnorm 0.8855 (0.8926)\n",
            "Epoch: [96][140/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7657 (0.7657)\tD(fake) 0.2315 (0.2309)\tgrad(D) penalty 0.0177 (0.0153)\tRec loss 3544.8372 (3961.1171)\tnorm 0.8788 (0.8923)\n",
            "Epoch: [96][150/195]\tTime  0.162 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7658 (0.7657)\tD(fake) 0.2446 (0.2315)\tgrad(D) penalty 0.0119 (0.0152)\tRec loss 4255.0713 (3974.5048)\tnorm 0.8988 (0.8924)\n",
            "Epoch: [96][160/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7658 (0.7657)\tD(fake) 0.2370 (0.2314)\tgrad(D) penalty 0.0145 (0.0151)\tRec loss 3962.7695 (3979.3200)\tnorm 0.8971 (0.8925)\n",
            "Epoch: [96][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7658 (0.7657)\tD(fake) 0.2347 (0.2316)\tgrad(D) penalty 0.0134 (0.0150)\tRec loss 3994.6672 (3978.2765)\tnorm 0.8870 (0.8927)\n",
            "Epoch: [96][180/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7658 (0.7657)\tD(fake) 0.2367 (0.2322)\tgrad(D) penalty 0.0128 (0.0149)\tRec loss 4020.1226 (3979.5562)\tnorm 0.8880 (0.8925)\n",
            "Epoch: [96][190/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7658 (0.7657)\tD(fake) 0.2172 (0.2317)\tgrad(D) penalty 0.0140 (0.0149)\tRec loss 4004.5239 (3971.1328)\tnorm 0.8877 (0.8922)\n",
            "Epoch: [97][  0/195]\tTime  0.433 ( 0.433)\tData  0.240 ( 0.240)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4486.6221 (4486.6221)\tnorm 0.9114 (0.9114)\n",
            "Epoch: [97][ 10/195]\tTime  0.149 ( 0.178)\tData  0.000 ( 0.022)\tD(real) 0.7656 (0.7657)\tD(fake) 0.2364 (0.2368)\tgrad(D) penalty 0.0147 (0.0148)\tRec loss 4161.0923 (3945.6481)\tnorm 0.8929 (0.8936)\n",
            "Epoch: [97][ 20/195]\tTime  0.149 ( 0.164)\tData  0.000 ( 0.012)\tD(real) 0.7658 (0.7657)\tD(fake) 0.2252 (0.2302)\tgrad(D) penalty 0.0138 (0.0149)\tRec loss 3963.2605 (3942.5504)\tnorm 0.8931 (0.8950)\n",
            "Epoch: [97][ 30/195]\tTime  0.149 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7658 (0.7657)\tD(fake) 0.2209 (0.2286)\tgrad(D) penalty 0.0152 (0.0148)\tRec loss 3750.8855 (3959.1073)\tnorm 0.8947 (0.8930)\n",
            "Epoch: [97][ 40/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7658 (0.7657)\tD(fake) 0.2381 (0.2312)\tgrad(D) penalty 0.0135 (0.0145)\tRec loss 3859.4951 (3963.6395)\tnorm 0.8819 (0.8921)\n",
            "Epoch: [97][ 50/195]\tTime  0.165 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7657 (0.7657)\tD(fake) 0.2305 (0.2306)\tgrad(D) penalty 0.0139 (0.0147)\tRec loss 3874.2197 (3954.2668)\tnorm 0.8837 (0.8916)\n",
            "Epoch: [97][ 60/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7658 (0.7657)\tD(fake) 0.2301 (0.2306)\tgrad(D) penalty 0.0119 (0.0145)\tRec loss 4111.8076 (3951.8943)\tnorm 0.8978 (0.8918)\n",
            "Epoch: [97][ 70/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7659 (0.7658)\tD(fake) 0.2132 (0.2286)\tgrad(D) penalty 0.0175 (0.0148)\tRec loss 3879.8501 (3948.4942)\tnorm 0.8981 (0.8918)\n",
            "Epoch: [97][ 80/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7659 (0.7658)\tD(fake) 0.2130 (0.2282)\tgrad(D) penalty 0.0142 (0.0148)\tRec loss 3906.5488 (3962.0666)\tnorm 0.8919 (0.8918)\n",
            "Epoch: [97][ 90/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7659 (0.7658)\tD(fake) 0.2395 (0.2281)\tgrad(D) penalty 0.0137 (0.0147)\tRec loss 3796.4238 (3954.5337)\tnorm 0.8806 (0.8913)\n",
            "Epoch: [97][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7659 (0.7658)\tD(fake) 0.2147 (0.2286)\tgrad(D) penalty 0.0129 (0.0144)\tRec loss 3933.6367 (3951.4044)\tnorm 0.8928 (0.8910)\n",
            "Epoch: [97][110/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7659 (0.7658)\tD(fake) 0.2392 (0.2287)\tgrad(D) penalty 0.0140 (0.0145)\tRec loss 4117.4810 (3956.1231)\tnorm 0.8897 (0.8906)\n",
            "Epoch: [97][120/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7659 (0.7658)\tD(fake) 0.2324 (0.2296)\tgrad(D) penalty 0.0149 (0.0145)\tRec loss 4068.4097 (3956.5537)\tnorm 0.8856 (0.8904)\n",
            "Epoch: [97][130/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7659 (0.7658)\tD(fake) 0.2184 (0.2291)\tgrad(D) penalty 0.0143 (0.0144)\tRec loss 3797.9043 (3962.1174)\tnorm 0.8965 (0.8906)\n",
            "Epoch: [97][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7659 (0.7658)\tD(fake) 0.2308 (0.2297)\tgrad(D) penalty 0.0149 (0.0144)\tRec loss 3815.9006 (3960.5927)\tnorm 0.8944 (0.8901)\n",
            "Epoch: [97][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7660 (0.7658)\tD(fake) 0.2336 (0.2296)\tgrad(D) penalty 0.0136 (0.0144)\tRec loss 4317.6797 (3964.3155)\tnorm 0.8871 (0.8900)\n",
            "Epoch: [97][160/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7660 (0.7658)\tD(fake) 0.2208 (0.2298)\tgrad(D) penalty 0.0137 (0.0143)\tRec loss 3773.6123 (3963.3940)\tnorm 0.8780 (0.8897)\n",
            "Epoch: [97][170/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7660 (0.7659)\tD(fake) 0.2410 (0.2300)\tgrad(D) penalty 0.0162 (0.0144)\tRec loss 3990.2188 (3967.3892)\tnorm 0.8893 (0.8897)\n",
            "Epoch: [97][180/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7660 (0.7659)\tD(fake) 0.2354 (0.2303)\tgrad(D) penalty 0.0123 (0.0142)\tRec loss 3956.3228 (3965.1678)\tnorm 0.8978 (0.8897)\n",
            "Epoch: [97][190/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7659 (0.7659)\tD(fake) 0.2328 (0.2304)\tgrad(D) penalty 0.0132 (0.0142)\tRec loss 3912.9441 (3963.7132)\tnorm 0.8857 (0.8898)\n",
            "Epoch: [98][  0/195]\tTime  0.425 ( 0.425)\tData  0.229 ( 0.229)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4126.7300 (4126.7300)\tnorm 0.8947 (0.8947)\n",
            "Epoch: [98][ 10/195]\tTime  0.149 ( 0.174)\tData  0.000 ( 0.021)\tD(real) 0.7660 (0.7660)\tD(fake) 0.2548 (0.2389)\tgrad(D) penalty 0.0123 (0.0148)\tRec loss 3960.1958 (3950.5697)\tnorm 0.8980 (0.8891)\n",
            "Epoch: [98][ 20/195]\tTime  0.146 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7661 (0.7660)\tD(fake) 0.2106 (0.2300)\tgrad(D) penalty 0.0174 (0.0146)\tRec loss 3819.5256 (3947.8237)\tnorm 0.8932 (0.8892)\n",
            "Epoch: [98][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7659 (0.7660)\tD(fake) 0.2493 (0.2337)\tgrad(D) penalty 0.0121 (0.0140)\tRec loss 3839.5679 (3964.9459)\tnorm 0.8960 (0.8907)\n",
            "Epoch: [98][ 40/195]\tTime  0.153 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7660 (0.7660)\tD(fake) 0.2259 (0.2301)\tgrad(D) penalty 0.0201 (0.0151)\tRec loss 3775.2183 (3960.2092)\tnorm 0.9023 (0.8910)\n",
            "Epoch: [98][ 50/195]\tTime  0.170 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7662 (0.7660)\tD(fake) 0.2305 (0.2317)\tgrad(D) penalty 0.0161 (0.0150)\tRec loss 3963.2798 (3946.9459)\tnorm 0.8861 (0.8924)\n",
            "Epoch: [98][ 60/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7662 (0.7660)\tD(fake) 0.2380 (0.2300)\tgrad(D) penalty 0.0151 (0.0152)\tRec loss 3940.4902 (3956.0013)\tnorm 0.8976 (0.8916)\n",
            "Epoch: [98][ 70/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7660 (0.7660)\tD(fake) 0.2137 (0.2288)\tgrad(D) penalty 0.0157 (0.0152)\tRec loss 4322.5615 (3964.8259)\tnorm 0.8863 (0.8912)\n",
            "Epoch: [98][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7661 (0.7660)\tD(fake) 0.2507 (0.2298)\tgrad(D) penalty 0.0122 (0.0152)\tRec loss 3931.7590 (3968.5358)\tnorm 0.8914 (0.8913)\n",
            "Epoch: [98][ 90/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7661 (0.7660)\tD(fake) 0.2143 (0.2290)\tgrad(D) penalty 0.0188 (0.0153)\tRec loss 3950.4561 (3974.3517)\tnorm 0.8940 (0.8910)\n",
            "Epoch: [98][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7662 (0.7661)\tD(fake) 0.2284 (0.2290)\tgrad(D) penalty 0.0144 (0.0152)\tRec loss 3866.4458 (3964.7724)\tnorm 0.9028 (0.8905)\n",
            "Epoch: [98][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7662 (0.7661)\tD(fake) 0.2386 (0.2295)\tgrad(D) penalty 0.0138 (0.0151)\tRec loss 4150.8906 (3968.8584)\tnorm 0.8882 (0.8907)\n",
            "Epoch: [98][120/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7663 (0.7661)\tD(fake) 0.2164 (0.2296)\tgrad(D) penalty 0.0155 (0.0149)\tRec loss 4242.3042 (3974.0452)\tnorm 0.8912 (0.8910)\n",
            "Epoch: [98][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7663 (0.7661)\tD(fake) 0.2498 (0.2303)\tgrad(D) penalty 0.0130 (0.0148)\tRec loss 4023.6489 (3967.5346)\tnorm 0.8748 (0.8905)\n",
            "Epoch: [98][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7662 (0.7661)\tD(fake) 0.2116 (0.2297)\tgrad(D) penalty 0.0154 (0.0147)\tRec loss 3842.8372 (3961.1833)\tnorm 0.8757 (0.8904)\n",
            "Epoch: [98][150/195]\tTime  0.161 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7663 (0.7661)\tD(fake) 0.2610 (0.2311)\tgrad(D) penalty 0.0148 (0.0147)\tRec loss 3650.9458 (3958.2976)\tnorm 0.8956 (0.8899)\n",
            "Epoch: [98][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7663 (0.7661)\tD(fake) 0.2262 (0.2305)\tgrad(D) penalty 0.0153 (0.0147)\tRec loss 4038.8926 (3956.7343)\tnorm 0.8889 (0.8901)\n",
            "Epoch: [98][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7662 (0.7661)\tD(fake) 0.2252 (0.2308)\tgrad(D) penalty 0.0124 (0.0146)\tRec loss 4095.5859 (3959.5117)\tnorm 0.8939 (0.8901)\n",
            "Epoch: [98][180/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7661 (0.7661)\tD(fake) 0.2295 (0.2311)\tgrad(D) penalty 0.0121 (0.0146)\tRec loss 4140.7256 (3955.8163)\tnorm 0.8822 (0.8899)\n",
            "Epoch: [98][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7660 (0.7661)\tD(fake) 0.2326 (0.2309)\tgrad(D) penalty 0.0149 (0.0146)\tRec loss 4019.1599 (3963.4453)\tnorm 0.8959 (0.8901)\n",
            "Epoch: [99][  0/195]\tTime  0.401 ( 0.401)\tData  0.223 ( 0.223)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4376.9600 (4376.9600)\tnorm 0.8928 (0.8928)\n",
            "Epoch: [99][ 10/195]\tTime  0.149 ( 0.172)\tData  0.000 ( 0.021)\tD(real) 0.7663 (0.7663)\tD(fake) 0.2455 (0.2402)\tgrad(D) penalty 0.0146 (0.0155)\tRec loss 3901.6179 (3923.7797)\tnorm 0.8825 (0.8900)\n",
            "Epoch: [99][ 20/195]\tTime  0.146 ( 0.160)\tData  0.000 ( 0.011)\tD(real) 0.7664 (0.7663)\tD(fake) 0.2294 (0.2317)\tgrad(D) penalty 0.0140 (0.0140)\tRec loss 3903.7070 (3911.9081)\tnorm 0.8794 (0.8895)\n",
            "Epoch: [99][ 30/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7664 (0.7663)\tD(fake) 0.2323 (0.2326)\tgrad(D) penalty 0.0117 (0.0137)\tRec loss 3978.7251 (3928.7396)\tnorm 0.8913 (0.8893)\n",
            "Epoch: [99][ 40/195]\tTime  0.154 ( 0.154)\tData  0.000 ( 0.006)\tD(real) 0.7663 (0.7663)\tD(fake) 0.2223 (0.2287)\tgrad(D) penalty 0.0123 (0.0138)\tRec loss 4280.4897 (3943.3267)\tnorm 0.9022 (0.8889)\n",
            "Epoch: [99][ 50/195]\tTime  0.162 ( 0.153)\tData  0.000 ( 0.005)\tD(real) 0.7663 (0.7663)\tD(fake) 0.2255 (0.2281)\tgrad(D) penalty 0.0135 (0.0140)\tRec loss 3827.4968 (3939.1299)\tnorm 0.8990 (0.8889)\n",
            "Epoch: [99][ 60/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7664 (0.7663)\tD(fake) 0.2437 (0.2292)\tgrad(D) penalty 0.0120 (0.0139)\tRec loss 3938.1357 (3935.0616)\tnorm 0.8887 (0.8891)\n",
            "Epoch: [99][ 70/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7664 (0.7663)\tD(fake) 0.2211 (0.2291)\tgrad(D) penalty 0.0145 (0.0140)\tRec loss 3657.6616 (3930.2754)\tnorm 0.8787 (0.8889)\n",
            "Epoch: [99][ 80/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7664 (0.7663)\tD(fake) 0.2509 (0.2298)\tgrad(D) penalty 0.0134 (0.0142)\tRec loss 3897.6335 (3933.2402)\tnorm 0.8960 (0.8892)\n",
            "Epoch: [99][ 90/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7664 (0.7664)\tD(fake) 0.2164 (0.2290)\tgrad(D) penalty 0.0152 (0.0143)\tRec loss 4034.3267 (3933.0603)\tnorm 0.9031 (0.8896)\n",
            "Epoch: [99][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7665 (0.7664)\tD(fake) 0.2316 (0.2288)\tgrad(D) penalty 0.0156 (0.0144)\tRec loss 4111.5190 (3941.3551)\tnorm 0.8885 (0.8903)\n",
            "Epoch: [99][110/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7665 (0.7664)\tD(fake) 0.2268 (0.2288)\tgrad(D) penalty 0.0133 (0.0143)\tRec loss 3716.1113 (3945.1816)\tnorm 0.8965 (0.8909)\n",
            "Epoch: [99][120/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7666 (0.7664)\tD(fake) 0.2103 (0.2285)\tgrad(D) penalty 0.0116 (0.0142)\tRec loss 4037.2566 (3948.8865)\tnorm 0.8898 (0.8915)\n",
            "Epoch: [99][130/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7666 (0.7664)\tD(fake) 0.2263 (0.2277)\tgrad(D) penalty 0.0142 (0.0143)\tRec loss 3825.8857 (3949.6115)\tnorm 0.9058 (0.8917)\n",
            "Epoch: [99][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7664 (0.7664)\tD(fake) 0.2282 (0.2285)\tgrad(D) penalty 0.0116 (0.0144)\tRec loss 4073.4963 (3957.5665)\tnorm 0.8905 (0.8918)\n",
            "Epoch: [99][150/195]\tTime  0.166 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7667 (0.7664)\tD(fake) 0.2252 (0.2283)\tgrad(D) penalty 0.0144 (0.0144)\tRec loss 4243.0781 (3961.8378)\tnorm 0.8990 (0.8921)\n",
            "Epoch: [99][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7666 (0.7664)\tD(fake) 0.2335 (0.2284)\tgrad(D) penalty 0.0128 (0.0144)\tRec loss 4001.1450 (3960.1257)\tnorm 0.9009 (0.8922)\n",
            "Epoch: [99][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7666 (0.7664)\tD(fake) 0.2049 (0.2273)\tgrad(D) penalty 0.0168 (0.0145)\tRec loss 3967.1455 (3958.6158)\tnorm 0.8973 (0.8923)\n",
            "Epoch: [99][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7667 (0.7664)\tD(fake) 0.2331 (0.2278)\tgrad(D) penalty 0.0153 (0.0144)\tRec loss 3785.5708 (3958.7267)\tnorm 0.8850 (0.8922)\n",
            "Epoch: [99][190/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7665 (0.7665)\tD(fake) 0.2283 (0.2278)\tgrad(D) penalty 0.0126 (0.0143)\tRec loss 3944.8589 (3959.4447)\tnorm 0.8756 (0.8917)\n",
            "Epoch: [100][  0/195]\tTime  0.413 ( 0.413)\tData  0.220 ( 0.220)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4029.7380 (4029.7380)\tnorm 0.8988 (0.8988)\n",
            "Epoch: [100][ 10/195]\tTime  0.156 ( 0.174)\tData  0.000 ( 0.020)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2605 (0.2491)\tgrad(D) penalty 0.0111 (0.0134)\tRec loss 3961.6082 (3957.3606)\tnorm 0.8832 (0.8904)\n",
            "Epoch: [100][ 20/195]\tTime  0.152 ( 0.163)\tData  0.000 ( 0.011)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2279 (0.2395)\tgrad(D) penalty 0.0202 (0.0152)\tRec loss 3779.8936 (3932.7274)\tnorm 0.8868 (0.8917)\n",
            "Epoch: [100][ 30/195]\tTime  0.151 ( 0.160)\tData  0.000 ( 0.007)\tD(real) 0.7666 (0.7667)\tD(fake) 0.2232 (0.2374)\tgrad(D) penalty 0.0131 (0.0146)\tRec loss 3824.3306 (3905.7212)\tnorm 0.9028 (0.8918)\n",
            "Epoch: [100][ 40/195]\tTime  0.149 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2408 (0.2344)\tgrad(D) penalty 0.0128 (0.0151)\tRec loss 4156.6523 (3934.2430)\tnorm 0.9017 (0.8923)\n",
            "Epoch: [100][ 50/195]\tTime  0.162 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2268 (0.2340)\tgrad(D) penalty 0.0139 (0.0150)\tRec loss 3839.6499 (3913.7791)\tnorm 0.8874 (0.8917)\n",
            "Epoch: [100][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2340 (0.2332)\tgrad(D) penalty 0.0125 (0.0149)\tRec loss 3939.4946 (3936.4838)\tnorm 0.8782 (0.8899)\n",
            "Epoch: [100][ 70/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2095 (0.2311)\tgrad(D) penalty 0.0171 (0.0152)\tRec loss 3974.3540 (3939.7828)\tnorm 0.8868 (0.8902)\n",
            "Epoch: [100][ 80/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2206 (0.2313)\tgrad(D) penalty 0.0139 (0.0150)\tRec loss 3882.4387 (3942.9323)\tnorm 0.8989 (0.8903)\n",
            "Epoch: [100][ 90/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2317 (0.2309)\tgrad(D) penalty 0.0164 (0.0151)\tRec loss 3691.5298 (3938.4522)\tnorm 0.8877 (0.8899)\n",
            "Epoch: [100][100/195]\tTime  0.169 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2290 (0.2313)\tgrad(D) penalty 0.0126 (0.0148)\tRec loss 3741.9971 (3936.7849)\tnorm 0.8889 (0.8898)\n",
            "Epoch: [100][110/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7666 (0.7667)\tD(fake) 0.2285 (0.2310)\tgrad(D) penalty 0.0169 (0.0148)\tRec loss 3779.4790 (3937.8666)\tnorm 0.8879 (0.8897)\n",
            "Epoch: [100][120/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2149 (0.2315)\tgrad(D) penalty 0.0137 (0.0147)\tRec loss 4012.0562 (3942.5971)\tnorm 0.8968 (0.8891)\n",
            "Epoch: [100][130/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7666 (0.7667)\tD(fake) 0.2338 (0.2312)\tgrad(D) penalty 0.0133 (0.0147)\tRec loss 3756.9585 (3940.5214)\tnorm 0.8885 (0.8893)\n",
            "Epoch: [100][140/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2167 (0.2305)\tgrad(D) penalty 0.0193 (0.0148)\tRec loss 3835.4814 (3950.1325)\tnorm 0.8797 (0.8887)\n",
            "Epoch: [100][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2507 (0.2316)\tgrad(D) penalty 0.0141 (0.0147)\tRec loss 3909.4280 (3951.4507)\tnorm 0.8809 (0.8886)\n",
            "Epoch: [100][160/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7666 (0.7667)\tD(fake) 0.2315 (0.2306)\tgrad(D) penalty 0.0158 (0.0149)\tRec loss 3871.9878 (3950.4092)\tnorm 0.8810 (0.8884)\n",
            "Epoch: [100][170/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2349 (0.2311)\tgrad(D) penalty 0.0155 (0.0149)\tRec loss 3828.6089 (3953.3843)\tnorm 0.8858 (0.8883)\n",
            "Epoch: [100][180/195]\tTime  0.144 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7666 (0.7667)\tD(fake) 0.2192 (0.2303)\tgrad(D) penalty 0.0180 (0.0150)\tRec loss 3927.3374 (3953.5334)\tnorm 0.8894 (0.8882)\n",
            "Epoch: [100][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2239 (0.2300)\tgrad(D) penalty 0.0171 (0.0151)\tRec loss 4013.4634 (3952.2408)\tnorm 0.8834 (0.8882)\n",
            "Epoch: [101][  0/195]\tTime  0.410 ( 0.410)\tData  0.216 ( 0.216)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3691.2637 (3691.2637)\tnorm 0.9007 (0.9007)\n",
            "Epoch: [101][ 10/195]\tTime  0.155 ( 0.177)\tData  0.000 ( 0.020)\tD(real) 0.7666 (0.7667)\tD(fake) 0.2641 (0.2523)\tgrad(D) penalty 0.0141 (0.0153)\tRec loss 3952.5347 (3891.9971)\tnorm 0.8959 (0.8916)\n",
            "Epoch: [101][ 20/195]\tTime  0.150 ( 0.163)\tData  0.000 ( 0.011)\tD(real) 0.7666 (0.7667)\tD(fake) 0.2208 (0.2368)\tgrad(D) penalty 0.0155 (0.0158)\tRec loss 3998.9043 (3899.5646)\tnorm 0.8890 (0.8885)\n",
            "Epoch: [101][ 30/195]\tTime  0.147 ( 0.159)\tData  0.000 ( 0.007)\tD(real) 0.7669 (0.7667)\tD(fake) 0.2491 (0.2398)\tgrad(D) penalty 0.0132 (0.0154)\tRec loss 4001.9141 (3927.2034)\tnorm 0.8868 (0.8880)\n",
            "Epoch: [101][ 40/195]\tTime  0.145 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2411 (0.2350)\tgrad(D) penalty 0.0145 (0.0156)\tRec loss 3921.7710 (3938.9268)\tnorm 0.8836 (0.8886)\n",
            "Epoch: [101][ 50/195]\tTime  0.165 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2213 (0.2348)\tgrad(D) penalty 0.0144 (0.0152)\tRec loss 3888.6025 (3954.2640)\tnorm 0.8951 (0.8887)\n",
            "Epoch: [101][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2287 (0.2335)\tgrad(D) penalty 0.0151 (0.0152)\tRec loss 3933.4060 (3948.5189)\tnorm 0.8842 (0.8882)\n",
            "Epoch: [101][ 70/195]\tTime  0.153 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2348 (0.2333)\tgrad(D) penalty 0.0157 (0.0151)\tRec loss 4020.3530 (3955.0672)\tnorm 0.8880 (0.8881)\n",
            "Epoch: [101][ 80/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2119 (0.2325)\tgrad(D) penalty 0.0176 (0.0152)\tRec loss 3806.4983 (3940.7463)\tnorm 0.8989 (0.8883)\n",
            "Epoch: [101][ 90/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2504 (0.2335)\tgrad(D) penalty 0.0134 (0.0151)\tRec loss 4194.9209 (3945.8853)\tnorm 0.8895 (0.8881)\n",
            "Epoch: [101][100/195]\tTime  0.162 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7665 (0.7668)\tD(fake) 0.2208 (0.2314)\tgrad(D) penalty 0.0190 (0.0153)\tRec loss 3789.9685 (3941.5288)\tnorm 0.8960 (0.8882)\n",
            "Epoch: [101][110/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7665 (0.7667)\tD(fake) 0.2499 (0.2330)\tgrad(D) penalty 0.0138 (0.0152)\tRec loss 4043.9424 (3940.8732)\tnorm 0.8902 (0.8876)\n",
            "Epoch: [101][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2287 (0.2321)\tgrad(D) penalty 0.0182 (0.0154)\tRec loss 3775.3965 (3945.1489)\tnorm 0.8906 (0.8873)\n",
            "Epoch: [101][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2363 (0.2324)\tgrad(D) penalty 0.0174 (0.0155)\tRec loss 3821.7480 (3945.2041)\tnorm 0.8970 (0.8874)\n",
            "Epoch: [101][140/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2282 (0.2319)\tgrad(D) penalty 0.0160 (0.0154)\tRec loss 3867.0085 (3945.7775)\tnorm 0.8814 (0.8872)\n",
            "Epoch: [101][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2409 (0.2322)\tgrad(D) penalty 0.0170 (0.0155)\tRec loss 4135.5361 (3948.8384)\tnorm 0.8890 (0.8874)\n",
            "Epoch: [101][160/195]\tTime  0.144 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2210 (0.2320)\tgrad(D) penalty 0.0169 (0.0154)\tRec loss 3713.6060 (3950.8115)\tnorm 0.9036 (0.8878)\n",
            "Epoch: [101][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2433 (0.2325)\tgrad(D) penalty 0.0107 (0.0153)\tRec loss 4115.6641 (3951.2614)\tnorm 0.8900 (0.8881)\n",
            "Epoch: [101][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2319 (0.2319)\tgrad(D) penalty 0.0143 (0.0153)\tRec loss 3909.6211 (3950.3525)\tnorm 0.8769 (0.8882)\n",
            "Epoch: [101][190/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2234 (0.2319)\tgrad(D) penalty 0.0159 (0.0153)\tRec loss 3897.4211 (3952.9935)\tnorm 0.8731 (0.8879)\n",
            "Epoch: [102][  0/195]\tTime  0.415 ( 0.415)\tData  0.216 ( 0.216)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3830.8452 (3830.8452)\tnorm 0.8920 (0.8920)\n",
            "Epoch: [102][ 10/195]\tTime  0.146 ( 0.173)\tData  0.000 ( 0.020)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2361 (0.2337)\tgrad(D) penalty 0.0138 (0.0134)\tRec loss 4162.1919 (3951.3809)\tnorm 0.8891 (0.8857)\n",
            "Epoch: [102][ 20/195]\tTime  0.152 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2432 (0.2288)\tgrad(D) penalty 0.0156 (0.0144)\tRec loss 3713.4048 (3897.6822)\tnorm 0.8883 (0.8881)\n",
            "Epoch: [102][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2347 (0.2317)\tgrad(D) penalty 0.0151 (0.0144)\tRec loss 3992.5461 (3897.6458)\tnorm 0.8794 (0.8876)\n",
            "Epoch: [102][ 40/195]\tTime  0.152 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2272 (0.2318)\tgrad(D) penalty 0.0140 (0.0143)\tRec loss 4203.5957 (3908.6274)\tnorm 0.8898 (0.8874)\n",
            "Epoch: [102][ 50/195]\tTime  0.166 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2432 (0.2316)\tgrad(D) penalty 0.0152 (0.0144)\tRec loss 3923.5142 (3901.2734)\tnorm 0.8988 (0.8882)\n",
            "Epoch: [102][ 60/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2086 (0.2317)\tgrad(D) penalty 0.0154 (0.0145)\tRec loss 4065.2993 (3917.7787)\tnorm 0.8840 (0.8869)\n",
            "Epoch: [102][ 70/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2265 (0.2300)\tgrad(D) penalty 0.0144 (0.0147)\tRec loss 3720.8608 (3928.8022)\tnorm 0.8847 (0.8862)\n",
            "Epoch: [102][ 80/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2176 (0.2309)\tgrad(D) penalty 0.0142 (0.0147)\tRec loss 3922.6951 (3930.8466)\tnorm 0.8823 (0.8860)\n",
            "Epoch: [102][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2437 (0.2313)\tgrad(D) penalty 0.0140 (0.0149)\tRec loss 3985.9707 (3938.2314)\tnorm 0.8779 (0.8859)\n",
            "Epoch: [102][100/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7666 (0.7668)\tD(fake) 0.2111 (0.2307)\tgrad(D) penalty 0.0169 (0.0150)\tRec loss 4175.5669 (3942.4811)\tnorm 0.8842 (0.8858)\n",
            "Epoch: [102][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2368 (0.2307)\tgrad(D) penalty 0.0159 (0.0151)\tRec loss 4041.3630 (3952.7264)\tnorm 0.8976 (0.8859)\n",
            "Epoch: [102][120/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2175 (0.2305)\tgrad(D) penalty 0.0171 (0.0151)\tRec loss 4164.3335 (3950.4877)\tnorm 0.8955 (0.8861)\n",
            "Epoch: [102][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2355 (0.2307)\tgrad(D) penalty 0.0139 (0.0151)\tRec loss 3910.1792 (3945.4866)\tnorm 0.8877 (0.8863)\n",
            "Epoch: [102][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2162 (0.2305)\tgrad(D) penalty 0.0148 (0.0151)\tRec loss 3970.2883 (3944.3051)\tnorm 0.8812 (0.8860)\n",
            "Epoch: [102][150/195]\tTime  0.161 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2500 (0.2308)\tgrad(D) penalty 0.0134 (0.0152)\tRec loss 4016.6775 (3945.4709)\tnorm 0.8865 (0.8860)\n",
            "Epoch: [102][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2102 (0.2308)\tgrad(D) penalty 0.0173 (0.0152)\tRec loss 3930.6831 (3945.3542)\tnorm 0.9004 (0.8862)\n",
            "Epoch: [102][170/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2503 (0.2310)\tgrad(D) penalty 0.0147 (0.0153)\tRec loss 4080.8999 (3945.9332)\tnorm 0.8835 (0.8861)\n",
            "Epoch: [102][180/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2115 (0.2310)\tgrad(D) penalty 0.0146 (0.0152)\tRec loss 3817.9688 (3947.8790)\tnorm 0.8911 (0.8860)\n",
            "Epoch: [102][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2718 (0.2322)\tgrad(D) penalty 0.0133 (0.0152)\tRec loss 4076.4531 (3944.7060)\tnorm 0.9022 (0.8866)\n",
            "Epoch: [103][  0/195]\tTime  0.400 ( 0.400)\tData  0.219 ( 0.219)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3732.5234 (3732.5234)\tnorm 0.8897 (0.8897)\n",
            "Epoch: [103][ 10/195]\tTime  0.146 ( 0.172)\tData  0.000 ( 0.020)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2575 (0.2442)\tgrad(D) penalty 0.0126 (0.0142)\tRec loss 3674.4126 (3921.0190)\tnorm 0.8856 (0.8883)\n",
            "Epoch: [103][ 20/195]\tTime  0.150 ( 0.160)\tData  0.000 ( 0.011)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2140 (0.2284)\tgrad(D) penalty 0.0163 (0.0148)\tRec loss 4502.1689 (3952.2589)\tnorm 0.8669 (0.8866)\n",
            "Epoch: [103][ 30/195]\tTime  0.150 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2450 (0.2319)\tgrad(D) penalty 0.0176 (0.0154)\tRec loss 4010.3564 (3944.9115)\tnorm 0.8827 (0.8857)\n",
            "Epoch: [103][ 40/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2060 (0.2305)\tgrad(D) penalty 0.0177 (0.0154)\tRec loss 3877.8306 (3927.8267)\tnorm 0.9009 (0.8875)\n",
            "Epoch: [103][ 50/195]\tTime  0.168 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2545 (0.2317)\tgrad(D) penalty 0.0140 (0.0157)\tRec loss 4155.0127 (3925.5896)\tnorm 0.9046 (0.8889)\n",
            "Epoch: [103][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2186 (0.2313)\tgrad(D) penalty 0.0182 (0.0159)\tRec loss 3770.4880 (3920.2143)\tnorm 0.8806 (0.8893)\n",
            "Epoch: [103][ 70/195]\tTime  0.153 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2366 (0.2316)\tgrad(D) penalty 0.0147 (0.0158)\tRec loss 3866.6768 (3926.8569)\tnorm 0.8885 (0.8885)\n",
            "Epoch: [103][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2275 (0.2310)\tgrad(D) penalty 0.0139 (0.0156)\tRec loss 4002.7290 (3934.2825)\tnorm 0.9030 (0.8899)\n",
            "Epoch: [103][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2080 (0.2300)\tgrad(D) penalty 0.0171 (0.0156)\tRec loss 3942.7944 (3939.1653)\tnorm 0.8983 (0.8908)\n",
            "Epoch: [103][100/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7670 (0.7668)\tD(fake) 0.2352 (0.2302)\tgrad(D) penalty 0.0146 (0.0157)\tRec loss 3920.9854 (3934.7399)\tnorm 0.8899 (0.8902)\n",
            "Epoch: [103][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2266 (0.2300)\tgrad(D) penalty 0.0170 (0.0157)\tRec loss 3937.8335 (3942.8810)\tnorm 0.8813 (0.8899)\n",
            "Epoch: [103][120/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2321 (0.2301)\tgrad(D) penalty 0.0165 (0.0156)\tRec loss 3899.9570 (3942.7229)\tnorm 0.8941 (0.8899)\n",
            "Epoch: [103][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7666 (0.7668)\tD(fake) 0.2250 (0.2302)\tgrad(D) penalty 0.0162 (0.0156)\tRec loss 3720.6943 (3949.7141)\tnorm 0.8789 (0.8897)\n",
            "Epoch: [103][140/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7670 (0.7668)\tD(fake) 0.2540 (0.2306)\tgrad(D) penalty 0.0151 (0.0156)\tRec loss 3837.6113 (3943.6484)\tnorm 0.9000 (0.8896)\n",
            "Epoch: [103][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2346 (0.2313)\tgrad(D) penalty 0.0147 (0.0155)\tRec loss 3716.8198 (3950.1882)\tnorm 0.8786 (0.8891)\n",
            "Epoch: [103][160/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2421 (0.2309)\tgrad(D) penalty 0.0154 (0.0156)\tRec loss 3874.4824 (3950.3186)\tnorm 0.8738 (0.8892)\n",
            "Epoch: [103][170/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2390 (0.2312)\tgrad(D) penalty 0.0166 (0.0156)\tRec loss 4286.4824 (3955.3052)\tnorm 0.8919 (0.8888)\n",
            "Epoch: [103][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2291 (0.2314)\tgrad(D) penalty 0.0159 (0.0156)\tRec loss 3760.2795 (3952.5225)\tnorm 0.8794 (0.8886)\n",
            "Epoch: [103][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7666 (0.7668)\tD(fake) 0.2384 (0.2314)\tgrad(D) penalty 0.0146 (0.0156)\tRec loss 3481.9790 (3946.3476)\tnorm 0.8780 (0.8885)\n",
            "Epoch: [104][  0/195]\tTime  0.414 ( 0.414)\tData  0.219 ( 0.219)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4038.0984 (4038.0984)\tnorm 0.8830 (0.8830)\n",
            "Epoch: [104][ 10/195]\tTime  0.146 ( 0.173)\tData  0.000 ( 0.020)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2383 (0.2362)\tgrad(D) penalty 0.0144 (0.0138)\tRec loss 4124.2183 (3993.4116)\tnorm 0.8914 (0.8903)\n",
            "Epoch: [104][ 20/195]\tTime  0.150 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2431 (0.2308)\tgrad(D) penalty 0.0183 (0.0152)\tRec loss 3933.4570 (3938.5221)\tnorm 0.8762 (0.8883)\n",
            "Epoch: [104][ 30/195]\tTime  0.151 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2214 (0.2327)\tgrad(D) penalty 0.0134 (0.0148)\tRec loss 3885.1274 (3917.0276)\tnorm 0.8809 (0.8861)\n",
            "Epoch: [104][ 40/195]\tTime  0.146 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2474 (0.2323)\tgrad(D) penalty 0.0149 (0.0152)\tRec loss 3771.8088 (3926.4385)\tnorm 0.8863 (0.8861)\n",
            "Epoch: [104][ 50/195]\tTime  0.162 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2279 (0.2323)\tgrad(D) penalty 0.0163 (0.0152)\tRec loss 3789.8608 (3931.2087)\tnorm 0.8825 (0.8863)\n",
            "Epoch: [104][ 60/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2417 (0.2328)\tgrad(D) penalty 0.0136 (0.0154)\tRec loss 4008.3855 (3939.8947)\tnorm 0.8845 (0.8862)\n",
            "Epoch: [104][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2094 (0.2313)\tgrad(D) penalty 0.0167 (0.0154)\tRec loss 4217.2202 (3939.7510)\tnorm 0.8755 (0.8865)\n",
            "Epoch: [104][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2475 (0.2313)\tgrad(D) penalty 0.0158 (0.0155)\tRec loss 4102.1211 (3942.5680)\tnorm 0.8846 (0.8864)\n",
            "Epoch: [104][ 90/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2188 (0.2311)\tgrad(D) penalty 0.0140 (0.0154)\tRec loss 3683.4670 (3938.4665)\tnorm 0.8926 (0.8859)\n",
            "Epoch: [104][100/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2413 (0.2314)\tgrad(D) penalty 0.0137 (0.0155)\tRec loss 3865.6853 (3943.6383)\tnorm 0.8860 (0.8862)\n",
            "Epoch: [104][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2187 (0.2313)\tgrad(D) penalty 0.0157 (0.0155)\tRec loss 3944.2493 (3945.2713)\tnorm 0.8931 (0.8867)\n",
            "Epoch: [104][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2344 (0.2312)\tgrad(D) penalty 0.0168 (0.0155)\tRec loss 4114.0522 (3943.3216)\tnorm 0.8899 (0.8872)\n",
            "Epoch: [104][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2222 (0.2309)\tgrad(D) penalty 0.0157 (0.0155)\tRec loss 3993.2441 (3947.3811)\tnorm 0.8765 (0.8873)\n",
            "Epoch: [104][140/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2130 (0.2306)\tgrad(D) penalty 0.0169 (0.0156)\tRec loss 3869.9326 (3945.2791)\tnorm 0.8819 (0.8875)\n",
            "Epoch: [104][150/195]\tTime  0.162 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2595 (0.2314)\tgrad(D) penalty 0.0125 (0.0155)\tRec loss 3859.1494 (3943.2543)\tnorm 0.8726 (0.8873)\n",
            "Epoch: [104][160/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2418 (0.2314)\tgrad(D) penalty 0.0140 (0.0155)\tRec loss 3900.8931 (3946.7639)\tnorm 0.8826 (0.8873)\n",
            "Epoch: [104][170/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2281 (0.2317)\tgrad(D) penalty 0.0170 (0.0156)\tRec loss 4157.5366 (3941.1500)\tnorm 0.8731 (0.8872)\n",
            "Epoch: [104][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2474 (0.2319)\tgrad(D) penalty 0.0134 (0.0155)\tRec loss 3815.9619 (3940.4693)\tnorm 0.8948 (0.8868)\n",
            "Epoch: [104][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2180 (0.2318)\tgrad(D) penalty 0.0165 (0.0155)\tRec loss 3776.7727 (3940.0635)\tnorm 0.8803 (0.8866)\n",
            "Epoch: [105][  0/195]\tTime  0.405 ( 0.405)\tData  0.225 ( 0.225)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3833.4517 (3833.4517)\tnorm 0.8733 (0.8733)\n",
            "Epoch: [105][ 10/195]\tTime  0.149 ( 0.173)\tData  0.000 ( 0.021)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2371 (0.2341)\tgrad(D) penalty 0.0177 (0.0165)\tRec loss 3907.4402 (3886.5873)\tnorm 0.8813 (0.8806)\n",
            "Epoch: [105][ 20/195]\tTime  0.150 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2209 (0.2257)\tgrad(D) penalty 0.0149 (0.0166)\tRec loss 4219.9600 (3921.9532)\tnorm 0.8855 (0.8819)\n",
            "Epoch: [105][ 30/195]\tTime  0.148 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.7669 (0.7669)\tD(fake) 0.2370 (0.2285)\tgrad(D) penalty 0.0137 (0.0161)\tRec loss 3943.9395 (3898.2396)\tnorm 0.8773 (0.8849)\n",
            "Epoch: [105][ 40/195]\tTime  0.146 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2183 (0.2278)\tgrad(D) penalty 0.0154 (0.0157)\tRec loss 3945.5803 (3918.4871)\tnorm 0.8847 (0.8867)\n",
            "Epoch: [105][ 50/195]\tTime  0.165 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2304 (0.2278)\tgrad(D) penalty 0.0158 (0.0157)\tRec loss 4035.0864 (3917.8809)\tnorm 0.8887 (0.8876)\n",
            "Epoch: [105][ 60/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7669 (0.7669)\tD(fake) 0.2246 (0.2274)\tgrad(D) penalty 0.0164 (0.0157)\tRec loss 3707.5303 (3912.9968)\tnorm 0.8749 (0.8874)\n",
            "Epoch: [105][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7668 (0.7669)\tD(fake) 0.2377 (0.2285)\tgrad(D) penalty 0.0195 (0.0159)\tRec loss 3914.8633 (3904.3057)\tnorm 0.8689 (0.8863)\n",
            "Epoch: [105][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2299 (0.2283)\tgrad(D) penalty 0.0172 (0.0160)\tRec loss 4168.4492 (3917.5191)\tnorm 0.8847 (0.8869)\n",
            "Epoch: [105][ 90/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7669 (0.7669)\tD(fake) 0.2377 (0.2297)\tgrad(D) penalty 0.0158 (0.0159)\tRec loss 3870.7339 (3918.9462)\tnorm 0.8847 (0.8865)\n",
            "Epoch: [105][100/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7669)\tD(fake) 0.2292 (0.2296)\tgrad(D) penalty 0.0152 (0.0159)\tRec loss 4215.3711 (3926.7600)\tnorm 0.8679 (0.8861)\n",
            "Epoch: [105][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7669)\tD(fake) 0.2227 (0.2298)\tgrad(D) penalty 0.0146 (0.0158)\tRec loss 3667.8389 (3935.3418)\tnorm 0.8861 (0.8865)\n",
            "Epoch: [105][120/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7670 (0.7669)\tD(fake) 0.2382 (0.2295)\tgrad(D) penalty 0.0132 (0.0157)\tRec loss 4161.6445 (3943.2925)\tnorm 0.9004 (0.8862)\n",
            "Epoch: [105][130/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7666 (0.7669)\tD(fake) 0.2325 (0.2298)\tgrad(D) penalty 0.0163 (0.0157)\tRec loss 3940.6047 (3938.3173)\tnorm 0.8830 (0.8859)\n",
            "Epoch: [105][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7670 (0.7669)\tD(fake) 0.2445 (0.2299)\tgrad(D) penalty 0.0154 (0.0157)\tRec loss 3810.7349 (3945.5513)\tnorm 0.8948 (0.8858)\n",
            "Epoch: [105][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7670 (0.7669)\tD(fake) 0.2412 (0.2308)\tgrad(D) penalty 0.0148 (0.0157)\tRec loss 4034.4355 (3949.2335)\tnorm 0.8762 (0.8860)\n",
            "Epoch: [105][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7669)\tD(fake) 0.2433 (0.2305)\tgrad(D) penalty 0.0166 (0.0158)\tRec loss 4081.8469 (3947.8336)\tnorm 0.8945 (0.8859)\n",
            "Epoch: [105][170/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7669)\tD(fake) 0.2174 (0.2304)\tgrad(D) penalty 0.0138 (0.0158)\tRec loss 4112.5137 (3948.7467)\tnorm 0.8847 (0.8858)\n",
            "Epoch: [105][180/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7667 (0.7669)\tD(fake) 0.2485 (0.2300)\tgrad(D) penalty 0.0191 (0.0159)\tRec loss 3891.5857 (3946.0960)\tnorm 0.8915 (0.8857)\n",
            "Epoch: [105][190/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7669 (0.7669)\tD(fake) 0.2401 (0.2309)\tgrad(D) penalty 0.0167 (0.0159)\tRec loss 3967.3174 (3943.0355)\tnorm 0.8870 (0.8857)\n",
            "Epoch: [106][  0/195]\tTime  0.413 ( 0.413)\tData  0.234 ( 0.234)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3841.7722 (3841.7722)\tnorm 0.8876 (0.8876)\n",
            "Epoch: [106][ 10/195]\tTime  0.150 ( 0.174)\tData  0.000 ( 0.022)\tD(real) 0.7668 (0.7666)\tD(fake) 0.2562 (0.2426)\tgrad(D) penalty 0.0184 (0.0186)\tRec loss 3841.2483 (3890.4479)\tnorm 0.8788 (0.8799)\n",
            "Epoch: [106][ 20/195]\tTime  0.145 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2034 (0.2359)\tgrad(D) penalty 0.0207 (0.0180)\tRec loss 3913.0688 (3902.5011)\tnorm 0.8715 (0.8810)\n",
            "Epoch: [106][ 30/195]\tTime  0.145 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2516 (0.2350)\tgrad(D) penalty 0.0172 (0.0180)\tRec loss 3780.6655 (3894.6534)\tnorm 0.8774 (0.8810)\n",
            "Epoch: [106][ 40/195]\tTime  0.146 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2043 (0.2343)\tgrad(D) penalty 0.0185 (0.0175)\tRec loss 4050.1997 (3911.0574)\tnorm 0.8969 (0.8807)\n",
            "Epoch: [106][ 50/195]\tTime  0.168 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2387 (0.2332)\tgrad(D) penalty 0.0172 (0.0176)\tRec loss 4001.6309 (3925.1643)\tnorm 0.8886 (0.8811)\n",
            "Epoch: [106][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2228 (0.2335)\tgrad(D) penalty 0.0169 (0.0172)\tRec loss 3937.4434 (3936.1058)\tnorm 0.8836 (0.8814)\n",
            "Epoch: [106][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7666 (0.7668)\tD(fake) 0.2329 (0.2326)\tgrad(D) penalty 0.0161 (0.0170)\tRec loss 4062.3035 (3940.3339)\tnorm 0.8736 (0.8819)\n",
            "Epoch: [106][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2130 (0.2323)\tgrad(D) penalty 0.0146 (0.0165)\tRec loss 4106.6855 (3941.7869)\tnorm 0.8855 (0.8823)\n",
            "Epoch: [106][ 90/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2505 (0.2327)\tgrad(D) penalty 0.0159 (0.0165)\tRec loss 4103.5781 (3941.1629)\tnorm 0.8887 (0.8828)\n",
            "Epoch: [106][100/195]\tTime  0.174 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2173 (0.2314)\tgrad(D) penalty 0.0181 (0.0164)\tRec loss 4237.7949 (3940.5548)\tnorm 0.8895 (0.8833)\n",
            "Epoch: [106][110/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2361 (0.2321)\tgrad(D) penalty 0.0144 (0.0163)\tRec loss 3760.7380 (3946.0195)\tnorm 0.8897 (0.8841)\n",
            "Epoch: [106][120/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2359 (0.2315)\tgrad(D) penalty 0.0150 (0.0163)\tRec loss 3632.8008 (3942.9486)\tnorm 0.9052 (0.8846)\n",
            "Epoch: [106][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2180 (0.2308)\tgrad(D) penalty 0.0179 (0.0164)\tRec loss 3911.9287 (3942.4338)\tnorm 0.8864 (0.8851)\n",
            "Epoch: [106][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2353 (0.2317)\tgrad(D) penalty 0.0152 (0.0163)\tRec loss 3722.8518 (3939.7753)\tnorm 0.8968 (0.8854)\n",
            "Epoch: [106][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7666 (0.7668)\tD(fake) 0.2059 (0.2304)\tgrad(D) penalty 0.0199 (0.0164)\tRec loss 3668.5122 (3939.3356)\tnorm 0.9026 (0.8855)\n",
            "Epoch: [106][160/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2509 (0.2317)\tgrad(D) penalty 0.0173 (0.0165)\tRec loss 3701.9236 (3939.9964)\tnorm 0.8818 (0.8856)\n",
            "Epoch: [106][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2229 (0.2314)\tgrad(D) penalty 0.0237 (0.0167)\tRec loss 4067.3665 (3936.2756)\tnorm 0.9021 (0.8858)\n",
            "Epoch: [106][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2279 (0.2316)\tgrad(D) penalty 0.0161 (0.0167)\tRec loss 3831.8430 (3938.2203)\tnorm 0.8874 (0.8862)\n",
            "Epoch: [106][190/195]\tTime  0.154 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2167 (0.2307)\tgrad(D) penalty 0.0188 (0.0167)\tRec loss 3801.0693 (3938.7879)\tnorm 0.8902 (0.8864)\n",
            "Epoch: [107][  0/195]\tTime  0.437 ( 0.437)\tData  0.234 ( 0.234)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4328.5723 (4328.5723)\tnorm 0.8876 (0.8876)\n",
            "Epoch: [107][ 10/195]\tTime  0.147 ( 0.176)\tData  0.000 ( 0.021)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2216 (0.2164)\tgrad(D) penalty 0.0183 (0.0176)\tRec loss 3671.3298 (3929.0981)\tnorm 0.8969 (0.8938)\n",
            "Epoch: [107][ 20/195]\tTime  0.149 ( 0.163)\tData  0.000 ( 0.011)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2295 (0.2334)\tgrad(D) penalty 0.0182 (0.0167)\tRec loss 3909.5898 (3891.2588)\tnorm 0.8836 (0.8943)\n",
            "Epoch: [107][ 30/195]\tTime  0.150 ( 0.160)\tData  0.000 ( 0.008)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2388 (0.2324)\tgrad(D) penalty 0.0154 (0.0168)\tRec loss 4149.4927 (3924.5141)\tnorm 0.8864 (0.8908)\n",
            "Epoch: [107][ 40/195]\tTime  0.153 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2289 (0.2307)\tgrad(D) penalty 0.0191 (0.0167)\tRec loss 3968.0918 (3937.4985)\tnorm 0.8940 (0.8897)\n",
            "Epoch: [107][ 50/195]\tTime  0.166 ( 0.157)\tData  0.000 ( 0.005)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2290 (0.2310)\tgrad(D) penalty 0.0133 (0.0164)\tRec loss 4042.9399 (3946.8962)\tnorm 0.8907 (0.8891)\n",
            "Epoch: [107][ 60/195]\tTime  0.152 ( 0.156)\tData  0.000 ( 0.004)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2308 (0.2283)\tgrad(D) penalty 0.0193 (0.0164)\tRec loss 3872.5227 (3953.9695)\tnorm 0.8850 (0.8890)\n",
            "Epoch: [107][ 70/195]\tTime  0.150 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2499 (0.2304)\tgrad(D) penalty 0.0159 (0.0164)\tRec loss 3869.7300 (3946.5191)\tnorm 0.8813 (0.8881)\n",
            "Epoch: [107][ 80/195]\tTime  0.151 ( 0.155)\tData  0.000 ( 0.003)\tD(real) 0.7665 (0.7667)\tD(fake) 0.2234 (0.2290)\tgrad(D) penalty 0.0189 (0.0165)\tRec loss 3666.5254 (3943.2197)\tnorm 0.8756 (0.8869)\n",
            "Epoch: [107][ 90/195]\tTime  0.153 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2312 (0.2295)\tgrad(D) penalty 0.0153 (0.0164)\tRec loss 4169.8721 (3937.1738)\tnorm 0.8834 (0.8862)\n",
            "Epoch: [107][100/195]\tTime  0.167 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2292 (0.2290)\tgrad(D) penalty 0.0186 (0.0166)\tRec loss 3867.3242 (3935.5293)\tnorm 0.8691 (0.8856)\n",
            "Epoch: [107][110/195]\tTime  0.156 ( 0.154)\tData  0.000 ( 0.002)\tD(real) 0.7663 (0.7667)\tD(fake) 0.2259 (0.2287)\tgrad(D) penalty 0.0179 (0.0167)\tRec loss 3707.0430 (3930.8625)\tnorm 0.8717 (0.8848)\n",
            "Epoch: [107][120/195]\tTime  0.152 ( 0.154)\tData  0.000 ( 0.002)\tD(real) 0.7666 (0.7667)\tD(fake) 0.2292 (0.2295)\tgrad(D) penalty 0.0152 (0.0167)\tRec loss 3884.0698 (3932.1851)\tnorm 0.8789 (0.8847)\n",
            "Epoch: [107][130/195]\tTime  0.152 ( 0.154)\tData  0.000 ( 0.002)\tD(real) 0.7666 (0.7667)\tD(fake) 0.2267 (0.2288)\tgrad(D) penalty 0.0188 (0.0168)\tRec loss 3969.0154 (3926.9683)\tnorm 0.8704 (0.8846)\n",
            "Epoch: [107][140/195]\tTime  0.155 ( 0.154)\tData  0.000 ( 0.002)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2169 (0.2295)\tgrad(D) penalty 0.0160 (0.0167)\tRec loss 3918.9312 (3929.0487)\tnorm 0.8787 (0.8842)\n",
            "Epoch: [107][150/195]\tTime  0.166 ( 0.154)\tData  0.000 ( 0.002)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2338 (0.2292)\tgrad(D) penalty 0.0168 (0.0167)\tRec loss 4069.6665 (3932.0616)\tnorm 0.8730 (0.8843)\n",
            "Epoch: [107][160/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2280 (0.2296)\tgrad(D) penalty 0.0138 (0.0166)\tRec loss 3910.2891 (3929.8299)\tnorm 0.8788 (0.8841)\n",
            "Epoch: [107][170/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2218 (0.2292)\tgrad(D) penalty 0.0153 (0.0165)\tRec loss 3896.4775 (3929.2094)\tnorm 0.8735 (0.8841)\n",
            "Epoch: [107][180/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7663 (0.7667)\tD(fake) 0.2434 (0.2297)\tgrad(D) penalty 0.0147 (0.0164)\tRec loss 3955.7031 (3931.2194)\tnorm 0.8749 (0.8839)\n",
            "Epoch: [107][190/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.001)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2180 (0.2294)\tgrad(D) penalty 0.0162 (0.0164)\tRec loss 4076.7881 (3931.2571)\tnorm 0.8958 (0.8840)\n",
            "Epoch: [108][  0/195]\tTime  0.441 ( 0.441)\tData  0.247 ( 0.247)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3869.3774 (3869.3774)\tnorm 0.8919 (0.8919)\n",
            "Epoch: [108][ 10/195]\tTime  0.148 ( 0.177)\tData  0.000 ( 0.023)\tD(real) 0.7666 (0.7667)\tD(fake) 0.2434 (0.2359)\tgrad(D) penalty 0.0177 (0.0167)\tRec loss 3574.4619 (3888.2701)\tnorm 0.8844 (0.8854)\n",
            "Epoch: [108][ 20/195]\tTime  0.149 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2253 (0.2349)\tgrad(D) penalty 0.0137 (0.0161)\tRec loss 4093.1138 (3908.5258)\tnorm 0.8802 (0.8824)\n",
            "Epoch: [108][ 30/195]\tTime  0.147 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2299 (0.2323)\tgrad(D) penalty 0.0164 (0.0160)\tRec loss 3771.1909 (3899.0243)\tnorm 0.8816 (0.8828)\n",
            "Epoch: [108][ 40/195]\tTime  0.152 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2297 (0.2315)\tgrad(D) penalty 0.0152 (0.0163)\tRec loss 4110.4287 (3904.7850)\tnorm 0.8868 (0.8821)\n",
            "Epoch: [108][ 50/195]\tTime  0.168 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2314 (0.2320)\tgrad(D) penalty 0.0142 (0.0161)\tRec loss 4389.9873 (3909.5016)\tnorm 0.8627 (0.8807)\n",
            "Epoch: [108][ 60/195]\tTime  0.151 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2477 (0.2330)\tgrad(D) penalty 0.0178 (0.0166)\tRec loss 3990.4878 (3917.1767)\tnorm 0.8871 (0.8814)\n",
            "Epoch: [108][ 70/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2181 (0.2319)\tgrad(D) penalty 0.0173 (0.0167)\tRec loss 3722.5042 (3918.0344)\tnorm 0.8794 (0.8811)\n",
            "Epoch: [108][ 80/195]\tTime  0.152 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2337 (0.2314)\tgrad(D) penalty 0.0161 (0.0168)\tRec loss 3867.0369 (3927.4014)\tnorm 0.8816 (0.8815)\n",
            "Epoch: [108][ 90/195]\tTime  0.154 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7666 (0.7667)\tD(fake) 0.2403 (0.2322)\tgrad(D) penalty 0.0192 (0.0168)\tRec loss 3992.9883 (3925.1733)\tnorm 0.8674 (0.8809)\n",
            "Epoch: [108][100/195]\tTime  0.169 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2164 (0.2307)\tgrad(D) penalty 0.0196 (0.0172)\tRec loss 3995.3530 (3931.2160)\tnorm 0.8819 (0.8809)\n",
            "Epoch: [108][110/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7667)\tD(fake) 0.2388 (0.2313)\tgrad(D) penalty 0.0170 (0.0171)\tRec loss 3750.7876 (3928.1124)\tnorm 0.9019 (0.8814)\n",
            "Epoch: [108][120/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2190 (0.2303)\tgrad(D) penalty 0.0193 (0.0171)\tRec loss 3745.0117 (3928.2622)\tnorm 0.8752 (0.8816)\n",
            "Epoch: [108][130/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2321 (0.2303)\tgrad(D) penalty 0.0154 (0.0170)\tRec loss 4057.9009 (3931.5115)\tnorm 0.8773 (0.8815)\n",
            "Epoch: [108][140/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2309 (0.2309)\tgrad(D) penalty 0.0142 (0.0169)\tRec loss 3792.2212 (3929.5703)\tnorm 0.9027 (0.8819)\n",
            "Epoch: [108][150/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7665 (0.7667)\tD(fake) 0.2272 (0.2309)\tgrad(D) penalty 0.0171 (0.0168)\tRec loss 4073.8169 (3928.3271)\tnorm 0.8934 (0.8820)\n",
            "Epoch: [108][160/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2387 (0.2310)\tgrad(D) penalty 0.0162 (0.0167)\tRec loss 4048.1833 (3931.5874)\tnorm 0.8862 (0.8822)\n",
            "Epoch: [108][170/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2291 (0.2314)\tgrad(D) penalty 0.0139 (0.0166)\tRec loss 4064.7314 (3931.4140)\tnorm 0.8912 (0.8826)\n",
            "Epoch: [108][180/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7667 (0.7667)\tD(fake) 0.2327 (0.2311)\tgrad(D) penalty 0.0156 (0.0167)\tRec loss 4018.0403 (3933.2545)\tnorm 0.8772 (0.8824)\n",
            "Epoch: [108][190/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7668 (0.7667)\tD(fake) 0.2423 (0.2313)\tgrad(D) penalty 0.0188 (0.0167)\tRec loss 4103.9873 (3929.2955)\tnorm 0.8889 (0.8827)\n",
            "Epoch: [109][  0/195]\tTime  0.443 ( 0.443)\tData  0.245 ( 0.245)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3955.3762 (3955.3762)\tnorm 0.8776 (0.8776)\n",
            "Epoch: [109][ 10/195]\tTime  0.148 ( 0.176)\tData  0.000 ( 0.023)\tD(real) 0.7666 (0.7667)\tD(fake) 0.2167 (0.2162)\tgrad(D) penalty 0.0159 (0.0161)\tRec loss 3957.0281 (3917.7832)\tnorm 0.8895 (0.8822)\n",
            "Epoch: [109][ 20/195]\tTime  0.151 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7667 (0.7668)\tD(fake) 0.2257 (0.2259)\tgrad(D) penalty 0.0155 (0.0168)\tRec loss 4275.5400 (3933.3531)\tnorm 0.8770 (0.8841)\n",
            "Epoch: [109][ 30/195]\tTime  0.148 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7665 (0.7667)\tD(fake) 0.2121 (0.2216)\tgrad(D) penalty 0.0159 (0.0166)\tRec loss 3998.9790 (3960.7129)\tnorm 0.8762 (0.8825)\n",
            "Epoch: [109][ 40/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7668 (0.7668)\tD(fake) 0.2315 (0.2261)\tgrad(D) penalty 0.0155 (0.0167)\tRec loss 3745.1685 (3935.6037)\tnorm 0.8895 (0.8835)\n",
            "Epoch: [109][ 50/195]\tTime  0.170 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7669 (0.7668)\tD(fake) 0.2352 (0.2274)\tgrad(D) penalty 0.0171 (0.0169)\tRec loss 3987.8696 (3935.9479)\tnorm 0.8836 (0.8839)\n",
            "Epoch: [109][ 60/195]\tTime  0.150 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7670 (0.7668)\tD(fake) 0.2192 (0.2271)\tgrad(D) penalty 0.0142 (0.0164)\tRec loss 3678.3027 (3918.6940)\tnorm 0.8778 (0.8833)\n",
            "Epoch: [109][ 70/195]\tTime  0.156 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7670 (0.7668)\tD(fake) 0.2419 (0.2281)\tgrad(D) penalty 0.0184 (0.0166)\tRec loss 4069.5044 (3918.5761)\tnorm 0.8919 (0.8831)\n",
            "Epoch: [109][ 80/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7669 (0.7669)\tD(fake) 0.2232 (0.2274)\tgrad(D) penalty 0.0168 (0.0165)\tRec loss 3559.1301 (3914.9479)\tnorm 0.8844 (0.8840)\n",
            "Epoch: [109][ 90/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7667 (0.7669)\tD(fake) 0.2277 (0.2274)\tgrad(D) penalty 0.0166 (0.0164)\tRec loss 4084.1377 (3920.3514)\tnorm 0.8831 (0.8840)\n",
            "Epoch: [109][100/195]\tTime  0.167 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7670 (0.7669)\tD(fake) 0.2413 (0.2285)\tgrad(D) penalty 0.0170 (0.0164)\tRec loss 3966.6821 (3921.8752)\tnorm 0.8847 (0.8848)\n",
            "Epoch: [109][110/195]\tTime  0.153 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7670 (0.7669)\tD(fake) 0.2186 (0.2281)\tgrad(D) penalty 0.0184 (0.0164)\tRec loss 3783.7402 (3923.9232)\tnorm 0.8749 (0.8843)\n",
            "Epoch: [109][120/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7669 (0.7669)\tD(fake) 0.2097 (0.2270)\tgrad(D) penalty 0.0202 (0.0165)\tRec loss 3572.8013 (3919.3673)\tnorm 0.8883 (0.8842)\n",
            "Epoch: [109][130/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7671 (0.7669)\tD(fake) 0.2233 (0.2267)\tgrad(D) penalty 0.0208 (0.0166)\tRec loss 4217.8164 (3926.8619)\tnorm 0.8754 (0.8834)\n",
            "Epoch: [109][140/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7671 (0.7669)\tD(fake) 0.2206 (0.2270)\tgrad(D) penalty 0.0171 (0.0167)\tRec loss 3897.5220 (3923.7304)\tnorm 0.8743 (0.8831)\n",
            "Epoch: [109][150/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7673 (0.7669)\tD(fake) 0.2364 (0.2270)\tgrad(D) penalty 0.0169 (0.0168)\tRec loss 4090.8169 (3921.5679)\tnorm 0.8869 (0.8832)\n",
            "Epoch: [109][160/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7672 (0.7669)\tD(fake) 0.2297 (0.2276)\tgrad(D) penalty 0.0154 (0.0168)\tRec loss 3923.8484 (3924.6593)\tnorm 0.8724 (0.8832)\n",
            "Epoch: [109][170/195]\tTime  0.156 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7672 (0.7670)\tD(fake) 0.2228 (0.2271)\tgrad(D) penalty 0.0161 (0.0168)\tRec loss 4112.4570 (3928.0592)\tnorm 0.8997 (0.8836)\n",
            "Epoch: [109][180/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7670 (0.7670)\tD(fake) 0.2149 (0.2274)\tgrad(D) penalty 0.0157 (0.0167)\tRec loss 3569.3667 (3925.0241)\tnorm 0.8721 (0.8834)\n",
            "Epoch: [109][190/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7674 (0.7670)\tD(fake) 0.2319 (0.2272)\tgrad(D) penalty 0.0183 (0.0168)\tRec loss 3801.4324 (3925.6171)\tnorm 0.8876 (0.8832)\n",
            "Epoch: [110][  0/195]\tTime  0.425 ( 0.425)\tData  0.245 ( 0.245)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3840.1562 (3840.1562)\tnorm 0.8906 (0.8906)\n",
            "Epoch: [110][ 10/195]\tTime  0.150 ( 0.177)\tData  0.000 ( 0.023)\tD(real) 0.7673 (0.7673)\tD(fake) 0.2290 (0.2176)\tgrad(D) penalty 0.0205 (0.0193)\tRec loss 3606.9282 (3818.5222)\tnorm 0.8686 (0.8795)\n",
            "Epoch: [110][ 20/195]\tTime  0.148 ( 0.164)\tData  0.000 ( 0.012)\tD(real) 0.7674 (0.7673)\tD(fake) 0.2157 (0.2293)\tgrad(D) penalty 0.0171 (0.0173)\tRec loss 3886.4556 (3835.5331)\tnorm 0.8790 (0.8804)\n",
            "Epoch: [110][ 30/195]\tTime  0.150 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7671 (0.7673)\tD(fake) 0.2371 (0.2273)\tgrad(D) penalty 0.0195 (0.0179)\tRec loss 3978.5356 (3869.8862)\tnorm 0.8803 (0.8790)\n",
            "Epoch: [110][ 40/195]\tTime  0.150 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7673 (0.7673)\tD(fake) 0.2097 (0.2302)\tgrad(D) penalty 0.0150 (0.0171)\tRec loss 3952.8660 (3894.5387)\tnorm 0.8664 (0.8798)\n",
            "Epoch: [110][ 50/195]\tTime  0.167 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7672 (0.7673)\tD(fake) 0.2453 (0.2303)\tgrad(D) penalty 0.0148 (0.0171)\tRec loss 3669.6665 (3913.1464)\tnorm 0.8863 (0.8809)\n",
            "Epoch: [110][ 60/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7673 (0.7673)\tD(fake) 0.2284 (0.2308)\tgrad(D) penalty 0.0177 (0.0169)\tRec loss 3887.7893 (3905.6865)\tnorm 0.8756 (0.8817)\n",
            "Epoch: [110][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7674 (0.7673)\tD(fake) 0.2347 (0.2312)\tgrad(D) penalty 0.0147 (0.0168)\tRec loss 4125.7559 (3908.9534)\tnorm 0.8830 (0.8820)\n",
            "Epoch: [110][ 80/195]\tTime  0.154 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7673 (0.7673)\tD(fake) 0.2413 (0.2308)\tgrad(D) penalty 0.0155 (0.0168)\tRec loss 4114.5732 (3919.6806)\tnorm 0.8836 (0.8827)\n",
            "Epoch: [110][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7673 (0.7673)\tD(fake) 0.2332 (0.2309)\tgrad(D) penalty 0.0153 (0.0166)\tRec loss 3923.0659 (3915.9767)\tnorm 0.8816 (0.8828)\n",
            "Epoch: [110][100/195]\tTime  0.168 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7672 (0.7673)\tD(fake) 0.2356 (0.2322)\tgrad(D) penalty 0.0202 (0.0166)\tRec loss 4080.3801 (3917.7309)\tnorm 0.8885 (0.8831)\n",
            "Epoch: [110][110/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7673 (0.7673)\tD(fake) 0.2335 (0.2322)\tgrad(D) penalty 0.0150 (0.0166)\tRec loss 4341.9312 (3919.3920)\tnorm 0.8872 (0.8829)\n",
            "Epoch: [110][120/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7673 (0.7673)\tD(fake) 0.2262 (0.2316)\tgrad(D) penalty 0.0156 (0.0164)\tRec loss 3917.7805 (3915.8795)\tnorm 0.8794 (0.8826)\n",
            "Epoch: [110][130/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7673 (0.7673)\tD(fake) 0.2358 (0.2317)\tgrad(D) penalty 0.0170 (0.0164)\tRec loss 3928.5552 (3911.7596)\tnorm 0.8896 (0.8828)\n",
            "Epoch: [110][140/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7672 (0.7673)\tD(fake) 0.2268 (0.2314)\tgrad(D) penalty 0.0167 (0.0164)\tRec loss 4038.0125 (3916.4638)\tnorm 0.8808 (0.8826)\n",
            "Epoch: [110][150/195]\tTime  0.161 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7674 (0.7673)\tD(fake) 0.2380 (0.2318)\tgrad(D) penalty 0.0157 (0.0164)\tRec loss 3836.6289 (3918.7259)\tnorm 0.9053 (0.8826)\n",
            "Epoch: [110][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7674 (0.7673)\tD(fake) 0.2280 (0.2317)\tgrad(D) penalty 0.0158 (0.0164)\tRec loss 3781.7461 (3919.3815)\tnorm 0.8697 (0.8827)\n",
            "Epoch: [110][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7673 (0.7673)\tD(fake) 0.2232 (0.2313)\tgrad(D) penalty 0.0153 (0.0163)\tRec loss 4131.0796 (3923.0148)\tnorm 0.8902 (0.8830)\n",
            "Epoch: [110][180/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7672 (0.7673)\tD(fake) 0.2175 (0.2307)\tgrad(D) penalty 0.0160 (0.0163)\tRec loss 3982.9861 (3923.4686)\tnorm 0.8832 (0.8832)\n",
            "Epoch: [110][190/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7674 (0.7673)\tD(fake) 0.2273 (0.2302)\tgrad(D) penalty 0.0166 (0.0163)\tRec loss 4153.9961 (3922.7355)\tnorm 0.8836 (0.8834)\n",
            "Epoch: [111][  0/195]\tTime  0.426 ( 0.426)\tData  0.243 ( 0.243)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3468.3435 (3468.3435)\tnorm 0.8960 (0.8960)\n",
            "Epoch: [111][ 10/195]\tTime  0.149 ( 0.177)\tData  0.000 ( 0.022)\tD(real) 0.7674 (0.7674)\tD(fake) 0.2373 (0.2275)\tgrad(D) penalty 0.0178 (0.0162)\tRec loss 3869.5686 (3893.8892)\tnorm 0.8927 (0.8815)\n",
            "Epoch: [111][ 20/195]\tTime  0.151 ( 0.164)\tData  0.000 ( 0.012)\tD(real) 0.7673 (0.7673)\tD(fake) 0.2179 (0.2235)\tgrad(D) penalty 0.0176 (0.0169)\tRec loss 3949.7290 (3901.7698)\tnorm 0.8723 (0.8813)\n",
            "Epoch: [111][ 30/195]\tTime  0.148 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7673 (0.7673)\tD(fake) 0.2392 (0.2275)\tgrad(D) penalty 0.0163 (0.0170)\tRec loss 3912.8730 (3935.9058)\tnorm 0.8727 (0.8800)\n",
            "Epoch: [111][ 40/195]\tTime  0.149 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7675 (0.7673)\tD(fake) 0.2166 (0.2252)\tgrad(D) penalty 0.0170 (0.0169)\tRec loss 3811.6040 (3910.6472)\tnorm 0.8743 (0.8818)\n",
            "Epoch: [111][ 50/195]\tTime  0.171 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7675 (0.7673)\tD(fake) 0.2370 (0.2262)\tgrad(D) penalty 0.0157 (0.0170)\tRec loss 4196.1826 (3897.2361)\tnorm 0.8870 (0.8820)\n",
            "Epoch: [111][ 60/195]\tTime  0.150 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7674 (0.7673)\tD(fake) 0.2397 (0.2268)\tgrad(D) penalty 0.0178 (0.0172)\tRec loss 3954.3667 (3910.7507)\tnorm 0.8878 (0.8828)\n",
            "Epoch: [111][ 70/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7673 (0.7673)\tD(fake) 0.2200 (0.2272)\tgrad(D) penalty 0.0150 (0.0171)\tRec loss 3642.9619 (3900.8825)\tnorm 0.8896 (0.8828)\n",
            "Epoch: [111][ 80/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7675 (0.7674)\tD(fake) 0.2336 (0.2264)\tgrad(D) penalty 0.0128 (0.0167)\tRec loss 3815.4885 (3903.7026)\tnorm 0.8801 (0.8831)\n",
            "Epoch: [111][ 90/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7675 (0.7674)\tD(fake) 0.2182 (0.2260)\tgrad(D) penalty 0.0142 (0.0166)\tRec loss 4166.9775 (3905.2584)\tnorm 0.8822 (0.8838)\n",
            "Epoch: [111][100/195]\tTime  0.166 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7676 (0.7674)\tD(fake) 0.2374 (0.2257)\tgrad(D) penalty 0.0143 (0.0164)\tRec loss 4060.8066 (3911.4398)\tnorm 0.8920 (0.8838)\n",
            "Epoch: [111][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7676 (0.7674)\tD(fake) 0.2378 (0.2263)\tgrad(D) penalty 0.0163 (0.0163)\tRec loss 4006.0713 (3914.8731)\tnorm 0.8925 (0.8835)\n",
            "Epoch: [111][120/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7676 (0.7674)\tD(fake) 0.2166 (0.2263)\tgrad(D) penalty 0.0147 (0.0163)\tRec loss 4315.4502 (3920.9083)\tnorm 0.8793 (0.8823)\n",
            "Epoch: [111][130/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7676 (0.7674)\tD(fake) 0.2304 (0.2262)\tgrad(D) penalty 0.0153 (0.0163)\tRec loss 3806.6274 (3918.5160)\tnorm 0.8737 (0.8817)\n",
            "Epoch: [111][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7676 (0.7674)\tD(fake) 0.2161 (0.2263)\tgrad(D) penalty 0.0193 (0.0163)\tRec loss 3944.8291 (3920.7240)\tnorm 0.8718 (0.8817)\n",
            "Epoch: [111][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7676 (0.7675)\tD(fake) 0.2294 (0.2262)\tgrad(D) penalty 0.0158 (0.0162)\tRec loss 3616.9717 (3917.8997)\tnorm 0.8810 (0.8818)\n",
            "Epoch: [111][160/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7676 (0.7675)\tD(fake) 0.2362 (0.2268)\tgrad(D) penalty 0.0151 (0.0162)\tRec loss 3864.1296 (3915.5069)\tnorm 0.8848 (0.8818)\n",
            "Epoch: [111][170/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7677 (0.7675)\tD(fake) 0.2468 (0.2273)\tgrad(D) penalty 0.0181 (0.0162)\tRec loss 4115.2100 (3913.9798)\tnorm 0.8808 (0.8817)\n",
            "Epoch: [111][180/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7676 (0.7675)\tD(fake) 0.2142 (0.2279)\tgrad(D) penalty 0.0200 (0.0163)\tRec loss 3931.3030 (3916.2443)\tnorm 0.8832 (0.8818)\n",
            "Epoch: [111][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7675 (0.7675)\tD(fake) 0.2386 (0.2278)\tgrad(D) penalty 0.0171 (0.0163)\tRec loss 3899.8545 (3919.0023)\tnorm 0.8899 (0.8821)\n",
            "Epoch: [112][  0/195]\tTime  0.425 ( 0.425)\tData  0.232 ( 0.232)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3912.9497 (3912.9497)\tnorm 0.8801 (0.8801)\n",
            "Epoch: [112][ 10/195]\tTime  0.149 ( 0.175)\tData  0.000 ( 0.021)\tD(real) 0.7676 (0.7676)\tD(fake) 0.2391 (0.2314)\tgrad(D) penalty 0.0165 (0.0168)\tRec loss 3927.3213 (3964.3318)\tnorm 0.8970 (0.8854)\n",
            "Epoch: [112][ 20/195]\tTime  0.148 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7675 (0.7675)\tD(fake) 0.2151 (0.2266)\tgrad(D) penalty 0.0175 (0.0162)\tRec loss 3910.0068 (3926.8047)\tnorm 0.8832 (0.8847)\n",
            "Epoch: [112][ 30/195]\tTime  0.146 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7676 (0.7676)\tD(fake) 0.2400 (0.2284)\tgrad(D) penalty 0.0146 (0.0158)\tRec loss 3899.4075 (3918.7626)\tnorm 0.8731 (0.8866)\n",
            "Epoch: [112][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7674 (0.7676)\tD(fake) 0.2082 (0.2253)\tgrad(D) penalty 0.0167 (0.0155)\tRec loss 4001.3220 (3903.2262)\tnorm 0.8838 (0.8850)\n",
            "Epoch: [112][ 50/195]\tTime  0.163 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7677 (0.7676)\tD(fake) 0.2505 (0.2287)\tgrad(D) penalty 0.0174 (0.0158)\tRec loss 3715.1089 (3892.9564)\tnorm 0.8743 (0.8845)\n",
            "Epoch: [112][ 60/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7676 (0.7676)\tD(fake) 0.2310 (0.2276)\tgrad(D) penalty 0.0154 (0.0161)\tRec loss 3896.8293 (3888.8044)\tnorm 0.8927 (0.8844)\n",
            "Epoch: [112][ 70/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7677 (0.7676)\tD(fake) 0.2153 (0.2270)\tgrad(D) penalty 0.0184 (0.0162)\tRec loss 3692.6177 (3894.7412)\tnorm 0.8937 (0.8837)\n",
            "Epoch: [112][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7676 (0.7676)\tD(fake) 0.2390 (0.2274)\tgrad(D) penalty 0.0205 (0.0167)\tRec loss 4011.7019 (3894.8473)\tnorm 0.8821 (0.8836)\n",
            "Epoch: [112][ 90/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7676 (0.7676)\tD(fake) 0.2227 (0.2277)\tgrad(D) penalty 0.0195 (0.0169)\tRec loss 3846.7559 (3907.0608)\tnorm 0.8821 (0.8825)\n",
            "Epoch: [112][100/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7677 (0.7676)\tD(fake) 0.2228 (0.2256)\tgrad(D) penalty 0.0154 (0.0170)\tRec loss 3982.3374 (3910.8019)\tnorm 0.8813 (0.8822)\n",
            "Epoch: [112][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7677 (0.7676)\tD(fake) 0.2457 (0.2269)\tgrad(D) penalty 0.0151 (0.0170)\tRec loss 3856.5151 (3910.6809)\tnorm 0.8918 (0.8824)\n",
            "Epoch: [112][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7676 (0.7676)\tD(fake) 0.2235 (0.2267)\tgrad(D) penalty 0.0143 (0.0168)\tRec loss 3668.7764 (3907.1427)\tnorm 0.8801 (0.8827)\n",
            "Epoch: [112][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7677 (0.7676)\tD(fake) 0.2122 (0.2266)\tgrad(D) penalty 0.0152 (0.0167)\tRec loss 3856.3994 (3912.6920)\tnorm 0.8866 (0.8825)\n",
            "Epoch: [112][140/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7678 (0.7676)\tD(fake) 0.2415 (0.2267)\tgrad(D) penalty 0.0140 (0.0166)\tRec loss 4155.0981 (3918.5011)\tnorm 0.8762 (0.8822)\n",
            "Epoch: [112][150/195]\tTime  0.162 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7677 (0.7676)\tD(fake) 0.2089 (0.2262)\tgrad(D) penalty 0.0194 (0.0166)\tRec loss 4036.5933 (3922.2752)\tnorm 0.8744 (0.8823)\n",
            "Epoch: [112][160/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7677 (0.7676)\tD(fake) 0.2564 (0.2267)\tgrad(D) penalty 0.0153 (0.0165)\tRec loss 3717.7275 (3918.0204)\tnorm 0.8816 (0.8818)\n",
            "Epoch: [112][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7678 (0.7676)\tD(fake) 0.2212 (0.2271)\tgrad(D) penalty 0.0161 (0.0165)\tRec loss 3910.7163 (3914.9510)\tnorm 0.8802 (0.8813)\n",
            "Epoch: [112][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7679 (0.7676)\tD(fake) 0.2443 (0.2269)\tgrad(D) penalty 0.0166 (0.0166)\tRec loss 4045.0422 (3914.4961)\tnorm 0.8810 (0.8808)\n",
            "Epoch: [112][190/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7678 (0.7677)\tD(fake) 0.2349 (0.2274)\tgrad(D) penalty 0.0166 (0.0166)\tRec loss 3884.9238 (3914.2405)\tnorm 0.8826 (0.8805)\n",
            "Epoch: [113][  0/195]\tTime  0.449 ( 0.449)\tData  0.247 ( 0.247)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4047.1074 (4047.1074)\tnorm 0.8722 (0.8722)\n",
            "Epoch: [113][ 10/195]\tTime  0.152 ( 0.179)\tData  0.000 ( 0.023)\tD(real) 0.7679 (0.7679)\tD(fake) 0.2513 (0.2386)\tgrad(D) penalty 0.0180 (0.0188)\tRec loss 3837.6768 (3818.9829)\tnorm 0.8685 (0.8772)\n",
            "Epoch: [113][ 20/195]\tTime  0.149 ( 0.165)\tData  0.000 ( 0.012)\tD(real) 0.7677 (0.7678)\tD(fake) 0.2093 (0.2320)\tgrad(D) penalty 0.0204 (0.0187)\tRec loss 4070.7678 (3889.2368)\tnorm 0.8728 (0.8768)\n",
            "Epoch: [113][ 30/195]\tTime  0.151 ( 0.160)\tData  0.000 ( 0.008)\tD(real) 0.7677 (0.7678)\tD(fake) 0.2364 (0.2314)\tgrad(D) penalty 0.0139 (0.0180)\tRec loss 3809.5259 (3892.6086)\tnorm 0.8692 (0.8786)\n",
            "Epoch: [113][ 40/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7678 (0.7678)\tD(fake) 0.2389 (0.2316)\tgrad(D) penalty 0.0174 (0.0170)\tRec loss 4066.9707 (3912.9522)\tnorm 0.8867 (0.8790)\n",
            "Epoch: [113][ 50/195]\tTime  0.165 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7678 (0.7678)\tD(fake) 0.2225 (0.2319)\tgrad(D) penalty 0.0171 (0.0167)\tRec loss 4028.0913 (3914.3351)\tnorm 0.8765 (0.8787)\n",
            "Epoch: [113][ 60/195]\tTime  0.152 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7679 (0.7678)\tD(fake) 0.2314 (0.2309)\tgrad(D) penalty 0.0195 (0.0166)\tRec loss 3901.6777 (3921.4510)\tnorm 0.8788 (0.8792)\n",
            "Epoch: [113][ 70/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7678 (0.7678)\tD(fake) 0.2212 (0.2307)\tgrad(D) penalty 0.0147 (0.0165)\tRec loss 4092.3977 (3928.3918)\tnorm 0.8905 (0.8798)\n",
            "Epoch: [113][ 80/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7678 (0.7678)\tD(fake) 0.2194 (0.2297)\tgrad(D) penalty 0.0162 (0.0163)\tRec loss 3902.2969 (3929.0705)\tnorm 0.8920 (0.8809)\n",
            "Epoch: [113][ 90/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7677 (0.7678)\tD(fake) 0.2312 (0.2296)\tgrad(D) penalty 0.0154 (0.0162)\tRec loss 4097.0205 (3928.0854)\tnorm 0.8780 (0.8807)\n",
            "Epoch: [113][100/195]\tTime  0.165 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7679 (0.7678)\tD(fake) 0.2194 (0.2285)\tgrad(D) penalty 0.0145 (0.0161)\tRec loss 4133.8745 (3936.7547)\tnorm 0.8740 (0.8812)\n",
            "Epoch: [113][110/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7678 (0.7678)\tD(fake) 0.2320 (0.2283)\tgrad(D) penalty 0.0171 (0.0161)\tRec loss 4094.8503 (3931.0216)\tnorm 0.8659 (0.8814)\n",
            "Epoch: [113][120/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7680 (0.7678)\tD(fake) 0.2318 (0.2287)\tgrad(D) penalty 0.0159 (0.0161)\tRec loss 3937.9290 (3930.7457)\tnorm 0.8689 (0.8814)\n",
            "Epoch: [113][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7679 (0.7678)\tD(fake) 0.2290 (0.2290)\tgrad(D) penalty 0.0151 (0.0160)\tRec loss 3997.0505 (3925.6108)\tnorm 0.8730 (0.8817)\n",
            "Epoch: [113][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7680 (0.7678)\tD(fake) 0.2332 (0.2288)\tgrad(D) penalty 0.0163 (0.0161)\tRec loss 3874.9202 (3920.4218)\tnorm 0.8795 (0.8820)\n",
            "Epoch: [113][150/195]\tTime  0.160 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7680 (0.7678)\tD(fake) 0.2161 (0.2283)\tgrad(D) penalty 0.0198 (0.0162)\tRec loss 3982.0173 (3920.0152)\tnorm 0.8594 (0.8819)\n",
            "Epoch: [113][160/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7677 (0.7678)\tD(fake) 0.2244 (0.2280)\tgrad(D) penalty 0.0173 (0.0163)\tRec loss 3891.9312 (3911.5957)\tnorm 0.8726 (0.8817)\n",
            "Epoch: [113][170/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7681 (0.7678)\tD(fake) 0.2197 (0.2274)\tgrad(D) penalty 0.0206 (0.0164)\tRec loss 3716.2358 (3907.9019)\tnorm 0.8770 (0.8813)\n",
            "Epoch: [113][180/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7678 (0.7679)\tD(fake) 0.2093 (0.2273)\tgrad(D) penalty 0.0169 (0.0165)\tRec loss 4028.5076 (3906.6819)\tnorm 0.8776 (0.8811)\n",
            "Epoch: [113][190/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7682 (0.7679)\tD(fake) 0.2377 (0.2272)\tgrad(D) penalty 0.0202 (0.0167)\tRec loss 3980.5154 (3910.8295)\tnorm 0.8856 (0.8812)\n",
            "Epoch: [114][  0/195]\tTime  0.419 ( 0.419)\tData  0.227 ( 0.227)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4033.0271 (4033.0271)\tnorm 0.8844 (0.8844)\n",
            "Epoch: [114][ 10/195]\tTime  0.149 ( 0.175)\tData  0.000 ( 0.021)\tD(real) 0.7682 (0.7682)\tD(fake) 0.2526 (0.2387)\tgrad(D) penalty 0.0156 (0.0200)\tRec loss 3924.6240 (3960.0658)\tnorm 0.8844 (0.8840)\n",
            "Epoch: [114][ 20/195]\tTime  0.148 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7682 (0.7682)\tD(fake) 0.2240 (0.2329)\tgrad(D) penalty 0.0178 (0.0176)\tRec loss 4116.5947 (3950.7127)\tnorm 0.8802 (0.8840)\n",
            "Epoch: [114][ 30/195]\tTime  0.153 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7683 (0.7682)\tD(fake) 0.2455 (0.2383)\tgrad(D) penalty 0.0175 (0.0174)\tRec loss 3978.4685 (3936.7988)\tnorm 0.8940 (0.8839)\n",
            "Epoch: [114][ 40/195]\tTime  0.159 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7683 (0.7682)\tD(fake) 0.2455 (0.2346)\tgrad(D) penalty 0.0149 (0.0174)\tRec loss 4046.1836 (3913.8705)\tnorm 0.8785 (0.8828)\n",
            "Epoch: [114][ 50/195]\tTime  0.169 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7683 (0.7683)\tD(fake) 0.2167 (0.2338)\tgrad(D) penalty 0.0157 (0.0171)\tRec loss 3901.3970 (3901.3611)\tnorm 0.8910 (0.8827)\n",
            "Epoch: [114][ 60/195]\tTime  0.151 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7683 (0.7683)\tD(fake) 0.2270 (0.2324)\tgrad(D) penalty 0.0168 (0.0170)\tRec loss 3979.2109 (3902.0033)\tnorm 0.8713 (0.8816)\n",
            "Epoch: [114][ 70/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7684 (0.7683)\tD(fake) 0.2270 (0.2315)\tgrad(D) penalty 0.0165 (0.0169)\tRec loss 3871.2937 (3900.7433)\tnorm 0.8804 (0.8812)\n",
            "Epoch: [114][ 80/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7683 (0.7683)\tD(fake) 0.2170 (0.2312)\tgrad(D) penalty 0.0147 (0.0167)\tRec loss 3816.6238 (3897.9913)\tnorm 0.8818 (0.8813)\n",
            "Epoch: [114][ 90/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7684 (0.7683)\tD(fake) 0.2204 (0.2301)\tgrad(D) penalty 0.0161 (0.0167)\tRec loss 3812.9485 (3896.3838)\tnorm 0.8949 (0.8807)\n",
            "Epoch: [114][100/195]\tTime  0.163 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7681 (0.7683)\tD(fake) 0.2207 (0.2288)\tgrad(D) penalty 0.0169 (0.0166)\tRec loss 3846.4546 (3900.2282)\tnorm 0.8836 (0.8802)\n",
            "Epoch: [114][110/195]\tTime  0.156 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7683 (0.7683)\tD(fake) 0.2374 (0.2293)\tgrad(D) penalty 0.0177 (0.0167)\tRec loss 3894.5737 (3901.8940)\tnorm 0.9045 (0.8802)\n",
            "Epoch: [114][120/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7684 (0.7683)\tD(fake) 0.2173 (0.2284)\tgrad(D) penalty 0.0168 (0.0165)\tRec loss 4128.0146 (3904.4331)\tnorm 0.8883 (0.8802)\n",
            "Epoch: [114][130/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7684 (0.7683)\tD(fake) 0.2432 (0.2290)\tgrad(D) penalty 0.0135 (0.0165)\tRec loss 3930.4272 (3906.9949)\tnorm 0.8852 (0.8801)\n",
            "Epoch: [114][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7685 (0.7683)\tD(fake) 0.2176 (0.2284)\tgrad(D) penalty 0.0177 (0.0166)\tRec loss 3890.1846 (3913.1682)\tnorm 0.8899 (0.8801)\n",
            "Epoch: [114][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7676 (0.7683)\tD(fake) 0.2422 (0.2290)\tgrad(D) penalty 0.0145 (0.0165)\tRec loss 4033.6304 (3909.4492)\tnorm 0.8791 (0.8800)\n",
            "Epoch: [114][160/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7683 (0.7683)\tD(fake) 0.2274 (0.2281)\tgrad(D) penalty 0.0178 (0.0165)\tRec loss 3862.2415 (3908.4096)\tnorm 0.8825 (0.8801)\n",
            "Epoch: [114][170/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7684 (0.7683)\tD(fake) 0.2207 (0.2281)\tgrad(D) penalty 0.0146 (0.0164)\tRec loss 3979.0439 (3911.4798)\tnorm 0.8879 (0.8802)\n",
            "Epoch: [114][180/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7685 (0.7683)\tD(fake) 0.2260 (0.2281)\tgrad(D) penalty 0.0175 (0.0164)\tRec loss 4012.9292 (3912.0569)\tnorm 0.8808 (0.8802)\n",
            "Epoch: [114][190/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7679 (0.7683)\tD(fake) 0.2120 (0.2274)\tgrad(D) penalty 0.0223 (0.0165)\tRec loss 4073.3347 (3907.6527)\tnorm 0.8752 (0.8806)\n",
            "Epoch: [115][  0/195]\tTime  0.421 ( 0.421)\tData  0.226 ( 0.226)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3943.6387 (3943.6387)\tnorm 0.8768 (0.8768)\n",
            "Epoch: [115][ 10/195]\tTime  0.148 ( 0.174)\tData  0.000 ( 0.021)\tD(real) 0.7686 (0.7685)\tD(fake) 0.2443 (0.2388)\tgrad(D) penalty 0.0198 (0.0202)\tRec loss 3993.0049 (3872.4474)\tnorm 0.8897 (0.8819)\n",
            "Epoch: [115][ 20/195]\tTime  0.146 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7686 (0.7685)\tD(fake) 0.2150 (0.2277)\tgrad(D) penalty 0.0222 (0.0209)\tRec loss 3729.6555 (3873.3223)\tnorm 0.8788 (0.8818)\n",
            "Epoch: [115][ 30/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7686 (0.7686)\tD(fake) 0.2258 (0.2270)\tgrad(D) penalty 0.0188 (0.0205)\tRec loss 3691.1108 (3867.1746)\tnorm 0.8870 (0.8799)\n",
            "Epoch: [115][ 40/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7685 (0.7686)\tD(fake) 0.2098 (0.2250)\tgrad(D) penalty 0.0187 (0.0198)\tRec loss 4174.9395 (3876.8694)\tnorm 0.8718 (0.8803)\n",
            "Epoch: [115][ 50/195]\tTime  0.163 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7687 (0.7686)\tD(fake) 0.2597 (0.2288)\tgrad(D) penalty 0.0144 (0.0191)\tRec loss 3848.8647 (3876.8133)\tnorm 0.8825 (0.8802)\n",
            "Epoch: [115][ 60/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7687 (0.7686)\tD(fake) 0.2306 (0.2290)\tgrad(D) penalty 0.0170 (0.0187)\tRec loss 3704.2678 (3884.6210)\tnorm 0.8704 (0.8793)\n",
            "Epoch: [115][ 70/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7688 (0.7686)\tD(fake) 0.2164 (0.2279)\tgrad(D) penalty 0.0199 (0.0185)\tRec loss 3725.5901 (3892.0446)\tnorm 0.8689 (0.8795)\n",
            "Epoch: [115][ 80/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7687 (0.7686)\tD(fake) 0.2291 (0.2288)\tgrad(D) penalty 0.0128 (0.0178)\tRec loss 3971.9023 (3898.1607)\tnorm 0.8744 (0.8788)\n",
            "Epoch: [115][ 90/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7687 (0.7686)\tD(fake) 0.2181 (0.2274)\tgrad(D) penalty 0.0170 (0.0176)\tRec loss 3988.5332 (3893.5998)\tnorm 0.8885 (0.8797)\n",
            "Epoch: [115][100/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7688 (0.7686)\tD(fake) 0.2082 (0.2283)\tgrad(D) penalty 0.0163 (0.0173)\tRec loss 3773.4497 (3897.4046)\tnorm 0.8752 (0.8798)\n",
            "Epoch: [115][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7688 (0.7686)\tD(fake) 0.2402 (0.2283)\tgrad(D) penalty 0.0161 (0.0174)\tRec loss 3844.2693 (3890.5666)\tnorm 0.8744 (0.8797)\n",
            "Epoch: [115][120/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7687 (0.7687)\tD(fake) 0.2282 (0.2286)\tgrad(D) penalty 0.0159 (0.0174)\tRec loss 3688.6807 (3891.1779)\tnorm 0.8862 (0.8802)\n",
            "Epoch: [115][130/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7688 (0.7687)\tD(fake) 0.2295 (0.2284)\tgrad(D) penalty 0.0186 (0.0174)\tRec loss 3537.2905 (3891.5713)\tnorm 0.8862 (0.8811)\n",
            "Epoch: [115][140/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7687 (0.7687)\tD(fake) 0.2409 (0.2289)\tgrad(D) penalty 0.0187 (0.0174)\tRec loss 3560.3975 (3891.7797)\tnorm 0.8805 (0.8808)\n",
            "Epoch: [115][150/195]\tTime  0.170 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7688 (0.7687)\tD(fake) 0.2287 (0.2291)\tgrad(D) penalty 0.0169 (0.0173)\tRec loss 4142.7852 (3900.0986)\tnorm 0.8828 (0.8811)\n",
            "Epoch: [115][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7689 (0.7687)\tD(fake) 0.2457 (0.2289)\tgrad(D) penalty 0.0168 (0.0174)\tRec loss 3898.0876 (3901.5485)\tnorm 0.8897 (0.8811)\n",
            "Epoch: [115][170/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7688 (0.7687)\tD(fake) 0.2285 (0.2294)\tgrad(D) penalty 0.0144 (0.0172)\tRec loss 3880.2849 (3908.4392)\tnorm 0.8821 (0.8811)\n",
            "Epoch: [115][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7687 (0.7687)\tD(fake) 0.2512 (0.2294)\tgrad(D) penalty 0.0149 (0.0172)\tRec loss 4025.1294 (3909.5508)\tnorm 0.8781 (0.8811)\n",
            "Epoch: [115][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7688 (0.7687)\tD(fake) 0.2130 (0.2292)\tgrad(D) penalty 0.0155 (0.0171)\tRec loss 3931.4622 (3911.3227)\tnorm 0.8947 (0.8810)\n",
            "Epoch: [116][  0/195]\tTime  0.448 ( 0.448)\tData  0.250 ( 0.250)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3794.0830 (3794.0830)\tnorm 0.8935 (0.8935)\n",
            "Epoch: [116][ 10/195]\tTime  0.151 ( 0.177)\tData  0.000 ( 0.023)\tD(real) 0.7690 (0.7688)\tD(fake) 0.2388 (0.2316)\tgrad(D) penalty 0.0151 (0.0160)\tRec loss 3795.1958 (3874.4009)\tnorm 0.8865 (0.8825)\n",
            "Epoch: [116][ 20/195]\tTime  0.146 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7686 (0.7688)\tD(fake) 0.2159 (0.2237)\tgrad(D) penalty 0.0175 (0.0162)\tRec loss 3676.0566 (3847.3912)\tnorm 0.8813 (0.8804)\n",
            "Epoch: [116][ 30/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7688 (0.7688)\tD(fake) 0.2359 (0.2260)\tgrad(D) penalty 0.0144 (0.0159)\tRec loss 3912.0466 (3899.5784)\tnorm 0.8819 (0.8808)\n",
            "Epoch: [116][ 40/195]\tTime  0.152 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7688 (0.7688)\tD(fake) 0.2338 (0.2277)\tgrad(D) penalty 0.0193 (0.0165)\tRec loss 3901.5488 (3918.3848)\tnorm 0.8903 (0.8810)\n",
            "Epoch: [116][ 50/195]\tTime  0.166 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7690 (0.7688)\tD(fake) 0.2413 (0.2314)\tgrad(D) penalty 0.0178 (0.0164)\tRec loss 4418.5845 (3944.1522)\tnorm 0.8870 (0.8820)\n",
            "Epoch: [116][ 60/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7690 (0.7689)\tD(fake) 0.2296 (0.2291)\tgrad(D) penalty 0.0158 (0.0165)\tRec loss 3959.5181 (3928.5408)\tnorm 0.8915 (0.8832)\n",
            "Epoch: [116][ 70/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7690 (0.7689)\tD(fake) 0.2278 (0.2293)\tgrad(D) penalty 0.0148 (0.0164)\tRec loss 3801.4561 (3920.9313)\tnorm 0.8756 (0.8828)\n",
            "Epoch: [116][ 80/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7689 (0.7689)\tD(fake) 0.2368 (0.2286)\tgrad(D) penalty 0.0212 (0.0169)\tRec loss 3853.9663 (3906.7019)\tnorm 0.8808 (0.8827)\n",
            "Epoch: [116][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7691 (0.7689)\tD(fake) 0.2069 (0.2280)\tgrad(D) penalty 0.0154 (0.0168)\tRec loss 4116.6240 (3907.5086)\tnorm 0.8804 (0.8822)\n",
            "Epoch: [116][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7691 (0.7689)\tD(fake) 0.2484 (0.2271)\tgrad(D) penalty 0.0183 (0.0173)\tRec loss 4156.5898 (3913.1968)\tnorm 0.8722 (0.8816)\n",
            "Epoch: [116][110/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7690 (0.7689)\tD(fake) 0.2033 (0.2267)\tgrad(D) penalty 0.0177 (0.0173)\tRec loss 3866.8223 (3915.9674)\tnorm 0.8862 (0.8820)\n",
            "Epoch: [116][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7692 (0.7689)\tD(fake) 0.2528 (0.2257)\tgrad(D) penalty 0.0175 (0.0176)\tRec loss 3785.4849 (3910.2714)\tnorm 0.8923 (0.8823)\n",
            "Epoch: [116][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7693 (0.7689)\tD(fake) 0.2124 (0.2261)\tgrad(D) penalty 0.0153 (0.0175)\tRec loss 3610.3066 (3908.5084)\tnorm 0.8769 (0.8819)\n",
            "Epoch: [116][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7692 (0.7690)\tD(fake) 0.2342 (0.2271)\tgrad(D) penalty 0.0168 (0.0175)\tRec loss 3917.9609 (3908.4775)\tnorm 0.8916 (0.8819)\n",
            "Epoch: [116][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7692 (0.7690)\tD(fake) 0.2366 (0.2270)\tgrad(D) penalty 0.0153 (0.0174)\tRec loss 3826.9565 (3904.2626)\tnorm 0.8745 (0.8816)\n",
            "Epoch: [116][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7691 (0.7690)\tD(fake) 0.2268 (0.2275)\tgrad(D) penalty 0.0135 (0.0171)\tRec loss 3769.1714 (3898.2203)\tnorm 0.8886 (0.8811)\n",
            "Epoch: [116][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7691 (0.7690)\tD(fake) 0.2295 (0.2275)\tgrad(D) penalty 0.0126 (0.0170)\tRec loss 3799.7607 (3903.6202)\tnorm 0.8719 (0.8809)\n",
            "Epoch: [116][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7693 (0.7690)\tD(fake) 0.2272 (0.2277)\tgrad(D) penalty 0.0149 (0.0168)\tRec loss 3706.9678 (3902.9075)\tnorm 0.8776 (0.8809)\n",
            "Epoch: [116][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7688 (0.7690)\tD(fake) 0.2410 (0.2280)\tgrad(D) penalty 0.0130 (0.0167)\tRec loss 3870.1733 (3906.0189)\tnorm 0.8783 (0.8807)\n",
            "Epoch: [117][  0/195]\tTime  0.431 ( 0.431)\tData  0.235 ( 0.235)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3694.9541 (3694.9541)\tnorm 0.8757 (0.8757)\n",
            "Epoch: [117][ 10/195]\tTime  0.146 ( 0.174)\tData  0.000 ( 0.022)\tD(real) 0.7691 (0.7692)\tD(fake) 0.2515 (0.2521)\tgrad(D) penalty 0.0147 (0.0153)\tRec loss 4125.7788 (3864.5401)\tnorm 0.8774 (0.8800)\n",
            "Epoch: [117][ 20/195]\tTime  0.147 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7692 (0.7692)\tD(fake) 0.2370 (0.2374)\tgrad(D) penalty 0.0144 (0.0149)\tRec loss 3971.7097 (3851.3303)\tnorm 0.8801 (0.8783)\n",
            "Epoch: [117][ 30/195]\tTime  0.148 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7688 (0.7691)\tD(fake) 0.2290 (0.2360)\tgrad(D) penalty 0.0169 (0.0151)\tRec loss 4136.9863 (3873.2429)\tnorm 0.8696 (0.8769)\n",
            "Epoch: [117][ 40/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7691 (0.7691)\tD(fake) 0.2384 (0.2345)\tgrad(D) penalty 0.0141 (0.0155)\tRec loss 3717.8594 (3863.0452)\tnorm 0.8753 (0.8758)\n",
            "Epoch: [117][ 50/195]\tTime  0.164 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7690 (0.7691)\tD(fake) 0.2217 (0.2332)\tgrad(D) penalty 0.0151 (0.0154)\tRec loss 4004.4966 (3869.1993)\tnorm 0.8670 (0.8762)\n",
            "Epoch: [117][ 60/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7691 (0.7691)\tD(fake) 0.2303 (0.2338)\tgrad(D) penalty 0.0136 (0.0154)\tRec loss 3932.8403 (3871.4616)\tnorm 0.8731 (0.8763)\n",
            "Epoch: [117][ 70/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7691 (0.7691)\tD(fake) 0.2304 (0.2330)\tgrad(D) penalty 0.0169 (0.0156)\tRec loss 4011.0696 (3864.1967)\tnorm 0.8795 (0.8753)\n",
            "Epoch: [117][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7692 (0.7691)\tD(fake) 0.2445 (0.2335)\tgrad(D) penalty 0.0152 (0.0156)\tRec loss 3706.8721 (3875.7516)\tnorm 0.8763 (0.8754)\n",
            "Epoch: [117][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7692 (0.7691)\tD(fake) 0.2220 (0.2334)\tgrad(D) penalty 0.0176 (0.0157)\tRec loss 3843.5249 (3877.1236)\tnorm 0.8882 (0.8758)\n",
            "Epoch: [117][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7693 (0.7691)\tD(fake) 0.2471 (0.2339)\tgrad(D) penalty 0.0163 (0.0160)\tRec loss 4091.0557 (3890.7427)\tnorm 0.8731 (0.8763)\n",
            "Epoch: [117][110/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7692 (0.7691)\tD(fake) 0.2207 (0.2335)\tgrad(D) penalty 0.0167 (0.0160)\tRec loss 3749.3848 (3890.9610)\tnorm 0.8759 (0.8768)\n",
            "Epoch: [117][120/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7693 (0.7691)\tD(fake) 0.2263 (0.2333)\tgrad(D) penalty 0.0161 (0.0160)\tRec loss 3949.4636 (3894.0839)\tnorm 0.8897 (0.8772)\n",
            "Epoch: [117][130/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7694 (0.7692)\tD(fake) 0.2324 (0.2329)\tgrad(D) penalty 0.0142 (0.0159)\tRec loss 3965.7107 (3896.8631)\tnorm 0.8751 (0.8774)\n",
            "Epoch: [117][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7693 (0.7692)\tD(fake) 0.2280 (0.2332)\tgrad(D) penalty 0.0158 (0.0159)\tRec loss 4107.7524 (3901.9258)\tnorm 0.8757 (0.8777)\n",
            "Epoch: [117][150/195]\tTime  0.162 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7693 (0.7692)\tD(fake) 0.2339 (0.2326)\tgrad(D) penalty 0.0139 (0.0158)\tRec loss 3766.9976 (3894.7436)\tnorm 0.8790 (0.8781)\n",
            "Epoch: [117][160/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7692 (0.7692)\tD(fake) 0.2256 (0.2319)\tgrad(D) penalty 0.0153 (0.0157)\tRec loss 3761.0088 (3899.1395)\tnorm 0.8739 (0.8776)\n",
            "Epoch: [117][170/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7692 (0.7692)\tD(fake) 0.2329 (0.2319)\tgrad(D) penalty 0.0149 (0.0157)\tRec loss 3915.4458 (3896.1355)\tnorm 0.8733 (0.8776)\n",
            "Epoch: [117][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7692 (0.7692)\tD(fake) 0.2220 (0.2316)\tgrad(D) penalty 0.0188 (0.0159)\tRec loss 3792.8174 (3896.5909)\tnorm 0.8852 (0.8778)\n",
            "Epoch: [117][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7694 (0.7692)\tD(fake) 0.2244 (0.2311)\tgrad(D) penalty 0.0162 (0.0159)\tRec loss 3979.0161 (3899.1016)\tnorm 0.8886 (0.8784)\n",
            "Epoch: [118][  0/195]\tTime  0.425 ( 0.425)\tData  0.236 ( 0.236)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3889.7319 (3889.7319)\tnorm 0.8896 (0.8896)\n",
            "Epoch: [118][ 10/195]\tTime  0.150 ( 0.175)\tData  0.000 ( 0.022)\tD(real) 0.7694 (0.7694)\tD(fake) 0.2411 (0.2327)\tgrad(D) penalty 0.0145 (0.0153)\tRec loss 4021.8877 (3960.9935)\tnorm 0.8779 (0.8869)\n",
            "Epoch: [118][ 20/195]\tTime  0.150 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7693 (0.7694)\tD(fake) 0.2117 (0.2290)\tgrad(D) penalty 0.0187 (0.0157)\tRec loss 3965.9976 (3915.5431)\tnorm 0.8834 (0.8876)\n",
            "Epoch: [118][ 30/195]\tTime  0.147 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7692 (0.7693)\tD(fake) 0.2400 (0.2336)\tgrad(D) penalty 0.0133 (0.0150)\tRec loss 3858.3193 (3909.6514)\tnorm 0.8927 (0.8860)\n",
            "Epoch: [118][ 40/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7693 (0.7693)\tD(fake) 0.2525 (0.2287)\tgrad(D) penalty 0.0145 (0.0160)\tRec loss 4220.5054 (3918.1473)\tnorm 0.8561 (0.8842)\n",
            "Epoch: [118][ 50/195]\tTime  0.166 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7693 (0.7693)\tD(fake) 0.2269 (0.2308)\tgrad(D) penalty 0.0160 (0.0159)\tRec loss 3733.1680 (3908.4319)\tnorm 0.8826 (0.8837)\n",
            "Epoch: [118][ 60/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7693 (0.7693)\tD(fake) 0.2474 (0.2297)\tgrad(D) penalty 0.0179 (0.0167)\tRec loss 3733.3425 (3907.9302)\tnorm 0.8668 (0.8823)\n",
            "Epoch: [118][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7693 (0.7693)\tD(fake) 0.2186 (0.2295)\tgrad(D) penalty 0.0139 (0.0164)\tRec loss 3877.5354 (3898.7027)\tnorm 0.8747 (0.8820)\n",
            "Epoch: [118][ 80/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7693 (0.7693)\tD(fake) 0.2412 (0.2296)\tgrad(D) penalty 0.0149 (0.0165)\tRec loss 4119.0859 (3898.5787)\tnorm 0.8692 (0.8812)\n",
            "Epoch: [118][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7692 (0.7693)\tD(fake) 0.2288 (0.2295)\tgrad(D) penalty 0.0203 (0.0166)\tRec loss 3945.2251 (3914.5439)\tnorm 0.8919 (0.8809)\n",
            "Epoch: [118][100/195]\tTime  0.163 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7692 (0.7693)\tD(fake) 0.2168 (0.2303)\tgrad(D) penalty 0.0142 (0.0162)\tRec loss 3839.2310 (3910.8291)\tnorm 0.8880 (0.8810)\n",
            "Epoch: [118][110/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7693 (0.7693)\tD(fake) 0.2445 (0.2303)\tgrad(D) penalty 0.0139 (0.0162)\tRec loss 4056.0488 (3914.1056)\tnorm 0.8786 (0.8809)\n",
            "Epoch: [118][120/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7693 (0.7693)\tD(fake) 0.2143 (0.2306)\tgrad(D) penalty 0.0186 (0.0162)\tRec loss 4001.7778 (3918.8176)\tnorm 0.8789 (0.8810)\n",
            "Epoch: [118][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7693 (0.7693)\tD(fake) 0.2466 (0.2308)\tgrad(D) penalty 0.0126 (0.0161)\tRec loss 3879.7178 (3911.0199)\tnorm 0.8715 (0.8811)\n",
            "Epoch: [118][140/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7692 (0.7693)\tD(fake) 0.2302 (0.2309)\tgrad(D) penalty 0.0137 (0.0160)\tRec loss 3937.4133 (3909.1479)\tnorm 0.8814 (0.8815)\n",
            "Epoch: [118][150/195]\tTime  0.168 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7691 (0.7693)\tD(fake) 0.2274 (0.2307)\tgrad(D) penalty 0.0161 (0.0161)\tRec loss 4064.2390 (3903.8307)\tnorm 0.8862 (0.8814)\n",
            "Epoch: [118][160/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7693 (0.7693)\tD(fake) 0.2226 (0.2305)\tgrad(D) penalty 0.0176 (0.0161)\tRec loss 3824.6650 (3902.0013)\tnorm 0.8857 (0.8812)\n",
            "Epoch: [118][170/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7693 (0.7693)\tD(fake) 0.2117 (0.2296)\tgrad(D) penalty 0.0160 (0.0161)\tRec loss 4064.9937 (3900.7219)\tnorm 0.8862 (0.8813)\n",
            "Epoch: [118][180/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7693 (0.7693)\tD(fake) 0.2239 (0.2300)\tgrad(D) penalty 0.0157 (0.0159)\tRec loss 4018.5339 (3900.4725)\tnorm 0.8906 (0.8816)\n",
            "Epoch: [118][190/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7691 (0.7693)\tD(fake) 0.2211 (0.2296)\tgrad(D) penalty 0.0157 (0.0160)\tRec loss 3681.1475 (3899.0032)\tnorm 0.8889 (0.8817)\n",
            "Epoch: [119][  0/195]\tTime  0.442 ( 0.442)\tData  0.242 ( 0.242)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3813.0205 (3813.0205)\tnorm 0.8894 (0.8894)\n",
            "Epoch: [119][ 10/195]\tTime  0.148 ( 0.179)\tData  0.000 ( 0.022)\tD(real) 0.7695 (0.7695)\tD(fake) 0.2470 (0.2413)\tgrad(D) penalty 0.0151 (0.0165)\tRec loss 4131.5732 (3851.8164)\tnorm 0.8846 (0.8877)\n",
            "Epoch: [119][ 20/195]\tTime  0.147 ( 0.165)\tData  0.000 ( 0.012)\tD(real) 0.7695 (0.7694)\tD(fake) 0.2228 (0.2281)\tgrad(D) penalty 0.0193 (0.0171)\tRec loss 3930.2188 (3873.2867)\tnorm 0.8935 (0.8852)\n",
            "Epoch: [119][ 30/195]\tTime  0.146 ( 0.160)\tData  0.000 ( 0.008)\tD(real) 0.7695 (0.7694)\tD(fake) 0.2223 (0.2284)\tgrad(D) penalty 0.0138 (0.0165)\tRec loss 4198.9438 (3885.1024)\tnorm 0.8762 (0.8828)\n",
            "Epoch: [119][ 40/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7694 (0.7694)\tD(fake) 0.2283 (0.2266)\tgrad(D) penalty 0.0162 (0.0168)\tRec loss 3853.1299 (3884.5846)\tnorm 0.8867 (0.8828)\n",
            "Epoch: [119][ 50/195]\tTime  0.167 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7696 (0.7694)\tD(fake) 0.2217 (0.2264)\tgrad(D) penalty 0.0159 (0.0166)\tRec loss 3920.2827 (3899.6029)\tnorm 0.8949 (0.8820)\n",
            "Epoch: [119][ 60/195]\tTime  0.152 ( 0.156)\tData  0.000 ( 0.004)\tD(real) 0.7696 (0.7695)\tD(fake) 0.2357 (0.2273)\tgrad(D) penalty 0.0138 (0.0165)\tRec loss 3864.6001 (3892.6458)\tnorm 0.8788 (0.8813)\n",
            "Epoch: [119][ 70/195]\tTime  0.153 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7696 (0.7695)\tD(fake) 0.2296 (0.2272)\tgrad(D) penalty 0.0171 (0.0164)\tRec loss 3883.3862 (3889.7850)\tnorm 0.8918 (0.8815)\n",
            "Epoch: [119][ 80/195]\tTime  0.152 ( 0.155)\tData  0.000 ( 0.003)\tD(real) 0.7695 (0.7695)\tD(fake) 0.2104 (0.2277)\tgrad(D) penalty 0.0151 (0.0161)\tRec loss 4086.0466 (3891.9331)\tnorm 0.8918 (0.8812)\n",
            "Epoch: [119][ 90/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7696 (0.7695)\tD(fake) 0.2464 (0.2287)\tgrad(D) penalty 0.0137 (0.0161)\tRec loss 3819.9158 (3890.2254)\tnorm 0.8785 (0.8811)\n",
            "Epoch: [119][100/195]\tTime  0.166 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7693 (0.7695)\tD(fake) 0.2570 (0.2291)\tgrad(D) penalty 0.0140 (0.0159)\tRec loss 3998.1531 (3886.4758)\tnorm 0.8741 (0.8809)\n",
            "Epoch: [119][110/195]\tTime  0.153 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7694 (0.7695)\tD(fake) 0.2183 (0.2293)\tgrad(D) penalty 0.0163 (0.0158)\tRec loss 3741.9861 (3887.3337)\tnorm 0.8760 (0.8804)\n",
            "Epoch: [119][120/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7693 (0.7695)\tD(fake) 0.2469 (0.2292)\tgrad(D) penalty 0.0143 (0.0158)\tRec loss 3792.3530 (3887.8704)\tnorm 0.8790 (0.8807)\n",
            "Epoch: [119][130/195]\tTime  0.159 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7694 (0.7695)\tD(fake) 0.2202 (0.2287)\tgrad(D) penalty 0.0162 (0.0159)\tRec loss 3934.0764 (3894.4989)\tnorm 0.8773 (0.8805)\n",
            "Epoch: [119][140/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7695 (0.7695)\tD(fake) 0.2258 (0.2290)\tgrad(D) penalty 0.0134 (0.0157)\tRec loss 3703.8940 (3888.5900)\tnorm 0.8845 (0.8805)\n",
            "Epoch: [119][150/195]\tTime  0.171 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7694 (0.7695)\tD(fake) 0.2283 (0.2289)\tgrad(D) penalty 0.0138 (0.0156)\tRec loss 3974.5376 (3890.4828)\tnorm 0.8778 (0.8803)\n",
            "Epoch: [119][160/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7694 (0.7695)\tD(fake) 0.2236 (0.2288)\tgrad(D) penalty 0.0176 (0.0155)\tRec loss 3892.5078 (3891.4661)\tnorm 0.8890 (0.8806)\n",
            "Epoch: [119][170/195]\tTime  0.153 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7695 (0.7695)\tD(fake) 0.2263 (0.2292)\tgrad(D) penalty 0.0143 (0.0154)\tRec loss 3711.3555 (3891.0099)\tnorm 0.8844 (0.8810)\n",
            "Epoch: [119][180/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7694 (0.7695)\tD(fake) 0.2405 (0.2291)\tgrad(D) penalty 0.0127 (0.0155)\tRec loss 3752.8333 (3892.4270)\tnorm 0.8802 (0.8809)\n",
            "Epoch: [119][190/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7694 (0.7695)\tD(fake) 0.2324 (0.2289)\tgrad(D) penalty 0.0174 (0.0155)\tRec loss 3981.4109 (3894.8597)\tnorm 0.8915 (0.8811)\n",
            "Epoch: [120][  0/195]\tTime  0.437 ( 0.437)\tData  0.240 ( 0.240)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4052.3022 (4052.3022)\tnorm 0.8781 (0.8781)\n",
            "Epoch: [120][ 10/195]\tTime  0.150 ( 0.179)\tData  0.000 ( 0.022)\tD(real) 0.7693 (0.7694)\tD(fake) 0.2244 (0.2209)\tgrad(D) penalty 0.0139 (0.0162)\tRec loss 3593.6528 (3885.6360)\tnorm 0.8783 (0.8842)\n",
            "Epoch: [120][ 20/195]\tTime  0.150 ( 0.164)\tData  0.000 ( 0.012)\tD(real) 0.7694 (0.7694)\tD(fake) 0.2546 (0.2350)\tgrad(D) penalty 0.0128 (0.0156)\tRec loss 3899.2324 (3908.3341)\tnorm 0.8922 (0.8834)\n",
            "Epoch: [120][ 30/195]\tTime  0.151 ( 0.160)\tData  0.000 ( 0.008)\tD(real) 0.7694 (0.7694)\tD(fake) 0.2096 (0.2291)\tgrad(D) penalty 0.0166 (0.0160)\tRec loss 3718.7883 (3899.0636)\tnorm 0.8772 (0.8822)\n",
            "Epoch: [120][ 40/195]\tTime  0.154 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7695 (0.7694)\tD(fake) 0.2367 (0.2292)\tgrad(D) penalty 0.0163 (0.0160)\tRec loss 3625.9419 (3890.6347)\tnorm 0.8841 (0.8827)\n",
            "Epoch: [120][ 50/195]\tTime  0.164 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7693 (0.7694)\tD(fake) 0.2125 (0.2274)\tgrad(D) penalty 0.0138 (0.0156)\tRec loss 3768.0952 (3898.4304)\tnorm 0.8869 (0.8820)\n",
            "Epoch: [120][ 60/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7693 (0.7694)\tD(fake) 0.2366 (0.2295)\tgrad(D) penalty 0.0131 (0.0155)\tRec loss 3810.7458 (3895.2843)\tnorm 0.8841 (0.8823)\n",
            "Epoch: [120][ 70/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7695 (0.7694)\tD(fake) 0.2316 (0.2289)\tgrad(D) penalty 0.0158 (0.0155)\tRec loss 3773.3193 (3894.7089)\tnorm 0.8848 (0.8825)\n",
            "Epoch: [120][ 80/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7693 (0.7694)\tD(fake) 0.2145 (0.2284)\tgrad(D) penalty 0.0169 (0.0155)\tRec loss 3793.4812 (3884.8392)\tnorm 0.8858 (0.8819)\n",
            "Epoch: [120][ 90/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7696 (0.7695)\tD(fake) 0.2284 (0.2286)\tgrad(D) penalty 0.0131 (0.0153)\tRec loss 3888.9575 (3885.7518)\tnorm 0.8830 (0.8812)\n",
            "Epoch: [120][100/195]\tTime  0.166 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7692 (0.7695)\tD(fake) 0.2391 (0.2278)\tgrad(D) penalty 0.0141 (0.0152)\tRec loss 4057.0271 (3893.4100)\tnorm 0.8787 (0.8815)\n",
            "Epoch: [120][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7694 (0.7695)\tD(fake) 0.2184 (0.2282)\tgrad(D) penalty 0.0131 (0.0151)\tRec loss 3885.5862 (3895.5228)\tnorm 0.8914 (0.8820)\n",
            "Epoch: [120][120/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7696 (0.7695)\tD(fake) 0.2252 (0.2278)\tgrad(D) penalty 0.0154 (0.0151)\tRec loss 3851.6895 (3891.6963)\tnorm 0.8861 (0.8822)\n",
            "Epoch: [120][130/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7695 (0.7695)\tD(fake) 0.2232 (0.2278)\tgrad(D) penalty 0.0144 (0.0151)\tRec loss 3795.7051 (3884.0024)\tnorm 0.8847 (0.8830)\n",
            "Epoch: [120][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7694 (0.7695)\tD(fake) 0.2410 (0.2281)\tgrad(D) penalty 0.0160 (0.0152)\tRec loss 4091.6592 (3880.3542)\tnorm 0.8904 (0.8833)\n",
            "Epoch: [120][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7697 (0.7695)\tD(fake) 0.2116 (0.2277)\tgrad(D) penalty 0.0142 (0.0151)\tRec loss 4131.4414 (3885.7309)\tnorm 0.8812 (0.8830)\n",
            "Epoch: [120][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7696 (0.7695)\tD(fake) 0.2573 (0.2284)\tgrad(D) penalty 0.0133 (0.0152)\tRec loss 4014.8779 (3886.3150)\tnorm 0.8669 (0.8828)\n",
            "Epoch: [120][170/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7696 (0.7695)\tD(fake) 0.2199 (0.2280)\tgrad(D) penalty 0.0159 (0.0152)\tRec loss 4384.6611 (3891.3241)\tnorm 0.8705 (0.8828)\n",
            "Epoch: [120][180/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7696 (0.7695)\tD(fake) 0.2267 (0.2277)\tgrad(D) penalty 0.0144 (0.0151)\tRec loss 4064.6179 (3890.2434)\tnorm 0.8792 (0.8830)\n",
            "Epoch: [120][190/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7697 (0.7695)\tD(fake) 0.2210 (0.2274)\tgrad(D) penalty 0.0169 (0.0152)\tRec loss 4255.0020 (3896.7583)\tnorm 0.8728 (0.8825)\n",
            "Epoch: [121][  0/195]\tTime  0.447 ( 0.447)\tData  0.242 ( 0.242)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4025.6475 (4025.6475)\tnorm 0.8821 (0.8821)\n",
            "Epoch: [121][ 10/195]\tTime  0.150 ( 0.178)\tData  0.000 ( 0.022)\tD(real) 0.7697 (0.7697)\tD(fake) 0.2269 (0.2209)\tgrad(D) penalty 0.0148 (0.0152)\tRec loss 3912.7285 (3883.2976)\tnorm 0.8764 (0.8827)\n",
            "Epoch: [121][ 20/195]\tTime  0.148 ( 0.165)\tData  0.000 ( 0.012)\tD(real) 0.7697 (0.7696)\tD(fake) 0.2316 (0.2257)\tgrad(D) penalty 0.0160 (0.0150)\tRec loss 3822.5063 (3892.8169)\tnorm 0.8927 (0.8859)\n",
            "Epoch: [121][ 30/195]\tTime  0.153 ( 0.160)\tData  0.000 ( 0.008)\tD(real) 0.7697 (0.7697)\tD(fake) 0.2193 (0.2258)\tgrad(D) penalty 0.0134 (0.0148)\tRec loss 4346.8647 (3907.3840)\tnorm 0.8893 (0.8875)\n",
            "Epoch: [121][ 40/195]\tTime  0.151 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7697 (0.7697)\tD(fake) 0.2301 (0.2290)\tgrad(D) penalty 0.0146 (0.0147)\tRec loss 4024.1472 (3908.1015)\tnorm 0.8720 (0.8864)\n",
            "Epoch: [121][ 50/195]\tTime  0.165 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7697 (0.7697)\tD(fake) 0.2329 (0.2272)\tgrad(D) penalty 0.0164 (0.0148)\tRec loss 4088.8564 (3894.5236)\tnorm 0.8705 (0.8857)\n",
            "Epoch: [121][ 60/195]\tTime  0.153 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7695 (0.7697)\tD(fake) 0.2182 (0.2284)\tgrad(D) penalty 0.0149 (0.0149)\tRec loss 3979.5698 (3898.7221)\tnorm 0.8909 (0.8860)\n",
            "Epoch: [121][ 70/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7697 (0.7697)\tD(fake) 0.2419 (0.2287)\tgrad(D) penalty 0.0150 (0.0152)\tRec loss 3570.0317 (3903.4508)\tnorm 0.8670 (0.8850)\n",
            "Epoch: [121][ 80/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7697 (0.7697)\tD(fake) 0.2250 (0.2276)\tgrad(D) penalty 0.0159 (0.0152)\tRec loss 4135.7207 (3899.6590)\tnorm 0.8752 (0.8846)\n",
            "Epoch: [121][ 90/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7697 (0.7697)\tD(fake) 0.2241 (0.2281)\tgrad(D) penalty 0.0149 (0.0152)\tRec loss 4117.3843 (3905.2790)\tnorm 0.8731 (0.8844)\n",
            "Epoch: [121][100/195]\tTime  0.167 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7697 (0.7697)\tD(fake) 0.2333 (0.2284)\tgrad(D) penalty 0.0121 (0.0151)\tRec loss 4171.6431 (3908.3231)\tnorm 0.8749 (0.8840)\n",
            "Epoch: [121][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7697 (0.7697)\tD(fake) 0.2300 (0.2281)\tgrad(D) penalty 0.0147 (0.0151)\tRec loss 3843.6826 (3911.9686)\tnorm 0.8696 (0.8838)\n",
            "Epoch: [121][120/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7697 (0.7697)\tD(fake) 0.2296 (0.2282)\tgrad(D) penalty 0.0144 (0.0149)\tRec loss 3900.2700 (3900.8374)\tnorm 0.8830 (0.8838)\n",
            "Epoch: [121][130/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7698 (0.7697)\tD(fake) 0.2234 (0.2281)\tgrad(D) penalty 0.0156 (0.0150)\tRec loss 3815.3447 (3895.8166)\tnorm 0.8854 (0.8830)\n",
            "Epoch: [121][140/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7699 (0.7697)\tD(fake) 0.2136 (0.2279)\tgrad(D) penalty 0.0173 (0.0151)\tRec loss 3924.4292 (3899.1739)\tnorm 0.8845 (0.8828)\n",
            "Epoch: [121][150/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7698 (0.7697)\tD(fake) 0.2430 (0.2283)\tgrad(D) penalty 0.0142 (0.0151)\tRec loss 3619.1501 (3896.2644)\tnorm 0.8852 (0.8830)\n",
            "Epoch: [121][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7698 (0.7697)\tD(fake) 0.2240 (0.2283)\tgrad(D) penalty 0.0146 (0.0150)\tRec loss 3603.7239 (3896.6623)\tnorm 0.8882 (0.8830)\n",
            "Epoch: [121][170/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7698 (0.7697)\tD(fake) 0.2388 (0.2286)\tgrad(D) penalty 0.0167 (0.0151)\tRec loss 3855.8062 (3893.9984)\tnorm 0.8886 (0.8828)\n",
            "Epoch: [121][180/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7699 (0.7697)\tD(fake) 0.2299 (0.2279)\tgrad(D) penalty 0.0161 (0.0152)\tRec loss 3850.1074 (3892.9080)\tnorm 0.8712 (0.8825)\n",
            "Epoch: [121][190/195]\tTime  0.157 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7698 (0.7697)\tD(fake) 0.2497 (0.2288)\tgrad(D) penalty 0.0142 (0.0152)\tRec loss 3760.7041 (3890.8198)\tnorm 0.8804 (0.8822)\n",
            "Epoch: [122][  0/195]\tTime  0.429 ( 0.429)\tData  0.245 ( 0.245)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3862.4585 (3862.4585)\tnorm 0.8622 (0.8622)\n",
            "Epoch: [122][ 10/195]\tTime  0.148 ( 0.175)\tData  0.000 ( 0.023)\tD(real) 0.7698 (0.7698)\tD(fake) 0.2431 (0.2381)\tgrad(D) penalty 0.0125 (0.0140)\tRec loss 3608.7642 (3844.2548)\tnorm 0.8907 (0.8802)\n",
            "Epoch: [122][ 20/195]\tTime  0.150 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7699 (0.7698)\tD(fake) 0.2238 (0.2302)\tgrad(D) penalty 0.0124 (0.0148)\tRec loss 4000.6753 (3855.6799)\tnorm 0.8780 (0.8765)\n",
            "Epoch: [122][ 30/195]\tTime  0.149 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7698 (0.7698)\tD(fake) 0.2466 (0.2328)\tgrad(D) penalty 0.0125 (0.0145)\tRec loss 4093.8101 (3864.2000)\tnorm 0.8712 (0.8748)\n",
            "Epoch: [122][ 40/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7698 (0.7698)\tD(fake) 0.2412 (0.2321)\tgrad(D) penalty 0.0148 (0.0146)\tRec loss 3735.7549 (3859.5481)\tnorm 0.8803 (0.8757)\n",
            "Epoch: [122][ 50/195]\tTime  0.166 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7698 (0.7698)\tD(fake) 0.2103 (0.2305)\tgrad(D) penalty 0.0183 (0.0148)\tRec loss 4066.4268 (3887.5717)\tnorm 0.8699 (0.8764)\n",
            "Epoch: [122][ 60/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7697 (0.7698)\tD(fake) 0.2201 (0.2303)\tgrad(D) penalty 0.0135 (0.0149)\tRec loss 4036.5176 (3879.4588)\tnorm 0.8769 (0.8775)\n",
            "Epoch: [122][ 70/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7698 (0.7698)\tD(fake) 0.2189 (0.2287)\tgrad(D) penalty 0.0161 (0.0149)\tRec loss 4069.4683 (3885.7054)\tnorm 0.8831 (0.8780)\n",
            "Epoch: [122][ 80/195]\tTime  0.156 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7699 (0.7698)\tD(fake) 0.2201 (0.2282)\tgrad(D) penalty 0.0149 (0.0149)\tRec loss 4012.5444 (3887.5983)\tnorm 0.8788 (0.8782)\n",
            "Epoch: [122][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7698 (0.7698)\tD(fake) 0.2311 (0.2282)\tgrad(D) penalty 0.0158 (0.0150)\tRec loss 3704.1919 (3885.2597)\tnorm 0.8889 (0.8778)\n",
            "Epoch: [122][100/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7696 (0.7698)\tD(fake) 0.2337 (0.2299)\tgrad(D) penalty 0.0144 (0.0150)\tRec loss 3949.9309 (3889.8049)\tnorm 0.8884 (0.8777)\n",
            "Epoch: [122][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7698 (0.7698)\tD(fake) 0.2273 (0.2294)\tgrad(D) penalty 0.0155 (0.0150)\tRec loss 4083.9153 (3888.5700)\tnorm 0.8824 (0.8778)\n",
            "Epoch: [122][120/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7698 (0.7698)\tD(fake) 0.2260 (0.2290)\tgrad(D) penalty 0.0146 (0.0148)\tRec loss 3879.6890 (3881.0230)\tnorm 0.8747 (0.8779)\n",
            "Epoch: [122][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7697 (0.7698)\tD(fake) 0.2339 (0.2290)\tgrad(D) penalty 0.0174 (0.0150)\tRec loss 3948.5168 (3881.6817)\tnorm 0.8782 (0.8780)\n",
            "Epoch: [122][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7699 (0.7698)\tD(fake) 0.2129 (0.2285)\tgrad(D) penalty 0.0178 (0.0150)\tRec loss 3602.0610 (3886.7656)\tnorm 0.8775 (0.8778)\n",
            "Epoch: [122][150/195]\tTime  0.160 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7699 (0.7698)\tD(fake) 0.2359 (0.2292)\tgrad(D) penalty 0.0142 (0.0149)\tRec loss 3757.2161 (3886.0343)\tnorm 0.8768 (0.8777)\n",
            "Epoch: [122][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7697 (0.7698)\tD(fake) 0.2392 (0.2287)\tgrad(D) penalty 0.0134 (0.0149)\tRec loss 3747.6069 (3883.2062)\tnorm 0.8683 (0.8778)\n",
            "Epoch: [122][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7698 (0.7698)\tD(fake) 0.2106 (0.2284)\tgrad(D) penalty 0.0150 (0.0148)\tRec loss 4080.8071 (3887.2353)\tnorm 0.8658 (0.8777)\n",
            "Epoch: [122][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7700 (0.7698)\tD(fake) 0.2440 (0.2290)\tgrad(D) penalty 0.0135 (0.0148)\tRec loss 3448.6279 (3884.2910)\tnorm 0.8713 (0.8776)\n",
            "Epoch: [122][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7699 (0.7698)\tD(fake) 0.2294 (0.2287)\tgrad(D) penalty 0.0188 (0.0149)\tRec loss 3760.3748 (3884.3890)\tnorm 0.8686 (0.8772)\n",
            "Epoch: [123][  0/195]\tTime  0.442 ( 0.442)\tData  0.253 ( 0.253)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3589.5464 (3589.5464)\tnorm 0.8887 (0.8887)\n",
            "Epoch: [123][ 10/195]\tTime  0.149 ( 0.176)\tData  0.000 ( 0.023)\tD(real) 0.7699 (0.7700)\tD(fake) 0.2321 (0.2250)\tgrad(D) penalty 0.0152 (0.0155)\tRec loss 3886.3206 (3890.7601)\tnorm 0.8707 (0.8785)\n",
            "Epoch: [123][ 20/195]\tTime  0.148 ( 0.164)\tData  0.000 ( 0.012)\tD(real) 0.7698 (0.7699)\tD(fake) 0.2242 (0.2294)\tgrad(D) penalty 0.0174 (0.0154)\tRec loss 3743.2969 (3865.3320)\tnorm 0.8753 (0.8811)\n",
            "Epoch: [123][ 30/195]\tTime  0.148 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7699 (0.7699)\tD(fake) 0.2201 (0.2294)\tgrad(D) penalty 0.0150 (0.0150)\tRec loss 3890.5732 (3883.9455)\tnorm 0.8850 (0.8811)\n",
            "Epoch: [123][ 40/195]\tTime  0.149 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7700 (0.7699)\tD(fake) 0.2406 (0.2310)\tgrad(D) penalty 0.0126 (0.0146)\tRec loss 3779.4661 (3867.0378)\tnorm 0.8882 (0.8795)\n",
            "Epoch: [123][ 50/195]\tTime  0.166 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7698 (0.7699)\tD(fake) 0.2300 (0.2295)\tgrad(D) penalty 0.0160 (0.0151)\tRec loss 3999.9026 (3871.8542)\tnorm 0.8837 (0.8787)\n",
            "Epoch: [123][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7700 (0.7699)\tD(fake) 0.2134 (0.2283)\tgrad(D) penalty 0.0168 (0.0149)\tRec loss 3795.9214 (3867.4701)\tnorm 0.8813 (0.8788)\n",
            "Epoch: [123][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7700 (0.7699)\tD(fake) 0.2353 (0.2289)\tgrad(D) penalty 0.0154 (0.0149)\tRec loss 4019.4443 (3876.6267)\tnorm 0.8725 (0.8785)\n",
            "Epoch: [123][ 80/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7700 (0.7699)\tD(fake) 0.2274 (0.2281)\tgrad(D) penalty 0.0143 (0.0148)\tRec loss 3749.0459 (3874.7076)\tnorm 0.8789 (0.8784)\n",
            "Epoch: [123][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7700 (0.7700)\tD(fake) 0.2287 (0.2281)\tgrad(D) penalty 0.0141 (0.0148)\tRec loss 3770.1484 (3873.0505)\tnorm 0.8744 (0.8781)\n",
            "Epoch: [123][100/195]\tTime  0.168 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2358 (0.2275)\tgrad(D) penalty 0.0150 (0.0150)\tRec loss 3814.5229 (3870.1804)\tnorm 0.8583 (0.8775)\n",
            "Epoch: [123][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7700 (0.7700)\tD(fake) 0.2237 (0.2271)\tgrad(D) penalty 0.0179 (0.0151)\tRec loss 3622.0837 (3874.3056)\tnorm 0.8628 (0.8769)\n",
            "Epoch: [123][120/195]\tTime  0.153 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2353 (0.2276)\tgrad(D) penalty 0.0157 (0.0150)\tRec loss 3660.1270 (3868.5010)\tnorm 0.8817 (0.8771)\n",
            "Epoch: [123][130/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7697 (0.7700)\tD(fake) 0.2358 (0.2284)\tgrad(D) penalty 0.0167 (0.0151)\tRec loss 3888.4529 (3864.2254)\tnorm 0.8882 (0.8772)\n",
            "Epoch: [123][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7702 (0.7700)\tD(fake) 0.2218 (0.2286)\tgrad(D) penalty 0.0153 (0.0151)\tRec loss 4125.3042 (3873.2724)\tnorm 0.8769 (0.8771)\n",
            "Epoch: [123][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2378 (0.2289)\tgrad(D) penalty 0.0136 (0.0151)\tRec loss 3894.0977 (3875.5530)\tnorm 0.8675 (0.8773)\n",
            "Epoch: [123][160/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2385 (0.2290)\tgrad(D) penalty 0.0142 (0.0149)\tRec loss 4041.4302 (3878.2414)\tnorm 0.8723 (0.8775)\n",
            "Epoch: [123][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7699 (0.7700)\tD(fake) 0.2364 (0.2294)\tgrad(D) penalty 0.0163 (0.0150)\tRec loss 3959.9863 (3882.7028)\tnorm 0.8683 (0.8776)\n",
            "Epoch: [123][180/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2260 (0.2292)\tgrad(D) penalty 0.0144 (0.0149)\tRec loss 3782.2749 (3883.3983)\tnorm 0.8782 (0.8776)\n",
            "Epoch: [123][190/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7700 (0.7700)\tD(fake) 0.2222 (0.2290)\tgrad(D) penalty 0.0138 (0.0149)\tRec loss 3818.5017 (3883.1518)\tnorm 0.8630 (0.8776)\n",
            "Epoch: [124][  0/195]\tTime  0.438 ( 0.438)\tData  0.239 ( 0.239)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3988.9805 (3988.9805)\tnorm 0.8764 (0.8764)\n",
            "Epoch: [124][ 10/195]\tTime  0.150 ( 0.177)\tData  0.000 ( 0.022)\tD(real) 0.7699 (0.7699)\tD(fake) 0.2358 (0.2289)\tgrad(D) penalty 0.0145 (0.0140)\tRec loss 4120.2822 (3886.2393)\tnorm 0.8762 (0.8741)\n",
            "Epoch: [124][ 20/195]\tTime  0.147 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2357 (0.2346)\tgrad(D) penalty 0.0144 (0.0142)\tRec loss 3991.5935 (3847.0289)\tnorm 0.8719 (0.8741)\n",
            "Epoch: [124][ 30/195]\tTime  0.150 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7700 (0.7699)\tD(fake) 0.1939 (0.2270)\tgrad(D) penalty 0.0182 (0.0148)\tRec loss 3764.6592 (3817.9074)\tnorm 0.8703 (0.8742)\n",
            "Epoch: [124][ 40/195]\tTime  0.151 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7700 (0.7700)\tD(fake) 0.2529 (0.2316)\tgrad(D) penalty 0.0137 (0.0148)\tRec loss 3556.8491 (3814.7322)\tnorm 0.8822 (0.8748)\n",
            "Epoch: [124][ 50/195]\tTime  0.166 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7700 (0.7700)\tD(fake) 0.2220 (0.2303)\tgrad(D) penalty 0.0168 (0.0148)\tRec loss 3616.5933 (3814.0404)\tnorm 0.8838 (0.8740)\n",
            "Epoch: [124][ 60/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7700 (0.7700)\tD(fake) 0.2462 (0.2314)\tgrad(D) penalty 0.0157 (0.0148)\tRec loss 4013.6062 (3823.9709)\tnorm 0.8639 (0.8746)\n",
            "Epoch: [124][ 70/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7700 (0.7700)\tD(fake) 0.2043 (0.2298)\tgrad(D) penalty 0.0204 (0.0149)\tRec loss 4023.2544 (3834.1894)\tnorm 0.8672 (0.8743)\n",
            "Epoch: [124][ 80/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7699 (0.7699)\tD(fake) 0.2096 (0.2301)\tgrad(D) penalty 0.0155 (0.0149)\tRec loss 3739.5483 (3835.2029)\tnorm 0.8726 (0.8740)\n",
            "Epoch: [124][ 90/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7699 (0.7699)\tD(fake) 0.2516 (0.2304)\tgrad(D) penalty 0.0150 (0.0149)\tRec loss 3784.1460 (3834.4816)\tnorm 0.8906 (0.8747)\n",
            "Epoch: [124][100/195]\tTime  0.167 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7700 (0.7700)\tD(fake) 0.2134 (0.2296)\tgrad(D) penalty 0.0183 (0.0149)\tRec loss 3762.0693 (3839.8292)\tnorm 0.8803 (0.8753)\n",
            "Epoch: [124][110/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2293 (0.2295)\tgrad(D) penalty 0.0131 (0.0147)\tRec loss 4163.2300 (3847.5323)\tnorm 0.8706 (0.8754)\n",
            "Epoch: [124][120/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7697 (0.7700)\tD(fake) 0.2520 (0.2299)\tgrad(D) penalty 0.0143 (0.0148)\tRec loss 3939.6531 (3851.4850)\tnorm 0.8773 (0.8757)\n",
            "Epoch: [124][130/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2126 (0.2293)\tgrad(D) penalty 0.0160 (0.0148)\tRec loss 3865.2764 (3863.2580)\tnorm 0.8779 (0.8755)\n",
            "Epoch: [124][140/195]\tTime  0.153 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2342 (0.2299)\tgrad(D) penalty 0.0133 (0.0148)\tRec loss 4029.0935 (3870.2978)\tnorm 0.8755 (0.8757)\n",
            "Epoch: [124][150/195]\tTime  0.162 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2086 (0.2287)\tgrad(D) penalty 0.0148 (0.0148)\tRec loss 3690.8940 (3875.7332)\tnorm 0.9022 (0.8765)\n",
            "Epoch: [124][160/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2145 (0.2294)\tgrad(D) penalty 0.0149 (0.0147)\tRec loss 4024.6306 (3883.3289)\tnorm 0.8822 (0.8770)\n",
            "Epoch: [124][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7700 (0.7700)\tD(fake) 0.2248 (0.2292)\tgrad(D) penalty 0.0116 (0.0147)\tRec loss 3926.2190 (3883.7282)\tnorm 0.8723 (0.8771)\n",
            "Epoch: [124][180/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2360 (0.2289)\tgrad(D) penalty 0.0154 (0.0147)\tRec loss 3620.0413 (3880.0980)\tnorm 0.8788 (0.8772)\n",
            "Epoch: [124][190/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7699 (0.7700)\tD(fake) 0.2130 (0.2287)\tgrad(D) penalty 0.0136 (0.0146)\tRec loss 3980.9478 (3881.0541)\tnorm 0.8746 (0.8773)\n",
            "Epoch: [125][  0/195]\tTime  0.401 ( 0.401)\tData  0.228 ( 0.228)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3642.9282 (3642.9282)\tnorm 0.8822 (0.8822)\n",
            "Epoch: [125][ 10/195]\tTime  0.147 ( 0.170)\tData  0.000 ( 0.021)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2237 (0.2292)\tgrad(D) penalty 0.0165 (0.0148)\tRec loss 4080.3877 (3776.9062)\tnorm 0.8732 (0.8776)\n",
            "Epoch: [125][ 20/195]\tTime  0.145 ( 0.159)\tData  0.000 ( 0.011)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2325 (0.2277)\tgrad(D) penalty 0.0147 (0.0151)\tRec loss 3846.1475 (3818.2161)\tnorm 0.8874 (0.8794)\n",
            "Epoch: [125][ 30/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.008)\tD(real) 0.7701 (0.7700)\tD(fake) 0.2276 (0.2275)\tgrad(D) penalty 0.0144 (0.0151)\tRec loss 3916.9038 (3818.7675)\tnorm 0.8873 (0.8781)\n",
            "Epoch: [125][ 40/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.006)\tD(real) 0.7701 (0.7701)\tD(fake) 0.2159 (0.2289)\tgrad(D) penalty 0.0142 (0.0150)\tRec loss 3979.1030 (3833.6765)\tnorm 0.8850 (0.8792)\n",
            "Epoch: [125][ 50/195]\tTime  0.162 ( 0.153)\tData  0.000 ( 0.005)\tD(real) 0.7702 (0.7701)\tD(fake) 0.2382 (0.2270)\tgrad(D) penalty 0.0139 (0.0151)\tRec loss 3601.0901 (3838.7806)\tnorm 0.8905 (0.8808)\n",
            "Epoch: [125][ 60/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7700 (0.7701)\tD(fake) 0.2094 (0.2274)\tgrad(D) penalty 0.0149 (0.0148)\tRec loss 3671.7114 (3824.8326)\tnorm 0.8862 (0.8812)\n",
            "Epoch: [125][ 70/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7699 (0.7701)\tD(fake) 0.2380 (0.2292)\tgrad(D) penalty 0.0119 (0.0146)\tRec loss 3756.9639 (3843.1795)\tnorm 0.8721 (0.8798)\n",
            "Epoch: [125][ 80/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7703 (0.7701)\tD(fake) 0.2484 (0.2299)\tgrad(D) penalty 0.0139 (0.0147)\tRec loss 3944.4670 (3854.0171)\tnorm 0.8832 (0.8802)\n",
            "Epoch: [125][ 90/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7701 (0.7701)\tD(fake) 0.2086 (0.2283)\tgrad(D) penalty 0.0146 (0.0145)\tRec loss 3932.9338 (3864.3432)\tnorm 0.8805 (0.8802)\n",
            "Epoch: [125][100/195]\tTime  0.168 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7701 (0.7701)\tD(fake) 0.2240 (0.2300)\tgrad(D) penalty 0.0113 (0.0144)\tRec loss 4077.2773 (3860.7448)\tnorm 0.8755 (0.8800)\n",
            "Epoch: [125][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7702 (0.7701)\tD(fake) 0.2414 (0.2295)\tgrad(D) penalty 0.0181 (0.0148)\tRec loss 3823.2598 (3864.9308)\tnorm 0.8836 (0.8796)\n",
            "Epoch: [125][120/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7700 (0.7701)\tD(fake) 0.2194 (0.2301)\tgrad(D) penalty 0.0155 (0.0146)\tRec loss 3893.3076 (3863.2633)\tnorm 0.8771 (0.8796)\n",
            "Epoch: [125][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7701 (0.7701)\tD(fake) 0.2355 (0.2308)\tgrad(D) penalty 0.0116 (0.0145)\tRec loss 4117.0918 (3864.4790)\tnorm 0.8672 (0.8796)\n",
            "Epoch: [125][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7702 (0.7701)\tD(fake) 0.2409 (0.2311)\tgrad(D) penalty 0.0108 (0.0144)\tRec loss 4215.2549 (3865.5302)\tnorm 0.8813 (0.8796)\n",
            "Epoch: [125][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7702 (0.7701)\tD(fake) 0.2286 (0.2307)\tgrad(D) penalty 0.0138 (0.0144)\tRec loss 3942.6833 (3867.5176)\tnorm 0.8798 (0.8796)\n",
            "Epoch: [125][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7700 (0.7701)\tD(fake) 0.2186 (0.2305)\tgrad(D) penalty 0.0142 (0.0142)\tRec loss 4077.3091 (3867.6009)\tnorm 0.8721 (0.8798)\n",
            "Epoch: [125][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7703 (0.7701)\tD(fake) 0.2346 (0.2307)\tgrad(D) penalty 0.0122 (0.0142)\tRec loss 3888.9966 (3871.9509)\tnorm 0.8880 (0.8795)\n",
            "Epoch: [125][180/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7699 (0.7701)\tD(fake) 0.2405 (0.2307)\tgrad(D) penalty 0.0139 (0.0142)\tRec loss 4209.6191 (3875.2221)\tnorm 0.8833 (0.8797)\n",
            "Epoch: [125][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7701 (0.7701)\tD(fake) 0.2352 (0.2304)\tgrad(D) penalty 0.0137 (0.0142)\tRec loss 3867.9028 (3877.0052)\tnorm 0.8810 (0.8795)\n",
            "Epoch: [126][  0/195]\tTime  0.454 ( 0.454)\tData  0.253 ( 0.253)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3876.4727 (3876.4727)\tnorm 0.8902 (0.8902)\n",
            "Epoch: [126][ 10/195]\tTime  0.149 ( 0.178)\tData  0.000 ( 0.023)\tD(real) 0.7703 (0.7702)\tD(fake) 0.2617 (0.2484)\tgrad(D) penalty 0.0132 (0.0145)\tRec loss 3915.3887 (3861.9039)\tnorm 0.8799 (0.8844)\n",
            "Epoch: [126][ 20/195]\tTime  0.146 ( 0.164)\tData  0.000 ( 0.012)\tD(real) 0.7702 (0.7702)\tD(fake) 0.2240 (0.2319)\tgrad(D) penalty 0.0159 (0.0151)\tRec loss 3763.9602 (3857.1936)\tnorm 0.8781 (0.8856)\n",
            "Epoch: [126][ 30/195]\tTime  0.146 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7703 (0.7702)\tD(fake) 0.2338 (0.2351)\tgrad(D) penalty 0.0168 (0.0150)\tRec loss 3732.2378 (3852.9192)\tnorm 0.8879 (0.8857)\n",
            "Epoch: [126][ 40/195]\tTime  0.145 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7703 (0.7702)\tD(fake) 0.2402 (0.2353)\tgrad(D) penalty 0.0145 (0.0152)\tRec loss 3974.2598 (3857.7515)\tnorm 0.8922 (0.8857)\n",
            "Epoch: [126][ 50/195]\tTime  0.161 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7701 (0.7702)\tD(fake) 0.2071 (0.2325)\tgrad(D) penalty 0.0135 (0.0148)\tRec loss 3746.2766 (3852.7776)\tnorm 0.8755 (0.8850)\n",
            "Epoch: [126][ 60/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7701 (0.7702)\tD(fake) 0.2198 (0.2316)\tgrad(D) penalty 0.0151 (0.0146)\tRec loss 3781.7778 (3848.9773)\tnorm 0.8793 (0.8836)\n",
            "Epoch: [126][ 70/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7700 (0.7702)\tD(fake) 0.2249 (0.2313)\tgrad(D) penalty 0.0123 (0.0143)\tRec loss 3815.4521 (3847.6806)\tnorm 0.8876 (0.8834)\n",
            "Epoch: [126][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7703 (0.7702)\tD(fake) 0.2260 (0.2298)\tgrad(D) penalty 0.0122 (0.0142)\tRec loss 3591.1409 (3852.7301)\tnorm 0.8756 (0.8829)\n",
            "Epoch: [126][ 90/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7703 (0.7702)\tD(fake) 0.2278 (0.2296)\tgrad(D) penalty 0.0132 (0.0142)\tRec loss 4013.2957 (3862.3621)\tnorm 0.8881 (0.8825)\n",
            "Epoch: [126][100/195]\tTime  0.170 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7704 (0.7702)\tD(fake) 0.2307 (0.2293)\tgrad(D) penalty 0.0135 (0.0141)\tRec loss 4228.0659 (3867.8429)\tnorm 0.8780 (0.8825)\n",
            "Epoch: [126][110/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7702 (0.7702)\tD(fake) 0.2228 (0.2295)\tgrad(D) penalty 0.0123 (0.0140)\tRec loss 3907.9287 (3871.5749)\tnorm 0.8739 (0.8822)\n",
            "Epoch: [126][120/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7702 (0.7702)\tD(fake) 0.2284 (0.2297)\tgrad(D) penalty 0.0117 (0.0140)\tRec loss 3791.2576 (3865.5817)\tnorm 0.8673 (0.8820)\n",
            "Epoch: [126][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7703 (0.7702)\tD(fake) 0.2127 (0.2287)\tgrad(D) penalty 0.0148 (0.0139)\tRec loss 3870.6025 (3863.9761)\tnorm 0.8907 (0.8819)\n",
            "Epoch: [126][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7704 (0.7702)\tD(fake) 0.2273 (0.2287)\tgrad(D) penalty 0.0144 (0.0139)\tRec loss 3859.7739 (3862.2617)\tnorm 0.8760 (0.8817)\n",
            "Epoch: [126][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7703 (0.7702)\tD(fake) 0.2199 (0.2283)\tgrad(D) penalty 0.0134 (0.0139)\tRec loss 3633.6401 (3860.5642)\tnorm 0.8774 (0.8816)\n",
            "Epoch: [126][160/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7703 (0.7702)\tD(fake) 0.2155 (0.2280)\tgrad(D) penalty 0.0157 (0.0139)\tRec loss 4256.8496 (3866.5037)\tnorm 0.8876 (0.8817)\n",
            "Epoch: [126][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7702 (0.7702)\tD(fake) 0.2295 (0.2280)\tgrad(D) penalty 0.0161 (0.0140)\tRec loss 4212.5176 (3872.2348)\tnorm 0.8743 (0.8816)\n",
            "Epoch: [126][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7705 (0.7702)\tD(fake) 0.2321 (0.2279)\tgrad(D) penalty 0.0116 (0.0140)\tRec loss 3738.0337 (3874.8323)\tnorm 0.8790 (0.8814)\n",
            "Epoch: [126][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7705 (0.7702)\tD(fake) 0.2294 (0.2278)\tgrad(D) penalty 0.0140 (0.0140)\tRec loss 3845.8806 (3877.3416)\tnorm 0.8780 (0.8809)\n",
            "Epoch: [127][  0/195]\tTime  0.451 ( 0.451)\tData  0.247 ( 0.247)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3675.2349 (3675.2349)\tnorm 0.8878 (0.8878)\n",
            "Epoch: [127][ 10/195]\tTime  0.158 ( 0.180)\tData  0.000 ( 0.023)\tD(real) 0.7706 (0.7706)\tD(fake) 0.2408 (0.2338)\tgrad(D) penalty 0.0120 (0.0130)\tRec loss 3623.9585 (3917.1434)\tnorm 0.8821 (0.8838)\n",
            "Epoch: [127][ 20/195]\tTime  0.149 ( 0.165)\tData  0.000 ( 0.012)\tD(real) 0.7704 (0.7704)\tD(fake) 0.2229 (0.2329)\tgrad(D) penalty 0.0130 (0.0131)\tRec loss 3699.6582 (3884.7711)\tnorm 0.8845 (0.8831)\n",
            "Epoch: [127][ 30/195]\tTime  0.151 ( 0.160)\tData  0.000 ( 0.008)\tD(real) 0.7704 (0.7704)\tD(fake) 0.2208 (0.2301)\tgrad(D) penalty 0.0133 (0.0132)\tRec loss 3874.6348 (3868.0416)\tnorm 0.8768 (0.8795)\n",
            "Epoch: [127][ 40/195]\tTime  0.150 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7705 (0.7704)\tD(fake) 0.2205 (0.2309)\tgrad(D) penalty 0.0141 (0.0134)\tRec loss 4226.2905 (3888.7058)\tnorm 0.8713 (0.8775)\n",
            "Epoch: [127][ 50/195]\tTime  0.165 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7705 (0.7704)\tD(fake) 0.2331 (0.2304)\tgrad(D) penalty 0.0150 (0.0136)\tRec loss 3688.2527 (3870.3843)\tnorm 0.8747 (0.8788)\n",
            "Epoch: [127][ 60/195]\tTime  0.151 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7705 (0.7704)\tD(fake) 0.2351 (0.2299)\tgrad(D) penalty 0.0130 (0.0135)\tRec loss 4143.0171 (3871.2518)\tnorm 0.8761 (0.8776)\n",
            "Epoch: [127][ 70/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7704 (0.7704)\tD(fake) 0.2208 (0.2285)\tgrad(D) penalty 0.0190 (0.0137)\tRec loss 3900.7878 (3863.9354)\tnorm 0.8691 (0.8776)\n",
            "Epoch: [127][ 80/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7705 (0.7704)\tD(fake) 0.2195 (0.2287)\tgrad(D) penalty 0.0152 (0.0137)\tRec loss 4327.8877 (3864.2273)\tnorm 0.8753 (0.8766)\n",
            "Epoch: [127][ 90/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7705 (0.7704)\tD(fake) 0.2364 (0.2293)\tgrad(D) penalty 0.0142 (0.0137)\tRec loss 3775.9595 (3864.5234)\tnorm 0.8723 (0.8768)\n",
            "Epoch: [127][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7705 (0.7704)\tD(fake) 0.2238 (0.2291)\tgrad(D) penalty 0.0137 (0.0138)\tRec loss 3822.6694 (3872.4637)\tnorm 0.8890 (0.8775)\n",
            "Epoch: [127][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7701 (0.7704)\tD(fake) 0.2206 (0.2283)\tgrad(D) penalty 0.0175 (0.0140)\tRec loss 3857.3267 (3873.1256)\tnorm 0.8858 (0.8779)\n",
            "Epoch: [127][120/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7705 (0.7704)\tD(fake) 0.2255 (0.2277)\tgrad(D) penalty 0.0131 (0.0139)\tRec loss 3706.6802 (3867.7962)\tnorm 0.8788 (0.8780)\n",
            "Epoch: [127][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7704 (0.7704)\tD(fake) 0.2347 (0.2278)\tgrad(D) penalty 0.0149 (0.0139)\tRec loss 3749.5298 (3863.4213)\tnorm 0.8767 (0.8782)\n",
            "Epoch: [127][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7705 (0.7704)\tD(fake) 0.2236 (0.2278)\tgrad(D) penalty 0.0131 (0.0138)\tRec loss 4025.5881 (3863.0027)\tnorm 0.8826 (0.8783)\n",
            "Epoch: [127][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7706 (0.7704)\tD(fake) 0.2264 (0.2281)\tgrad(D) penalty 0.0120 (0.0138)\tRec loss 4027.1001 (3867.8483)\tnorm 0.8890 (0.8786)\n",
            "Epoch: [127][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7706 (0.7704)\tD(fake) 0.2311 (0.2278)\tgrad(D) penalty 0.0150 (0.0138)\tRec loss 4033.6604 (3868.9116)\tnorm 0.8680 (0.8787)\n",
            "Epoch: [127][170/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7705 (0.7704)\tD(fake) 0.2249 (0.2280)\tgrad(D) penalty 0.0143 (0.0138)\tRec loss 4054.6104 (3869.2721)\tnorm 0.8779 (0.8788)\n",
            "Epoch: [127][180/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7704 (0.7705)\tD(fake) 0.2377 (0.2281)\tgrad(D) penalty 0.0148 (0.0139)\tRec loss 4054.6995 (3870.9253)\tnorm 0.8989 (0.8792)\n",
            "Epoch: [127][190/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7705 (0.7705)\tD(fake) 0.2350 (0.2283)\tgrad(D) penalty 0.0125 (0.0138)\tRec loss 3832.2622 (3869.8995)\tnorm 0.8772 (0.8793)\n",
            "Epoch: [128][  0/195]\tTime  0.422 ( 0.422)\tData  0.224 ( 0.224)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3625.8228 (3625.8228)\tnorm 0.8845 (0.8845)\n",
            "Epoch: [128][ 10/195]\tTime  0.147 ( 0.173)\tData  0.000 ( 0.021)\tD(real) 0.7706 (0.7705)\tD(fake) 0.2311 (0.2345)\tgrad(D) penalty 0.0123 (0.0135)\tRec loss 3650.0850 (3786.9576)\tnorm 0.8727 (0.8784)\n",
            "Epoch: [128][ 20/195]\tTime  0.148 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7706 (0.7706)\tD(fake) 0.2523 (0.2308)\tgrad(D) penalty 0.0131 (0.0151)\tRec loss 3795.8271 (3830.2354)\tnorm 0.8627 (0.8771)\n",
            "Epoch: [128][ 30/195]\tTime  0.149 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.7705 (0.7706)\tD(fake) 0.2241 (0.2318)\tgrad(D) penalty 0.0120 (0.0145)\tRec loss 3987.1738 (3861.7534)\tnorm 0.8790 (0.8768)\n",
            "Epoch: [128][ 40/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7706 (0.7706)\tD(fake) 0.2325 (0.2305)\tgrad(D) penalty 0.0158 (0.0145)\tRec loss 4189.2471 (3874.9887)\tnorm 0.8777 (0.8773)\n",
            "Epoch: [128][ 50/195]\tTime  0.163 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7705 (0.7706)\tD(fake) 0.2362 (0.2309)\tgrad(D) penalty 0.0125 (0.0143)\tRec loss 4085.3044 (3868.4736)\tnorm 0.8798 (0.8790)\n",
            "Epoch: [128][ 60/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7706 (0.7706)\tD(fake) 0.2335 (0.2290)\tgrad(D) penalty 0.0173 (0.0145)\tRec loss 3796.7700 (3860.6185)\tnorm 0.8627 (0.8782)\n",
            "Epoch: [128][ 70/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7707 (0.7706)\tD(fake) 0.2162 (0.2297)\tgrad(D) penalty 0.0110 (0.0141)\tRec loss 3652.7280 (3868.5459)\tnorm 0.8802 (0.8781)\n",
            "Epoch: [128][ 80/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7704 (0.7706)\tD(fake) 0.2326 (0.2301)\tgrad(D) penalty 0.0128 (0.0139)\tRec loss 3864.7615 (3856.3012)\tnorm 0.8678 (0.8783)\n",
            "Epoch: [128][ 90/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7707 (0.7706)\tD(fake) 0.2173 (0.2293)\tgrad(D) penalty 0.0155 (0.0140)\tRec loss 3708.6870 (3849.9080)\tnorm 0.8859 (0.8786)\n",
            "Epoch: [128][100/195]\tTime  0.162 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7705 (0.7706)\tD(fake) 0.2381 (0.2299)\tgrad(D) penalty 0.0140 (0.0142)\tRec loss 3964.8057 (3855.4329)\tnorm 0.8899 (0.8784)\n",
            "Epoch: [128][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7707 (0.7706)\tD(fake) 0.2055 (0.2278)\tgrad(D) penalty 0.0201 (0.0143)\tRec loss 3891.9666 (3864.9489)\tnorm 0.8795 (0.8788)\n",
            "Epoch: [128][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7704 (0.7706)\tD(fake) 0.2156 (0.2289)\tgrad(D) penalty 0.0167 (0.0143)\tRec loss 3808.9431 (3861.2931)\tnorm 0.8768 (0.8788)\n",
            "Epoch: [128][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7707 (0.7706)\tD(fake) 0.2537 (0.2295)\tgrad(D) penalty 0.0129 (0.0143)\tRec loss 3744.1719 (3859.0552)\tnorm 0.8604 (0.8786)\n",
            "Epoch: [128][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7707 (0.7706)\tD(fake) 0.2074 (0.2294)\tgrad(D) penalty 0.0136 (0.0143)\tRec loss 3877.0779 (3858.8464)\tnorm 0.8862 (0.8783)\n",
            "Epoch: [128][150/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7707 (0.7706)\tD(fake) 0.2323 (0.2294)\tgrad(D) penalty 0.0128 (0.0142)\tRec loss 3944.1594 (3863.3242)\tnorm 0.8922 (0.8785)\n",
            "Epoch: [128][160/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7706 (0.7706)\tD(fake) 0.2297 (0.2292)\tgrad(D) penalty 0.0110 (0.0142)\tRec loss 3829.0835 (3868.2040)\tnorm 0.8786 (0.8784)\n",
            "Epoch: [128][170/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7704 (0.7706)\tD(fake) 0.2322 (0.2288)\tgrad(D) penalty 0.0172 (0.0142)\tRec loss 4170.2500 (3871.2537)\tnorm 0.8783 (0.8784)\n",
            "Epoch: [128][180/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7707 (0.7706)\tD(fake) 0.2139 (0.2288)\tgrad(D) penalty 0.0150 (0.0142)\tRec loss 3614.3711 (3870.9046)\tnorm 0.8805 (0.8785)\n",
            "Epoch: [128][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7707 (0.7706)\tD(fake) 0.2507 (0.2293)\tgrad(D) penalty 0.0103 (0.0141)\tRec loss 3767.5623 (3870.0198)\tnorm 0.8710 (0.8784)\n",
            "Epoch: [129][  0/195]\tTime  0.433 ( 0.433)\tData  0.240 ( 0.240)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3963.4775 (3963.4775)\tnorm 0.8659 (0.8659)\n",
            "Epoch: [129][ 10/195]\tTime  0.151 ( 0.173)\tData  0.000 ( 0.022)\tD(real) 0.7707 (0.7707)\tD(fake) 0.2288 (0.2239)\tgrad(D) penalty 0.0129 (0.0137)\tRec loss 3784.6792 (3772.0792)\tnorm 0.8850 (0.8758)\n",
            "Epoch: [129][ 20/195]\tTime  0.148 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7708 (0.7707)\tD(fake) 0.2174 (0.2247)\tgrad(D) penalty 0.0144 (0.0141)\tRec loss 4105.4971 (3852.3497)\tnorm 0.8757 (0.8779)\n",
            "Epoch: [129][ 30/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7708 (0.7707)\tD(fake) 0.2375 (0.2268)\tgrad(D) penalty 0.0153 (0.0144)\tRec loss 3932.4170 (3877.1044)\tnorm 0.8810 (0.8794)\n",
            "Epoch: [129][ 40/195]\tTime  0.151 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7706 (0.7707)\tD(fake) 0.2250 (0.2259)\tgrad(D) penalty 0.0157 (0.0148)\tRec loss 4074.0981 (3877.5447)\tnorm 0.8821 (0.8790)\n",
            "Epoch: [129][ 50/195]\tTime  0.165 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7708 (0.7707)\tD(fake) 0.2364 (0.2273)\tgrad(D) penalty 0.0134 (0.0147)\tRec loss 3858.9563 (3858.9156)\tnorm 0.8926 (0.8794)\n",
            "Epoch: [129][ 60/195]\tTime  0.153 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7708 (0.7707)\tD(fake) 0.2238 (0.2275)\tgrad(D) penalty 0.0120 (0.0145)\tRec loss 3710.8647 (3854.1671)\tnorm 0.8963 (0.8796)\n",
            "Epoch: [129][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7707 (0.7707)\tD(fake) 0.2347 (0.2280)\tgrad(D) penalty 0.0157 (0.0145)\tRec loss 3837.1982 (3862.4542)\tnorm 0.8710 (0.8794)\n",
            "Epoch: [129][ 80/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7705 (0.7707)\tD(fake) 0.2304 (0.2262)\tgrad(D) penalty 0.0128 (0.0144)\tRec loss 4239.9888 (3868.0334)\tnorm 0.8765 (0.8785)\n",
            "Epoch: [129][ 90/195]\tTime  0.144 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7709 (0.7707)\tD(fake) 0.2168 (0.2259)\tgrad(D) penalty 0.0163 (0.0143)\tRec loss 3830.4983 (3864.3620)\tnorm 0.8833 (0.8779)\n",
            "Epoch: [129][100/195]\tTime  0.170 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7708 (0.7707)\tD(fake) 0.2089 (0.2260)\tgrad(D) penalty 0.0142 (0.0143)\tRec loss 3658.3674 (3859.0069)\tnorm 0.8706 (0.8773)\n",
            "Epoch: [129][110/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7708 (0.7707)\tD(fake) 0.2609 (0.2273)\tgrad(D) penalty 0.0133 (0.0144)\tRec loss 4135.4067 (3859.6914)\tnorm 0.8716 (0.8766)\n",
            "Epoch: [129][120/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7708 (0.7707)\tD(fake) 0.2149 (0.2262)\tgrad(D) penalty 0.0187 (0.0146)\tRec loss 3471.6084 (3861.2703)\tnorm 0.8900 (0.8763)\n",
            "Epoch: [129][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7707 (0.7707)\tD(fake) 0.2547 (0.2277)\tgrad(D) penalty 0.0170 (0.0147)\tRec loss 3469.4827 (3858.6721)\tnorm 0.8772 (0.8765)\n",
            "Epoch: [129][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7707 (0.7707)\tD(fake) 0.2233 (0.2267)\tgrad(D) penalty 0.0163 (0.0147)\tRec loss 3898.3984 (3858.2941)\tnorm 0.8624 (0.8763)\n",
            "Epoch: [129][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7708 (0.7707)\tD(fake) 0.2225 (0.2272)\tgrad(D) penalty 0.0105 (0.0146)\tRec loss 3717.6072 (3860.2198)\tnorm 0.8679 (0.8764)\n",
            "Epoch: [129][160/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7708 (0.7707)\tD(fake) 0.2474 (0.2275)\tgrad(D) penalty 0.0095 (0.0145)\tRec loss 3902.9595 (3860.1642)\tnorm 0.8767 (0.8762)\n",
            "Epoch: [129][170/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7709 (0.7707)\tD(fake) 0.2114 (0.2269)\tgrad(D) penalty 0.0169 (0.0145)\tRec loss 4070.5403 (3864.7730)\tnorm 0.8800 (0.8765)\n",
            "Epoch: [129][180/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7709 (0.7707)\tD(fake) 0.2169 (0.2272)\tgrad(D) penalty 0.0171 (0.0145)\tRec loss 3769.6362 (3865.2478)\tnorm 0.8800 (0.8770)\n",
            "Epoch: [129][190/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7708 (0.7707)\tD(fake) 0.2207 (0.2271)\tgrad(D) penalty 0.0148 (0.0145)\tRec loss 4024.0249 (3865.6758)\tnorm 0.8861 (0.8771)\n",
            "Epoch: [130][  0/195]\tTime  0.415 ( 0.415)\tData  0.223 ( 0.223)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3952.2454 (3952.2454)\tnorm 0.8836 (0.8836)\n",
            "Epoch: [130][ 10/195]\tTime  0.146 ( 0.173)\tData  0.000 ( 0.021)\tD(real) 0.7708 (0.7708)\tD(fake) 0.2233 (0.2243)\tgrad(D) penalty 0.0141 (0.0142)\tRec loss 4063.4146 (3889.6039)\tnorm 0.8753 (0.8785)\n",
            "Epoch: [130][ 20/195]\tTime  0.151 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7709 (0.7709)\tD(fake) 0.2168 (0.2281)\tgrad(D) penalty 0.0155 (0.0140)\tRec loss 3944.4468 (3853.8731)\tnorm 0.8748 (0.8762)\n",
            "Epoch: [130][ 30/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7709 (0.7709)\tD(fake) 0.2308 (0.2275)\tgrad(D) penalty 0.0122 (0.0141)\tRec loss 4265.0234 (3874.3974)\tnorm 0.8772 (0.8768)\n",
            "Epoch: [130][ 40/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7705 (0.7708)\tD(fake) 0.2265 (0.2290)\tgrad(D) penalty 0.0146 (0.0142)\tRec loss 3993.9961 (3867.2113)\tnorm 0.8872 (0.8770)\n",
            "Epoch: [130][ 50/195]\tTime  0.163 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7709 (0.7708)\tD(fake) 0.2244 (0.2287)\tgrad(D) penalty 0.0139 (0.0140)\tRec loss 3610.7229 (3866.9627)\tnorm 0.8814 (0.8770)\n",
            "Epoch: [130][ 60/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7710 (0.7709)\tD(fake) 0.2505 (0.2298)\tgrad(D) penalty 0.0140 (0.0145)\tRec loss 3879.4131 (3863.1362)\tnorm 0.8823 (0.8774)\n",
            "Epoch: [130][ 70/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7710 (0.7709)\tD(fake) 0.2195 (0.2288)\tgrad(D) penalty 0.0174 (0.0148)\tRec loss 3682.5571 (3873.7762)\tnorm 0.8762 (0.8771)\n",
            "Epoch: [130][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7710 (0.7709)\tD(fake) 0.2331 (0.2278)\tgrad(D) penalty 0.0172 (0.0151)\tRec loss 3623.3943 (3875.5477)\tnorm 0.8858 (0.8775)\n",
            "Epoch: [130][ 90/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7710 (0.7709)\tD(fake) 0.2203 (0.2273)\tgrad(D) penalty 0.0133 (0.0150)\tRec loss 3521.0112 (3870.9675)\tnorm 0.8805 (0.8774)\n",
            "Epoch: [130][100/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7708 (0.7709)\tD(fake) 0.2293 (0.2274)\tgrad(D) penalty 0.0143 (0.0148)\tRec loss 3667.1050 (3863.8927)\tnorm 0.8858 (0.8772)\n",
            "Epoch: [130][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7711 (0.7709)\tD(fake) 0.2271 (0.2274)\tgrad(D) penalty 0.0135 (0.0147)\tRec loss 3801.2944 (3867.2886)\tnorm 0.8745 (0.8766)\n",
            "Epoch: [130][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7710 (0.7709)\tD(fake) 0.2373 (0.2276)\tgrad(D) penalty 0.0160 (0.0147)\tRec loss 3826.2283 (3866.3321)\tnorm 0.8739 (0.8767)\n",
            "Epoch: [130][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7710 (0.7709)\tD(fake) 0.2206 (0.2275)\tgrad(D) penalty 0.0137 (0.0146)\tRec loss 3850.6367 (3868.3374)\tnorm 0.8900 (0.8770)\n",
            "Epoch: [130][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7710 (0.7709)\tD(fake) 0.2203 (0.2275)\tgrad(D) penalty 0.0137 (0.0145)\tRec loss 3985.4624 (3864.3794)\tnorm 0.8785 (0.8775)\n",
            "Epoch: [130][150/195]\tTime  0.170 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7710 (0.7709)\tD(fake) 0.2466 (0.2280)\tgrad(D) penalty 0.0175 (0.0147)\tRec loss 3875.4302 (3863.8017)\tnorm 0.8765 (0.8772)\n",
            "Epoch: [130][160/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7709 (0.7709)\tD(fake) 0.2018 (0.2272)\tgrad(D) penalty 0.0192 (0.0147)\tRec loss 3871.2256 (3863.7175)\tnorm 0.8659 (0.8768)\n",
            "Epoch: [130][170/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7711 (0.7710)\tD(fake) 0.2351 (0.2274)\tgrad(D) penalty 0.0152 (0.0148)\tRec loss 3967.7417 (3863.3418)\tnorm 0.8896 (0.8772)\n",
            "Epoch: [130][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7711 (0.7710)\tD(fake) 0.2386 (0.2270)\tgrad(D) penalty 0.0142 (0.0148)\tRec loss 4079.9741 (3864.0005)\tnorm 0.8785 (0.8776)\n",
            "Epoch: [130][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7711 (0.7710)\tD(fake) 0.2212 (0.2271)\tgrad(D) penalty 0.0165 (0.0148)\tRec loss 3888.5967 (3863.4987)\tnorm 0.8765 (0.8777)\n",
            "Epoch: [131][  0/195]\tTime  0.445 ( 0.445)\tData  0.253 ( 0.253)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3671.0459 (3671.0459)\tnorm 0.8781 (0.8781)\n",
            "Epoch: [131][ 10/195]\tTime  0.145 ( 0.176)\tData  0.000 ( 0.023)\tD(real) 0.7709 (0.7710)\tD(fake) 0.2370 (0.2337)\tgrad(D) penalty 0.0123 (0.0123)\tRec loss 3840.4668 (3810.3594)\tnorm 0.8727 (0.8780)\n",
            "Epoch: [131][ 20/195]\tTime  0.146 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7710 (0.7711)\tD(fake) 0.2170 (0.2292)\tgrad(D) penalty 0.0142 (0.0131)\tRec loss 3559.5808 (3859.7473)\tnorm 0.8887 (0.8785)\n",
            "Epoch: [131][ 30/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2249 (0.2278)\tgrad(D) penalty 0.0129 (0.0131)\tRec loss 3786.8184 (3873.6871)\tnorm 0.8940 (0.8801)\n",
            "Epoch: [131][ 40/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2128 (0.2261)\tgrad(D) penalty 0.0152 (0.0136)\tRec loss 3907.8027 (3868.9099)\tnorm 0.8676 (0.8796)\n",
            "Epoch: [131][ 50/195]\tTime  0.164 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2281 (0.2265)\tgrad(D) penalty 0.0140 (0.0137)\tRec loss 3844.1843 (3861.5941)\tnorm 0.8682 (0.8789)\n",
            "Epoch: [131][ 60/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2280 (0.2276)\tgrad(D) penalty 0.0163 (0.0140)\tRec loss 3725.9158 (3876.1631)\tnorm 0.8709 (0.8796)\n",
            "Epoch: [131][ 70/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2195 (0.2265)\tgrad(D) penalty 0.0164 (0.0143)\tRec loss 3712.4453 (3877.8291)\tnorm 0.8862 (0.8796)\n",
            "Epoch: [131][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7712 (0.7711)\tD(fake) 0.2089 (0.2258)\tgrad(D) penalty 0.0144 (0.0146)\tRec loss 4082.9578 (3876.8231)\tnorm 0.8854 (0.8800)\n",
            "Epoch: [131][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7712 (0.7711)\tD(fake) 0.2486 (0.2268)\tgrad(D) penalty 0.0156 (0.0147)\tRec loss 3966.2046 (3869.3109)\tnorm 0.8928 (0.8806)\n",
            "Epoch: [131][100/195]\tTime  0.169 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7712 (0.7711)\tD(fake) 0.2217 (0.2270)\tgrad(D) penalty 0.0152 (0.0148)\tRec loss 3890.0366 (3864.5858)\tnorm 0.8813 (0.8804)\n",
            "Epoch: [131][110/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2261 (0.2273)\tgrad(D) penalty 0.0137 (0.0147)\tRec loss 3905.4524 (3864.0429)\tnorm 0.8793 (0.8799)\n",
            "Epoch: [131][120/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7710 (0.7711)\tD(fake) 0.2426 (0.2280)\tgrad(D) penalty 0.0130 (0.0146)\tRec loss 4029.1011 (3861.0621)\tnorm 0.8838 (0.8797)\n",
            "Epoch: [131][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2058 (0.2272)\tgrad(D) penalty 0.0159 (0.0146)\tRec loss 3914.3628 (3864.7440)\tnorm 0.8754 (0.8793)\n",
            "Epoch: [131][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7710 (0.7711)\tD(fake) 0.2316 (0.2289)\tgrad(D) penalty 0.0129 (0.0145)\tRec loss 3867.7168 (3864.8388)\tnorm 0.8893 (0.8798)\n",
            "Epoch: [131][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2297 (0.2281)\tgrad(D) penalty 0.0187 (0.0147)\tRec loss 3878.3596 (3865.6748)\tnorm 0.8675 (0.8795)\n",
            "Epoch: [131][160/195]\tTime  0.144 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7712 (0.7711)\tD(fake) 0.2028 (0.2289)\tgrad(D) penalty 0.0153 (0.0147)\tRec loss 4090.2673 (3868.7676)\tnorm 0.8729 (0.8791)\n",
            "Epoch: [131][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2446 (0.2289)\tgrad(D) penalty 0.0133 (0.0147)\tRec loss 3705.3232 (3866.0646)\tnorm 0.8708 (0.8787)\n",
            "Epoch: [131][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2180 (0.2289)\tgrad(D) penalty 0.0156 (0.0148)\tRec loss 3928.7693 (3863.1470)\tnorm 0.8726 (0.8788)\n",
            "Epoch: [131][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2432 (0.2291)\tgrad(D) penalty 0.0134 (0.0148)\tRec loss 3830.9822 (3862.7168)\tnorm 0.8741 (0.8788)\n",
            "Epoch: [132][  0/195]\tTime  0.413 ( 0.413)\tData  0.235 ( 0.235)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3836.9531 (3836.9531)\tnorm 0.8708 (0.8708)\n",
            "Epoch: [132][ 10/195]\tTime  0.151 ( 0.173)\tData  0.000 ( 0.022)\tD(real) 0.7709 (0.7710)\tD(fake) 0.2374 (0.2335)\tgrad(D) penalty 0.0111 (0.0135)\tRec loss 3951.9226 (3926.1897)\tnorm 0.8814 (0.8747)\n",
            "Epoch: [132][ 20/195]\tTime  0.149 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7712 (0.7711)\tD(fake) 0.2246 (0.2281)\tgrad(D) penalty 0.0148 (0.0144)\tRec loss 4078.1953 (3851.9625)\tnorm 0.8667 (0.8764)\n",
            "Epoch: [132][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2169 (0.2258)\tgrad(D) penalty 0.0171 (0.0145)\tRec loss 3958.0256 (3832.6959)\tnorm 0.8847 (0.8753)\n",
            "Epoch: [132][ 40/195]\tTime  0.146 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7712 (0.7711)\tD(fake) 0.2373 (0.2290)\tgrad(D) penalty 0.0156 (0.0145)\tRec loss 4086.8989 (3818.9789)\tnorm 0.8662 (0.8762)\n",
            "Epoch: [132][ 50/195]\tTime  0.167 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2287 (0.2291)\tgrad(D) penalty 0.0157 (0.0147)\tRec loss 3738.5645 (3828.1965)\tnorm 0.8799 (0.8763)\n",
            "Epoch: [132][ 60/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2563 (0.2303)\tgrad(D) penalty 0.0141 (0.0147)\tRec loss 3942.1499 (3826.5081)\tnorm 0.8853 (0.8765)\n",
            "Epoch: [132][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7712 (0.7711)\tD(fake) 0.2238 (0.2308)\tgrad(D) penalty 0.0179 (0.0149)\tRec loss 3870.6743 (3825.5009)\tnorm 0.8758 (0.8761)\n",
            "Epoch: [132][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2341 (0.2305)\tgrad(D) penalty 0.0157 (0.0153)\tRec loss 3732.2212 (3826.7545)\tnorm 0.8722 (0.8759)\n",
            "Epoch: [132][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7712 (0.7711)\tD(fake) 0.2045 (0.2293)\tgrad(D) penalty 0.0167 (0.0154)\tRec loss 3654.6572 (3832.0316)\tnorm 0.8683 (0.8762)\n",
            "Epoch: [132][100/195]\tTime  0.163 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7712 (0.7711)\tD(fake) 0.2391 (0.2304)\tgrad(D) penalty 0.0129 (0.0154)\tRec loss 3919.9717 (3840.8956)\tnorm 0.8731 (0.8761)\n",
            "Epoch: [132][110/195]\tTime  0.153 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2357 (0.2298)\tgrad(D) penalty 0.0157 (0.0153)\tRec loss 3819.6802 (3850.8299)\tnorm 0.8870 (0.8761)\n",
            "Epoch: [132][120/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2229 (0.2299)\tgrad(D) penalty 0.0164 (0.0152)\tRec loss 4315.1113 (3851.2078)\tnorm 0.8890 (0.8763)\n",
            "Epoch: [132][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2335 (0.2303)\tgrad(D) penalty 0.0148 (0.0152)\tRec loss 3929.4448 (3857.3460)\tnorm 0.8766 (0.8764)\n",
            "Epoch: [132][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7709 (0.7711)\tD(fake) 0.2244 (0.2294)\tgrad(D) penalty 0.0158 (0.0154)\tRec loss 4019.4592 (3853.4302)\tnorm 0.8663 (0.8761)\n",
            "Epoch: [132][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7712 (0.7711)\tD(fake) 0.2276 (0.2293)\tgrad(D) penalty 0.0191 (0.0155)\tRec loss 3741.8455 (3855.8519)\tnorm 0.8818 (0.8764)\n",
            "Epoch: [132][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7711 (0.7711)\tD(fake) 0.2289 (0.2289)\tgrad(D) penalty 0.0172 (0.0155)\tRec loss 3830.8984 (3854.8493)\tnorm 0.8748 (0.8765)\n",
            "Epoch: [132][170/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7712 (0.7711)\tD(fake) 0.2126 (0.2288)\tgrad(D) penalty 0.0188 (0.0155)\tRec loss 3835.6355 (3850.8141)\tnorm 0.8627 (0.8762)\n",
            "Epoch: [132][180/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7712 (0.7711)\tD(fake) 0.2411 (0.2296)\tgrad(D) penalty 0.0145 (0.0154)\tRec loss 4184.7793 (3854.3551)\tnorm 0.8794 (0.8762)\n",
            "Epoch: [132][190/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7713 (0.7711)\tD(fake) 0.2386 (0.2296)\tgrad(D) penalty 0.0145 (0.0154)\tRec loss 3961.1538 (3857.5502)\tnorm 0.8804 (0.8762)\n",
            "Epoch: [133][  0/195]\tTime  0.441 ( 0.441)\tData  0.239 ( 0.239)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4250.2476 (4250.2476)\tnorm 0.8714 (0.8714)\n",
            "Epoch: [133][ 10/195]\tTime  0.148 ( 0.176)\tData  0.000 ( 0.022)\tD(real) 0.7713 (0.7713)\tD(fake) 0.2387 (0.2288)\tgrad(D) penalty 0.0114 (0.0129)\tRec loss 3819.1799 (3879.2914)\tnorm 0.8764 (0.8749)\n",
            "Epoch: [133][ 20/195]\tTime  0.155 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7711 (0.7712)\tD(fake) 0.2196 (0.2260)\tgrad(D) penalty 0.0137 (0.0132)\tRec loss 3857.8003 (3873.9757)\tnorm 0.8909 (0.8762)\n",
            "Epoch: [133][ 30/195]\tTime  0.148 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7712 (0.7712)\tD(fake) 0.2260 (0.2258)\tgrad(D) penalty 0.0165 (0.0138)\tRec loss 3503.8196 (3887.4701)\tnorm 0.8765 (0.8784)\n",
            "Epoch: [133][ 40/195]\tTime  0.159 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7710 (0.7712)\tD(fake) 0.2256 (0.2287)\tgrad(D) penalty 0.0173 (0.0146)\tRec loss 3870.9856 (3899.0110)\tnorm 0.8816 (0.8796)\n",
            "Epoch: [133][ 50/195]\tTime  0.167 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7711 (0.7712)\tD(fake) 0.2196 (0.2270)\tgrad(D) penalty 0.0191 (0.0154)\tRec loss 3994.7261 (3879.4647)\tnorm 0.8937 (0.8806)\n",
            "Epoch: [133][ 60/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7707 (0.7712)\tD(fake) 0.2115 (0.2262)\tgrad(D) penalty 0.0189 (0.0157)\tRec loss 3657.0640 (3863.3905)\tnorm 0.8830 (0.8800)\n",
            "Epoch: [133][ 70/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7713 (0.7712)\tD(fake) 0.2366 (0.2267)\tgrad(D) penalty 0.0151 (0.0157)\tRec loss 4439.7100 (3867.5204)\tnorm 0.8734 (0.8804)\n",
            "Epoch: [133][ 80/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7713 (0.7712)\tD(fake) 0.2318 (0.2273)\tgrad(D) penalty 0.0163 (0.0157)\tRec loss 3856.3755 (3868.3996)\tnorm 0.8794 (0.8804)\n",
            "Epoch: [133][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7713 (0.7712)\tD(fake) 0.2133 (0.2266)\tgrad(D) penalty 0.0147 (0.0155)\tRec loss 3900.9500 (3862.5473)\tnorm 0.8715 (0.8795)\n",
            "Epoch: [133][100/195]\tTime  0.169 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7710 (0.7712)\tD(fake) 0.2045 (0.2278)\tgrad(D) penalty 0.0155 (0.0153)\tRec loss 3694.8884 (3858.6223)\tnorm 0.8646 (0.8785)\n",
            "Epoch: [133][110/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7713 (0.7712)\tD(fake) 0.2491 (0.2277)\tgrad(D) penalty 0.0140 (0.0153)\tRec loss 3958.0757 (3858.2842)\tnorm 0.8872 (0.8783)\n",
            "Epoch: [133][120/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7709 (0.7712)\tD(fake) 0.2265 (0.2275)\tgrad(D) penalty 0.0190 (0.0155)\tRec loss 3699.8354 (3856.2913)\tnorm 0.8935 (0.8783)\n",
            "Epoch: [133][130/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7712 (0.7712)\tD(fake) 0.2274 (0.2278)\tgrad(D) penalty 0.0164 (0.0155)\tRec loss 3810.9451 (3854.8048)\tnorm 0.8676 (0.8781)\n",
            "Epoch: [133][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7712 (0.7712)\tD(fake) 0.2424 (0.2282)\tgrad(D) penalty 0.0168 (0.0157)\tRec loss 3980.2822 (3853.0749)\tnorm 0.8788 (0.8785)\n",
            "Epoch: [133][150/195]\tTime  0.168 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7712 (0.7712)\tD(fake) 0.2183 (0.2280)\tgrad(D) penalty 0.0188 (0.0158)\tRec loss 3766.5042 (3853.8008)\tnorm 0.8835 (0.8785)\n",
            "Epoch: [133][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7712 (0.7712)\tD(fake) 0.2221 (0.2284)\tgrad(D) penalty 0.0165 (0.0159)\tRec loss 4014.8616 (3855.5048)\tnorm 0.8918 (0.8784)\n",
            "Epoch: [133][170/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7712 (0.7712)\tD(fake) 0.2279 (0.2282)\tgrad(D) penalty 0.0152 (0.0159)\tRec loss 4026.5974 (3854.1389)\tnorm 0.8800 (0.8782)\n",
            "Epoch: [133][180/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7711 (0.7712)\tD(fake) 0.2246 (0.2280)\tgrad(D) penalty 0.0179 (0.0159)\tRec loss 4074.3525 (3854.4609)\tnorm 0.8719 (0.8782)\n",
            "Epoch: [133][190/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7712 (0.7712)\tD(fake) 0.2158 (0.2280)\tgrad(D) penalty 0.0133 (0.0158)\tRec loss 3749.3040 (3854.7698)\tnorm 0.8949 (0.8780)\n",
            "Epoch: [134][  0/195]\tTime  0.414 ( 0.414)\tData  0.240 ( 0.240)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3919.3276 (3919.3276)\tnorm 0.8554 (0.8554)\n",
            "Epoch: [134][ 10/195]\tTime  0.148 ( 0.173)\tData  0.000 ( 0.022)\tD(real) 0.7713 (0.7713)\tD(fake) 0.2266 (0.2251)\tgrad(D) penalty 0.0157 (0.0140)\tRec loss 4225.0903 (3917.0211)\tnorm 0.8791 (0.8818)\n",
            "Epoch: [134][ 20/195]\tTime  0.147 ( 0.161)\tData  0.000 ( 0.012)\tD(real) 0.7712 (0.7712)\tD(fake) 0.2150 (0.2268)\tgrad(D) penalty 0.0142 (0.0144)\tRec loss 3721.8613 (3930.1828)\tnorm 0.8791 (0.8770)\n",
            "Epoch: [134][ 30/195]\tTime  0.146 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7713 (0.7712)\tD(fake) 0.2378 (0.2276)\tgrad(D) penalty 0.0131 (0.0145)\tRec loss 3536.2700 (3911.6362)\tnorm 0.8927 (0.8785)\n",
            "Epoch: [134][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7712 (0.7712)\tD(fake) 0.2116 (0.2249)\tgrad(D) penalty 0.0156 (0.0145)\tRec loss 3983.6409 (3915.2035)\tnorm 0.8700 (0.8792)\n",
            "Epoch: [134][ 50/195]\tTime  0.168 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7713 (0.7712)\tD(fake) 0.2357 (0.2257)\tgrad(D) penalty 0.0127 (0.0145)\tRec loss 3813.9185 (3902.1529)\tnorm 0.8812 (0.8799)\n",
            "Epoch: [134][ 60/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7712 (0.7712)\tD(fake) 0.2215 (0.2258)\tgrad(D) penalty 0.0153 (0.0146)\tRec loss 3964.2017 (3892.9293)\tnorm 0.8903 (0.8795)\n",
            "Epoch: [134][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7712 (0.7712)\tD(fake) 0.2317 (0.2259)\tgrad(D) penalty 0.0160 (0.0147)\tRec loss 3906.8105 (3887.2422)\tnorm 0.8777 (0.8799)\n",
            "Epoch: [134][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7711 (0.7712)\tD(fake) 0.2176 (0.2250)\tgrad(D) penalty 0.0176 (0.0148)\tRec loss 3930.1660 (3876.3135)\tnorm 0.8758 (0.8789)\n",
            "Epoch: [134][ 90/195]\tTime  0.157 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7712 (0.7712)\tD(fake) 0.2329 (0.2256)\tgrad(D) penalty 0.0171 (0.0151)\tRec loss 4043.5286 (3882.8406)\tnorm 0.8662 (0.8781)\n",
            "Epoch: [134][100/195]\tTime  0.164 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7714 (0.7712)\tD(fake) 0.2314 (0.2249)\tgrad(D) penalty 0.0165 (0.0153)\tRec loss 3892.8384 (3884.3036)\tnorm 0.8838 (0.8780)\n",
            "Epoch: [134][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7714 (0.7712)\tD(fake) 0.2295 (0.2254)\tgrad(D) penalty 0.0161 (0.0153)\tRec loss 3816.3984 (3873.1777)\tnorm 0.8839 (0.8783)\n",
            "Epoch: [134][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7713 (0.7713)\tD(fake) 0.2364 (0.2260)\tgrad(D) penalty 0.0149 (0.0153)\tRec loss 3985.4612 (3864.8561)\tnorm 0.8702 (0.8782)\n",
            "Epoch: [134][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7713 (0.7713)\tD(fake) 0.2418 (0.2260)\tgrad(D) penalty 0.0132 (0.0152)\tRec loss 3714.5127 (3865.9005)\tnorm 0.8792 (0.8784)\n",
            "Epoch: [134][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7713)\tD(fake) 0.2223 (0.2265)\tgrad(D) penalty 0.0165 (0.0152)\tRec loss 3620.9885 (3864.5288)\tnorm 0.8789 (0.8785)\n",
            "Epoch: [134][150/195]\tTime  0.165 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7713)\tD(fake) 0.2377 (0.2271)\tgrad(D) penalty 0.0177 (0.0153)\tRec loss 3447.4771 (3858.2998)\tnorm 0.8629 (0.8783)\n",
            "Epoch: [134][160/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7713)\tD(fake) 0.2230 (0.2270)\tgrad(D) penalty 0.0176 (0.0152)\tRec loss 3968.2422 (3862.4310)\tnorm 0.8812 (0.8782)\n",
            "Epoch: [134][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7713)\tD(fake) 0.2352 (0.2271)\tgrad(D) penalty 0.0150 (0.0152)\tRec loss 3813.2317 (3858.1367)\tnorm 0.8796 (0.8781)\n",
            "Epoch: [134][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7714 (0.7713)\tD(fake) 0.2198 (0.2269)\tgrad(D) penalty 0.0164 (0.0153)\tRec loss 3780.4429 (3855.5292)\tnorm 0.8728 (0.8781)\n",
            "Epoch: [134][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7714 (0.7713)\tD(fake) 0.2409 (0.2275)\tgrad(D) penalty 0.0168 (0.0154)\tRec loss 3648.7295 (3852.6200)\tnorm 0.8714 (0.8780)\n",
            "Epoch: [135][  0/195]\tTime  0.433 ( 0.433)\tData  0.239 ( 0.239)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3788.4995 (3788.4995)\tnorm 0.8663 (0.8663)\n",
            "Epoch: [135][ 10/195]\tTime  0.151 ( 0.175)\tData  0.000 ( 0.022)\tD(real) 0.7714 (0.7714)\tD(fake) 0.2450 (0.2361)\tgrad(D) penalty 0.0156 (0.0156)\tRec loss 3672.7764 (3884.9509)\tnorm 0.8694 (0.8707)\n",
            "Epoch: [135][ 20/195]\tTime  0.145 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2284 (0.2332)\tgrad(D) penalty 0.0162 (0.0156)\tRec loss 3685.0151 (3878.4696)\tnorm 0.8737 (0.8744)\n",
            "Epoch: [135][ 30/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7714 (0.7714)\tD(fake) 0.2146 (0.2295)\tgrad(D) penalty 0.0157 (0.0155)\tRec loss 3832.4558 (3869.5364)\tnorm 0.8677 (0.8773)\n",
            "Epoch: [135][ 40/195]\tTime  0.150 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7713 (0.7714)\tD(fake) 0.2222 (0.2303)\tgrad(D) penalty 0.0144 (0.0157)\tRec loss 3734.7195 (3840.0689)\tnorm 0.8779 (0.8771)\n",
            "Epoch: [135][ 50/195]\tTime  0.165 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7712 (0.7714)\tD(fake) 0.2348 (0.2287)\tgrad(D) penalty 0.0155 (0.0159)\tRec loss 4101.9404 (3845.3060)\tnorm 0.8753 (0.8765)\n",
            "Epoch: [135][ 60/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7713 (0.7714)\tD(fake) 0.2112 (0.2275)\tgrad(D) penalty 0.0174 (0.0159)\tRec loss 3861.3306 (3864.8079)\tnorm 0.8844 (0.8765)\n",
            "Epoch: [135][ 70/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7716 (0.7714)\tD(fake) 0.2466 (0.2286)\tgrad(D) penalty 0.0170 (0.0161)\tRec loss 3724.3291 (3862.7691)\tnorm 0.8701 (0.8758)\n",
            "Epoch: [135][ 80/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7713 (0.7714)\tD(fake) 0.2443 (0.2292)\tgrad(D) penalty 0.0166 (0.0162)\tRec loss 3840.7026 (3851.7480)\tnorm 0.8645 (0.8748)\n",
            "Epoch: [135][ 90/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7715 (0.7714)\tD(fake) 0.2146 (0.2295)\tgrad(D) penalty 0.0166 (0.0162)\tRec loss 4055.2502 (3851.1391)\tnorm 0.8646 (0.8745)\n",
            "Epoch: [135][100/195]\tTime  0.161 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7715 (0.7714)\tD(fake) 0.2232 (0.2288)\tgrad(D) penalty 0.0141 (0.0161)\tRec loss 3818.0386 (3843.4697)\tnorm 0.8726 (0.8748)\n",
            "Epoch: [135][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7714 (0.7714)\tD(fake) 0.2379 (0.2291)\tgrad(D) penalty 0.0127 (0.0159)\tRec loss 3395.8203 (3838.7418)\tnorm 0.8675 (0.8745)\n",
            "Epoch: [135][120/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7714 (0.7714)\tD(fake) 0.2382 (0.2285)\tgrad(D) penalty 0.0157 (0.0158)\tRec loss 3679.1758 (3839.2477)\tnorm 0.8779 (0.8746)\n",
            "Epoch: [135][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7714 (0.7714)\tD(fake) 0.2181 (0.2282)\tgrad(D) penalty 0.0164 (0.0158)\tRec loss 3674.3723 (3836.2939)\tnorm 0.8752 (0.8748)\n",
            "Epoch: [135][140/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7714 (0.7714)\tD(fake) 0.2297 (0.2288)\tgrad(D) penalty 0.0174 (0.0159)\tRec loss 3794.5459 (3834.5415)\tnorm 0.8676 (0.8746)\n",
            "Epoch: [135][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7713 (0.7714)\tD(fake) 0.2196 (0.2282)\tgrad(D) penalty 0.0209 (0.0161)\tRec loss 3890.3560 (3835.8170)\tnorm 0.8825 (0.8749)\n",
            "Epoch: [135][160/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7714 (0.7714)\tD(fake) 0.2261 (0.2285)\tgrad(D) penalty 0.0137 (0.0161)\tRec loss 3760.9014 (3839.7019)\tnorm 0.8754 (0.8750)\n",
            "Epoch: [135][170/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7712 (0.7714)\tD(fake) 0.2416 (0.2282)\tgrad(D) penalty 0.0155 (0.0161)\tRec loss 3753.2124 (3844.8477)\tnorm 0.8784 (0.8749)\n",
            "Epoch: [135][180/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7714)\tD(fake) 0.2129 (0.2283)\tgrad(D) penalty 0.0133 (0.0160)\tRec loss 3791.1475 (3843.1955)\tnorm 0.8843 (0.8753)\n",
            "Epoch: [135][190/195]\tTime  0.144 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7714 (0.7714)\tD(fake) 0.2408 (0.2285)\tgrad(D) penalty 0.0129 (0.0159)\tRec loss 3825.6514 (3848.0736)\tnorm 0.8774 (0.8755)\n",
            "Epoch: [136][  0/195]\tTime  0.437 ( 0.437)\tData  0.241 ( 0.241)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3971.7107 (3971.7107)\tnorm 0.8786 (0.8786)\n",
            "Epoch: [136][ 10/195]\tTime  0.150 ( 0.178)\tData  0.000 ( 0.022)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2535 (0.2471)\tgrad(D) penalty 0.0118 (0.0125)\tRec loss 4008.1021 (3867.9225)\tnorm 0.8761 (0.8799)\n",
            "Epoch: [136][ 20/195]\tTime  0.151 ( 0.164)\tData  0.000 ( 0.012)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2369 (0.2290)\tgrad(D) penalty 0.0155 (0.0148)\tRec loss 3839.8796 (3843.1423)\tnorm 0.8769 (0.8772)\n",
            "Epoch: [136][ 30/195]\tTime  0.153 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7714 (0.7715)\tD(fake) 0.2242 (0.2306)\tgrad(D) penalty 0.0149 (0.0149)\tRec loss 3866.9978 (3863.1841)\tnorm 0.8751 (0.8771)\n",
            "Epoch: [136][ 40/195]\tTime  0.154 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7716 (0.7714)\tD(fake) 0.2322 (0.2276)\tgrad(D) penalty 0.0177 (0.0160)\tRec loss 3801.6780 (3846.6493)\tnorm 0.8726 (0.8777)\n",
            "Epoch: [136][ 50/195]\tTime  0.163 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7715 (0.7714)\tD(fake) 0.2216 (0.2263)\tgrad(D) penalty 0.0174 (0.0164)\tRec loss 3778.8877 (3850.9317)\tnorm 0.8718 (0.8778)\n",
            "Epoch: [136][ 60/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2270 (0.2260)\tgrad(D) penalty 0.0163 (0.0165)\tRec loss 3684.3262 (3836.0268)\tnorm 0.8753 (0.8774)\n",
            "Epoch: [136][ 70/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7714 (0.7715)\tD(fake) 0.2142 (0.2254)\tgrad(D) penalty 0.0138 (0.0164)\tRec loss 3517.1738 (3825.4657)\tnorm 0.8542 (0.8776)\n",
            "Epoch: [136][ 80/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2261 (0.2253)\tgrad(D) penalty 0.0138 (0.0160)\tRec loss 3535.2607 (3824.3027)\tnorm 0.8777 (0.8774)\n",
            "Epoch: [136][ 90/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2355 (0.2261)\tgrad(D) penalty 0.0164 (0.0159)\tRec loss 3833.8198 (3820.9862)\tnorm 0.8779 (0.8780)\n",
            "Epoch: [136][100/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2249 (0.2264)\tgrad(D) penalty 0.0174 (0.0159)\tRec loss 3720.2859 (3828.2386)\tnorm 0.8947 (0.8782)\n",
            "Epoch: [136][110/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2253 (0.2268)\tgrad(D) penalty 0.0160 (0.0159)\tRec loss 3710.0620 (3832.1304)\tnorm 0.8872 (0.8787)\n",
            "Epoch: [136][120/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7714 (0.7715)\tD(fake) 0.2485 (0.2262)\tgrad(D) penalty 0.0147 (0.0161)\tRec loss 4023.2871 (3838.6406)\tnorm 0.8748 (0.8784)\n",
            "Epoch: [136][130/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2158 (0.2260)\tgrad(D) penalty 0.0184 (0.0162)\tRec loss 3704.4390 (3837.2836)\tnorm 0.8687 (0.8787)\n",
            "Epoch: [136][140/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7714 (0.7715)\tD(fake) 0.2204 (0.2262)\tgrad(D) penalty 0.0144 (0.0162)\tRec loss 3902.8076 (3843.2732)\tnorm 0.8787 (0.8787)\n",
            "Epoch: [136][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2428 (0.2267)\tgrad(D) penalty 0.0140 (0.0162)\tRec loss 3947.6970 (3844.4084)\tnorm 0.8855 (0.8787)\n",
            "Epoch: [136][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2296 (0.2268)\tgrad(D) penalty 0.0150 (0.0161)\tRec loss 3944.9871 (3848.9357)\tnorm 0.8760 (0.8785)\n",
            "Epoch: [136][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2454 (0.2272)\tgrad(D) penalty 0.0148 (0.0161)\tRec loss 3857.2451 (3850.6150)\tnorm 0.8753 (0.8780)\n",
            "Epoch: [136][180/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2293 (0.2271)\tgrad(D) penalty 0.0168 (0.0161)\tRec loss 3667.2415 (3848.4077)\tnorm 0.8714 (0.8778)\n",
            "Epoch: [136][190/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2135 (0.2269)\tgrad(D) penalty 0.0145 (0.0160)\tRec loss 3890.0818 (3849.1618)\tnorm 0.8944 (0.8778)\n",
            "Epoch: [137][  0/195]\tTime  0.439 ( 0.439)\tData  0.236 ( 0.236)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3788.1729 (3788.1729)\tnorm 0.8912 (0.8912)\n",
            "Epoch: [137][ 10/195]\tTime  0.145 ( 0.176)\tData  0.000 ( 0.022)\tD(real) 0.7712 (0.7713)\tD(fake) 0.2155 (0.2245)\tgrad(D) penalty 0.0163 (0.0178)\tRec loss 3818.7451 (3874.0369)\tnorm 0.8872 (0.8785)\n",
            "Epoch: [137][ 20/195]\tTime  0.149 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7716 (0.7714)\tD(fake) 0.2292 (0.2257)\tgrad(D) penalty 0.0176 (0.0180)\tRec loss 3919.0142 (3874.7349)\tnorm 0.8944 (0.8813)\n",
            "Epoch: [137][ 30/195]\tTime  0.152 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2178 (0.2250)\tgrad(D) penalty 0.0155 (0.0175)\tRec loss 3837.3672 (3863.3117)\tnorm 0.8798 (0.8806)\n",
            "Epoch: [137][ 40/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7717 (0.7715)\tD(fake) 0.2202 (0.2243)\tgrad(D) penalty 0.0146 (0.0172)\tRec loss 3879.2910 (3863.8466)\tnorm 0.8689 (0.8799)\n",
            "Epoch: [137][ 50/195]\tTime  0.162 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2452 (0.2257)\tgrad(D) penalty 0.0157 (0.0173)\tRec loss 3762.9365 (3860.1402)\tnorm 0.8725 (0.8787)\n",
            "Epoch: [137][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7717 (0.7715)\tD(fake) 0.2206 (0.2256)\tgrad(D) penalty 0.0169 (0.0169)\tRec loss 3923.1211 (3854.2493)\tnorm 0.8784 (0.8785)\n",
            "Epoch: [137][ 70/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2315 (0.2262)\tgrad(D) penalty 0.0147 (0.0166)\tRec loss 3625.6445 (3841.4605)\tnorm 0.8885 (0.8777)\n",
            "Epoch: [137][ 80/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2279 (0.2266)\tgrad(D) penalty 0.0150 (0.0165)\tRec loss 3693.1821 (3836.8417)\tnorm 0.8689 (0.8771)\n",
            "Epoch: [137][ 90/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2278 (0.2268)\tgrad(D) penalty 0.0140 (0.0164)\tRec loss 3614.0894 (3836.7175)\tnorm 0.8633 (0.8769)\n",
            "Epoch: [137][100/195]\tTime  0.161 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2186 (0.2280)\tgrad(D) penalty 0.0132 (0.0162)\tRec loss 4066.2983 (3838.9023)\tnorm 0.8694 (0.8765)\n",
            "Epoch: [137][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2295 (0.2274)\tgrad(D) penalty 0.0158 (0.0162)\tRec loss 3827.3545 (3831.1999)\tnorm 0.8774 (0.8769)\n",
            "Epoch: [137][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2261 (0.2274)\tgrad(D) penalty 0.0232 (0.0163)\tRec loss 3785.9968 (3839.2147)\tnorm 0.8916 (0.8770)\n",
            "Epoch: [137][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2302 (0.2280)\tgrad(D) penalty 0.0172 (0.0162)\tRec loss 3821.3149 (3838.1141)\tnorm 0.8735 (0.8773)\n",
            "Epoch: [137][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2407 (0.2271)\tgrad(D) penalty 0.0152 (0.0162)\tRec loss 4017.7119 (3846.8469)\tnorm 0.8788 (0.8770)\n",
            "Epoch: [137][150/195]\tTime  0.171 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2355 (0.2279)\tgrad(D) penalty 0.0159 (0.0162)\tRec loss 3532.5920 (3851.4934)\tnorm 0.8723 (0.8774)\n",
            "Epoch: [137][160/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2434 (0.2275)\tgrad(D) penalty 0.0139 (0.0160)\tRec loss 3892.9707 (3848.3387)\tnorm 0.8701 (0.8772)\n",
            "Epoch: [137][170/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2101 (0.2270)\tgrad(D) penalty 0.0163 (0.0160)\tRec loss 3948.8384 (3847.6741)\tnorm 0.8780 (0.8773)\n",
            "Epoch: [137][180/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2313 (0.2274)\tgrad(D) penalty 0.0155 (0.0159)\tRec loss 3870.1313 (3846.9561)\tnorm 0.8870 (0.8772)\n",
            "Epoch: [137][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2346 (0.2273)\tgrad(D) penalty 0.0139 (0.0158)\tRec loss 3791.5942 (3845.2849)\tnorm 0.8847 (0.8770)\n",
            "Epoch: [138][  0/195]\tTime  0.421 ( 0.421)\tData  0.231 ( 0.231)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3626.3247 (3626.3247)\tnorm 0.8784 (0.8784)\n",
            "Epoch: [138][ 10/195]\tTime  0.148 ( 0.175)\tData  0.000 ( 0.021)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2530 (0.2352)\tgrad(D) penalty 0.0155 (0.0168)\tRec loss 3852.1538 (3765.7780)\tnorm 0.8763 (0.8667)\n",
            "Epoch: [138][ 20/195]\tTime  0.146 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2310 (0.2312)\tgrad(D) penalty 0.0165 (0.0164)\tRec loss 3908.7703 (3800.0536)\tnorm 0.8710 (0.8721)\n",
            "Epoch: [138][ 30/195]\tTime  0.149 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7709 (0.7716)\tD(fake) 0.2252 (0.2324)\tgrad(D) penalty 0.0142 (0.0162)\tRec loss 3813.8618 (3802.6882)\tnorm 0.8671 (0.8732)\n",
            "Epoch: [138][ 40/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7718 (0.7716)\tD(fake) 0.2604 (0.2308)\tgrad(D) penalty 0.0146 (0.0165)\tRec loss 3661.9414 (3794.3308)\tnorm 0.8652 (0.8733)\n",
            "Epoch: [138][ 50/195]\tTime  0.165 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7717 (0.7717)\tD(fake) 0.1992 (0.2282)\tgrad(D) penalty 0.0142 (0.0158)\tRec loss 3837.7327 (3802.4830)\tnorm 0.8735 (0.8739)\n",
            "Epoch: [138][ 60/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2023 (0.2284)\tgrad(D) penalty 0.0148 (0.0153)\tRec loss 3659.0574 (3806.3895)\tnorm 0.8610 (0.8733)\n",
            "Epoch: [138][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2582 (0.2294)\tgrad(D) penalty 0.0138 (0.0154)\tRec loss 3671.1335 (3797.2544)\tnorm 0.8661 (0.8731)\n",
            "Epoch: [138][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2208 (0.2280)\tgrad(D) penalty 0.0127 (0.0153)\tRec loss 4033.9326 (3808.1645)\tnorm 0.8704 (0.8723)\n",
            "Epoch: [138][ 90/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2369 (0.2285)\tgrad(D) penalty 0.0125 (0.0151)\tRec loss 3711.0166 (3803.0855)\tnorm 0.8676 (0.8721)\n",
            "Epoch: [138][100/195]\tTime  0.169 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2068 (0.2282)\tgrad(D) penalty 0.0135 (0.0150)\tRec loss 3628.2300 (3810.2068)\tnorm 0.8728 (0.8716)\n",
            "Epoch: [138][110/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2529 (0.2290)\tgrad(D) penalty 0.0129 (0.0149)\tRec loss 4114.2256 (3818.8047)\tnorm 0.8698 (0.8719)\n",
            "Epoch: [138][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2214 (0.2277)\tgrad(D) penalty 0.0139 (0.0148)\tRec loss 3920.2668 (3821.3221)\tnorm 0.8724 (0.8722)\n",
            "Epoch: [138][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2346 (0.2277)\tgrad(D) penalty 0.0189 (0.0150)\tRec loss 3875.4595 (3828.2242)\tnorm 0.8752 (0.8725)\n",
            "Epoch: [138][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2207 (0.2282)\tgrad(D) penalty 0.0173 (0.0151)\tRec loss 3835.3970 (3824.4429)\tnorm 0.8837 (0.8728)\n",
            "Epoch: [138][150/195]\tTime  0.170 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7717)\tD(fake) 0.2240 (0.2281)\tgrad(D) penalty 0.0192 (0.0152)\tRec loss 4174.4414 (3833.0582)\tnorm 0.8834 (0.8732)\n",
            "Epoch: [138][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7717)\tD(fake) 0.2353 (0.2276)\tgrad(D) penalty 0.0159 (0.0153)\tRec loss 3715.2495 (3833.6786)\tnorm 0.8744 (0.8734)\n",
            "Epoch: [138][170/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2026 (0.2271)\tgrad(D) penalty 0.0151 (0.0153)\tRec loss 3696.7786 (3830.6468)\tnorm 0.8629 (0.8734)\n",
            "Epoch: [138][180/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2421 (0.2277)\tgrad(D) penalty 0.0158 (0.0154)\tRec loss 3984.3484 (3836.8023)\tnorm 0.8779 (0.8736)\n",
            "Epoch: [138][190/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7714 (0.7717)\tD(fake) 0.1892 (0.2263)\tgrad(D) penalty 0.0184 (0.0156)\tRec loss 3846.1001 (3834.9415)\tnorm 0.8890 (0.8739)\n",
            "Epoch: [139][  0/195]\tTime  0.425 ( 0.425)\tData  0.229 ( 0.229)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3767.4670 (3767.4670)\tnorm 0.8790 (0.8790)\n",
            "Epoch: [139][ 10/195]\tTime  0.147 ( 0.173)\tData  0.000 ( 0.021)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2069 (0.2134)\tgrad(D) penalty 0.0163 (0.0158)\tRec loss 3780.9048 (3859.8291)\tnorm 0.8822 (0.8836)\n",
            "Epoch: [139][ 20/195]\tTime  0.148 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2230 (0.2243)\tgrad(D) penalty 0.0148 (0.0154)\tRec loss 3576.4221 (3846.1922)\tnorm 0.8845 (0.8793)\n",
            "Epoch: [139][ 30/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2293 (0.2264)\tgrad(D) penalty 0.0136 (0.0151)\tRec loss 4073.7788 (3851.4257)\tnorm 0.8859 (0.8807)\n",
            "Epoch: [139][ 40/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.006)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2195 (0.2254)\tgrad(D) penalty 0.0151 (0.0151)\tRec loss 3720.1519 (3833.4280)\tnorm 0.8703 (0.8793)\n",
            "Epoch: [139][ 50/195]\tTime  0.164 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2140 (0.2242)\tgrad(D) penalty 0.0161 (0.0150)\tRec loss 3994.6472 (3824.7886)\tnorm 0.8688 (0.8774)\n",
            "Epoch: [139][ 60/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2180 (0.2259)\tgrad(D) penalty 0.0134 (0.0148)\tRec loss 3464.6968 (3824.1319)\tnorm 0.8707 (0.8761)\n",
            "Epoch: [139][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2394 (0.2256)\tgrad(D) penalty 0.0154 (0.0148)\tRec loss 3670.5620 (3809.7953)\tnorm 0.8758 (0.8763)\n",
            "Epoch: [139][ 80/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2515 (0.2261)\tgrad(D) penalty 0.0144 (0.0149)\tRec loss 3566.5898 (3806.6081)\tnorm 0.8859 (0.8764)\n",
            "Epoch: [139][ 90/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2188 (0.2277)\tgrad(D) penalty 0.0143 (0.0148)\tRec loss 4214.1816 (3804.7531)\tnorm 0.8882 (0.8763)\n",
            "Epoch: [139][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7718)\tD(fake) 0.2533 (0.2287)\tgrad(D) penalty 0.0135 (0.0150)\tRec loss 3837.1177 (3812.9614)\tnorm 0.8688 (0.8758)\n",
            "Epoch: [139][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2226 (0.2289)\tgrad(D) penalty 0.0167 (0.0150)\tRec loss 3835.1099 (3816.6813)\tnorm 0.8682 (0.8759)\n",
            "Epoch: [139][120/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2318 (0.2292)\tgrad(D) penalty 0.0128 (0.0151)\tRec loss 4027.3208 (3820.0484)\tnorm 0.8656 (0.8758)\n",
            "Epoch: [139][130/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2405 (0.2288)\tgrad(D) penalty 0.0135 (0.0151)\tRec loss 4043.4436 (3823.0019)\tnorm 0.8885 (0.8758)\n",
            "Epoch: [139][140/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2240 (0.2288)\tgrad(D) penalty 0.0163 (0.0151)\tRec loss 3741.5603 (3825.5229)\tnorm 0.8746 (0.8757)\n",
            "Epoch: [139][150/195]\tTime  0.167 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2322 (0.2296)\tgrad(D) penalty 0.0150 (0.0151)\tRec loss 3701.0547 (3829.3977)\tnorm 0.8812 (0.8758)\n",
            "Epoch: [139][160/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2377 (0.2293)\tgrad(D) penalty 0.0134 (0.0150)\tRec loss 3864.2251 (3830.9089)\tnorm 0.8835 (0.8756)\n",
            "Epoch: [139][170/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2328 (0.2293)\tgrad(D) penalty 0.0160 (0.0150)\tRec loss 3786.7012 (3830.6430)\tnorm 0.8574 (0.8752)\n",
            "Epoch: [139][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2389 (0.2293)\tgrad(D) penalty 0.0179 (0.0152)\tRec loss 3645.3762 (3831.1373)\tnorm 0.8778 (0.8752)\n",
            "Epoch: [139][190/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2262 (0.2294)\tgrad(D) penalty 0.0148 (0.0151)\tRec loss 4081.2227 (3838.1022)\tnorm 0.8724 (0.8750)\n",
            "Epoch: [140][  0/195]\tTime  0.422 ( 0.422)\tData  0.230 ( 0.230)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3966.7461 (3966.7461)\tnorm 0.8633 (0.8633)\n",
            "Epoch: [140][ 10/195]\tTime  0.149 ( 0.174)\tData  0.000 ( 0.021)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2561 (0.2429)\tgrad(D) penalty 0.0144 (0.0151)\tRec loss 4049.9670 (3876.3551)\tnorm 0.8874 (0.8780)\n",
            "Epoch: [140][ 20/195]\tTime  0.147 ( 0.163)\tData  0.000 ( 0.011)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2287 (0.2287)\tgrad(D) penalty 0.0165 (0.0158)\tRec loss 4146.0996 (3832.3541)\tnorm 0.8727 (0.8781)\n",
            "Epoch: [140][ 30/195]\tTime  0.147 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2405 (0.2334)\tgrad(D) penalty 0.0126 (0.0150)\tRec loss 3523.8865 (3826.7528)\tnorm 0.8766 (0.8770)\n",
            "Epoch: [140][ 40/195]\tTime  0.145 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2369 (0.2312)\tgrad(D) penalty 0.0133 (0.0150)\tRec loss 3635.4399 (3826.6533)\tnorm 0.8787 (0.8788)\n",
            "Epoch: [140][ 50/195]\tTime  0.169 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2190 (0.2310)\tgrad(D) penalty 0.0147 (0.0149)\tRec loss 3878.5483 (3817.5451)\tnorm 0.8739 (0.8781)\n",
            "Epoch: [140][ 60/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7715 (0.7717)\tD(fake) 0.2162 (0.2311)\tgrad(D) penalty 0.0132 (0.0147)\tRec loss 3732.1602 (3809.3728)\tnorm 0.8695 (0.8773)\n",
            "Epoch: [140][ 70/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2331 (0.2296)\tgrad(D) penalty 0.0182 (0.0151)\tRec loss 3693.2734 (3815.4332)\tnorm 0.8617 (0.8765)\n",
            "Epoch: [140][ 80/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2336 (0.2310)\tgrad(D) penalty 0.0155 (0.0152)\tRec loss 4023.1086 (3824.2379)\tnorm 0.8727 (0.8756)\n",
            "Epoch: [140][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2304 (0.2308)\tgrad(D) penalty 0.0170 (0.0154)\tRec loss 3510.9331 (3818.7560)\tnorm 0.8761 (0.8755)\n",
            "Epoch: [140][100/195]\tTime  0.163 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2178 (0.2305)\tgrad(D) penalty 0.0156 (0.0155)\tRec loss 3850.0063 (3826.6576)\tnorm 0.8716 (0.8753)\n",
            "Epoch: [140][110/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2289 (0.2303)\tgrad(D) penalty 0.0154 (0.0155)\tRec loss 3733.7261 (3827.6274)\tnorm 0.8714 (0.8752)\n",
            "Epoch: [140][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2443 (0.2303)\tgrad(D) penalty 0.0147 (0.0153)\tRec loss 3904.9600 (3822.8315)\tnorm 0.8777 (0.8752)\n",
            "Epoch: [140][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2086 (0.2294)\tgrad(D) penalty 0.0175 (0.0154)\tRec loss 3715.9919 (3833.6822)\tnorm 0.8825 (0.8747)\n",
            "Epoch: [140][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2263 (0.2294)\tgrad(D) penalty 0.0150 (0.0153)\tRec loss 3858.1924 (3838.6055)\tnorm 0.8718 (0.8747)\n",
            "Epoch: [140][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2325 (0.2294)\tgrad(D) penalty 0.0147 (0.0153)\tRec loss 3746.3994 (3832.2322)\tnorm 0.8817 (0.8746)\n",
            "Epoch: [140][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2362 (0.2294)\tgrad(D) penalty 0.0133 (0.0153)\tRec loss 3916.1934 (3828.7819)\tnorm 0.8717 (0.8748)\n",
            "Epoch: [140][170/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2173 (0.2290)\tgrad(D) penalty 0.0186 (0.0153)\tRec loss 3876.5029 (3830.1276)\tnorm 0.8711 (0.8746)\n",
            "Epoch: [140][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.1964 (0.2288)\tgrad(D) penalty 0.0158 (0.0153)\tRec loss 3918.4214 (3835.1464)\tnorm 0.8714 (0.8749)\n",
            "Epoch: [140][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7719 (0.7717)\tD(fake) 0.2360 (0.2286)\tgrad(D) penalty 0.0146 (0.0153)\tRec loss 3549.8909 (3835.2871)\tnorm 0.8792 (0.8751)\n",
            "Epoch: [141][  0/195]\tTime  0.455 ( 0.455)\tData  0.257 ( 0.257)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3887.6235 (3887.6235)\tnorm 0.8770 (0.8770)\n",
            "Epoch: [141][ 10/195]\tTime  0.148 ( 0.178)\tData  0.000 ( 0.024)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2483 (0.2326)\tgrad(D) penalty 0.0150 (0.0169)\tRec loss 3800.3569 (3698.5723)\tnorm 0.8709 (0.8746)\n",
            "Epoch: [141][ 20/195]\tTime  0.150 ( 0.164)\tData  0.000 ( 0.012)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2203 (0.2269)\tgrad(D) penalty 0.0224 (0.0176)\tRec loss 3740.6680 (3788.9527)\tnorm 0.8888 (0.8778)\n",
            "Epoch: [141][ 30/195]\tTime  0.149 ( 0.159)\tData  0.000 ( 0.009)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2374 (0.2315)\tgrad(D) penalty 0.0112 (0.0164)\tRec loss 3879.8665 (3786.9934)\tnorm 0.8733 (0.8780)\n",
            "Epoch: [141][ 40/195]\tTime  0.155 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2376 (0.2268)\tgrad(D) penalty 0.0139 (0.0161)\tRec loss 3946.0735 (3802.4966)\tnorm 0.8634 (0.8769)\n",
            "Epoch: [141][ 50/195]\tTime  0.166 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2234 (0.2275)\tgrad(D) penalty 0.0142 (0.0155)\tRec loss 3876.0840 (3801.0527)\tnorm 0.8669 (0.8761)\n",
            "Epoch: [141][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2303 (0.2291)\tgrad(D) penalty 0.0125 (0.0150)\tRec loss 4094.2585 (3817.3407)\tnorm 0.8810 (0.8757)\n",
            "Epoch: [141][ 70/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2352 (0.2286)\tgrad(D) penalty 0.0150 (0.0152)\tRec loss 3839.5898 (3821.8992)\tnorm 0.8882 (0.8763)\n",
            "Epoch: [141][ 80/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2169 (0.2287)\tgrad(D) penalty 0.0154 (0.0149)\tRec loss 3573.4961 (3809.0987)\tnorm 0.8774 (0.8757)\n",
            "Epoch: [141][ 90/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2401 (0.2292)\tgrad(D) penalty 0.0140 (0.0150)\tRec loss 4057.6831 (3811.0468)\tnorm 0.8827 (0.8753)\n",
            "Epoch: [141][100/195]\tTime  0.169 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2253 (0.2297)\tgrad(D) penalty 0.0153 (0.0151)\tRec loss 3977.2158 (3811.9886)\tnorm 0.8884 (0.8750)\n",
            "Epoch: [141][110/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2347 (0.2292)\tgrad(D) penalty 0.0160 (0.0152)\tRec loss 3966.2329 (3819.4464)\tnorm 0.8839 (0.8748)\n",
            "Epoch: [141][120/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2278 (0.2294)\tgrad(D) penalty 0.0174 (0.0152)\tRec loss 3631.3325 (3824.1462)\tnorm 0.8701 (0.8748)\n",
            "Epoch: [141][130/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2389 (0.2299)\tgrad(D) penalty 0.0161 (0.0152)\tRec loss 4010.2388 (3826.0140)\tnorm 0.8708 (0.8746)\n",
            "Epoch: [141][140/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2400 (0.2306)\tgrad(D) penalty 0.0145 (0.0152)\tRec loss 3908.1475 (3823.7561)\tnorm 0.8662 (0.8747)\n",
            "Epoch: [141][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2123 (0.2298)\tgrad(D) penalty 0.0163 (0.0153)\tRec loss 3929.9785 (3828.4161)\tnorm 0.8787 (0.8745)\n",
            "Epoch: [141][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2280 (0.2295)\tgrad(D) penalty 0.0130 (0.0152)\tRec loss 3638.6719 (3827.3973)\tnorm 0.8695 (0.8750)\n",
            "Epoch: [141][170/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2208 (0.2293)\tgrad(D) penalty 0.0101 (0.0150)\tRec loss 4150.7251 (3826.2571)\tnorm 0.8770 (0.8751)\n",
            "Epoch: [141][180/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2332 (0.2294)\tgrad(D) penalty 0.0128 (0.0150)\tRec loss 3951.3306 (3827.1178)\tnorm 0.8768 (0.8750)\n",
            "Epoch: [141][190/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2253 (0.2288)\tgrad(D) penalty 0.0173 (0.0151)\tRec loss 4010.6514 (3831.4377)\tnorm 0.8823 (0.8752)\n",
            "Epoch: [142][  0/195]\tTime  0.410 ( 0.410)\tData  0.235 ( 0.235)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3570.2480 (3570.2480)\tnorm 0.8893 (0.8893)\n",
            "Epoch: [142][ 10/195]\tTime  0.146 ( 0.172)\tData  0.000 ( 0.022)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2712 (0.2575)\tgrad(D) penalty 0.0145 (0.0167)\tRec loss 3791.7412 (3714.5905)\tnorm 0.8655 (0.8741)\n",
            "Epoch: [142][ 20/195]\tTime  0.145 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2333 (0.2392)\tgrad(D) penalty 0.0186 (0.0168)\tRec loss 3732.4480 (3762.9933)\tnorm 0.8804 (0.8722)\n",
            "Epoch: [142][ 30/195]\tTime  0.152 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2238 (0.2368)\tgrad(D) penalty 0.0176 (0.0165)\tRec loss 3885.0273 (3757.5327)\tnorm 0.8665 (0.8734)\n",
            "Epoch: [142][ 40/195]\tTime  0.154 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2446 (0.2346)\tgrad(D) penalty 0.0155 (0.0168)\tRec loss 3862.0300 (3788.1558)\tnorm 0.8747 (0.8733)\n",
            "Epoch: [142][ 50/195]\tTime  0.169 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2100 (0.2323)\tgrad(D) penalty 0.0149 (0.0166)\tRec loss 3745.3396 (3800.1233)\tnorm 0.8825 (0.8744)\n",
            "Epoch: [142][ 60/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2453 (0.2324)\tgrad(D) penalty 0.0149 (0.0166)\tRec loss 3759.0132 (3800.4781)\tnorm 0.8884 (0.8744)\n",
            "Epoch: [142][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2267 (0.2319)\tgrad(D) penalty 0.0157 (0.0164)\tRec loss 3980.1006 (3813.7085)\tnorm 0.8751 (0.8745)\n",
            "Epoch: [142][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2232 (0.2314)\tgrad(D) penalty 0.0169 (0.0163)\tRec loss 3420.9565 (3807.2398)\tnorm 0.8711 (0.8750)\n",
            "Epoch: [142][ 90/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2247 (0.2320)\tgrad(D) penalty 0.0133 (0.0160)\tRec loss 3844.4468 (3805.3898)\tnorm 0.8686 (0.8748)\n",
            "Epoch: [142][100/195]\tTime  0.169 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2280 (0.2309)\tgrad(D) penalty 0.0190 (0.0160)\tRec loss 3722.2012 (3811.6737)\tnorm 0.8760 (0.8745)\n",
            "Epoch: [142][110/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7718)\tD(fake) 0.2173 (0.2307)\tgrad(D) penalty 0.0155 (0.0159)\tRec loss 3822.6587 (3818.4370)\tnorm 0.8712 (0.8745)\n",
            "Epoch: [142][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2332 (0.2307)\tgrad(D) penalty 0.0148 (0.0159)\tRec loss 3851.8076 (3827.6369)\tnorm 0.8837 (0.8750)\n",
            "Epoch: [142][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7718)\tD(fake) 0.2307 (0.2301)\tgrad(D) penalty 0.0210 (0.0161)\tRec loss 3906.6697 (3834.5539)\tnorm 0.8810 (0.8751)\n",
            "Epoch: [142][140/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2191 (0.2318)\tgrad(D) penalty 0.0168 (0.0161)\tRec loss 3827.7966 (3830.9802)\tnorm 0.8741 (0.8749)\n",
            "Epoch: [142][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2298 (0.2312)\tgrad(D) penalty 0.0191 (0.0162)\tRec loss 3817.7654 (3827.8613)\tnorm 0.8740 (0.8746)\n",
            "Epoch: [142][160/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2198 (0.2312)\tgrad(D) penalty 0.0156 (0.0161)\tRec loss 3774.5461 (3827.1286)\tnorm 0.8664 (0.8746)\n",
            "Epoch: [142][170/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2207 (0.2309)\tgrad(D) penalty 0.0152 (0.0161)\tRec loss 3645.0459 (3828.8243)\tnorm 0.8682 (0.8743)\n",
            "Epoch: [142][180/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2317 (0.2306)\tgrad(D) penalty 0.0136 (0.0160)\tRec loss 3744.0981 (3828.5071)\tnorm 0.8816 (0.8742)\n",
            "Epoch: [142][190/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2449 (0.2307)\tgrad(D) penalty 0.0143 (0.0160)\tRec loss 3727.8145 (3832.0549)\tnorm 0.8726 (0.8743)\n",
            "Epoch: [143][  0/195]\tTime  0.413 ( 0.413)\tData  0.231 ( 0.231)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3938.6670 (3938.6670)\tnorm 0.8741 (0.8741)\n",
            "Epoch: [143][ 10/195]\tTime  0.154 ( 0.181)\tData  0.000 ( 0.021)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2518 (0.2446)\tgrad(D) penalty 0.0142 (0.0152)\tRec loss 3712.7563 (3811.0433)\tnorm 0.8727 (0.8753)\n",
            "Epoch: [143][ 20/195]\tTime  0.151 ( 0.167)\tData  0.000 ( 0.011)\tD(real) 0.7713 (0.7716)\tD(fake) 0.2056 (0.2282)\tgrad(D) penalty 0.0166 (0.0163)\tRec loss 3858.7783 (3777.2289)\tnorm 0.8719 (0.8725)\n",
            "Epoch: [143][ 30/195]\tTime  0.148 ( 0.161)\tData  0.000 ( 0.008)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2582 (0.2328)\tgrad(D) penalty 0.0138 (0.0163)\tRec loss 3911.3003 (3797.1827)\tnorm 0.8838 (0.8723)\n",
            "Epoch: [143][ 40/195]\tTime  0.154 ( 0.158)\tData  0.000 ( 0.006)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2202 (0.2326)\tgrad(D) penalty 0.0164 (0.0161)\tRec loss 3673.2290 (3798.1374)\tnorm 0.8701 (0.8727)\n",
            "Epoch: [143][ 50/195]\tTime  0.161 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2225 (0.2320)\tgrad(D) penalty 0.0162 (0.0160)\tRec loss 4172.1602 (3819.5142)\tnorm 0.8565 (0.8734)\n",
            "Epoch: [143][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2367 (0.2311)\tgrad(D) penalty 0.0137 (0.0157)\tRec loss 3899.6982 (3823.0839)\tnorm 0.8645 (0.8741)\n",
            "Epoch: [143][ 70/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2196 (0.2300)\tgrad(D) penalty 0.0137 (0.0157)\tRec loss 3796.9580 (3832.3908)\tnorm 0.8768 (0.8744)\n",
            "Epoch: [143][ 80/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7713 (0.7717)\tD(fake) 0.2162 (0.2282)\tgrad(D) penalty 0.0162 (0.0155)\tRec loss 3845.7356 (3824.0387)\tnorm 0.8917 (0.8750)\n",
            "Epoch: [143][ 90/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7715 (0.7717)\tD(fake) 0.2248 (0.2286)\tgrad(D) penalty 0.0129 (0.0154)\tRec loss 3798.5823 (3832.4029)\tnorm 0.8674 (0.8752)\n",
            "Epoch: [143][100/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2373 (0.2277)\tgrad(D) penalty 0.0122 (0.0153)\tRec loss 4073.2373 (3830.8286)\tnorm 0.8772 (0.8759)\n",
            "Epoch: [143][110/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2188 (0.2273)\tgrad(D) penalty 0.0145 (0.0153)\tRec loss 4005.8708 (3826.8037)\tnorm 0.8735 (0.8764)\n",
            "Epoch: [143][120/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2232 (0.2277)\tgrad(D) penalty 0.0152 (0.0152)\tRec loss 3969.5171 (3823.7632)\tnorm 0.8538 (0.8761)\n",
            "Epoch: [143][130/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2335 (0.2273)\tgrad(D) penalty 0.0188 (0.0155)\tRec loss 3928.3333 (3821.3138)\tnorm 0.8851 (0.8762)\n",
            "Epoch: [143][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2131 (0.2275)\tgrad(D) penalty 0.0176 (0.0155)\tRec loss 3493.8188 (3825.6661)\tnorm 0.8750 (0.8757)\n",
            "Epoch: [143][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2329 (0.2276)\tgrad(D) penalty 0.0170 (0.0157)\tRec loss 4046.3887 (3823.0248)\tnorm 0.8723 (0.8757)\n",
            "Epoch: [143][160/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2495 (0.2279)\tgrad(D) penalty 0.0154 (0.0157)\tRec loss 3836.1953 (3827.5713)\tnorm 0.8794 (0.8757)\n",
            "Epoch: [143][170/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2094 (0.2279)\tgrad(D) penalty 0.0148 (0.0157)\tRec loss 3806.7717 (3829.1311)\tnorm 0.8685 (0.8754)\n",
            "Epoch: [143][180/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2252 (0.2283)\tgrad(D) penalty 0.0156 (0.0157)\tRec loss 3880.9641 (3828.4893)\tnorm 0.8757 (0.8755)\n",
            "Epoch: [143][190/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7713 (0.7717)\tD(fake) 0.2356 (0.2281)\tgrad(D) penalty 0.0136 (0.0157)\tRec loss 3932.3247 (3830.7542)\tnorm 0.8693 (0.8751)\n",
            "Epoch: [144][  0/195]\tTime  0.422 ( 0.422)\tData  0.223 ( 0.223)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3905.7859 (3905.7859)\tnorm 0.8644 (0.8644)\n",
            "Epoch: [144][ 10/195]\tTime  0.152 ( 0.176)\tData  0.000 ( 0.021)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2505 (0.2474)\tgrad(D) penalty 0.0130 (0.0135)\tRec loss 3638.1240 (3818.2956)\tnorm 0.8853 (0.8754)\n",
            "Epoch: [144][ 20/195]\tTime  0.150 ( 0.164)\tData  0.000 ( 0.011)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2380 (0.2308)\tgrad(D) penalty 0.0141 (0.0147)\tRec loss 3710.5767 (3821.2580)\tnorm 0.8857 (0.8788)\n",
            "Epoch: [144][ 30/195]\tTime  0.153 ( 0.160)\tData  0.000 ( 0.007)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2232 (0.2291)\tgrad(D) penalty 0.0127 (0.0143)\tRec loss 3890.3633 (3812.0994)\tnorm 0.8887 (0.8782)\n",
            "Epoch: [144][ 40/195]\tTime  0.147 ( 0.158)\tData  0.000 ( 0.006)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2236 (0.2286)\tgrad(D) penalty 0.0158 (0.0146)\tRec loss 3682.8723 (3792.5198)\tnorm 0.8622 (0.8776)\n",
            "Epoch: [144][ 50/195]\tTime  0.164 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7714 (0.7717)\tD(fake) 0.2363 (0.2285)\tgrad(D) penalty 0.0167 (0.0148)\tRec loss 3638.9990 (3793.9992)\tnorm 0.8755 (0.8766)\n",
            "Epoch: [144][ 60/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2208 (0.2301)\tgrad(D) penalty 0.0175 (0.0154)\tRec loss 3689.0308 (3791.4158)\tnorm 0.8823 (0.8766)\n",
            "Epoch: [144][ 70/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2187 (0.2288)\tgrad(D) penalty 0.0174 (0.0158)\tRec loss 3897.6357 (3805.3756)\tnorm 0.8716 (0.8755)\n",
            "Epoch: [144][ 80/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2358 (0.2302)\tgrad(D) penalty 0.0187 (0.0159)\tRec loss 3789.1816 (3801.4803)\tnorm 0.8752 (0.8759)\n",
            "Epoch: [144][ 90/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7715 (0.7717)\tD(fake) 0.2222 (0.2302)\tgrad(D) penalty 0.0199 (0.0161)\tRec loss 3855.1699 (3810.6419)\tnorm 0.8689 (0.8763)\n",
            "Epoch: [144][100/195]\tTime  0.163 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2305 (0.2298)\tgrad(D) penalty 0.0165 (0.0159)\tRec loss 3856.0181 (3809.2340)\tnorm 0.8764 (0.8766)\n",
            "Epoch: [144][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2236 (0.2291)\tgrad(D) penalty 0.0151 (0.0158)\tRec loss 3873.0447 (3811.8315)\tnorm 0.8758 (0.8766)\n",
            "Epoch: [144][120/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2420 (0.2288)\tgrad(D) penalty 0.0137 (0.0157)\tRec loss 4025.4045 (3823.3947)\tnorm 0.8844 (0.8767)\n",
            "Epoch: [144][130/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2129 (0.2288)\tgrad(D) penalty 0.0141 (0.0155)\tRec loss 3651.7727 (3820.1147)\tnorm 0.8852 (0.8771)\n",
            "Epoch: [144][140/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2185 (0.2281)\tgrad(D) penalty 0.0138 (0.0153)\tRec loss 4146.3760 (3824.5370)\tnorm 0.8751 (0.8772)\n",
            "Epoch: [144][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2206 (0.2280)\tgrad(D) penalty 0.0128 (0.0152)\tRec loss 4201.4531 (3829.7924)\tnorm 0.8620 (0.8767)\n",
            "Epoch: [144][160/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7716)\tD(fake) 0.2395 (0.2274)\tgrad(D) penalty 0.0143 (0.0153)\tRec loss 3798.3613 (3830.3476)\tnorm 0.8680 (0.8764)\n",
            "Epoch: [144][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2255 (0.2272)\tgrad(D) penalty 0.0152 (0.0153)\tRec loss 3884.0701 (3830.3302)\tnorm 0.8766 (0.8764)\n",
            "Epoch: [144][180/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2240 (0.2273)\tgrad(D) penalty 0.0163 (0.0154)\tRec loss 4156.4209 (3832.1259)\tnorm 0.8824 (0.8763)\n",
            "Epoch: [144][190/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2383 (0.2275)\tgrad(D) penalty 0.0154 (0.0154)\tRec loss 3786.4956 (3830.8443)\tnorm 0.8782 (0.8762)\n",
            "Epoch: [145][  0/195]\tTime  0.436 ( 0.436)\tData  0.244 ( 0.244)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3750.9355 (3750.9355)\tnorm 0.8706 (0.8706)\n",
            "Epoch: [145][ 10/195]\tTime  0.149 ( 0.176)\tData  0.000 ( 0.022)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2370 (0.2204)\tgrad(D) penalty 0.0171 (0.0181)\tRec loss 4017.4207 (3807.5952)\tnorm 0.8786 (0.8714)\n",
            "Epoch: [145][ 20/195]\tTime  0.144 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7716 (0.7717)\tD(fake) 0.1926 (0.2257)\tgrad(D) penalty 0.0152 (0.0170)\tRec loss 3979.6416 (3849.2438)\tnorm 0.8684 (0.8709)\n",
            "Epoch: [145][ 30/195]\tTime  0.146 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2665 (0.2312)\tgrad(D) penalty 0.0158 (0.0170)\tRec loss 3871.3228 (3826.1415)\tnorm 0.8736 (0.8723)\n",
            "Epoch: [145][ 40/195]\tTime  0.146 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7716 (0.7717)\tD(fake) 0.1961 (0.2271)\tgrad(D) penalty 0.0216 (0.0173)\tRec loss 3825.4731 (3821.1907)\tnorm 0.8779 (0.8732)\n",
            "Epoch: [145][ 50/195]\tTime  0.163 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2422 (0.2280)\tgrad(D) penalty 0.0156 (0.0170)\tRec loss 3919.1719 (3819.9393)\tnorm 0.8834 (0.8740)\n",
            "Epoch: [145][ 60/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2213 (0.2271)\tgrad(D) penalty 0.0185 (0.0169)\tRec loss 3969.9668 (3820.5001)\tnorm 0.8667 (0.8730)\n",
            "Epoch: [145][ 70/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2241 (0.2277)\tgrad(D) penalty 0.0148 (0.0167)\tRec loss 3780.3076 (3817.4317)\tnorm 0.8563 (0.8727)\n",
            "Epoch: [145][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2283 (0.2283)\tgrad(D) penalty 0.0126 (0.0163)\tRec loss 3833.6299 (3815.7216)\tnorm 0.8734 (0.8725)\n",
            "Epoch: [145][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2294 (0.2277)\tgrad(D) penalty 0.0149 (0.0162)\tRec loss 3651.0210 (3812.6548)\tnorm 0.8839 (0.8728)\n",
            "Epoch: [145][100/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2180 (0.2276)\tgrad(D) penalty 0.0117 (0.0158)\tRec loss 3762.2363 (3820.9430)\tnorm 0.8692 (0.8724)\n",
            "Epoch: [145][110/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2358 (0.2278)\tgrad(D) penalty 0.0127 (0.0157)\tRec loss 4078.6572 (3818.5852)\tnorm 0.8513 (0.8719)\n",
            "Epoch: [145][120/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2164 (0.2269)\tgrad(D) penalty 0.0128 (0.0155)\tRec loss 4037.0791 (3814.4038)\tnorm 0.8852 (0.8724)\n",
            "Epoch: [145][130/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7717)\tD(fake) 0.2332 (0.2270)\tgrad(D) penalty 0.0146 (0.0155)\tRec loss 3937.0488 (3810.0095)\tnorm 0.8717 (0.8722)\n",
            "Epoch: [145][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2067 (0.2264)\tgrad(D) penalty 0.0160 (0.0155)\tRec loss 3907.6797 (3811.9677)\tnorm 0.8794 (0.8723)\n",
            "Epoch: [145][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2603 (0.2278)\tgrad(D) penalty 0.0144 (0.0155)\tRec loss 3944.5474 (3819.3549)\tnorm 0.8711 (0.8727)\n",
            "Epoch: [145][160/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2236 (0.2275)\tgrad(D) penalty 0.0217 (0.0158)\tRec loss 4067.1440 (3815.9928)\tnorm 0.8795 (0.8728)\n",
            "Epoch: [145][170/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2499 (0.2287)\tgrad(D) penalty 0.0165 (0.0158)\tRec loss 3608.1255 (3819.2693)\tnorm 0.8748 (0.8729)\n",
            "Epoch: [145][180/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2324 (0.2282)\tgrad(D) penalty 0.0147 (0.0158)\tRec loss 3947.9827 (3819.7719)\tnorm 0.8674 (0.8732)\n",
            "Epoch: [145][190/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2178 (0.2282)\tgrad(D) penalty 0.0155 (0.0159)\tRec loss 3834.5864 (3826.6602)\tnorm 0.8890 (0.8737)\n",
            "Epoch: [146][  0/195]\tTime  0.458 ( 0.458)\tData  0.260 ( 0.260)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3716.6152 (3716.6152)\tnorm 0.8626 (0.8626)\n",
            "Epoch: [146][ 10/195]\tTime  0.146 ( 0.176)\tData  0.000 ( 0.024)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2481 (0.2416)\tgrad(D) penalty 0.0146 (0.0147)\tRec loss 3667.8857 (3783.0282)\tnorm 0.8710 (0.8735)\n",
            "Epoch: [146][ 20/195]\tTime  0.147 ( 0.163)\tData  0.000 ( 0.013)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2552 (0.2338)\tgrad(D) penalty 0.0140 (0.0145)\tRec loss 3921.2803 (3808.8611)\tnorm 0.8791 (0.8764)\n",
            "Epoch: [146][ 30/195]\tTime  0.148 ( 0.158)\tData  0.000 ( 0.009)\tD(real) 0.7714 (0.7716)\tD(fake) 0.2076 (0.2322)\tgrad(D) penalty 0.0153 (0.0142)\tRec loss 4086.5491 (3824.1676)\tnorm 0.8794 (0.8765)\n",
            "Epoch: [146][ 40/195]\tTime  0.150 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2390 (0.2321)\tgrad(D) penalty 0.0155 (0.0142)\tRec loss 3445.6353 (3814.6763)\tnorm 0.8759 (0.8779)\n",
            "Epoch: [146][ 50/195]\tTime  0.167 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2147 (0.2301)\tgrad(D) penalty 0.0179 (0.0147)\tRec loss 3672.1863 (3817.9606)\tnorm 0.8825 (0.8789)\n",
            "Epoch: [146][ 60/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2212 (0.2320)\tgrad(D) penalty 0.0156 (0.0151)\tRec loss 4091.7874 (3813.7897)\tnorm 0.8793 (0.8786)\n",
            "Epoch: [146][ 70/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2242 (0.2302)\tgrad(D) penalty 0.0189 (0.0156)\tRec loss 3743.7336 (3809.5159)\tnorm 0.8700 (0.8777)\n",
            "Epoch: [146][ 80/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2153 (0.2299)\tgrad(D) penalty 0.0145 (0.0155)\tRec loss 3876.4072 (3811.3552)\tnorm 0.8793 (0.8783)\n",
            "Epoch: [146][ 90/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2503 (0.2307)\tgrad(D) penalty 0.0172 (0.0157)\tRec loss 3990.1340 (3815.0897)\tnorm 0.8840 (0.8787)\n",
            "Epoch: [146][100/195]\tTime  0.169 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2370 (0.2312)\tgrad(D) penalty 0.0126 (0.0158)\tRec loss 3876.5532 (3815.0580)\tnorm 0.8641 (0.8782)\n",
            "Epoch: [146][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2338 (0.2310)\tgrad(D) penalty 0.0157 (0.0158)\tRec loss 3939.2773 (3817.6318)\tnorm 0.8686 (0.8773)\n",
            "Epoch: [146][120/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7714 (0.7715)\tD(fake) 0.2101 (0.2308)\tgrad(D) penalty 0.0196 (0.0157)\tRec loss 3936.4944 (3820.1976)\tnorm 0.8797 (0.8773)\n",
            "Epoch: [146][130/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2392 (0.2313)\tgrad(D) penalty 0.0158 (0.0157)\tRec loss 3826.1062 (3825.8249)\tnorm 0.8743 (0.8772)\n",
            "Epoch: [146][140/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2334 (0.2310)\tgrad(D) penalty 0.0144 (0.0158)\tRec loss 3920.4360 (3825.5412)\tnorm 0.8714 (0.8771)\n",
            "Epoch: [146][150/195]\tTime  0.161 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2347 (0.2311)\tgrad(D) penalty 0.0191 (0.0159)\tRec loss 3979.8052 (3822.3840)\tnorm 0.8755 (0.8770)\n",
            "Epoch: [146][160/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2113 (0.2306)\tgrad(D) penalty 0.0185 (0.0160)\tRec loss 4195.2197 (3823.2991)\tnorm 0.8673 (0.8765)\n",
            "Epoch: [146][170/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2282 (0.2306)\tgrad(D) penalty 0.0158 (0.0160)\tRec loss 3917.4956 (3822.8257)\tnorm 0.8867 (0.8763)\n",
            "Epoch: [146][180/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2306 (0.2303)\tgrad(D) penalty 0.0168 (0.0161)\tRec loss 4086.3274 (3825.5870)\tnorm 0.8768 (0.8762)\n",
            "Epoch: [146][190/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7714 (0.7715)\tD(fake) 0.2169 (0.2298)\tgrad(D) penalty 0.0171 (0.0161)\tRec loss 3820.5181 (3823.9215)\tnorm 0.8671 (0.8761)\n",
            "Epoch: [147][  0/195]\tTime  0.432 ( 0.432)\tData  0.235 ( 0.235)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3642.5322 (3642.5322)\tnorm 0.8690 (0.8690)\n",
            "Epoch: [147][ 10/195]\tTime  0.150 ( 0.179)\tData  0.000 ( 0.022)\tD(real) 0.7714 (0.7713)\tD(fake) 0.2284 (0.2248)\tgrad(D) penalty 0.0147 (0.0155)\tRec loss 3956.2993 (3844.0309)\tnorm 0.8716 (0.8726)\n",
            "Epoch: [147][ 20/195]\tTime  0.150 ( 0.164)\tData  0.000 ( 0.011)\tD(real) 0.7715 (0.7714)\tD(fake) 0.2339 (0.2306)\tgrad(D) penalty 0.0174 (0.0159)\tRec loss 3675.1860 (3833.1722)\tnorm 0.8776 (0.8742)\n",
            "Epoch: [147][ 30/195]\tTime  0.148 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7715 (0.7714)\tD(fake) 0.2227 (0.2309)\tgrad(D) penalty 0.0160 (0.0155)\tRec loss 3650.3882 (3812.7396)\tnorm 0.8851 (0.8742)\n",
            "Epoch: [147][ 40/195]\tTime  0.150 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7715 (0.7714)\tD(fake) 0.2454 (0.2315)\tgrad(D) penalty 0.0174 (0.0154)\tRec loss 4027.8022 (3801.6156)\tnorm 0.8576 (0.8732)\n",
            "Epoch: [147][ 50/195]\tTime  0.164 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2241 (0.2316)\tgrad(D) penalty 0.0138 (0.0152)\tRec loss 3716.8799 (3812.2067)\tnorm 0.8675 (0.8726)\n",
            "Epoch: [147][ 60/195]\tTime  0.149 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7715 (0.7714)\tD(fake) 0.2379 (0.2312)\tgrad(D) penalty 0.0164 (0.0152)\tRec loss 3641.2358 (3816.5042)\tnorm 0.8700 (0.8733)\n",
            "Epoch: [147][ 70/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7714 (0.7714)\tD(fake) 0.2209 (0.2307)\tgrad(D) penalty 0.0162 (0.0154)\tRec loss 3846.9194 (3820.4905)\tnorm 0.8760 (0.8733)\n",
            "Epoch: [147][ 80/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2244 (0.2302)\tgrad(D) penalty 0.0172 (0.0157)\tRec loss 3804.1309 (3817.8838)\tnorm 0.8872 (0.8736)\n",
            "Epoch: [147][ 90/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2237 (0.2285)\tgrad(D) penalty 0.0188 (0.0159)\tRec loss 3935.3438 (3818.1702)\tnorm 0.8755 (0.8736)\n",
            "Epoch: [147][100/195]\tTime  0.166 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7714 (0.7715)\tD(fake) 0.2158 (0.2284)\tgrad(D) penalty 0.0146 (0.0158)\tRec loss 3853.8389 (3821.6064)\tnorm 0.8699 (0.8735)\n",
            "Epoch: [147][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2391 (0.2287)\tgrad(D) penalty 0.0118 (0.0157)\tRec loss 3662.6672 (3817.1448)\tnorm 0.8803 (0.8737)\n",
            "Epoch: [147][120/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7715)\tD(fake) 0.2204 (0.2279)\tgrad(D) penalty 0.0178 (0.0158)\tRec loss 3586.7832 (3817.1115)\tnorm 0.8796 (0.8736)\n",
            "Epoch: [147][130/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2258 (0.2284)\tgrad(D) penalty 0.0180 (0.0158)\tRec loss 3893.0884 (3815.5879)\tnorm 0.8701 (0.8741)\n",
            "Epoch: [147][140/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2340 (0.2289)\tgrad(D) penalty 0.0144 (0.0158)\tRec loss 3687.9004 (3813.2876)\tnorm 0.8676 (0.8740)\n",
            "Epoch: [147][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2140 (0.2281)\tgrad(D) penalty 0.0140 (0.0158)\tRec loss 3878.8101 (3815.8102)\tnorm 0.8694 (0.8737)\n",
            "Epoch: [147][160/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7714 (0.7715)\tD(fake) 0.2197 (0.2282)\tgrad(D) penalty 0.0177 (0.0159)\tRec loss 3944.2014 (3817.2561)\tnorm 0.8668 (0.8740)\n",
            "Epoch: [147][170/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2260 (0.2277)\tgrad(D) penalty 0.0188 (0.0160)\tRec loss 4095.0544 (3821.5010)\tnorm 0.8937 (0.8739)\n",
            "Epoch: [147][180/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7715)\tD(fake) 0.2269 (0.2280)\tgrad(D) penalty 0.0182 (0.0161)\tRec loss 3865.9355 (3821.7769)\tnorm 0.8651 (0.8740)\n",
            "Epoch: [147][190/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7716 (0.7715)\tD(fake) 0.2237 (0.2277)\tgrad(D) penalty 0.0188 (0.0162)\tRec loss 3752.0107 (3819.7822)\tnorm 0.8818 (0.8742)\n",
            "Epoch: [148][  0/195]\tTime  0.434 ( 0.434)\tData  0.244 ( 0.244)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3737.6509 (3737.6509)\tnorm 0.8742 (0.8742)\n",
            "Epoch: [148][ 10/195]\tTime  0.147 ( 0.175)\tData  0.000 ( 0.022)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2265 (0.2332)\tgrad(D) penalty 0.0130 (0.0141)\tRec loss 3982.3943 (3809.4791)\tnorm 0.8676 (0.8687)\n",
            "Epoch: [148][ 20/195]\tTime  0.148 ( 0.161)\tData  0.000 ( 0.012)\tD(real) 0.7715 (0.7716)\tD(fake) 0.2300 (0.2294)\tgrad(D) penalty 0.0176 (0.0147)\tRec loss 3838.6790 (3791.9072)\tnorm 0.8815 (0.8711)\n",
            "Epoch: [148][ 30/195]\tTime  0.145 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2186 (0.2301)\tgrad(D) penalty 0.0180 (0.0146)\tRec loss 3752.1074 (3793.8447)\tnorm 0.8810 (0.8718)\n",
            "Epoch: [148][ 40/195]\tTime  0.153 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2196 (0.2296)\tgrad(D) penalty 0.0152 (0.0146)\tRec loss 3745.9976 (3783.7511)\tnorm 0.8680 (0.8720)\n",
            "Epoch: [148][ 50/195]\tTime  0.160 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2284 (0.2300)\tgrad(D) penalty 0.0142 (0.0145)\tRec loss 3656.9409 (3780.2474)\tnorm 0.8693 (0.8717)\n",
            "Epoch: [148][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7715 (0.7716)\tD(fake) 0.2136 (0.2283)\tgrad(D) penalty 0.0150 (0.0146)\tRec loss 3675.6125 (3802.9498)\tnorm 0.8767 (0.8729)\n",
            "Epoch: [148][ 70/195]\tTime  0.144 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2240 (0.2282)\tgrad(D) penalty 0.0145 (0.0145)\tRec loss 4079.4229 (3806.2384)\tnorm 0.8670 (0.8735)\n",
            "Epoch: [148][ 80/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2304 (0.2280)\tgrad(D) penalty 0.0171 (0.0149)\tRec loss 3609.6982 (3808.8342)\tnorm 0.8824 (0.8738)\n",
            "Epoch: [148][ 90/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7715 (0.7716)\tD(fake) 0.2208 (0.2276)\tgrad(D) penalty 0.0166 (0.0150)\tRec loss 3946.4854 (3809.5043)\tnorm 0.8702 (0.8736)\n",
            "Epoch: [148][100/195]\tTime  0.162 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2242 (0.2281)\tgrad(D) penalty 0.0141 (0.0150)\tRec loss 3719.8403 (3808.0820)\tnorm 0.8580 (0.8737)\n",
            "Epoch: [148][110/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2264 (0.2282)\tgrad(D) penalty 0.0170 (0.0150)\tRec loss 3973.1711 (3809.2359)\tnorm 0.8631 (0.8734)\n",
            "Epoch: [148][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2332 (0.2282)\tgrad(D) penalty 0.0183 (0.0152)\tRec loss 3955.6487 (3806.7365)\tnorm 0.8758 (0.8734)\n",
            "Epoch: [148][130/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2159 (0.2280)\tgrad(D) penalty 0.0146 (0.0152)\tRec loss 3646.4949 (3810.8279)\tnorm 0.8714 (0.8734)\n",
            "Epoch: [148][140/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7716)\tD(fake) 0.2316 (0.2284)\tgrad(D) penalty 0.0139 (0.0152)\tRec loss 3714.4861 (3809.8210)\tnorm 0.8662 (0.8734)\n",
            "Epoch: [148][150/195]\tTime  0.163 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2155 (0.2276)\tgrad(D) penalty 0.0165 (0.0152)\tRec loss 3931.3975 (3813.2267)\tnorm 0.8851 (0.8735)\n",
            "Epoch: [148][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2206 (0.2281)\tgrad(D) penalty 0.0141 (0.0152)\tRec loss 3643.1626 (3817.5495)\tnorm 0.8650 (0.8738)\n",
            "Epoch: [148][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2590 (0.2286)\tgrad(D) penalty 0.0143 (0.0153)\tRec loss 3990.7427 (3819.3170)\tnorm 0.8747 (0.8733)\n",
            "Epoch: [148][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2367 (0.2279)\tgrad(D) penalty 0.0133 (0.0152)\tRec loss 3772.2451 (3818.5269)\tnorm 0.8711 (0.8734)\n",
            "Epoch: [148][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7716)\tD(fake) 0.2256 (0.2282)\tgrad(D) penalty 0.0133 (0.0151)\tRec loss 4044.8645 (3819.7753)\tnorm 0.8673 (0.8736)\n",
            "Epoch: [149][  0/195]\tTime  0.412 ( 0.412)\tData  0.237 ( 0.237)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4208.5557 (4208.5557)\tnorm 0.8683 (0.8683)\n",
            "Epoch: [149][ 10/195]\tTime  0.146 ( 0.173)\tData  0.000 ( 0.022)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2316 (0.2348)\tgrad(D) penalty 0.0151 (0.0153)\tRec loss 3867.6643 (3952.7623)\tnorm 0.8760 (0.8741)\n",
            "Epoch: [149][ 20/195]\tTime  0.152 ( 0.161)\tData  0.000 ( 0.012)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2275 (0.2293)\tgrad(D) penalty 0.0158 (0.0161)\tRec loss 3848.6472 (3903.8861)\tnorm 0.8678 (0.8741)\n",
            "Epoch: [149][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2172 (0.2259)\tgrad(D) penalty 0.0164 (0.0162)\tRec loss 3659.8696 (3868.1778)\tnorm 0.8833 (0.8761)\n",
            "Epoch: [149][ 40/195]\tTime  0.150 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2571 (0.2280)\tgrad(D) penalty 0.0158 (0.0169)\tRec loss 3656.7371 (3826.2428)\tnorm 0.8790 (0.8754)\n",
            "Epoch: [149][ 50/195]\tTime  0.168 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2235 (0.2286)\tgrad(D) penalty 0.0167 (0.0167)\tRec loss 3543.4932 (3816.6072)\tnorm 0.8696 (0.8750)\n",
            "Epoch: [149][ 60/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2334 (0.2298)\tgrad(D) penalty 0.0153 (0.0167)\tRec loss 3849.9722 (3812.0691)\tnorm 0.8773 (0.8751)\n",
            "Epoch: [149][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2207 (0.2293)\tgrad(D) penalty 0.0177 (0.0167)\tRec loss 3923.9155 (3799.3884)\tnorm 0.8840 (0.8747)\n",
            "Epoch: [149][ 80/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2218 (0.2302)\tgrad(D) penalty 0.0129 (0.0162)\tRec loss 3585.6636 (3795.1380)\tnorm 0.8797 (0.8748)\n",
            "Epoch: [149][ 90/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2461 (0.2299)\tgrad(D) penalty 0.0138 (0.0161)\tRec loss 3699.4260 (3786.8793)\tnorm 0.8662 (0.8739)\n",
            "Epoch: [149][100/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2415 (0.2310)\tgrad(D) penalty 0.0150 (0.0160)\tRec loss 4251.6895 (3796.2176)\tnorm 0.8760 (0.8737)\n",
            "Epoch: [149][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2236 (0.2306)\tgrad(D) penalty 0.0190 (0.0160)\tRec loss 3493.9138 (3800.1241)\tnorm 0.8741 (0.8734)\n",
            "Epoch: [149][120/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2231 (0.2296)\tgrad(D) penalty 0.0159 (0.0159)\tRec loss 3679.0608 (3796.2538)\tnorm 0.8794 (0.8730)\n",
            "Epoch: [149][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7715 (0.7716)\tD(fake) 0.2410 (0.2302)\tgrad(D) penalty 0.0149 (0.0158)\tRec loss 4071.3247 (3801.4639)\tnorm 0.8692 (0.8725)\n",
            "Epoch: [149][140/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7716)\tD(fake) 0.2449 (0.2303)\tgrad(D) penalty 0.0144 (0.0158)\tRec loss 3708.6877 (3811.8011)\tnorm 0.8782 (0.8727)\n",
            "Epoch: [149][150/195]\tTime  0.168 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2248 (0.2303)\tgrad(D) penalty 0.0153 (0.0157)\tRec loss 3795.7231 (3809.4019)\tnorm 0.8663 (0.8731)\n",
            "Epoch: [149][160/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2230 (0.2297)\tgrad(D) penalty 0.0172 (0.0156)\tRec loss 3965.9985 (3809.7888)\tnorm 0.8846 (0.8733)\n",
            "Epoch: [149][170/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2414 (0.2300)\tgrad(D) penalty 0.0140 (0.0156)\tRec loss 3619.3735 (3811.9848)\tnorm 0.8849 (0.8734)\n",
            "Epoch: [149][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2275 (0.2295)\tgrad(D) penalty 0.0192 (0.0157)\tRec loss 4156.6504 (3816.6367)\tnorm 0.8651 (0.8730)\n",
            "Epoch: [149][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7714 (0.7716)\tD(fake) 0.2227 (0.2293)\tgrad(D) penalty 0.0193 (0.0158)\tRec loss 3574.1094 (3813.3497)\tnorm 0.8858 (0.8729)\n",
            "Epoch: [150][  0/195]\tTime  0.426 ( 0.426)\tData  0.248 ( 0.248)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3526.7769 (3526.7769)\tnorm 0.8754 (0.8754)\n",
            "Epoch: [150][ 10/195]\tTime  0.150 ( 0.176)\tData  0.000 ( 0.023)\tD(real) 0.7716 (0.7716)\tD(fake) 0.2441 (0.2407)\tgrad(D) penalty 0.0136 (0.0159)\tRec loss 3953.7021 (3754.0368)\tnorm 0.8707 (0.8714)\n",
            "Epoch: [150][ 20/195]\tTime  0.147 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2369 (0.2298)\tgrad(D) penalty 0.0154 (0.0156)\tRec loss 3948.7380 (3761.4913)\tnorm 0.8701 (0.8724)\n",
            "Epoch: [150][ 30/195]\tTime  0.147 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7715 (0.7717)\tD(fake) 0.2086 (0.2276)\tgrad(D) penalty 0.0177 (0.0154)\tRec loss 3556.9373 (3769.8238)\tnorm 0.8816 (0.8745)\n",
            "Epoch: [150][ 40/195]\tTime  0.146 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2275 (0.2286)\tgrad(D) penalty 0.0167 (0.0157)\tRec loss 4125.5703 (3786.0695)\tnorm 0.8728 (0.8749)\n",
            "Epoch: [150][ 50/195]\tTime  0.164 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2256 (0.2277)\tgrad(D) penalty 0.0184 (0.0162)\tRec loss 3948.6206 (3791.3689)\tnorm 0.8705 (0.8756)\n",
            "Epoch: [150][ 60/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2283 (0.2290)\tgrad(D) penalty 0.0166 (0.0162)\tRec loss 3745.9990 (3787.1618)\tnorm 0.8701 (0.8752)\n",
            "Epoch: [150][ 70/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2268 (0.2288)\tgrad(D) penalty 0.0167 (0.0160)\tRec loss 3788.0344 (3788.9239)\tnorm 0.8784 (0.8752)\n",
            "Epoch: [150][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2077 (0.2286)\tgrad(D) penalty 0.0148 (0.0156)\tRec loss 3965.4558 (3805.3239)\tnorm 0.8729 (0.8752)\n",
            "Epoch: [150][ 90/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2404 (0.2294)\tgrad(D) penalty 0.0122 (0.0155)\tRec loss 3938.0232 (3813.5556)\tnorm 0.8595 (0.8749)\n",
            "Epoch: [150][100/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2344 (0.2291)\tgrad(D) penalty 0.0156 (0.0155)\tRec loss 3945.5894 (3809.3128)\tnorm 0.8765 (0.8746)\n",
            "Epoch: [150][110/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2187 (0.2291)\tgrad(D) penalty 0.0164 (0.0155)\tRec loss 3962.6912 (3802.1718)\tnorm 0.8727 (0.8743)\n",
            "Epoch: [150][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2596 (0.2302)\tgrad(D) penalty 0.0125 (0.0154)\tRec loss 3924.4600 (3811.3904)\tnorm 0.8906 (0.8741)\n",
            "Epoch: [150][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2098 (0.2294)\tgrad(D) penalty 0.0197 (0.0155)\tRec loss 3734.7600 (3810.8056)\tnorm 0.8738 (0.8744)\n",
            "Epoch: [150][140/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2265 (0.2297)\tgrad(D) penalty 0.0151 (0.0155)\tRec loss 3854.7173 (3811.1695)\tnorm 0.8673 (0.8742)\n",
            "Epoch: [150][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2110 (0.2285)\tgrad(D) penalty 0.0206 (0.0157)\tRec loss 4016.9722 (3810.2603)\tnorm 0.8728 (0.8738)\n",
            "Epoch: [150][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2129 (0.2289)\tgrad(D) penalty 0.0152 (0.0157)\tRec loss 3839.1343 (3806.1754)\tnorm 0.8659 (0.8739)\n",
            "Epoch: [150][170/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7717)\tD(fake) 0.2340 (0.2286)\tgrad(D) penalty 0.0152 (0.0157)\tRec loss 3917.2329 (3810.6165)\tnorm 0.8831 (0.8737)\n",
            "Epoch: [150][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2247 (0.2286)\tgrad(D) penalty 0.0139 (0.0156)\tRec loss 4268.1816 (3811.3734)\tnorm 0.8720 (0.8738)\n",
            "Epoch: [150][190/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2292 (0.2285)\tgrad(D) penalty 0.0137 (0.0156)\tRec loss 3534.0745 (3811.1650)\tnorm 0.8773 (0.8740)\n",
            "Epoch: [151][  0/195]\tTime  0.432 ( 0.432)\tData  0.240 ( 0.240)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3718.6931 (3718.6931)\tnorm 0.8695 (0.8695)\n",
            "Epoch: [151][ 10/195]\tTime  0.148 ( 0.175)\tData  0.000 ( 0.022)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2287 (0.2193)\tgrad(D) penalty 0.0132 (0.0147)\tRec loss 3737.1587 (3789.9142)\tnorm 0.8585 (0.8773)\n",
            "Epoch: [151][ 20/195]\tTime  0.146 ( 0.161)\tData  0.000 ( 0.012)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2166 (0.2277)\tgrad(D) penalty 0.0155 (0.0155)\tRec loss 3983.0200 (3792.4644)\tnorm 0.8757 (0.8758)\n",
            "Epoch: [151][ 30/195]\tTime  0.146 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2478 (0.2319)\tgrad(D) penalty 0.0138 (0.0152)\tRec loss 3985.6504 (3795.2688)\tnorm 0.8621 (0.8749)\n",
            "Epoch: [151][ 40/195]\tTime  0.154 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2410 (0.2310)\tgrad(D) penalty 0.0141 (0.0154)\tRec loss 3839.6833 (3792.1669)\tnorm 0.8626 (0.8745)\n",
            "Epoch: [151][ 50/195]\tTime  0.168 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2264 (0.2298)\tgrad(D) penalty 0.0150 (0.0155)\tRec loss 3612.6353 (3785.4614)\tnorm 0.8661 (0.8746)\n",
            "Epoch: [151][ 60/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2192 (0.2299)\tgrad(D) penalty 0.0160 (0.0158)\tRec loss 3757.2522 (3772.7790)\tnorm 0.8795 (0.8745)\n",
            "Epoch: [151][ 70/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2486 (0.2296)\tgrad(D) penalty 0.0170 (0.0162)\tRec loss 3898.3811 (3774.3038)\tnorm 0.8791 (0.8745)\n",
            "Epoch: [151][ 80/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2211 (0.2299)\tgrad(D) penalty 0.0160 (0.0159)\tRec loss 3856.7095 (3788.8527)\tnorm 0.8702 (0.8745)\n",
            "Epoch: [151][ 90/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7714 (0.7717)\tD(fake) 0.2374 (0.2313)\tgrad(D) penalty 0.0139 (0.0158)\tRec loss 3820.3965 (3798.1548)\tnorm 0.8744 (0.8745)\n",
            "Epoch: [151][100/195]\tTime  0.171 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2457 (0.2313)\tgrad(D) penalty 0.0141 (0.0156)\tRec loss 3807.2942 (3792.6357)\tnorm 0.8753 (0.8746)\n",
            "Epoch: [151][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2202 (0.2308)\tgrad(D) penalty 0.0160 (0.0156)\tRec loss 3751.3313 (3787.3670)\tnorm 0.8722 (0.8744)\n",
            "Epoch: [151][120/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2180 (0.2304)\tgrad(D) penalty 0.0159 (0.0156)\tRec loss 3700.2339 (3783.4643)\tnorm 0.8783 (0.8742)\n",
            "Epoch: [151][130/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2407 (0.2305)\tgrad(D) penalty 0.0151 (0.0157)\tRec loss 4031.1309 (3787.1901)\tnorm 0.8704 (0.8739)\n",
            "Epoch: [151][140/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2189 (0.2299)\tgrad(D) penalty 0.0195 (0.0157)\tRec loss 3656.4460 (3787.9129)\tnorm 0.8613 (0.8738)\n",
            "Epoch: [151][150/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2320 (0.2305)\tgrad(D) penalty 0.0144 (0.0156)\tRec loss 3854.5876 (3793.8943)\tnorm 0.8751 (0.8735)\n",
            "Epoch: [151][160/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2187 (0.2301)\tgrad(D) penalty 0.0156 (0.0156)\tRec loss 3775.3120 (3802.1813)\tnorm 0.8763 (0.8741)\n",
            "Epoch: [151][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2277 (0.2295)\tgrad(D) penalty 0.0152 (0.0155)\tRec loss 3853.8552 (3799.8260)\tnorm 0.8713 (0.8743)\n",
            "Epoch: [151][180/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2204 (0.2297)\tgrad(D) penalty 0.0136 (0.0155)\tRec loss 3880.9910 (3806.4766)\tnorm 0.8763 (0.8744)\n",
            "Epoch: [151][190/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2319 (0.2299)\tgrad(D) penalty 0.0125 (0.0154)\tRec loss 3856.0078 (3810.2283)\tnorm 0.8817 (0.8743)\n",
            "Epoch: [152][  0/195]\tTime  0.430 ( 0.430)\tData  0.239 ( 0.239)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3786.7593 (3786.7593)\tnorm 0.8679 (0.8679)\n",
            "Epoch: [152][ 10/195]\tTime  0.150 ( 0.175)\tData  0.000 ( 0.022)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2296 (0.2282)\tgrad(D) penalty 0.0158 (0.0163)\tRec loss 3750.5732 (3810.1584)\tnorm 0.8801 (0.8728)\n",
            "Epoch: [152][ 20/195]\tTime  0.146 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2281 (0.2267)\tgrad(D) penalty 0.0156 (0.0156)\tRec loss 3588.0295 (3814.6409)\tnorm 0.8757 (0.8735)\n",
            "Epoch: [152][ 30/195]\tTime  0.147 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2242 (0.2243)\tgrad(D) penalty 0.0117 (0.0148)\tRec loss 3819.8672 (3807.7156)\tnorm 0.8746 (0.8738)\n",
            "Epoch: [152][ 40/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7715 (0.7717)\tD(fake) 0.2348 (0.2247)\tgrad(D) penalty 0.0155 (0.0150)\tRec loss 3850.1926 (3811.3175)\tnorm 0.8638 (0.8744)\n",
            "Epoch: [152][ 50/195]\tTime  0.170 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2246 (0.2241)\tgrad(D) penalty 0.0157 (0.0150)\tRec loss 3716.7456 (3810.9964)\tnorm 0.8827 (0.8746)\n",
            "Epoch: [152][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2149 (0.2257)\tgrad(D) penalty 0.0152 (0.0150)\tRec loss 3771.2471 (3802.6056)\tnorm 0.8755 (0.8748)\n",
            "Epoch: [152][ 70/195]\tTime  0.153 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7715 (0.7717)\tD(fake) 0.2291 (0.2255)\tgrad(D) penalty 0.0173 (0.0152)\tRec loss 3841.2339 (3797.8597)\tnorm 0.8710 (0.8743)\n",
            "Epoch: [152][ 80/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2363 (0.2267)\tgrad(D) penalty 0.0172 (0.0154)\tRec loss 3978.1001 (3797.1197)\tnorm 0.8645 (0.8739)\n",
            "Epoch: [152][ 90/195]\tTime  0.158 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2141 (0.2263)\tgrad(D) penalty 0.0165 (0.0155)\tRec loss 4041.9458 (3796.7808)\tnorm 0.8704 (0.8737)\n",
            "Epoch: [152][100/195]\tTime  0.164 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7716 (0.7717)\tD(fake) 0.2329 (0.2260)\tgrad(D) penalty 0.0169 (0.0158)\tRec loss 3864.0952 (3797.7405)\tnorm 0.8615 (0.8739)\n",
            "Epoch: [152][110/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2177 (0.2252)\tgrad(D) penalty 0.0181 (0.0159)\tRec loss 4072.7302 (3800.1764)\tnorm 0.8842 (0.8741)\n",
            "Epoch: [152][120/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7717)\tD(fake) 0.2064 (0.2260)\tgrad(D) penalty 0.0178 (0.0158)\tRec loss 3751.1980 (3801.7346)\tnorm 0.8899 (0.8744)\n",
            "Epoch: [152][130/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2466 (0.2271)\tgrad(D) penalty 0.0137 (0.0157)\tRec loss 3940.2451 (3806.1736)\tnorm 0.8767 (0.8745)\n",
            "Epoch: [152][140/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7717)\tD(fake) 0.2573 (0.2274)\tgrad(D) penalty 0.0140 (0.0157)\tRec loss 3930.5552 (3806.7301)\tnorm 0.8676 (0.8743)\n",
            "Epoch: [152][150/195]\tTime  0.168 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7717)\tD(fake) 0.2161 (0.2276)\tgrad(D) penalty 0.0157 (0.0156)\tRec loss 3798.4402 (3803.7724)\tnorm 0.8797 (0.8742)\n",
            "Epoch: [152][160/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7717)\tD(fake) 0.2337 (0.2279)\tgrad(D) penalty 0.0138 (0.0157)\tRec loss 4090.7839 (3808.1580)\tnorm 0.8920 (0.8741)\n",
            "Epoch: [152][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2353 (0.2278)\tgrad(D) penalty 0.0156 (0.0158)\tRec loss 3852.5059 (3806.6020)\tnorm 0.8608 (0.8741)\n",
            "Epoch: [152][180/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7717)\tD(fake) 0.2411 (0.2278)\tgrad(D) penalty 0.0160 (0.0158)\tRec loss 3824.6187 (3806.1735)\tnorm 0.8842 (0.8742)\n",
            "Epoch: [152][190/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7719 (0.7717)\tD(fake) 0.2243 (0.2278)\tgrad(D) penalty 0.0142 (0.0158)\tRec loss 3757.7671 (3804.5130)\tnorm 0.8647 (0.8740)\n",
            "Epoch: [153][  0/195]\tTime  0.433 ( 0.433)\tData  0.238 ( 0.238)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3895.3367 (3895.3367)\tnorm 0.8899 (0.8899)\n",
            "Epoch: [153][ 10/195]\tTime  0.150 ( 0.174)\tData  0.000 ( 0.022)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2398 (0.2316)\tgrad(D) penalty 0.0150 (0.0153)\tRec loss 3965.5667 (3743.2996)\tnorm 0.8519 (0.8773)\n",
            "Epoch: [153][ 20/195]\tTime  0.151 ( 0.161)\tData  0.000 ( 0.012)\tD(real) 0.7715 (0.7717)\tD(fake) 0.2280 (0.2318)\tgrad(D) penalty 0.0122 (0.0140)\tRec loss 3619.6340 (3758.3506)\tnorm 0.8758 (0.8743)\n",
            "Epoch: [153][ 30/195]\tTime  0.146 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2362 (0.2309)\tgrad(D) penalty 0.0189 (0.0149)\tRec loss 3988.5745 (3789.2136)\tnorm 0.8732 (0.8742)\n",
            "Epoch: [153][ 40/195]\tTime  0.150 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2230 (0.2281)\tgrad(D) penalty 0.0146 (0.0151)\tRec loss 3611.8254 (3805.8058)\tnorm 0.8889 (0.8744)\n",
            "Epoch: [153][ 50/195]\tTime  0.162 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2319 (0.2286)\tgrad(D) penalty 0.0159 (0.0149)\tRec loss 3634.8184 (3796.8543)\tnorm 0.8654 (0.8751)\n",
            "Epoch: [153][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2193 (0.2287)\tgrad(D) penalty 0.0150 (0.0150)\tRec loss 3724.0024 (3796.1285)\tnorm 0.8832 (0.8755)\n",
            "Epoch: [153][ 70/195]\tTime  0.153 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7715 (0.7718)\tD(fake) 0.2361 (0.2295)\tgrad(D) penalty 0.0165 (0.0152)\tRec loss 3838.8862 (3799.1848)\tnorm 0.8774 (0.8754)\n",
            "Epoch: [153][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2228 (0.2298)\tgrad(D) penalty 0.0128 (0.0153)\tRec loss 3793.3765 (3800.2536)\tnorm 0.8842 (0.8756)\n",
            "Epoch: [153][ 90/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2425 (0.2306)\tgrad(D) penalty 0.0131 (0.0152)\tRec loss 3673.4587 (3790.9203)\tnorm 0.8825 (0.8755)\n",
            "Epoch: [153][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7720 (0.7718)\tD(fake) 0.2358 (0.2305)\tgrad(D) penalty 0.0144 (0.0154)\tRec loss 3720.1458 (3794.0296)\tnorm 0.8813 (0.8748)\n",
            "Epoch: [153][110/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2269 (0.2303)\tgrad(D) penalty 0.0164 (0.0154)\tRec loss 3962.6448 (3793.3895)\tnorm 0.8669 (0.8742)\n",
            "Epoch: [153][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2475 (0.2306)\tgrad(D) penalty 0.0129 (0.0155)\tRec loss 3865.0486 (3799.5925)\tnorm 0.8663 (0.8738)\n",
            "Epoch: [153][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2242 (0.2300)\tgrad(D) penalty 0.0201 (0.0156)\tRec loss 3994.8772 (3801.5727)\tnorm 0.8861 (0.8740)\n",
            "Epoch: [153][140/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2419 (0.2306)\tgrad(D) penalty 0.0153 (0.0156)\tRec loss 3872.1677 (3803.1068)\tnorm 0.8640 (0.8734)\n",
            "Epoch: [153][150/195]\tTime  0.167 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2300 (0.2306)\tgrad(D) penalty 0.0126 (0.0155)\tRec loss 3857.1709 (3799.9007)\tnorm 0.8700 (0.8735)\n",
            "Epoch: [153][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2313 (0.2306)\tgrad(D) penalty 0.0127 (0.0154)\tRec loss 3999.0002 (3802.6870)\tnorm 0.8542 (0.8732)\n",
            "Epoch: [153][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2221 (0.2298)\tgrad(D) penalty 0.0165 (0.0154)\tRec loss 3719.6987 (3804.2722)\tnorm 0.8536 (0.8728)\n",
            "Epoch: [153][180/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7718)\tD(fake) 0.2123 (0.2298)\tgrad(D) penalty 0.0175 (0.0154)\tRec loss 4090.0103 (3807.7623)\tnorm 0.8770 (0.8727)\n",
            "Epoch: [153][190/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2449 (0.2300)\tgrad(D) penalty 0.0159 (0.0155)\tRec loss 3755.0471 (3807.1141)\tnorm 0.8669 (0.8724)\n",
            "Epoch: [154][  0/195]\tTime  0.437 ( 0.437)\tData  0.244 ( 0.244)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3932.3979 (3932.3979)\tnorm 0.8728 (0.8728)\n",
            "Epoch: [154][ 10/195]\tTime  0.146 ( 0.176)\tData  0.000 ( 0.022)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2440 (0.2355)\tgrad(D) penalty 0.0159 (0.0156)\tRec loss 4077.0444 (3760.7568)\tnorm 0.8695 (0.8663)\n",
            "Epoch: [154][ 20/195]\tTime  0.151 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2273 (0.2262)\tgrad(D) penalty 0.0169 (0.0169)\tRec loss 3646.7336 (3749.1038)\tnorm 0.8717 (0.8672)\n",
            "Epoch: [154][ 30/195]\tTime  0.148 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7719 (0.7719)\tD(fake) 0.2396 (0.2306)\tgrad(D) penalty 0.0164 (0.0167)\tRec loss 3608.9998 (3767.1884)\tnorm 0.8731 (0.8683)\n",
            "Epoch: [154][ 40/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7715 (0.7718)\tD(fake) 0.2532 (0.2309)\tgrad(D) penalty 0.0124 (0.0162)\tRec loss 3872.0298 (3763.4933)\tnorm 0.8789 (0.8709)\n",
            "Epoch: [154][ 50/195]\tTime  0.164 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2086 (0.2280)\tgrad(D) penalty 0.0177 (0.0160)\tRec loss 4043.9014 (3767.5627)\tnorm 0.8772 (0.8712)\n",
            "Epoch: [154][ 60/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7716 (0.7718)\tD(fake) 0.2204 (0.2292)\tgrad(D) penalty 0.0124 (0.0152)\tRec loss 3728.8816 (3769.9055)\tnorm 0.8759 (0.8720)\n",
            "Epoch: [154][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2330 (0.2287)\tgrad(D) penalty 0.0170 (0.0153)\tRec loss 3860.5161 (3781.1237)\tnorm 0.8728 (0.8714)\n",
            "Epoch: [154][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2215 (0.2288)\tgrad(D) penalty 0.0169 (0.0153)\tRec loss 4012.7188 (3788.5336)\tnorm 0.8745 (0.8720)\n",
            "Epoch: [154][ 90/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2225 (0.2287)\tgrad(D) penalty 0.0191 (0.0154)\tRec loss 4131.8896 (3789.9790)\tnorm 0.8765 (0.8730)\n",
            "Epoch: [154][100/195]\tTime  0.164 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2306 (0.2286)\tgrad(D) penalty 0.0152 (0.0154)\tRec loss 3716.7139 (3791.7032)\tnorm 0.8784 (0.8731)\n",
            "Epoch: [154][110/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2354 (0.2290)\tgrad(D) penalty 0.0171 (0.0155)\tRec loss 3677.9187 (3788.0274)\tnorm 0.8703 (0.8730)\n",
            "Epoch: [154][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2311 (0.2289)\tgrad(D) penalty 0.0171 (0.0156)\tRec loss 3851.4692 (3789.2662)\tnorm 0.8819 (0.8733)\n",
            "Epoch: [154][130/195]\tTime  0.155 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2248 (0.2291)\tgrad(D) penalty 0.0172 (0.0156)\tRec loss 3848.9834 (3794.2298)\tnorm 0.8595 (0.8731)\n",
            "Epoch: [154][140/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2349 (0.2292)\tgrad(D) penalty 0.0159 (0.0155)\tRec loss 3670.4736 (3798.8385)\tnorm 0.8767 (0.8730)\n",
            "Epoch: [154][150/195]\tTime  0.168 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2179 (0.2286)\tgrad(D) penalty 0.0130 (0.0154)\tRec loss 4133.6250 (3805.2236)\tnorm 0.8769 (0.8733)\n",
            "Epoch: [154][160/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7718)\tD(fake) 0.2186 (0.2286)\tgrad(D) penalty 0.0154 (0.0153)\tRec loss 3807.9231 (3805.9815)\tnorm 0.8733 (0.8732)\n",
            "Epoch: [154][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2438 (0.2290)\tgrad(D) penalty 0.0171 (0.0153)\tRec loss 3929.1965 (3805.7459)\tnorm 0.8723 (0.8733)\n",
            "Epoch: [154][180/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2294 (0.2288)\tgrad(D) penalty 0.0115 (0.0152)\tRec loss 3831.7070 (3802.7319)\tnorm 0.8723 (0.8733)\n",
            "Epoch: [154][190/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2109 (0.2283)\tgrad(D) penalty 0.0146 (0.0152)\tRec loss 3692.0000 (3800.9791)\tnorm 0.8713 (0.8732)\n",
            "Epoch: [155][  0/195]\tTime  0.430 ( 0.430)\tData  0.245 ( 0.245)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3884.7275 (3884.7275)\tnorm 0.8749 (0.8749)\n",
            "Epoch: [155][ 10/195]\tTime  0.145 ( 0.175)\tData  0.000 ( 0.023)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2200 (0.2187)\tgrad(D) penalty 0.0157 (0.0174)\tRec loss 3748.7051 (3841.0837)\tnorm 0.8681 (0.8725)\n",
            "Epoch: [155][ 20/195]\tTime  0.147 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2173 (0.2184)\tgrad(D) penalty 0.0208 (0.0184)\tRec loss 3760.1753 (3840.2122)\tnorm 0.8736 (0.8721)\n",
            "Epoch: [155][ 30/195]\tTime  0.145 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2195 (0.2201)\tgrad(D) penalty 0.0167 (0.0185)\tRec loss 3868.4504 (3838.0832)\tnorm 0.8790 (0.8747)\n",
            "Epoch: [155][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2372 (0.2220)\tgrad(D) penalty 0.0143 (0.0175)\tRec loss 4055.9106 (3844.8150)\tnorm 0.8734 (0.8758)\n",
            "Epoch: [155][ 50/195]\tTime  0.164 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2078 (0.2205)\tgrad(D) penalty 0.0177 (0.0173)\tRec loss 3813.2925 (3814.1422)\tnorm 0.8651 (0.8758)\n",
            "Epoch: [155][ 60/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2301 (0.2225)\tgrad(D) penalty 0.0127 (0.0164)\tRec loss 3881.4941 (3801.9472)\tnorm 0.8702 (0.8753)\n",
            "Epoch: [155][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2238 (0.2236)\tgrad(D) penalty 0.0157 (0.0161)\tRec loss 3716.3042 (3791.3285)\tnorm 0.8720 (0.8752)\n",
            "Epoch: [155][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2178 (0.2241)\tgrad(D) penalty 0.0138 (0.0159)\tRec loss 3621.6221 (3794.3506)\tnorm 0.8768 (0.8752)\n",
            "Epoch: [155][ 90/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2374 (0.2250)\tgrad(D) penalty 0.0137 (0.0158)\tRec loss 3842.4685 (3790.2598)\tnorm 0.8662 (0.8753)\n",
            "Epoch: [155][100/195]\tTime  0.171 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2341 (0.2253)\tgrad(D) penalty 0.0156 (0.0157)\tRec loss 3798.2715 (3789.7396)\tnorm 0.8747 (0.8750)\n",
            "Epoch: [155][110/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2364 (0.2263)\tgrad(D) penalty 0.0163 (0.0159)\tRec loss 3935.7729 (3796.5706)\tnorm 0.8764 (0.8751)\n",
            "Epoch: [155][120/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2483 (0.2268)\tgrad(D) penalty 0.0145 (0.0159)\tRec loss 3865.0601 (3794.8148)\tnorm 0.8717 (0.8753)\n",
            "Epoch: [155][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7718)\tD(fake) 0.2343 (0.2267)\tgrad(D) penalty 0.0169 (0.0159)\tRec loss 3930.9785 (3797.2322)\tnorm 0.8802 (0.8755)\n",
            "Epoch: [155][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2350 (0.2277)\tgrad(D) penalty 0.0189 (0.0160)\tRec loss 3656.8169 (3798.1163)\tnorm 0.8601 (0.8752)\n",
            "Epoch: [155][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7718)\tD(fake) 0.2313 (0.2281)\tgrad(D) penalty 0.0162 (0.0160)\tRec loss 4112.6675 (3796.9433)\tnorm 0.8785 (0.8750)\n",
            "Epoch: [155][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2342 (0.2289)\tgrad(D) penalty 0.0141 (0.0158)\tRec loss 3762.5823 (3797.0629)\tnorm 0.8671 (0.8746)\n",
            "Epoch: [155][170/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7718)\tD(fake) 0.2409 (0.2286)\tgrad(D) penalty 0.0134 (0.0157)\tRec loss 3664.7659 (3795.5665)\tnorm 0.8765 (0.8744)\n",
            "Epoch: [155][180/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2052 (0.2283)\tgrad(D) penalty 0.0181 (0.0157)\tRec loss 3731.8201 (3797.5779)\tnorm 0.8714 (0.8742)\n",
            "Epoch: [155][190/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2440 (0.2292)\tgrad(D) penalty 0.0132 (0.0156)\tRec loss 3849.7346 (3799.1450)\tnorm 0.8550 (0.8740)\n",
            "Epoch: [156][  0/195]\tTime  0.440 ( 0.440)\tData  0.235 ( 0.235)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3892.4146 (3892.4146)\tnorm 0.8877 (0.8877)\n",
            "Epoch: [156][ 10/195]\tTime  0.146 ( 0.175)\tData  0.000 ( 0.022)\tD(real) 0.7719 (0.7719)\tD(fake) 0.2735 (0.2767)\tgrad(D) penalty 0.0173 (0.0164)\tRec loss 4020.9932 (3743.3007)\tnorm 0.8864 (0.8782)\n",
            "Epoch: [156][ 20/195]\tTime  0.151 ( 0.163)\tData  0.000 ( 0.011)\tD(real) 0.7719 (0.7720)\tD(fake) 0.2429 (0.2446)\tgrad(D) penalty 0.0166 (0.0170)\tRec loss 3836.4900 (3719.9140)\tnorm 0.8705 (0.8756)\n",
            "Epoch: [156][ 30/195]\tTime  0.149 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7718 (0.7719)\tD(fake) 0.2371 (0.2464)\tgrad(D) penalty 0.0162 (0.0165)\tRec loss 3914.8611 (3750.5681)\tnorm 0.8799 (0.8743)\n",
            "Epoch: [156][ 40/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7718 (0.7719)\tD(fake) 0.2472 (0.2410)\tgrad(D) penalty 0.0130 (0.0165)\tRec loss 3836.3750 (3754.3066)\tnorm 0.8573 (0.8726)\n",
            "Epoch: [156][ 50/195]\tTime  0.170 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7719 (0.7719)\tD(fake) 0.2233 (0.2386)\tgrad(D) penalty 0.0181 (0.0167)\tRec loss 3414.2793 (3751.1966)\tnorm 0.8723 (0.8725)\n",
            "Epoch: [156][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2269 (0.2375)\tgrad(D) penalty 0.0148 (0.0165)\tRec loss 3841.0703 (3758.8809)\tnorm 0.8644 (0.8719)\n",
            "Epoch: [156][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7718 (0.7718)\tD(fake) 0.2384 (0.2366)\tgrad(D) penalty 0.0171 (0.0166)\tRec loss 3762.3125 (3769.0728)\tnorm 0.8729 (0.8721)\n",
            "Epoch: [156][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7716 (0.7718)\tD(fake) 0.2337 (0.2342)\tgrad(D) penalty 0.0143 (0.0163)\tRec loss 3814.7434 (3770.1179)\tnorm 0.8608 (0.8718)\n",
            "Epoch: [156][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2134 (0.2335)\tgrad(D) penalty 0.0178 (0.0163)\tRec loss 3760.0479 (3776.9979)\tnorm 0.8744 (0.8721)\n",
            "Epoch: [156][100/195]\tTime  0.164 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2211 (0.2321)\tgrad(D) penalty 0.0126 (0.0160)\tRec loss 3970.5249 (3780.1037)\tnorm 0.8879 (0.8722)\n",
            "Epoch: [156][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2415 (0.2330)\tgrad(D) penalty 0.0121 (0.0160)\tRec loss 4051.0278 (3785.1074)\tnorm 0.8648 (0.8721)\n",
            "Epoch: [156][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7719)\tD(fake) 0.2525 (0.2322)\tgrad(D) penalty 0.0127 (0.0159)\tRec loss 3888.8220 (3786.3199)\tnorm 0.8821 (0.8724)\n",
            "Epoch: [156][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2114 (0.2316)\tgrad(D) penalty 0.0177 (0.0158)\tRec loss 3724.4858 (3789.3520)\tnorm 0.8610 (0.8723)\n",
            "Epoch: [156][140/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7719)\tD(fake) 0.2223 (0.2317)\tgrad(D) penalty 0.0171 (0.0158)\tRec loss 3669.8540 (3787.0067)\tnorm 0.8765 (0.8723)\n",
            "Epoch: [156][150/195]\tTime  0.162 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7719)\tD(fake) 0.2515 (0.2322)\tgrad(D) penalty 0.0165 (0.0158)\tRec loss 3963.6846 (3788.8917)\tnorm 0.8745 (0.8724)\n",
            "Epoch: [156][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7719)\tD(fake) 0.2308 (0.2324)\tgrad(D) penalty 0.0154 (0.0158)\tRec loss 3807.2559 (3795.4766)\tnorm 0.8759 (0.8721)\n",
            "Epoch: [156][170/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7719)\tD(fake) 0.2354 (0.2324)\tgrad(D) penalty 0.0168 (0.0159)\tRec loss 3621.0864 (3792.8918)\tnorm 0.8520 (0.8722)\n",
            "Epoch: [156][180/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7718)\tD(fake) 0.2345 (0.2320)\tgrad(D) penalty 0.0150 (0.0160)\tRec loss 3855.7100 (3793.6832)\tnorm 0.8878 (0.8723)\n",
            "Epoch: [156][190/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2133 (0.2317)\tgrad(D) penalty 0.0149 (0.0159)\tRec loss 3740.7151 (3797.1574)\tnorm 0.8610 (0.8721)\n",
            "Epoch: [157][  0/195]\tTime  0.425 ( 0.425)\tData  0.244 ( 0.244)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3719.4038 (3719.4038)\tnorm 0.8658 (0.8658)\n",
            "Epoch: [157][ 10/195]\tTime  0.146 ( 0.174)\tData  0.000 ( 0.022)\tD(real) 0.7717 (0.7717)\tD(fake) 0.2241 (0.2214)\tgrad(D) penalty 0.0151 (0.0159)\tRec loss 3908.7764 (3750.3160)\tnorm 0.8695 (0.8652)\n",
            "Epoch: [157][ 20/195]\tTime  0.155 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7719 (0.7718)\tD(fake) 0.2335 (0.2235)\tgrad(D) penalty 0.0156 (0.0162)\tRec loss 3616.8252 (3760.9703)\tnorm 0.8809 (0.8685)\n",
            "Epoch: [157][ 30/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2263 (0.2227)\tgrad(D) penalty 0.0145 (0.0153)\tRec loss 3649.8896 (3754.0608)\tnorm 0.8700 (0.8684)\n",
            "Epoch: [157][ 40/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7719 (0.7719)\tD(fake) 0.2450 (0.2247)\tgrad(D) penalty 0.0182 (0.0158)\tRec loss 3641.5605 (3761.7219)\tnorm 0.8793 (0.8691)\n",
            "Epoch: [157][ 50/195]\tTime  0.163 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2207 (0.2262)\tgrad(D) penalty 0.0154 (0.0156)\tRec loss 3711.5303 (3773.4715)\tnorm 0.8621 (0.8710)\n",
            "Epoch: [157][ 60/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2345 (0.2264)\tgrad(D) penalty 0.0154 (0.0156)\tRec loss 3792.7561 (3774.0570)\tnorm 0.8689 (0.8718)\n",
            "Epoch: [157][ 70/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7718 (0.7719)\tD(fake) 0.2262 (0.2259)\tgrad(D) penalty 0.0157 (0.0156)\tRec loss 3790.6641 (3776.7442)\tnorm 0.8739 (0.8724)\n",
            "Epoch: [157][ 80/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2432 (0.2260)\tgrad(D) penalty 0.0138 (0.0155)\tRec loss 4004.1091 (3778.5430)\tnorm 0.8625 (0.8723)\n",
            "Epoch: [157][ 90/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7718 (0.7719)\tD(fake) 0.2227 (0.2264)\tgrad(D) penalty 0.0161 (0.0154)\tRec loss 3699.6182 (3778.8870)\tnorm 0.8615 (0.8719)\n",
            "Epoch: [157][100/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2407 (0.2273)\tgrad(D) penalty 0.0172 (0.0157)\tRec loss 3886.5815 (3783.7377)\tnorm 0.8751 (0.8724)\n",
            "Epoch: [157][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7719)\tD(fake) 0.2250 (0.2275)\tgrad(D) penalty 0.0170 (0.0158)\tRec loss 3795.3984 (3789.1003)\tnorm 0.8786 (0.8727)\n",
            "Epoch: [157][120/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2390 (0.2275)\tgrad(D) penalty 0.0162 (0.0158)\tRec loss 3943.5991 (3799.6816)\tnorm 0.8765 (0.8729)\n",
            "Epoch: [157][130/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7719)\tD(fake) 0.2212 (0.2272)\tgrad(D) penalty 0.0164 (0.0159)\tRec loss 3906.5784 (3803.2389)\tnorm 0.8775 (0.8730)\n",
            "Epoch: [157][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7719)\tD(fake) 0.2296 (0.2278)\tgrad(D) penalty 0.0143 (0.0156)\tRec loss 3621.3408 (3795.1858)\tnorm 0.8941 (0.8732)\n",
            "Epoch: [157][150/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2480 (0.2284)\tgrad(D) penalty 0.0142 (0.0155)\tRec loss 4017.0894 (3798.1283)\tnorm 0.8864 (0.8734)\n",
            "Epoch: [157][160/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7718 (0.7719)\tD(fake) 0.2426 (0.2285)\tgrad(D) penalty 0.0142 (0.0155)\tRec loss 3794.2666 (3795.4356)\tnorm 0.8543 (0.8730)\n",
            "Epoch: [157][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2209 (0.2284)\tgrad(D) penalty 0.0196 (0.0156)\tRec loss 3730.9363 (3796.6900)\tnorm 0.8825 (0.8731)\n",
            "Epoch: [157][180/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7719)\tD(fake) 0.2353 (0.2294)\tgrad(D) penalty 0.0143 (0.0156)\tRec loss 3618.9404 (3794.7336)\tnorm 0.8712 (0.8728)\n",
            "Epoch: [157][190/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7717 (0.7719)\tD(fake) 0.2243 (0.2289)\tgrad(D) penalty 0.0170 (0.0157)\tRec loss 3762.8428 (3791.4065)\tnorm 0.8703 (0.8729)\n",
            "Epoch: [158][  0/195]\tTime  0.419 ( 0.419)\tData  0.222 ( 0.222)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3786.1294 (3786.1294)\tnorm 0.8707 (0.8707)\n",
            "Epoch: [158][ 10/195]\tTime  0.147 ( 0.174)\tData  0.000 ( 0.020)\tD(real) 0.7720 (0.7720)\tD(fake) 0.2673 (0.2545)\tgrad(D) penalty 0.0124 (0.0147)\tRec loss 3703.8818 (3820.8210)\tnorm 0.8960 (0.8790)\n",
            "Epoch: [158][ 20/195]\tTime  0.148 ( 0.161)\tData  0.000 ( 0.011)\tD(real) 0.7720 (0.7719)\tD(fake) 0.2437 (0.2364)\tgrad(D) penalty 0.0133 (0.0148)\tRec loss 3756.6174 (3813.3132)\tnorm 0.8821 (0.8802)\n",
            "Epoch: [158][ 30/195]\tTime  0.146 ( 0.157)\tData  0.000 ( 0.007)\tD(real) 0.7718 (0.7719)\tD(fake) 0.2021 (0.2305)\tgrad(D) penalty 0.0180 (0.0148)\tRec loss 3890.0288 (3794.1611)\tnorm 0.8788 (0.8770)\n",
            "Epoch: [158][ 40/195]\tTime  0.145 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7720 (0.7720)\tD(fake) 0.2182 (0.2290)\tgrad(D) penalty 0.0127 (0.0142)\tRec loss 3842.2202 (3787.5425)\tnorm 0.8747 (0.8759)\n",
            "Epoch: [158][ 50/195]\tTime  0.163 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7720 (0.7720)\tD(fake) 0.2237 (0.2288)\tgrad(D) penalty 0.0121 (0.0139)\tRec loss 3832.4749 (3775.1846)\tnorm 0.8686 (0.8757)\n",
            "Epoch: [158][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7720 (0.7720)\tD(fake) 0.2283 (0.2281)\tgrad(D) penalty 0.0146 (0.0139)\tRec loss 3547.0244 (3771.2795)\tnorm 0.8669 (0.8735)\n",
            "Epoch: [158][ 70/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7721 (0.7720)\tD(fake) 0.2330 (0.2291)\tgrad(D) penalty 0.0156 (0.0141)\tRec loss 3717.7012 (3769.8472)\tnorm 0.8714 (0.8731)\n",
            "Epoch: [158][ 80/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7720 (0.7720)\tD(fake) 0.2445 (0.2292)\tgrad(D) penalty 0.0181 (0.0145)\tRec loss 3669.9421 (3770.8321)\tnorm 0.8603 (0.8723)\n",
            "Epoch: [158][ 90/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7720 (0.7720)\tD(fake) 0.2136 (0.2288)\tgrad(D) penalty 0.0149 (0.0145)\tRec loss 4015.8887 (3774.6781)\tnorm 0.8554 (0.8715)\n",
            "Epoch: [158][100/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7720)\tD(fake) 0.2316 (0.2306)\tgrad(D) penalty 0.0156 (0.0147)\tRec loss 3789.6206 (3778.6380)\tnorm 0.8870 (0.8714)\n",
            "Epoch: [158][110/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7720)\tD(fake) 0.2239 (0.2291)\tgrad(D) penalty 0.0203 (0.0151)\tRec loss 3693.7698 (3770.7986)\tnorm 0.8770 (0.8714)\n",
            "Epoch: [158][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7720)\tD(fake) 0.1895 (0.2288)\tgrad(D) penalty 0.0190 (0.0153)\tRec loss 3716.2495 (3772.8336)\tnorm 0.8789 (0.8718)\n",
            "Epoch: [158][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7720)\tD(fake) 0.2492 (0.2287)\tgrad(D) penalty 0.0167 (0.0157)\tRec loss 3664.3271 (3778.1366)\tnorm 0.8874 (0.8720)\n",
            "Epoch: [158][140/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7720)\tD(fake) 0.2059 (0.2283)\tgrad(D) penalty 0.0181 (0.0158)\tRec loss 3727.0718 (3774.7626)\tnorm 0.8762 (0.8722)\n",
            "Epoch: [158][150/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7720)\tD(fake) 0.2651 (0.2295)\tgrad(D) penalty 0.0148 (0.0159)\tRec loss 4081.3936 (3781.0602)\tnorm 0.8773 (0.8721)\n",
            "Epoch: [158][160/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7721 (0.7720)\tD(fake) 0.2508 (0.2296)\tgrad(D) penalty 0.0129 (0.0158)\tRec loss 3701.5669 (3783.3422)\tnorm 0.8639 (0.8722)\n",
            "Epoch: [158][170/195]\tTime  0.155 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7721 (0.7720)\tD(fake) 0.2220 (0.2294)\tgrad(D) penalty 0.0139 (0.0156)\tRec loss 3678.1724 (3785.0640)\tnorm 0.8767 (0.8721)\n",
            "Epoch: [158][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7722 (0.7720)\tD(fake) 0.2052 (0.2293)\tgrad(D) penalty 0.0138 (0.0155)\tRec loss 3956.0518 (3790.7174)\tnorm 0.8637 (0.8723)\n",
            "Epoch: [158][190/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7721 (0.7720)\tD(fake) 0.2489 (0.2302)\tgrad(D) penalty 0.0122 (0.0154)\tRec loss 3895.5742 (3791.3989)\tnorm 0.8769 (0.8727)\n",
            "Epoch: [159][  0/195]\tTime  0.450 ( 0.450)\tData  0.249 ( 0.249)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3748.5952 (3748.5952)\tnorm 0.8845 (0.8845)\n",
            "Epoch: [159][ 10/195]\tTime  0.151 ( 0.177)\tData  0.000 ( 0.023)\tD(real) 0.7722 (0.7721)\tD(fake) 0.2351 (0.2420)\tgrad(D) penalty 0.0115 (0.0118)\tRec loss 3651.3882 (3684.2702)\tnorm 0.8811 (0.8726)\n",
            "Epoch: [159][ 20/195]\tTime  0.151 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7721 (0.7721)\tD(fake) 0.2506 (0.2341)\tgrad(D) penalty 0.0126 (0.0138)\tRec loss 4023.5649 (3757.8895)\tnorm 0.8625 (0.8716)\n",
            "Epoch: [159][ 30/195]\tTime  0.147 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7720 (0.7721)\tD(fake) 0.2237 (0.2304)\tgrad(D) penalty 0.0178 (0.0141)\tRec loss 3654.9961 (3751.1967)\tnorm 0.8781 (0.8714)\n",
            "Epoch: [159][ 40/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7721 (0.7721)\tD(fake) 0.2107 (0.2296)\tgrad(D) penalty 0.0145 (0.0143)\tRec loss 3588.0518 (3767.9842)\tnorm 0.8678 (0.8738)\n",
            "Epoch: [159][ 50/195]\tTime  0.162 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7719 (0.7721)\tD(fake) 0.2090 (0.2259)\tgrad(D) penalty 0.0175 (0.0148)\tRec loss 3900.2568 (3787.3372)\tnorm 0.8776 (0.8745)\n",
            "Epoch: [159][ 60/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7720 (0.7720)\tD(fake) 0.2429 (0.2277)\tgrad(D) penalty 0.0208 (0.0155)\tRec loss 3825.3853 (3797.1642)\tnorm 0.8667 (0.8742)\n",
            "Epoch: [159][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7720 (0.7720)\tD(fake) 0.2197 (0.2273)\tgrad(D) penalty 0.0150 (0.0155)\tRec loss 3795.0171 (3802.7503)\tnorm 0.8752 (0.8744)\n",
            "Epoch: [159][ 80/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7721 (0.7720)\tD(fake) 0.2525 (0.2287)\tgrad(D) penalty 0.0159 (0.0158)\tRec loss 4112.8374 (3794.6344)\tnorm 0.8775 (0.8743)\n",
            "Epoch: [159][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7720 (0.7720)\tD(fake) 0.2104 (0.2276)\tgrad(D) penalty 0.0180 (0.0161)\tRec loss 3743.7795 (3787.7860)\tnorm 0.8770 (0.8737)\n",
            "Epoch: [159][100/195]\tTime  0.168 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7720 (0.7720)\tD(fake) 0.1949 (0.2269)\tgrad(D) penalty 0.0163 (0.0161)\tRec loss 3997.3035 (3778.4255)\tnorm 0.8782 (0.8739)\n",
            "Epoch: [159][110/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7721 (0.7720)\tD(fake) 0.2327 (0.2277)\tgrad(D) penalty 0.0128 (0.0159)\tRec loss 3641.1213 (3775.0908)\tnorm 0.8552 (0.8735)\n",
            "Epoch: [159][120/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7721 (0.7720)\tD(fake) 0.2589 (0.2281)\tgrad(D) penalty 0.0139 (0.0159)\tRec loss 3974.8303 (3775.4102)\tnorm 0.8676 (0.8732)\n",
            "Epoch: [159][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7721 (0.7720)\tD(fake) 0.2194 (0.2276)\tgrad(D) penalty 0.0177 (0.0159)\tRec loss 3676.4651 (3776.0034)\tnorm 0.8752 (0.8733)\n",
            "Epoch: [159][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7720)\tD(fake) 0.2265 (0.2280)\tgrad(D) penalty 0.0181 (0.0160)\tRec loss 3811.3296 (3773.5321)\tnorm 0.8658 (0.8729)\n",
            "Epoch: [159][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7722 (0.7720)\tD(fake) 0.2282 (0.2276)\tgrad(D) penalty 0.0193 (0.0161)\tRec loss 4051.5281 (3782.9353)\tnorm 0.8688 (0.8728)\n",
            "Epoch: [159][160/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7721 (0.7720)\tD(fake) 0.2261 (0.2282)\tgrad(D) penalty 0.0171 (0.0161)\tRec loss 3949.7466 (3785.6978)\tnorm 0.8789 (0.8731)\n",
            "Epoch: [159][170/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7720)\tD(fake) 0.2335 (0.2284)\tgrad(D) penalty 0.0165 (0.0162)\tRec loss 4156.7539 (3786.2163)\tnorm 0.8546 (0.8728)\n",
            "Epoch: [159][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7716 (0.7720)\tD(fake) 0.2333 (0.2283)\tgrad(D) penalty 0.0126 (0.0160)\tRec loss 3794.5391 (3789.9910)\tnorm 0.8759 (0.8729)\n",
            "Epoch: [159][190/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7722 (0.7720)\tD(fake) 0.2536 (0.2287)\tgrad(D) penalty 0.0142 (0.0160)\tRec loss 3579.4824 (3790.1635)\tnorm 0.8821 (0.8733)\n",
            "Epoch: [160][  0/195]\tTime  0.430 ( 0.430)\tData  0.237 ( 0.237)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3831.2693 (3831.2693)\tnorm 0.8690 (0.8690)\n",
            "Epoch: [160][ 10/195]\tTime  0.156 ( 0.177)\tData  0.000 ( 0.022)\tD(real) 0.7721 (0.7721)\tD(fake) 0.2418 (0.2509)\tgrad(D) penalty 0.0113 (0.0116)\tRec loss 3784.7427 (3728.0619)\tnorm 0.8679 (0.8739)\n",
            "Epoch: [160][ 20/195]\tTime  0.151 ( 0.164)\tData  0.000 ( 0.012)\tD(real) 0.7722 (0.7721)\tD(fake) 0.2441 (0.2413)\tgrad(D) penalty 0.0129 (0.0132)\tRec loss 3937.6826 (3720.6456)\tnorm 0.8683 (0.8726)\n",
            "Epoch: [160][ 30/195]\tTime  0.147 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7721 (0.7721)\tD(fake) 0.2268 (0.2359)\tgrad(D) penalty 0.0142 (0.0135)\tRec loss 3774.5029 (3747.6640)\tnorm 0.8763 (0.8721)\n",
            "Epoch: [160][ 40/195]\tTime  0.151 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7721 (0.7721)\tD(fake) 0.2063 (0.2321)\tgrad(D) penalty 0.0185 (0.0140)\tRec loss 3719.0659 (3749.3977)\tnorm 0.8736 (0.8717)\n",
            "Epoch: [160][ 50/195]\tTime  0.165 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7722 (0.7721)\tD(fake) 0.2645 (0.2368)\tgrad(D) penalty 0.0149 (0.0141)\tRec loss 3731.2686 (3753.8807)\tnorm 0.8797 (0.8725)\n",
            "Epoch: [160][ 60/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7720 (0.7721)\tD(fake) 0.2291 (0.2344)\tgrad(D) penalty 0.0187 (0.0148)\tRec loss 3826.8999 (3758.9171)\tnorm 0.8710 (0.8723)\n",
            "Epoch: [160][ 70/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7722 (0.7721)\tD(fake) 0.2251 (0.2345)\tgrad(D) penalty 0.0172 (0.0150)\tRec loss 3865.8438 (3755.7242)\tnorm 0.8738 (0.8717)\n",
            "Epoch: [160][ 80/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7721 (0.7721)\tD(fake) 0.2209 (0.2322)\tgrad(D) penalty 0.0191 (0.0157)\tRec loss 3652.4575 (3761.6966)\tnorm 0.8921 (0.8716)\n",
            "Epoch: [160][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7721 (0.7721)\tD(fake) 0.2297 (0.2323)\tgrad(D) penalty 0.0161 (0.0159)\tRec loss 4358.0664 (3778.8134)\tnorm 0.8877 (0.8718)\n",
            "Epoch: [160][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7722 (0.7721)\tD(fake) 0.2469 (0.2318)\tgrad(D) penalty 0.0153 (0.0159)\tRec loss 3670.5771 (3785.5047)\tnorm 0.8752 (0.8718)\n",
            "Epoch: [160][110/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7721)\tD(fake) 0.2082 (0.2307)\tgrad(D) penalty 0.0168 (0.0158)\tRec loss 4040.2593 (3789.5976)\tnorm 0.8672 (0.8719)\n",
            "Epoch: [160][120/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7721)\tD(fake) 0.2164 (0.2309)\tgrad(D) penalty 0.0155 (0.0156)\tRec loss 3542.2778 (3785.0363)\tnorm 0.8791 (0.8724)\n",
            "Epoch: [160][130/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7721)\tD(fake) 0.2506 (0.2315)\tgrad(D) penalty 0.0114 (0.0155)\tRec loss 3855.2795 (3790.9521)\tnorm 0.8754 (0.8725)\n",
            "Epoch: [160][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7720 (0.7721)\tD(fake) 0.2543 (0.2309)\tgrad(D) penalty 0.0095 (0.0154)\tRec loss 3658.5107 (3788.2562)\tnorm 0.8761 (0.8728)\n",
            "Epoch: [160][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7722 (0.7721)\tD(fake) 0.2044 (0.2297)\tgrad(D) penalty 0.0166 (0.0153)\tRec loss 3687.7297 (3787.8295)\tnorm 0.8725 (0.8728)\n",
            "Epoch: [160][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7722 (0.7721)\tD(fake) 0.2120 (0.2302)\tgrad(D) penalty 0.0173 (0.0152)\tRec loss 3442.7703 (3781.3013)\tnorm 0.8649 (0.8727)\n",
            "Epoch: [160][170/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7719 (0.7721)\tD(fake) 0.2343 (0.2300)\tgrad(D) penalty 0.0154 (0.0152)\tRec loss 4164.6963 (3780.8364)\tnorm 0.8648 (0.8726)\n",
            "Epoch: [160][180/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7721 (0.7721)\tD(fake) 0.2289 (0.2293)\tgrad(D) penalty 0.0175 (0.0153)\tRec loss 3787.6577 (3783.7130)\tnorm 0.8778 (0.8723)\n",
            "Epoch: [160][190/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7723 (0.7721)\tD(fake) 0.2217 (0.2292)\tgrad(D) penalty 0.0143 (0.0153)\tRec loss 3797.0820 (3785.7231)\tnorm 0.8689 (0.8727)\n",
            "Epoch: [161][  0/195]\tTime  0.444 ( 0.444)\tData  0.250 ( 0.250)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4162.3198 (4162.3198)\tnorm 0.8734 (0.8734)\n",
            "Epoch: [161][ 10/195]\tTime  0.146 ( 0.175)\tData  0.000 ( 0.023)\tD(real) 0.7722 (0.7721)\tD(fake) 0.2349 (0.2257)\tgrad(D) penalty 0.0168 (0.0180)\tRec loss 4005.7397 (3787.8444)\tnorm 0.8643 (0.8725)\n",
            "Epoch: [161][ 20/195]\tTime  0.146 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7720 (0.7721)\tD(fake) 0.2137 (0.2252)\tgrad(D) penalty 0.0199 (0.0174)\tRec loss 3823.1768 (3744.2724)\tnorm 0.8686 (0.8717)\n",
            "Epoch: [161][ 30/195]\tTime  0.148 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7722 (0.7721)\tD(fake) 0.2288 (0.2261)\tgrad(D) penalty 0.0145 (0.0166)\tRec loss 3702.5347 (3779.9086)\tnorm 0.8643 (0.8718)\n",
            "Epoch: [161][ 40/195]\tTime  0.160 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7721 (0.7721)\tD(fake) 0.2295 (0.2257)\tgrad(D) penalty 0.0173 (0.0166)\tRec loss 3733.4526 (3780.6977)\tnorm 0.8783 (0.8730)\n",
            "Epoch: [161][ 50/195]\tTime  0.167 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7720 (0.7721)\tD(fake) 0.2173 (0.2266)\tgrad(D) penalty 0.0156 (0.0164)\tRec loss 3913.8184 (3782.9030)\tnorm 0.8663 (0.8731)\n",
            "Epoch: [161][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7723 (0.7721)\tD(fake) 0.2396 (0.2279)\tgrad(D) penalty 0.0141 (0.0162)\tRec loss 3720.2251 (3786.3281)\tnorm 0.8676 (0.8737)\n",
            "Epoch: [161][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7719 (0.7721)\tD(fake) 0.2153 (0.2266)\tgrad(D) penalty 0.0145 (0.0160)\tRec loss 3791.8003 (3785.9418)\tnorm 0.8828 (0.8744)\n",
            "Epoch: [161][ 80/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7722 (0.7721)\tD(fake) 0.2088 (0.2256)\tgrad(D) penalty 0.0170 (0.0157)\tRec loss 3897.5747 (3783.1517)\tnorm 0.8818 (0.8746)\n",
            "Epoch: [161][ 90/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7723 (0.7722)\tD(fake) 0.2389 (0.2273)\tgrad(D) penalty 0.0122 (0.0154)\tRec loss 3721.6914 (3783.1899)\tnorm 0.8656 (0.8744)\n",
            "Epoch: [161][100/195]\tTime  0.164 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7721 (0.7722)\tD(fake) 0.2329 (0.2270)\tgrad(D) penalty 0.0124 (0.0152)\tRec loss 3906.9727 (3789.6941)\tnorm 0.8828 (0.8748)\n",
            "Epoch: [161][110/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7723 (0.7722)\tD(fake) 0.2198 (0.2265)\tgrad(D) penalty 0.0165 (0.0153)\tRec loss 3738.9800 (3796.9319)\tnorm 0.8646 (0.8747)\n",
            "Epoch: [161][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7723 (0.7722)\tD(fake) 0.2421 (0.2267)\tgrad(D) penalty 0.0158 (0.0156)\tRec loss 3728.2358 (3796.4783)\tnorm 0.8724 (0.8740)\n",
            "Epoch: [161][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7723 (0.7722)\tD(fake) 0.1981 (0.2254)\tgrad(D) penalty 0.0186 (0.0157)\tRec loss 3867.8770 (3790.4057)\tnorm 0.8731 (0.8738)\n",
            "Epoch: [161][140/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7721 (0.7722)\tD(fake) 0.2114 (0.2259)\tgrad(D) penalty 0.0140 (0.0156)\tRec loss 3695.8628 (3784.7395)\tnorm 0.8658 (0.8737)\n",
            "Epoch: [161][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7722 (0.7722)\tD(fake) 0.2336 (0.2260)\tgrad(D) penalty 0.0144 (0.0156)\tRec loss 3849.7744 (3785.4921)\tnorm 0.8712 (0.8734)\n",
            "Epoch: [161][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7724 (0.7722)\tD(fake) 0.2074 (0.2254)\tgrad(D) penalty 0.0196 (0.0156)\tRec loss 4075.3364 (3785.0701)\tnorm 0.8835 (0.8733)\n",
            "Epoch: [161][170/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7724 (0.7722)\tD(fake) 0.2320 (0.2261)\tgrad(D) penalty 0.0157 (0.0156)\tRec loss 3516.7849 (3782.3527)\tnorm 0.8801 (0.8731)\n",
            "Epoch: [161][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7724 (0.7722)\tD(fake) 0.2272 (0.2256)\tgrad(D) penalty 0.0156 (0.0157)\tRec loss 3961.7639 (3787.3990)\tnorm 0.8705 (0.8734)\n",
            "Epoch: [161][190/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7725 (0.7722)\tD(fake) 0.2284 (0.2256)\tgrad(D) penalty 0.0189 (0.0159)\tRec loss 3660.7070 (3787.2620)\tnorm 0.8946 (0.8734)\n",
            "Epoch: [162][  0/195]\tTime  0.415 ( 0.415)\tData  0.236 ( 0.236)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3815.5427 (3815.5427)\tnorm 0.8756 (0.8756)\n",
            "Epoch: [162][ 10/195]\tTime  0.151 ( 0.174)\tData  0.000 ( 0.022)\tD(real) 0.7725 (0.7725)\tD(fake) 0.2339 (0.2312)\tgrad(D) penalty 0.0171 (0.0192)\tRec loss 3890.9763 (3735.3378)\tnorm 0.8631 (0.8712)\n",
            "Epoch: [162][ 20/195]\tTime  0.150 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7725 (0.7725)\tD(fake) 0.2287 (0.2281)\tgrad(D) penalty 0.0171 (0.0176)\tRec loss 3926.5225 (3784.5769)\tnorm 0.8663 (0.8680)\n",
            "Epoch: [162][ 30/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7725 (0.7725)\tD(fake) 0.2256 (0.2287)\tgrad(D) penalty 0.0141 (0.0167)\tRec loss 3558.4370 (3764.5681)\tnorm 0.8750 (0.8695)\n",
            "Epoch: [162][ 40/195]\tTime  0.146 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7725 (0.7725)\tD(fake) 0.2227 (0.2293)\tgrad(D) penalty 0.0130 (0.0158)\tRec loss 3739.1577 (3766.7681)\tnorm 0.8678 (0.8695)\n",
            "Epoch: [162][ 50/195]\tTime  0.163 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7726 (0.7725)\tD(fake) 0.2298 (0.2301)\tgrad(D) penalty 0.0148 (0.0156)\tRec loss 3764.4387 (3765.3346)\tnorm 0.8725 (0.8704)\n",
            "Epoch: [162][ 60/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7726 (0.7725)\tD(fake) 0.2378 (0.2290)\tgrad(D) penalty 0.0120 (0.0152)\tRec loss 3952.0339 (3773.6612)\tnorm 0.8755 (0.8711)\n",
            "Epoch: [162][ 70/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7724 (0.7725)\tD(fake) 0.2064 (0.2284)\tgrad(D) penalty 0.0160 (0.0150)\tRec loss 3670.6143 (3767.1413)\tnorm 0.8723 (0.8714)\n",
            "Epoch: [162][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7725 (0.7725)\tD(fake) 0.2248 (0.2278)\tgrad(D) penalty 0.0126 (0.0150)\tRec loss 3923.4277 (3765.2914)\tnorm 0.8784 (0.8716)\n",
            "Epoch: [162][ 90/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7724 (0.7725)\tD(fake) 0.2287 (0.2274)\tgrad(D) penalty 0.0148 (0.0151)\tRec loss 3826.4788 (3769.8439)\tnorm 0.8769 (0.8718)\n",
            "Epoch: [162][100/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7727 (0.7725)\tD(fake) 0.2266 (0.2267)\tgrad(D) penalty 0.0224 (0.0156)\tRec loss 3553.4131 (3771.3290)\tnorm 0.8838 (0.8717)\n",
            "Epoch: [162][110/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7725 (0.7725)\tD(fake) 0.2111 (0.2254)\tgrad(D) penalty 0.0219 (0.0160)\tRec loss 3714.2207 (3780.4455)\tnorm 0.8753 (0.8721)\n",
            "Epoch: [162][120/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7727 (0.7725)\tD(fake) 0.2130 (0.2254)\tgrad(D) penalty 0.0220 (0.0164)\tRec loss 4067.9497 (3779.9241)\tnorm 0.8716 (0.8722)\n",
            "Epoch: [162][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7727 (0.7726)\tD(fake) 0.2147 (0.2246)\tgrad(D) penalty 0.0189 (0.0165)\tRec loss 3935.5129 (3778.8880)\tnorm 0.8711 (0.8723)\n",
            "Epoch: [162][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7728 (0.7726)\tD(fake) 0.2330 (0.2252)\tgrad(D) penalty 0.0169 (0.0166)\tRec loss 3668.4307 (3772.6013)\tnorm 0.8593 (0.8724)\n",
            "Epoch: [162][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7728 (0.7726)\tD(fake) 0.2379 (0.2255)\tgrad(D) penalty 0.0178 (0.0166)\tRec loss 4009.1682 (3774.2266)\tnorm 0.8856 (0.8724)\n",
            "Epoch: [162][160/195]\tTime  0.144 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7727 (0.7726)\tD(fake) 0.2453 (0.2262)\tgrad(D) penalty 0.0151 (0.0164)\tRec loss 3483.9805 (3776.0246)\tnorm 0.8748 (0.8728)\n",
            "Epoch: [162][170/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7728 (0.7726)\tD(fake) 0.2186 (0.2263)\tgrad(D) penalty 0.0169 (0.0163)\tRec loss 3741.7905 (3779.9173)\tnorm 0.8615 (0.8729)\n",
            "Epoch: [162][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7726)\tD(fake) 0.2332 (0.2265)\tgrad(D) penalty 0.0135 (0.0161)\tRec loss 3896.6606 (3781.9614)\tnorm 0.8780 (0.8732)\n",
            "Epoch: [162][190/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7728 (0.7726)\tD(fake) 0.2251 (0.2268)\tgrad(D) penalty 0.0145 (0.0160)\tRec loss 3819.2529 (3781.6000)\tnorm 0.8651 (0.8731)\n",
            "Epoch: [163][  0/195]\tTime  0.431 ( 0.431)\tData  0.236 ( 0.236)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3881.7041 (3881.7041)\tnorm 0.8819 (0.8819)\n",
            "Epoch: [163][ 10/195]\tTime  0.149 ( 0.174)\tData  0.000 ( 0.022)\tD(real) 0.7729 (0.7729)\tD(fake) 0.2217 (0.2207)\tgrad(D) penalty 0.0169 (0.0149)\tRec loss 3836.5208 (3744.6863)\tnorm 0.8710 (0.8793)\n",
            "Epoch: [163][ 20/195]\tTime  0.149 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7729 (0.7728)\tD(fake) 0.2156 (0.2203)\tgrad(D) penalty 0.0149 (0.0146)\tRec loss 4108.5254 (3758.8775)\tnorm 0.8676 (0.8766)\n",
            "Epoch: [163][ 30/195]\tTime  0.148 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7728 (0.7728)\tD(fake) 0.2403 (0.2263)\tgrad(D) penalty 0.0152 (0.0151)\tRec loss 4048.6431 (3793.1182)\tnorm 0.8795 (0.8748)\n",
            "Epoch: [163][ 40/195]\tTime  0.152 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7728 (0.7728)\tD(fake) 0.2457 (0.2309)\tgrad(D) penalty 0.0160 (0.0155)\tRec loss 3847.7695 (3789.2691)\tnorm 0.8667 (0.8740)\n",
            "Epoch: [163][ 50/195]\tTime  0.163 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7727 (0.7728)\tD(fake) 0.2421 (0.2336)\tgrad(D) penalty 0.0174 (0.0158)\tRec loss 3745.9709 (3772.4805)\tnorm 0.8705 (0.8732)\n",
            "Epoch: [163][ 60/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7727 (0.7728)\tD(fake) 0.2588 (0.2357)\tgrad(D) penalty 0.0143 (0.0158)\tRec loss 3915.8032 (3789.6023)\tnorm 0.8739 (0.8738)\n",
            "Epoch: [163][ 70/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7727 (0.7727)\tD(fake) 0.2112 (0.2335)\tgrad(D) penalty 0.0168 (0.0158)\tRec loss 3870.3545 (3790.3973)\tnorm 0.8797 (0.8742)\n",
            "Epoch: [163][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7722 (0.7727)\tD(fake) 0.2096 (0.2346)\tgrad(D) penalty 0.0148 (0.0158)\tRec loss 3445.7119 (3778.0620)\tnorm 0.8655 (0.8739)\n",
            "Epoch: [163][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7726 (0.7727)\tD(fake) 0.2455 (0.2341)\tgrad(D) penalty 0.0171 (0.0159)\tRec loss 3489.4988 (3769.5948)\tnorm 0.8753 (0.8742)\n",
            "Epoch: [163][100/195]\tTime  0.164 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7725 (0.7727)\tD(fake) 0.2247 (0.2326)\tgrad(D) penalty 0.0150 (0.0159)\tRec loss 4061.5352 (3776.2143)\tnorm 0.8567 (0.8735)\n",
            "Epoch: [163][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7726 (0.7726)\tD(fake) 0.2214 (0.2317)\tgrad(D) penalty 0.0139 (0.0157)\tRec loss 3891.0386 (3767.6884)\tnorm 0.8719 (0.8736)\n",
            "Epoch: [163][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7725 (0.7726)\tD(fake) 0.2373 (0.2322)\tgrad(D) penalty 0.0153 (0.0158)\tRec loss 3867.0723 (3778.1471)\tnorm 0.8600 (0.8730)\n",
            "Epoch: [163][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7725 (0.7726)\tD(fake) 0.2218 (0.2313)\tgrad(D) penalty 0.0142 (0.0158)\tRec loss 3821.0049 (3779.2314)\tnorm 0.8624 (0.8725)\n",
            "Epoch: [163][140/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7727 (0.7726)\tD(fake) 0.2263 (0.2314)\tgrad(D) penalty 0.0135 (0.0157)\tRec loss 3628.8369 (3783.0506)\tnorm 0.8519 (0.8724)\n",
            "Epoch: [163][150/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7726 (0.7726)\tD(fake) 0.2328 (0.2312)\tgrad(D) penalty 0.0133 (0.0157)\tRec loss 3814.3633 (3781.3623)\tnorm 0.8747 (0.8722)\n",
            "Epoch: [163][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7726 (0.7726)\tD(fake) 0.2552 (0.2310)\tgrad(D) penalty 0.0162 (0.0158)\tRec loss 3667.3564 (3785.2843)\tnorm 0.8875 (0.8724)\n",
            "Epoch: [163][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7726 (0.7726)\tD(fake) 0.2049 (0.2306)\tgrad(D) penalty 0.0186 (0.0158)\tRec loss 3570.2612 (3784.1542)\tnorm 0.8747 (0.8726)\n",
            "Epoch: [163][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7726 (0.7726)\tD(fake) 0.2152 (0.2305)\tgrad(D) penalty 0.0168 (0.0159)\tRec loss 3489.9529 (3788.8742)\tnorm 0.8648 (0.8728)\n",
            "Epoch: [163][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7727 (0.7726)\tD(fake) 0.2247 (0.2298)\tgrad(D) penalty 0.0171 (0.0161)\tRec loss 3827.5593 (3786.7176)\tnorm 0.8576 (0.8726)\n",
            "Epoch: [164][  0/195]\tTime  0.416 ( 0.416)\tData  0.236 ( 0.236)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3672.3821 (3672.3821)\tnorm 0.8777 (0.8777)\n",
            "Epoch: [164][ 10/195]\tTime  0.146 ( 0.173)\tData  0.000 ( 0.022)\tD(real) 0.7728 (0.7727)\tD(fake) 0.2275 (0.2281)\tgrad(D) penalty 0.0145 (0.0155)\tRec loss 3785.1162 (3799.7112)\tnorm 0.8800 (0.8731)\n",
            "Epoch: [164][ 20/195]\tTime  0.150 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7725 (0.7727)\tD(fake) 0.2204 (0.2285)\tgrad(D) penalty 0.0169 (0.0153)\tRec loss 3595.1943 (3768.3381)\tnorm 0.8668 (0.8725)\n",
            "Epoch: [164][ 30/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7725 (0.7727)\tD(fake) 0.2168 (0.2269)\tgrad(D) penalty 0.0141 (0.0152)\tRec loss 3636.1372 (3771.4814)\tnorm 0.8759 (0.8716)\n",
            "Epoch: [164][ 40/195]\tTime  0.146 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7728 (0.7727)\tD(fake) 0.2267 (0.2275)\tgrad(D) penalty 0.0128 (0.0149)\tRec loss 3663.3826 (3758.1290)\tnorm 0.8711 (0.8710)\n",
            "Epoch: [164][ 50/195]\tTime  0.166 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7728 (0.7727)\tD(fake) 0.2207 (0.2272)\tgrad(D) penalty 0.0143 (0.0147)\tRec loss 4038.3159 (3754.2444)\tnorm 0.8673 (0.8699)\n",
            "Epoch: [164][ 60/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7727 (0.7727)\tD(fake) 0.2050 (0.2268)\tgrad(D) penalty 0.0170 (0.0149)\tRec loss 4165.6665 (3759.3025)\tnorm 0.8847 (0.8698)\n",
            "Epoch: [164][ 70/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7727 (0.7727)\tD(fake) 0.2308 (0.2268)\tgrad(D) penalty 0.0131 (0.0147)\tRec loss 3826.9165 (3749.0104)\tnorm 0.8721 (0.8700)\n",
            "Epoch: [164][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7727 (0.7727)\tD(fake) 0.2196 (0.2266)\tgrad(D) penalty 0.0172 (0.0148)\tRec loss 3914.3022 (3756.1337)\tnorm 0.8746 (0.8701)\n",
            "Epoch: [164][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7729 (0.7727)\tD(fake) 0.2413 (0.2272)\tgrad(D) penalty 0.0166 (0.0150)\tRec loss 3941.3972 (3758.3993)\tnorm 0.8672 (0.8706)\n",
            "Epoch: [164][100/195]\tTime  0.170 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7728 (0.7727)\tD(fake) 0.2216 (0.2265)\tgrad(D) penalty 0.0190 (0.0153)\tRec loss 3682.5469 (3754.1669)\tnorm 0.8512 (0.8708)\n",
            "Epoch: [164][110/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7728 (0.7727)\tD(fake) 0.2144 (0.2257)\tgrad(D) penalty 0.0192 (0.0156)\tRec loss 3556.2646 (3756.3468)\tnorm 0.8624 (0.8703)\n",
            "Epoch: [164][120/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7727 (0.7727)\tD(fake) 0.2130 (0.2259)\tgrad(D) penalty 0.0164 (0.0159)\tRec loss 3727.5935 (3757.9701)\tnorm 0.8655 (0.8702)\n",
            "Epoch: [164][130/195]\tTime  0.153 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7727)\tD(fake) 0.2211 (0.2252)\tgrad(D) penalty 0.0182 (0.0161)\tRec loss 3673.6553 (3755.9991)\tnorm 0.8732 (0.8708)\n",
            "Epoch: [164][140/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7728 (0.7728)\tD(fake) 0.2362 (0.2258)\tgrad(D) penalty 0.0136 (0.0159)\tRec loss 3843.7617 (3756.1775)\tnorm 0.8691 (0.8710)\n",
            "Epoch: [164][150/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7728)\tD(fake) 0.2345 (0.2260)\tgrad(D) penalty 0.0169 (0.0159)\tRec loss 3687.5840 (3764.7738)\tnorm 0.8823 (0.8708)\n",
            "Epoch: [164][160/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7728)\tD(fake) 0.2199 (0.2256)\tgrad(D) penalty 0.0183 (0.0159)\tRec loss 3796.9917 (3768.9570)\tnorm 0.8654 (0.8708)\n",
            "Epoch: [164][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7728)\tD(fake) 0.2236 (0.2264)\tgrad(D) penalty 0.0138 (0.0157)\tRec loss 3745.3723 (3776.4226)\tnorm 0.8763 (0.8712)\n",
            "Epoch: [164][180/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7728)\tD(fake) 0.2293 (0.2264)\tgrad(D) penalty 0.0149 (0.0157)\tRec loss 3779.8193 (3773.2178)\tnorm 0.8748 (0.8715)\n",
            "Epoch: [164][190/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7728 (0.7728)\tD(fake) 0.2360 (0.2264)\tgrad(D) penalty 0.0153 (0.0157)\tRec loss 3963.2881 (3775.6864)\tnorm 0.8843 (0.8715)\n",
            "Epoch: [165][  0/195]\tTime  0.434 ( 0.434)\tData  0.257 ( 0.257)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3510.3511 (3510.3511)\tnorm 0.8693 (0.8693)\n",
            "Epoch: [165][ 10/195]\tTime  0.148 ( 0.175)\tData  0.000 ( 0.024)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2567 (0.2481)\tgrad(D) penalty 0.0165 (0.0168)\tRec loss 3714.1152 (3665.1192)\tnorm 0.8683 (0.8716)\n",
            "Epoch: [165][ 20/195]\tTime  0.149 ( 0.163)\tData  0.000 ( 0.013)\tD(real) 0.7729 (0.7729)\tD(fake) 0.2397 (0.2409)\tgrad(D) penalty 0.0173 (0.0177)\tRec loss 3778.6826 (3733.5612)\tnorm 0.8639 (0.8684)\n",
            "Epoch: [165][ 30/195]\tTime  0.151 ( 0.159)\tData  0.000 ( 0.009)\tD(real) 0.7729 (0.7729)\tD(fake) 0.2063 (0.2350)\tgrad(D) penalty 0.0146 (0.0171)\tRec loss 3678.6404 (3717.4625)\tnorm 0.8703 (0.8689)\n",
            "Epoch: [165][ 40/195]\tTime  0.153 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7729 (0.7729)\tD(fake) 0.2078 (0.2355)\tgrad(D) penalty 0.0132 (0.0160)\tRec loss 3455.7041 (3715.8850)\tnorm 0.8777 (0.8689)\n",
            "Epoch: [165][ 50/195]\tTime  0.165 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7729 (0.7729)\tD(fake) 0.2616 (0.2354)\tgrad(D) penalty 0.0153 (0.0159)\tRec loss 3862.0867 (3733.8700)\tnorm 0.8852 (0.8705)\n",
            "Epoch: [165][ 60/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7727 (0.7728)\tD(fake) 0.2455 (0.2352)\tgrad(D) penalty 0.0150 (0.0157)\tRec loss 3726.4951 (3738.0316)\tnorm 0.8642 (0.8705)\n",
            "Epoch: [165][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7728 (0.7728)\tD(fake) 0.2146 (0.2340)\tgrad(D) penalty 0.0173 (0.0157)\tRec loss 3788.3406 (3736.3658)\tnorm 0.8780 (0.8706)\n",
            "Epoch: [165][ 80/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7728 (0.7728)\tD(fake) 0.2146 (0.2328)\tgrad(D) penalty 0.0149 (0.0155)\tRec loss 3958.3525 (3750.3893)\tnorm 0.8744 (0.8709)\n",
            "Epoch: [165][ 90/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7727 (0.7728)\tD(fake) 0.2332 (0.2314)\tgrad(D) penalty 0.0140 (0.0155)\tRec loss 3684.0386 (3752.1171)\tnorm 0.8803 (0.8718)\n",
            "Epoch: [165][100/195]\tTime  0.163 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7728 (0.7728)\tD(fake) 0.2250 (0.2306)\tgrad(D) penalty 0.0143 (0.0155)\tRec loss 3711.0967 (3755.6649)\tnorm 0.8667 (0.8714)\n",
            "Epoch: [165][110/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7724 (0.7728)\tD(fake) 0.2240 (0.2297)\tgrad(D) penalty 0.0131 (0.0154)\tRec loss 3544.9207 (3756.2284)\tnorm 0.8685 (0.8710)\n",
            "Epoch: [165][120/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7728 (0.7728)\tD(fake) 0.2045 (0.2283)\tgrad(D) penalty 0.0157 (0.0154)\tRec loss 3665.6387 (3756.3917)\tnorm 0.8708 (0.8709)\n",
            "Epoch: [165][130/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7728)\tD(fake) 0.2483 (0.2287)\tgrad(D) penalty 0.0148 (0.0154)\tRec loss 3802.2646 (3756.7838)\tnorm 0.8755 (0.8709)\n",
            "Epoch: [165][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7728)\tD(fake) 0.2429 (0.2283)\tgrad(D) penalty 0.0174 (0.0155)\tRec loss 3781.7607 (3766.2433)\tnorm 0.8769 (0.8711)\n",
            "Epoch: [165][150/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7728)\tD(fake) 0.2257 (0.2289)\tgrad(D) penalty 0.0159 (0.0155)\tRec loss 3570.4229 (3769.6395)\tnorm 0.8643 (0.8709)\n",
            "Epoch: [165][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7728)\tD(fake) 0.2206 (0.2290)\tgrad(D) penalty 0.0169 (0.0156)\tRec loss 3928.1848 (3770.3277)\tnorm 0.8670 (0.8715)\n",
            "Epoch: [165][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7728 (0.7728)\tD(fake) 0.2262 (0.2290)\tgrad(D) penalty 0.0162 (0.0156)\tRec loss 4056.0723 (3776.2441)\tnorm 0.8669 (0.8715)\n",
            "Epoch: [165][180/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7728)\tD(fake) 0.2323 (0.2294)\tgrad(D) penalty 0.0125 (0.0155)\tRec loss 3818.5051 (3773.0782)\tnorm 0.8809 (0.8715)\n",
            "Epoch: [165][190/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7727 (0.7728)\tD(fake) 0.2402 (0.2297)\tgrad(D) penalty 0.0149 (0.0155)\tRec loss 4090.9583 (3775.6649)\tnorm 0.8865 (0.8716)\n",
            "Epoch: [166][  0/195]\tTime  0.435 ( 0.435)\tData  0.253 ( 0.253)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3668.8442 (3668.8442)\tnorm 0.8819 (0.8819)\n",
            "Epoch: [166][ 10/195]\tTime  0.150 ( 0.176)\tData  0.000 ( 0.023)\tD(real) 0.7730 (0.7728)\tD(fake) 0.2405 (0.2300)\tgrad(D) penalty 0.0166 (0.0177)\tRec loss 3610.5840 (3754.8210)\tnorm 0.8773 (0.8754)\n",
            "Epoch: [166][ 20/195]\tTime  0.148 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7727 (0.7729)\tD(fake) 0.2095 (0.2153)\tgrad(D) penalty 0.0216 (0.0194)\tRec loss 3865.1191 (3821.5071)\tnorm 0.8815 (0.8748)\n",
            "Epoch: [166][ 30/195]\tTime  0.152 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7731 (0.7729)\tD(fake) 0.2145 (0.2167)\tgrad(D) penalty 0.0191 (0.0196)\tRec loss 3761.7295 (3794.9615)\tnorm 0.8637 (0.8743)\n",
            "Epoch: [166][ 40/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7732 (0.7730)\tD(fake) 0.2350 (0.2160)\tgrad(D) penalty 0.0179 (0.0198)\tRec loss 3695.6077 (3788.7874)\tnorm 0.8751 (0.8721)\n",
            "Epoch: [166][ 50/195]\tTime  0.163 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2103 (0.2169)\tgrad(D) penalty 0.0147 (0.0191)\tRec loss 3969.0239 (3781.6185)\tnorm 0.8658 (0.8713)\n",
            "Epoch: [166][ 60/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7732 (0.7730)\tD(fake) 0.2465 (0.2221)\tgrad(D) penalty 0.0102 (0.0177)\tRec loss 4254.6450 (3781.2436)\tnorm 0.8685 (0.8711)\n",
            "Epoch: [166][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2370 (0.2236)\tgrad(D) penalty 0.0120 (0.0171)\tRec loss 4101.6729 (3785.8233)\tnorm 0.8727 (0.8716)\n",
            "Epoch: [166][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2243 (0.2246)\tgrad(D) penalty 0.0150 (0.0167)\tRec loss 3874.5615 (3791.5075)\tnorm 0.8643 (0.8721)\n",
            "Epoch: [166][ 90/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2340 (0.2253)\tgrad(D) penalty 0.0112 (0.0164)\tRec loss 3545.5474 (3791.2991)\tnorm 0.8683 (0.8726)\n",
            "Epoch: [166][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2397 (0.2262)\tgrad(D) penalty 0.0131 (0.0161)\tRec loss 3757.2695 (3780.9097)\tnorm 0.8591 (0.8729)\n",
            "Epoch: [166][110/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2237 (0.2258)\tgrad(D) penalty 0.0182 (0.0161)\tRec loss 3708.6052 (3775.8643)\tnorm 0.8868 (0.8726)\n",
            "Epoch: [166][120/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2263 (0.2257)\tgrad(D) penalty 0.0182 (0.0161)\tRec loss 3893.3772 (3778.6615)\tnorm 0.8836 (0.8728)\n",
            "Epoch: [166][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2404 (0.2270)\tgrad(D) penalty 0.0166 (0.0161)\tRec loss 3714.7031 (3778.0591)\tnorm 0.8677 (0.8731)\n",
            "Epoch: [166][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2487 (0.2279)\tgrad(D) penalty 0.0162 (0.0162)\tRec loss 3580.7595 (3781.0971)\tnorm 0.8703 (0.8728)\n",
            "Epoch: [166][150/195]\tTime  0.168 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2252 (0.2283)\tgrad(D) penalty 0.0173 (0.0162)\tRec loss 3742.4197 (3775.7984)\tnorm 0.8743 (0.8728)\n",
            "Epoch: [166][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2169 (0.2278)\tgrad(D) penalty 0.0157 (0.0161)\tRec loss 3549.7896 (3777.4432)\tnorm 0.8802 (0.8726)\n",
            "Epoch: [166][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7728 (0.7730)\tD(fake) 0.2387 (0.2277)\tgrad(D) penalty 0.0140 (0.0160)\tRec loss 3797.9675 (3776.2621)\tnorm 0.8756 (0.8724)\n",
            "Epoch: [166][180/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2473 (0.2281)\tgrad(D) penalty 0.0137 (0.0159)\tRec loss 3727.0901 (3775.9041)\tnorm 0.8727 (0.8723)\n",
            "Epoch: [166][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2102 (0.2275)\tgrad(D) penalty 0.0175 (0.0159)\tRec loss 3893.1484 (3775.9421)\tnorm 0.8700 (0.8720)\n",
            "Epoch: [167][  0/195]\tTime  0.446 ( 0.446)\tData  0.254 ( 0.254)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3840.3374 (3840.3374)\tnorm 0.8762 (0.8762)\n",
            "Epoch: [167][ 10/195]\tTime  0.151 ( 0.179)\tData  0.000 ( 0.023)\tD(real) 0.7729 (0.7728)\tD(fake) 0.2476 (0.2349)\tgrad(D) penalty 0.0121 (0.0162)\tRec loss 3804.1807 (3754.5777)\tnorm 0.8754 (0.8711)\n",
            "Epoch: [167][ 20/195]\tTime  0.148 ( 0.165)\tData  0.000 ( 0.012)\tD(real) 0.7729 (0.7729)\tD(fake) 0.2509 (0.2342)\tgrad(D) penalty 0.0155 (0.0168)\tRec loss 3952.7939 (3743.9476)\tnorm 0.8666 (0.8712)\n",
            "Epoch: [167][ 30/195]\tTime  0.148 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7730 (0.7729)\tD(fake) 0.2134 (0.2303)\tgrad(D) penalty 0.0156 (0.0168)\tRec loss 3689.6497 (3734.0606)\tnorm 0.8660 (0.8697)\n",
            "Epoch: [167][ 40/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7729 (0.7729)\tD(fake) 0.2268 (0.2303)\tgrad(D) penalty 0.0188 (0.0171)\tRec loss 3898.6499 (3735.3544)\tnorm 0.8637 (0.8690)\n",
            "Epoch: [167][ 50/195]\tTime  0.164 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7729 (0.7729)\tD(fake) 0.2197 (0.2279)\tgrad(D) penalty 0.0177 (0.0171)\tRec loss 3995.3794 (3751.1907)\tnorm 0.8743 (0.8694)\n",
            "Epoch: [167][ 60/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7730 (0.7729)\tD(fake) 0.2112 (0.2291)\tgrad(D) penalty 0.0155 (0.0169)\tRec loss 3833.1162 (3748.2648)\tnorm 0.8603 (0.8693)\n",
            "Epoch: [167][ 70/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7731 (0.7729)\tD(fake) 0.2710 (0.2327)\tgrad(D) penalty 0.0130 (0.0167)\tRec loss 3768.5188 (3750.3780)\tnorm 0.8702 (0.8688)\n",
            "Epoch: [167][ 80/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7729 (0.7729)\tD(fake) 0.2507 (0.2320)\tgrad(D) penalty 0.0117 (0.0164)\tRec loss 3625.9048 (3745.1644)\tnorm 0.8744 (0.8686)\n",
            "Epoch: [167][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7729 (0.7729)\tD(fake) 0.2106 (0.2305)\tgrad(D) penalty 0.0167 (0.0163)\tRec loss 3886.7368 (3743.5742)\tnorm 0.8565 (0.8689)\n",
            "Epoch: [167][100/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7730 (0.7729)\tD(fake) 0.2226 (0.2303)\tgrad(D) penalty 0.0163 (0.0161)\tRec loss 3931.1604 (3748.9056)\tnorm 0.8794 (0.8688)\n",
            "Epoch: [167][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7730 (0.7729)\tD(fake) 0.2270 (0.2295)\tgrad(D) penalty 0.0142 (0.0160)\tRec loss 3658.9011 (3749.5583)\tnorm 0.8692 (0.8691)\n",
            "Epoch: [167][120/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7729)\tD(fake) 0.2249 (0.2290)\tgrad(D) penalty 0.0180 (0.0161)\tRec loss 3850.8896 (3757.1756)\tnorm 0.8712 (0.8691)\n",
            "Epoch: [167][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7729)\tD(fake) 0.2219 (0.2283)\tgrad(D) penalty 0.0191 (0.0163)\tRec loss 3607.7505 (3757.7836)\tnorm 0.8800 (0.8690)\n",
            "Epoch: [167][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7726 (0.7729)\tD(fake) 0.2016 (0.2280)\tgrad(D) penalty 0.0199 (0.0165)\tRec loss 3471.1841 (3752.2490)\tnorm 0.8690 (0.8692)\n",
            "Epoch: [167][150/195]\tTime  0.176 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7729)\tD(fake) 0.2212 (0.2269)\tgrad(D) penalty 0.0224 (0.0169)\tRec loss 3671.4600 (3756.7601)\tnorm 0.8666 (0.8691)\n",
            "Epoch: [167][160/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7729)\tD(fake) 0.2199 (0.2274)\tgrad(D) penalty 0.0195 (0.0170)\tRec loss 3790.7622 (3763.5207)\tnorm 0.8731 (0.8691)\n",
            "Epoch: [167][170/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7729)\tD(fake) 0.2369 (0.2275)\tgrad(D) penalty 0.0194 (0.0171)\tRec loss 4014.8853 (3767.0253)\tnorm 0.8723 (0.8692)\n",
            "Epoch: [167][180/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2393 (0.2279)\tgrad(D) penalty 0.0191 (0.0171)\tRec loss 4042.8867 (3768.6450)\tnorm 0.8724 (0.8690)\n",
            "Epoch: [167][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2013 (0.2273)\tgrad(D) penalty 0.0192 (0.0171)\tRec loss 3792.8457 (3772.2558)\tnorm 0.8708 (0.8693)\n",
            "Epoch: [168][  0/195]\tTime  0.439 ( 0.439)\tData  0.242 ( 0.242)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3767.2393 (3767.2393)\tnorm 0.8817 (0.8817)\n",
            "Epoch: [168][ 10/195]\tTime  0.148 ( 0.174)\tData  0.000 ( 0.022)\tD(real) 0.7732 (0.7731)\tD(fake) 0.2317 (0.2193)\tgrad(D) penalty 0.0126 (0.0145)\tRec loss 4051.5710 (3777.2449)\tnorm 0.8759 (0.8726)\n",
            "Epoch: [168][ 20/195]\tTime  0.147 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7732 (0.7731)\tD(fake) 0.2277 (0.2196)\tgrad(D) penalty 0.0176 (0.0146)\tRec loss 4005.3330 (3768.3682)\tnorm 0.8597 (0.8706)\n",
            "Epoch: [168][ 30/195]\tTime  0.149 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7733 (0.7731)\tD(fake) 0.1943 (0.2179)\tgrad(D) penalty 0.0168 (0.0148)\tRec loss 3856.7112 (3803.1538)\tnorm 0.8879 (0.8711)\n",
            "Epoch: [168][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2080 (0.2211)\tgrad(D) penalty 0.0178 (0.0151)\tRec loss 3538.4133 (3784.2984)\tnorm 0.8719 (0.8718)\n",
            "Epoch: [168][ 50/195]\tTime  0.167 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7731 (0.7732)\tD(fake) 0.2389 (0.2227)\tgrad(D) penalty 0.0151 (0.0150)\tRec loss 3583.3940 (3770.0262)\tnorm 0.8601 (0.8706)\n",
            "Epoch: [168][ 60/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2236 (0.2232)\tgrad(D) penalty 0.0165 (0.0150)\tRec loss 3540.4619 (3764.3634)\tnorm 0.8811 (0.8707)\n",
            "Epoch: [168][ 70/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2289 (0.2244)\tgrad(D) penalty 0.0133 (0.0150)\tRec loss 3634.8037 (3761.9757)\tnorm 0.8749 (0.8701)\n",
            "Epoch: [168][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2380 (0.2249)\tgrad(D) penalty 0.0152 (0.0149)\tRec loss 3918.3560 (3766.9020)\tnorm 0.8595 (0.8700)\n",
            "Epoch: [168][ 90/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2207 (0.2249)\tgrad(D) penalty 0.0200 (0.0151)\tRec loss 4007.8955 (3778.6601)\tnorm 0.8851 (0.8708)\n",
            "Epoch: [168][100/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2502 (0.2256)\tgrad(D) penalty 0.0134 (0.0154)\tRec loss 3403.4932 (3772.9262)\tnorm 0.8766 (0.8711)\n",
            "Epoch: [168][110/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7732)\tD(fake) 0.2246 (0.2255)\tgrad(D) penalty 0.0188 (0.0154)\tRec loss 3655.1055 (3781.3380)\tnorm 0.8635 (0.8711)\n",
            "Epoch: [168][120/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2070 (0.2261)\tgrad(D) penalty 0.0160 (0.0155)\tRec loss 3632.9702 (3772.0073)\tnorm 0.8884 (0.8721)\n",
            "Epoch: [168][130/195]\tTime  0.144 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7732)\tD(fake) 0.2443 (0.2260)\tgrad(D) penalty 0.0174 (0.0157)\tRec loss 3831.5059 (3773.4167)\tnorm 0.8865 (0.8724)\n",
            "Epoch: [168][140/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7732)\tD(fake) 0.1976 (0.2257)\tgrad(D) penalty 0.0198 (0.0161)\tRec loss 3725.4609 (3767.9511)\tnorm 0.8772 (0.8724)\n",
            "Epoch: [168][150/195]\tTime  0.162 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2254 (0.2255)\tgrad(D) penalty 0.0207 (0.0163)\tRec loss 4041.6880 (3769.1950)\tnorm 0.8644 (0.8722)\n",
            "Epoch: [168][160/195]\tTime  0.144 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2074 (0.2241)\tgrad(D) penalty 0.0249 (0.0168)\tRec loss 3725.3108 (3769.0091)\tnorm 0.8727 (0.8721)\n",
            "Epoch: [168][170/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7732)\tD(fake) 0.2169 (0.2239)\tgrad(D) penalty 0.0191 (0.0170)\tRec loss 3777.2234 (3772.3419)\tnorm 0.8681 (0.8722)\n",
            "Epoch: [168][180/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2420 (0.2242)\tgrad(D) penalty 0.0191 (0.0170)\tRec loss 4157.5771 (3770.4340)\tnorm 0.8608 (0.8721)\n",
            "Epoch: [168][190/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7735 (0.7732)\tD(fake) 0.2109 (0.2241)\tgrad(D) penalty 0.0225 (0.0171)\tRec loss 3931.6240 (3768.9741)\tnorm 0.8589 (0.8719)\n",
            "Epoch: [169][  0/195]\tTime  0.457 ( 0.457)\tData  0.257 ( 0.257)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3737.4175 (3737.4175)\tnorm 0.8667 (0.8667)\n",
            "Epoch: [169][ 10/195]\tTime  0.146 ( 0.177)\tData  0.000 ( 0.024)\tD(real) 0.7734 (0.7733)\tD(fake) 0.2131 (0.2174)\tgrad(D) penalty 0.0195 (0.0185)\tRec loss 3553.5044 (3683.6394)\tnorm 0.8791 (0.8724)\n",
            "Epoch: [169][ 20/195]\tTime  0.147 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2120 (0.2244)\tgrad(D) penalty 0.0172 (0.0178)\tRec loss 3591.4478 (3701.9491)\tnorm 0.8710 (0.8720)\n",
            "Epoch: [169][ 30/195]\tTime  0.150 ( 0.159)\tData  0.000 ( 0.009)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2421 (0.2264)\tgrad(D) penalty 0.0137 (0.0172)\tRec loss 3735.1484 (3714.6514)\tnorm 0.8625 (0.8712)\n",
            "Epoch: [169][ 40/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2333 (0.2289)\tgrad(D) penalty 0.0148 (0.0163)\tRec loss 3956.8574 (3760.8240)\tnorm 0.8770 (0.8718)\n",
            "Epoch: [169][ 50/195]\tTime  0.163 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2200 (0.2294)\tgrad(D) penalty 0.0135 (0.0159)\tRec loss 3905.2358 (3779.9217)\tnorm 0.8700 (0.8725)\n",
            "Epoch: [169][ 60/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2250 (0.2308)\tgrad(D) penalty 0.0141 (0.0155)\tRec loss 3824.3164 (3789.4574)\tnorm 0.8783 (0.8736)\n",
            "Epoch: [169][ 70/195]\tTime  0.153 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7733 (0.7734)\tD(fake) 0.2216 (0.2289)\tgrad(D) penalty 0.0165 (0.0155)\tRec loss 3786.0359 (3794.2813)\tnorm 0.8699 (0.8732)\n",
            "Epoch: [169][ 80/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7733 (0.7734)\tD(fake) 0.1963 (0.2278)\tgrad(D) penalty 0.0172 (0.0154)\tRec loss 3754.4556 (3796.5948)\tnorm 0.8778 (0.8732)\n",
            "Epoch: [169][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2408 (0.2294)\tgrad(D) penalty 0.0158 (0.0154)\tRec loss 3829.0605 (3795.3232)\tnorm 0.8649 (0.8731)\n",
            "Epoch: [169][100/195]\tTime  0.163 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7733 (0.7734)\tD(fake) 0.2559 (0.2287)\tgrad(D) penalty 0.0172 (0.0156)\tRec loss 4254.8496 (3804.4648)\tnorm 0.8805 (0.8729)\n",
            "Epoch: [169][110/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2273 (0.2293)\tgrad(D) penalty 0.0162 (0.0156)\tRec loss 3749.8457 (3792.4853)\tnorm 0.8689 (0.8731)\n",
            "Epoch: [169][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7734)\tD(fake) 0.2353 (0.2306)\tgrad(D) penalty 0.0155 (0.0157)\tRec loss 3581.1199 (3787.0776)\tnorm 0.8669 (0.8726)\n",
            "Epoch: [169][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7726 (0.7734)\tD(fake) 0.2405 (0.2315)\tgrad(D) penalty 0.0149 (0.0156)\tRec loss 4091.5811 (3780.3565)\tnorm 0.8773 (0.8722)\n",
            "Epoch: [169][140/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7733)\tD(fake) 0.2472 (0.2309)\tgrad(D) penalty 0.0157 (0.0156)\tRec loss 3810.4993 (3783.5287)\tnorm 0.8813 (0.8718)\n",
            "Epoch: [169][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7733)\tD(fake) 0.2152 (0.2302)\tgrad(D) penalty 0.0172 (0.0156)\tRec loss 3735.9717 (3781.7201)\tnorm 0.8799 (0.8717)\n",
            "Epoch: [169][160/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7733)\tD(fake) 0.2003 (0.2302)\tgrad(D) penalty 0.0175 (0.0155)\tRec loss 3859.0786 (3783.5783)\tnorm 0.8684 (0.8721)\n",
            "Epoch: [169][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7733)\tD(fake) 0.2508 (0.2309)\tgrad(D) penalty 0.0154 (0.0156)\tRec loss 3729.1184 (3774.1273)\tnorm 0.8609 (0.8721)\n",
            "Epoch: [169][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7733)\tD(fake) 0.2904 (0.2319)\tgrad(D) penalty 0.0146 (0.0156)\tRec loss 3454.7400 (3772.4484)\tnorm 0.8855 (0.8725)\n",
            "Epoch: [169][190/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7733)\tD(fake) 0.2059 (0.2321)\tgrad(D) penalty 0.0191 (0.0157)\tRec loss 3611.3296 (3774.3012)\tnorm 0.8735 (0.8725)\n",
            "Epoch: [170][  0/195]\tTime  0.457 ( 0.457)\tData  0.259 ( 0.259)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3765.5730 (3765.5730)\tnorm 0.8678 (0.8678)\n",
            "Epoch: [170][ 10/195]\tTime  0.152 ( 0.178)\tData  0.000 ( 0.024)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2366 (0.2476)\tgrad(D) penalty 0.0155 (0.0149)\tRec loss 3930.4785 (3781.4414)\tnorm 0.8801 (0.8699)\n",
            "Epoch: [170][ 20/195]\tTime  0.148 ( 0.164)\tData  0.000 ( 0.013)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2286 (0.2310)\tgrad(D) penalty 0.0178 (0.0172)\tRec loss 3769.1592 (3743.5609)\tnorm 0.8900 (0.8731)\n",
            "Epoch: [170][ 30/195]\tTime  0.148 ( 0.159)\tData  0.000 ( 0.009)\tD(real) 0.7730 (0.7731)\tD(fake) 0.2221 (0.2288)\tgrad(D) penalty 0.0169 (0.0173)\tRec loss 3650.3970 (3758.6133)\tnorm 0.8771 (0.8734)\n",
            "Epoch: [170][ 40/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2238 (0.2294)\tgrad(D) penalty 0.0137 (0.0164)\tRec loss 3846.9956 (3769.0636)\tnorm 0.8765 (0.8735)\n",
            "Epoch: [170][ 50/195]\tTime  0.164 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2253 (0.2290)\tgrad(D) penalty 0.0174 (0.0164)\tRec loss 3499.1677 (3764.9711)\tnorm 0.8724 (0.8735)\n",
            "Epoch: [170][ 60/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2273 (0.2293)\tgrad(D) penalty 0.0140 (0.0158)\tRec loss 3870.4990 (3779.4173)\tnorm 0.8745 (0.8736)\n",
            "Epoch: [170][ 70/195]\tTime  0.153 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2248 (0.2284)\tgrad(D) penalty 0.0140 (0.0156)\tRec loss 3757.3843 (3760.9596)\tnorm 0.8804 (0.8741)\n",
            "Epoch: [170][ 80/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2292 (0.2276)\tgrad(D) penalty 0.0134 (0.0153)\tRec loss 3787.7568 (3761.1116)\tnorm 0.8635 (0.8736)\n",
            "Epoch: [170][ 90/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2226 (0.2267)\tgrad(D) penalty 0.0146 (0.0152)\tRec loss 3719.0391 (3762.4734)\tnorm 0.8699 (0.8736)\n",
            "Epoch: [170][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2280 (0.2272)\tgrad(D) penalty 0.0182 (0.0154)\tRec loss 3578.2026 (3755.7187)\tnorm 0.8585 (0.8736)\n",
            "Epoch: [170][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2237 (0.2272)\tgrad(D) penalty 0.0173 (0.0155)\tRec loss 3508.5271 (3755.6653)\tnorm 0.8667 (0.8732)\n",
            "Epoch: [170][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7728 (0.7730)\tD(fake) 0.2271 (0.2260)\tgrad(D) penalty 0.0183 (0.0158)\tRec loss 3776.0215 (3758.6082)\tnorm 0.8779 (0.8725)\n",
            "Epoch: [170][130/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2392 (0.2266)\tgrad(D) penalty 0.0147 (0.0158)\tRec loss 3902.2041 (3767.4739)\tnorm 0.8771 (0.8720)\n",
            "Epoch: [170][140/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2332 (0.2267)\tgrad(D) penalty 0.0146 (0.0157)\tRec loss 4231.8643 (3767.2216)\tnorm 0.8682 (0.8719)\n",
            "Epoch: [170][150/195]\tTime  0.161 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2211 (0.2266)\tgrad(D) penalty 0.0145 (0.0155)\tRec loss 3464.1104 (3763.6889)\tnorm 0.8819 (0.8720)\n",
            "Epoch: [170][160/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2232 (0.2270)\tgrad(D) penalty 0.0154 (0.0156)\tRec loss 3548.7642 (3766.3818)\tnorm 0.8811 (0.8722)\n",
            "Epoch: [170][170/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2146 (0.2264)\tgrad(D) penalty 0.0166 (0.0156)\tRec loss 3584.0854 (3764.6920)\tnorm 0.8765 (0.8722)\n",
            "Epoch: [170][180/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2116 (0.2265)\tgrad(D) penalty 0.0182 (0.0156)\tRec loss 3719.4751 (3764.5278)\tnorm 0.8681 (0.8719)\n",
            "Epoch: [170][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2235 (0.2263)\tgrad(D) penalty 0.0150 (0.0156)\tRec loss 3442.1914 (3767.6131)\tnorm 0.8769 (0.8719)\n",
            "Epoch: [171][  0/195]\tTime  0.448 ( 0.448)\tData  0.253 ( 0.253)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 4080.7480 (4080.7480)\tnorm 0.8642 (0.8642)\n",
            "Epoch: [171][ 10/195]\tTime  0.150 ( 0.177)\tData  0.000 ( 0.023)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2295 (0.2256)\tgrad(D) penalty 0.0169 (0.0178)\tRec loss 3952.9634 (3785.3882)\tnorm 0.8828 (0.8736)\n",
            "Epoch: [171][ 20/195]\tTime  0.149 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2152 (0.2210)\tgrad(D) penalty 0.0184 (0.0166)\tRec loss 3506.2817 (3764.4207)\tnorm 0.8831 (0.8735)\n",
            "Epoch: [171][ 30/195]\tTime  0.150 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2318 (0.2249)\tgrad(D) penalty 0.0176 (0.0168)\tRec loss 3523.9910 (3747.2870)\tnorm 0.8759 (0.8732)\n",
            "Epoch: [171][ 40/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2414 (0.2263)\tgrad(D) penalty 0.0143 (0.0164)\tRec loss 3634.2212 (3744.3021)\tnorm 0.8623 (0.8718)\n",
            "Epoch: [171][ 50/195]\tTime  0.164 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2135 (0.2260)\tgrad(D) penalty 0.0135 (0.0159)\tRec loss 3899.2617 (3749.9717)\tnorm 0.8624 (0.8708)\n",
            "Epoch: [171][ 60/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7730 (0.7731)\tD(fake) 0.2293 (0.2264)\tgrad(D) penalty 0.0134 (0.0157)\tRec loss 3786.6655 (3756.9536)\tnorm 0.8653 (0.8708)\n",
            "Epoch: [171][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7730 (0.7731)\tD(fake) 0.2481 (0.2265)\tgrad(D) penalty 0.0145 (0.0157)\tRec loss 3835.4663 (3765.1105)\tnorm 0.8656 (0.8708)\n",
            "Epoch: [171][ 80/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7732 (0.7731)\tD(fake) 0.2362 (0.2255)\tgrad(D) penalty 0.0154 (0.0157)\tRec loss 3719.8743 (3772.2881)\tnorm 0.8615 (0.8708)\n",
            "Epoch: [171][ 90/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7729 (0.7731)\tD(fake) 0.2366 (0.2274)\tgrad(D) penalty 0.0152 (0.0156)\tRec loss 3793.6821 (3773.4945)\tnorm 0.8630 (0.8711)\n",
            "Epoch: [171][100/195]\tTime  0.168 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7732 (0.7731)\tD(fake) 0.2684 (0.2272)\tgrad(D) penalty 0.0138 (0.0159)\tRec loss 3802.6338 (3773.5027)\tnorm 0.8701 (0.8709)\n",
            "Epoch: [171][110/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2057 (0.2275)\tgrad(D) penalty 0.0163 (0.0160)\tRec loss 3880.6687 (3776.0256)\tnorm 0.8732 (0.8709)\n",
            "Epoch: [171][120/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7731)\tD(fake) 0.2270 (0.2278)\tgrad(D) penalty 0.0169 (0.0159)\tRec loss 3911.9280 (3769.4327)\tnorm 0.8663 (0.8709)\n",
            "Epoch: [171][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2405 (0.2273)\tgrad(D) penalty 0.0197 (0.0161)\tRec loss 3789.2170 (3767.1028)\tnorm 0.8677 (0.8710)\n",
            "Epoch: [171][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7731)\tD(fake) 0.2234 (0.2279)\tgrad(D) penalty 0.0185 (0.0162)\tRec loss 3903.4810 (3764.4639)\tnorm 0.8735 (0.8713)\n",
            "Epoch: [171][150/195]\tTime  0.170 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7731)\tD(fake) 0.2394 (0.2281)\tgrad(D) penalty 0.0148 (0.0162)\tRec loss 3895.5928 (3767.3805)\tnorm 0.8781 (0.8714)\n",
            "Epoch: [171][160/195]\tTime  0.144 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2324 (0.2284)\tgrad(D) penalty 0.0149 (0.0162)\tRec loss 4030.4871 (3770.1275)\tnorm 0.8593 (0.8712)\n",
            "Epoch: [171][170/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2111 (0.2281)\tgrad(D) penalty 0.0167 (0.0161)\tRec loss 3782.9878 (3765.3162)\tnorm 0.8727 (0.8712)\n",
            "Epoch: [171][180/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2214 (0.2282)\tgrad(D) penalty 0.0132 (0.0159)\tRec loss 3804.6606 (3766.4665)\tnorm 0.8778 (0.8712)\n",
            "Epoch: [171][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2450 (0.2284)\tgrad(D) penalty 0.0150 (0.0159)\tRec loss 3682.9346 (3763.4452)\tnorm 0.8595 (0.8711)\n",
            "Epoch: [172][  0/195]\tTime  0.423 ( 0.423)\tData  0.232 ( 0.232)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3788.0981 (3788.0981)\tnorm 0.8661 (0.8661)\n",
            "Epoch: [172][ 10/195]\tTime  0.149 ( 0.176)\tData  0.000 ( 0.021)\tD(real) 0.7730 (0.7731)\tD(fake) 0.2764 (0.2781)\tgrad(D) penalty 0.0148 (0.0142)\tRec loss 3918.7300 (3804.7109)\tnorm 0.8697 (0.8726)\n",
            "Epoch: [172][ 20/195]\tTime  0.149 ( 0.163)\tData  0.000 ( 0.011)\tD(real) 0.7730 (0.7731)\tD(fake) 0.2507 (0.2416)\tgrad(D) penalty 0.0159 (0.0156)\tRec loss 3893.5483 (3789.3874)\tnorm 0.8585 (0.8733)\n",
            "Epoch: [172][ 30/195]\tTime  0.148 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2183 (0.2375)\tgrad(D) penalty 0.0150 (0.0156)\tRec loss 3646.3564 (3766.9078)\tnorm 0.8741 (0.8728)\n",
            "Epoch: [172][ 40/195]\tTime  0.150 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2560 (0.2347)\tgrad(D) penalty 0.0156 (0.0167)\tRec loss 3820.9600 (3773.2609)\tnorm 0.8781 (0.8720)\n",
            "Epoch: [172][ 50/195]\tTime  0.165 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2071 (0.2327)\tgrad(D) penalty 0.0176 (0.0168)\tRec loss 3950.4509 (3763.4317)\tnorm 0.8642 (0.8717)\n",
            "Epoch: [172][ 60/195]\tTime  0.146 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2254 (0.2311)\tgrad(D) penalty 0.0170 (0.0168)\tRec loss 3979.5117 (3762.3857)\tnorm 0.8652 (0.8720)\n",
            "Epoch: [172][ 70/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7728 (0.7730)\tD(fake) 0.2124 (0.2289)\tgrad(D) penalty 0.0180 (0.0169)\tRec loss 3961.6353 (3760.9327)\tnorm 0.8612 (0.8717)\n",
            "Epoch: [172][ 80/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2366 (0.2292)\tgrad(D) penalty 0.0136 (0.0166)\tRec loss 3863.8242 (3754.0709)\tnorm 0.8841 (0.8713)\n",
            "Epoch: [172][ 90/195]\tTime  0.154 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2349 (0.2287)\tgrad(D) penalty 0.0154 (0.0165)\tRec loss 3676.9451 (3761.7646)\tnorm 0.8752 (0.8712)\n",
            "Epoch: [172][100/195]\tTime  0.163 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2128 (0.2278)\tgrad(D) penalty 0.0169 (0.0165)\tRec loss 3791.4094 (3767.3573)\tnorm 0.8722 (0.8710)\n",
            "Epoch: [172][110/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2206 (0.2269)\tgrad(D) penalty 0.0149 (0.0165)\tRec loss 3476.8442 (3764.2731)\tnorm 0.8776 (0.8711)\n",
            "Epoch: [172][120/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2247 (0.2268)\tgrad(D) penalty 0.0176 (0.0165)\tRec loss 3848.7053 (3755.8431)\tnorm 0.8721 (0.8709)\n",
            "Epoch: [172][130/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2108 (0.2255)\tgrad(D) penalty 0.0185 (0.0165)\tRec loss 3869.7617 (3756.0825)\tnorm 0.8595 (0.8706)\n",
            "Epoch: [172][140/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2185 (0.2260)\tgrad(D) penalty 0.0175 (0.0164)\tRec loss 3522.1011 (3755.7228)\tnorm 0.8717 (0.8705)\n",
            "Epoch: [172][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2247 (0.2262)\tgrad(D) penalty 0.0151 (0.0164)\tRec loss 3723.5112 (3755.2706)\tnorm 0.8702 (0.8703)\n",
            "Epoch: [172][160/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2184 (0.2267)\tgrad(D) penalty 0.0154 (0.0162)\tRec loss 3788.5708 (3763.0208)\tnorm 0.8852 (0.8707)\n",
            "Epoch: [172][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2348 (0.2271)\tgrad(D) penalty 0.0137 (0.0161)\tRec loss 3810.1672 (3758.8442)\tnorm 0.8649 (0.8709)\n",
            "Epoch: [172][180/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2398 (0.2270)\tgrad(D) penalty 0.0154 (0.0160)\tRec loss 3746.3169 (3760.9005)\tnorm 0.8639 (0.8711)\n",
            "Epoch: [172][190/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2141 (0.2266)\tgrad(D) penalty 0.0167 (0.0160)\tRec loss 3569.8308 (3761.4563)\tnorm 0.8755 (0.8711)\n",
            "Epoch: [173][  0/195]\tTime  0.432 ( 0.432)\tData  0.237 ( 0.237)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3800.1050 (3800.1050)\tnorm 0.8757 (0.8757)\n",
            "Epoch: [173][ 10/195]\tTime  0.151 ( 0.177)\tData  0.000 ( 0.022)\tD(real) 0.7732 (0.7731)\tD(fake) 0.2274 (0.2234)\tgrad(D) penalty 0.0130 (0.0151)\tRec loss 3868.4419 (3756.4415)\tnorm 0.8551 (0.8705)\n",
            "Epoch: [173][ 20/195]\tTime  0.147 ( 0.164)\tData  0.000 ( 0.012)\tD(real) 0.7733 (0.7731)\tD(fake) 0.2328 (0.2261)\tgrad(D) penalty 0.0147 (0.0146)\tRec loss 3731.5505 (3738.3811)\tnorm 0.8752 (0.8713)\n",
            "Epoch: [173][ 30/195]\tTime  0.149 ( 0.160)\tData  0.000 ( 0.008)\tD(real) 0.7727 (0.7731)\tD(fake) 0.2288 (0.2261)\tgrad(D) penalty 0.0138 (0.0149)\tRec loss 4043.4648 (3744.0179)\tnorm 0.8766 (0.8702)\n",
            "Epoch: [173][ 40/195]\tTime  0.150 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7733 (0.7731)\tD(fake) 0.2223 (0.2245)\tgrad(D) penalty 0.0164 (0.0152)\tRec loss 3941.6279 (3750.4572)\tnorm 0.8687 (0.8707)\n",
            "Epoch: [173][ 50/195]\tTime  0.165 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7733 (0.7731)\tD(fake) 0.2367 (0.2255)\tgrad(D) penalty 0.0190 (0.0157)\tRec loss 3475.4651 (3733.1900)\tnorm 0.8630 (0.8706)\n",
            "Epoch: [173][ 60/195]\tTime  0.149 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2199 (0.2235)\tgrad(D) penalty 0.0187 (0.0164)\tRec loss 3761.9541 (3728.5373)\tnorm 0.8742 (0.8706)\n",
            "Epoch: [173][ 70/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2239 (0.2235)\tgrad(D) penalty 0.0187 (0.0168)\tRec loss 3637.1753 (3735.4748)\tnorm 0.8747 (0.8717)\n",
            "Epoch: [173][ 80/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7734 (0.7732)\tD(fake) 0.2205 (0.2232)\tgrad(D) penalty 0.0154 (0.0169)\tRec loss 3898.0684 (3734.2227)\tnorm 0.8662 (0.8713)\n",
            "Epoch: [173][ 90/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2201 (0.2235)\tgrad(D) penalty 0.0144 (0.0166)\tRec loss 3739.3262 (3734.6960)\tnorm 0.8687 (0.8718)\n",
            "Epoch: [173][100/195]\tTime  0.162 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2282 (0.2237)\tgrad(D) penalty 0.0123 (0.0163)\tRec loss 3687.0332 (3744.4105)\tnorm 0.8694 (0.8715)\n",
            "Epoch: [173][110/195]\tTime  0.155 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2352 (0.2244)\tgrad(D) penalty 0.0151 (0.0162)\tRec loss 3598.9355 (3752.0360)\tnorm 0.8660 (0.8714)\n",
            "Epoch: [173][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7733)\tD(fake) 0.2414 (0.2253)\tgrad(D) penalty 0.0140 (0.0160)\tRec loss 3695.9973 (3747.5466)\tnorm 0.8742 (0.8710)\n",
            "Epoch: [173][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2115 (0.2253)\tgrad(D) penalty 0.0144 (0.0159)\tRec loss 3922.2900 (3753.9842)\tnorm 0.8780 (0.8708)\n",
            "Epoch: [173][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7733)\tD(fake) 0.2251 (0.2249)\tgrad(D) penalty 0.0135 (0.0159)\tRec loss 3681.0718 (3752.0253)\tnorm 0.8622 (0.8706)\n",
            "Epoch: [173][150/195]\tTime  0.162 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7733)\tD(fake) 0.2381 (0.2250)\tgrad(D) penalty 0.0172 (0.0159)\tRec loss 3939.5801 (3753.4489)\tnorm 0.8824 (0.8710)\n",
            "Epoch: [173][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7733)\tD(fake) 0.2307 (0.2254)\tgrad(D) penalty 0.0187 (0.0159)\tRec loss 3999.9849 (3756.7479)\tnorm 0.8665 (0.8706)\n",
            "Epoch: [173][170/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7733)\tD(fake) 0.2280 (0.2257)\tgrad(D) penalty 0.0148 (0.0158)\tRec loss 3429.5342 (3756.4589)\tnorm 0.8595 (0.8705)\n",
            "Epoch: [173][180/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7733)\tD(fake) 0.2387 (0.2258)\tgrad(D) penalty 0.0150 (0.0159)\tRec loss 3570.3274 (3749.1165)\tnorm 0.8576 (0.8700)\n",
            "Epoch: [173][190/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7734 (0.7733)\tD(fake) 0.2186 (0.2257)\tgrad(D) penalty 0.0153 (0.0160)\tRec loss 3902.2354 (3753.4199)\tnorm 0.8685 (0.8699)\n",
            "Epoch: [174][  0/195]\tTime  0.432 ( 0.432)\tData  0.233 ( 0.233)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3740.2017 (3740.2017)\tnorm 0.8686 (0.8686)\n",
            "Epoch: [174][ 10/195]\tTime  0.151 ( 0.176)\tData  0.000 ( 0.022)\tD(real) 0.7731 (0.7732)\tD(fake) 0.2524 (0.2323)\tgrad(D) penalty 0.0145 (0.0160)\tRec loss 3601.1919 (3771.5793)\tnorm 0.8559 (0.8623)\n",
            "Epoch: [174][ 20/195]\tTime  0.145 ( 0.163)\tData  0.000 ( 0.011)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2270 (0.2262)\tgrad(D) penalty 0.0163 (0.0157)\tRec loss 3651.6772 (3726.0112)\tnorm 0.8702 (0.8666)\n",
            "Epoch: [174][ 30/195]\tTime  0.146 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2309 (0.2299)\tgrad(D) penalty 0.0159 (0.0155)\tRec loss 3906.3130 (3728.1845)\tnorm 0.8625 (0.8687)\n",
            "Epoch: [174][ 40/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2248 (0.2277)\tgrad(D) penalty 0.0128 (0.0155)\tRec loss 3431.2910 (3718.1607)\tnorm 0.8593 (0.8692)\n",
            "Epoch: [174][ 50/195]\tTime  0.172 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2483 (0.2286)\tgrad(D) penalty 0.0175 (0.0155)\tRec loss 3747.3618 (3736.8440)\tnorm 0.8677 (0.8691)\n",
            "Epoch: [174][ 60/195]\tTime  0.152 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2250 (0.2276)\tgrad(D) penalty 0.0166 (0.0155)\tRec loss 3993.8987 (3742.6948)\tnorm 0.8622 (0.8699)\n",
            "Epoch: [174][ 70/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2355 (0.2283)\tgrad(D) penalty 0.0176 (0.0156)\tRec loss 3711.1533 (3745.1526)\tnorm 0.8739 (0.8700)\n",
            "Epoch: [174][ 80/195]\tTime  0.144 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2125 (0.2276)\tgrad(D) penalty 0.0158 (0.0156)\tRec loss 3495.2017 (3737.9516)\tnorm 0.8590 (0.8697)\n",
            "Epoch: [174][ 90/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2364 (0.2291)\tgrad(D) penalty 0.0148 (0.0156)\tRec loss 3674.9038 (3744.3540)\tnorm 0.8703 (0.8700)\n",
            "Epoch: [174][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7729 (0.7732)\tD(fake) 0.2565 (0.2289)\tgrad(D) penalty 0.0149 (0.0156)\tRec loss 3734.8276 (3742.7778)\tnorm 0.8694 (0.8702)\n",
            "Epoch: [174][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7732)\tD(fake) 0.1983 (0.2276)\tgrad(D) penalty 0.0220 (0.0159)\tRec loss 3581.5327 (3741.9488)\tnorm 0.8649 (0.8698)\n",
            "Epoch: [174][120/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2192 (0.2278)\tgrad(D) penalty 0.0154 (0.0159)\tRec loss 3835.2524 (3745.7206)\tnorm 0.8573 (0.8695)\n",
            "Epoch: [174][130/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2194 (0.2271)\tgrad(D) penalty 0.0156 (0.0159)\tRec loss 3900.8850 (3747.5323)\tnorm 0.8775 (0.8695)\n",
            "Epoch: [174][140/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2265 (0.2279)\tgrad(D) penalty 0.0132 (0.0157)\tRec loss 3961.7039 (3751.8163)\tnorm 0.8725 (0.8698)\n",
            "Epoch: [174][150/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7732)\tD(fake) 0.2414 (0.2275)\tgrad(D) penalty 0.0147 (0.0157)\tRec loss 3614.1838 (3750.0900)\tnorm 0.8762 (0.8702)\n",
            "Epoch: [174][160/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2262 (0.2270)\tgrad(D) penalty 0.0180 (0.0157)\tRec loss 3762.3313 (3748.2609)\tnorm 0.8591 (0.8705)\n",
            "Epoch: [174][170/195]\tTime  0.144 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2057 (0.2270)\tgrad(D) penalty 0.0132 (0.0156)\tRec loss 3847.4060 (3754.6641)\tnorm 0.8780 (0.8703)\n",
            "Epoch: [174][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2298 (0.2268)\tgrad(D) penalty 0.0134 (0.0156)\tRec loss 3561.6016 (3755.1676)\tnorm 0.8684 (0.8704)\n",
            "Epoch: [174][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2432 (0.2267)\tgrad(D) penalty 0.0147 (0.0157)\tRec loss 3592.2583 (3759.5924)\tnorm 0.8715 (0.8706)\n",
            "Epoch: [175][  0/195]\tTime  0.424 ( 0.424)\tData  0.231 ( 0.231)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3642.4180 (3642.4180)\tnorm 0.8682 (0.8682)\n",
            "Epoch: [175][ 10/195]\tTime  0.147 ( 0.174)\tData  0.000 ( 0.021)\tD(real) 0.7733 (0.7733)\tD(fake) 0.2329 (0.2276)\tgrad(D) penalty 0.0162 (0.0169)\tRec loss 3864.2522 (3833.7902)\tnorm 0.8784 (0.8697)\n",
            "Epoch: [175][ 20/195]\tTime  0.147 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7731 (0.7732)\tD(fake) 0.2376 (0.2343)\tgrad(D) penalty 0.0196 (0.0179)\tRec loss 3778.7029 (3809.2506)\tnorm 0.8621 (0.8690)\n",
            "Epoch: [175][ 30/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2187 (0.2321)\tgrad(D) penalty 0.0157 (0.0173)\tRec loss 3766.6233 (3806.4738)\tnorm 0.8568 (0.8694)\n",
            "Epoch: [175][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7730 (0.7731)\tD(fake) 0.2271 (0.2324)\tgrad(D) penalty 0.0181 (0.0179)\tRec loss 3670.7766 (3797.3517)\tnorm 0.8774 (0.8716)\n",
            "Epoch: [175][ 50/195]\tTime  0.164 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2175 (0.2311)\tgrad(D) penalty 0.0139 (0.0174)\tRec loss 3683.0823 (3789.2147)\tnorm 0.8685 (0.8729)\n",
            "Epoch: [175][ 60/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7732 (0.7731)\tD(fake) 0.2175 (0.2308)\tgrad(D) penalty 0.0133 (0.0166)\tRec loss 3854.4263 (3775.5958)\tnorm 0.8658 (0.8722)\n",
            "Epoch: [175][ 70/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7732 (0.7731)\tD(fake) 0.2534 (0.2316)\tgrad(D) penalty 0.0131 (0.0164)\tRec loss 3581.3364 (3762.8417)\tnorm 0.8686 (0.8717)\n",
            "Epoch: [175][ 80/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2513 (0.2325)\tgrad(D) penalty 0.0138 (0.0161)\tRec loss 3887.2397 (3757.3002)\tnorm 0.8697 (0.8721)\n",
            "Epoch: [175][ 90/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7730 (0.7731)\tD(fake) 0.2074 (0.2307)\tgrad(D) penalty 0.0209 (0.0163)\tRec loss 3845.4287 (3764.0692)\tnorm 0.8742 (0.8724)\n",
            "Epoch: [175][100/195]\tTime  0.166 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7730 (0.7731)\tD(fake) 0.1921 (0.2307)\tgrad(D) penalty 0.0166 (0.0162)\tRec loss 3577.7612 (3759.1342)\tnorm 0.8642 (0.8721)\n",
            "Epoch: [175][110/195]\tTime  0.154 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2511 (0.2310)\tgrad(D) penalty 0.0138 (0.0161)\tRec loss 3715.0142 (3760.6910)\tnorm 0.8754 (0.8722)\n",
            "Epoch: [175][120/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2187 (0.2287)\tgrad(D) penalty 0.0197 (0.0164)\tRec loss 3653.5535 (3761.6732)\tnorm 0.8755 (0.8723)\n",
            "Epoch: [175][130/195]\tTime  0.154 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7731)\tD(fake) 0.2237 (0.2294)\tgrad(D) penalty 0.0159 (0.0164)\tRec loss 3528.2153 (3762.1672)\tnorm 0.8665 (0.8719)\n",
            "Epoch: [175][140/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7731)\tD(fake) 0.2582 (0.2286)\tgrad(D) penalty 0.0129 (0.0164)\tRec loss 3690.0974 (3762.5584)\tnorm 0.8777 (0.8718)\n",
            "Epoch: [175][150/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7731)\tD(fake) 0.1890 (0.2276)\tgrad(D) penalty 0.0166 (0.0164)\tRec loss 3566.6755 (3760.2331)\tnorm 0.8722 (0.8716)\n",
            "Epoch: [175][160/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2008 (0.2279)\tgrad(D) penalty 0.0161 (0.0163)\tRec loss 3922.4331 (3754.7788)\tnorm 0.8738 (0.8716)\n",
            "Epoch: [175][170/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2460 (0.2282)\tgrad(D) penalty 0.0136 (0.0163)\tRec loss 3782.8052 (3755.8545)\tnorm 0.8599 (0.8711)\n",
            "Epoch: [175][180/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7731)\tD(fake) 0.2405 (0.2283)\tgrad(D) penalty 0.0131 (0.0162)\tRec loss 3820.9570 (3755.2046)\tnorm 0.8674 (0.8709)\n",
            "Epoch: [175][190/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.001)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2015 (0.2275)\tgrad(D) penalty 0.0150 (0.0161)\tRec loss 3708.5398 (3756.0320)\tnorm 0.8642 (0.8707)\n",
            "Epoch: [176][  0/195]\tTime  0.426 ( 0.426)\tData  0.243 ( 0.243)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3563.9995 (3563.9995)\tnorm 0.8666 (0.8666)\n",
            "Epoch: [176][ 10/195]\tTime  0.148 ( 0.175)\tData  0.000 ( 0.022)\tD(real) 0.7730 (0.7731)\tD(fake) 0.2351 (0.2280)\tgrad(D) penalty 0.0163 (0.0162)\tRec loss 3449.7507 (3698.6270)\tnorm 0.8663 (0.8632)\n",
            "Epoch: [176][ 20/195]\tTime  0.150 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2330 (0.2267)\tgrad(D) penalty 0.0179 (0.0162)\tRec loss 3604.1704 (3724.9882)\tnorm 0.8688 (0.8666)\n",
            "Epoch: [176][ 30/195]\tTime  0.149 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7730 (0.7731)\tD(fake) 0.2158 (0.2285)\tgrad(D) penalty 0.0146 (0.0158)\tRec loss 3736.9556 (3720.8711)\tnorm 0.8742 (0.8657)\n",
            "Epoch: [176][ 40/195]\tTime  0.151 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2238 (0.2295)\tgrad(D) penalty 0.0161 (0.0157)\tRec loss 3673.9189 (3731.4526)\tnorm 0.8646 (0.8675)\n",
            "Epoch: [176][ 50/195]\tTime  0.166 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2335 (0.2271)\tgrad(D) penalty 0.0160 (0.0157)\tRec loss 3951.5964 (3747.9801)\tnorm 0.8796 (0.8665)\n",
            "Epoch: [176][ 60/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2113 (0.2265)\tgrad(D) penalty 0.0148 (0.0155)\tRec loss 3877.7832 (3747.1376)\tnorm 0.8776 (0.8668)\n",
            "Epoch: [176][ 70/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2283 (0.2265)\tgrad(D) penalty 0.0144 (0.0154)\tRec loss 3770.9116 (3748.4126)\tnorm 0.8651 (0.8672)\n",
            "Epoch: [176][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2384 (0.2270)\tgrad(D) penalty 0.0131 (0.0153)\tRec loss 3689.3157 (3752.6439)\tnorm 0.8704 (0.8679)\n",
            "Epoch: [176][ 90/195]\tTime  0.154 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2204 (0.2267)\tgrad(D) penalty 0.0201 (0.0155)\tRec loss 3885.2734 (3745.8705)\tnorm 0.8690 (0.8687)\n",
            "Epoch: [176][100/195]\tTime  0.171 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2297 (0.2274)\tgrad(D) penalty 0.0184 (0.0157)\tRec loss 3879.9390 (3756.2829)\tnorm 0.8699 (0.8691)\n",
            "Epoch: [176][110/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2498 (0.2288)\tgrad(D) penalty 0.0171 (0.0159)\tRec loss 3794.8789 (3748.3799)\tnorm 0.8643 (0.8693)\n",
            "Epoch: [176][120/195]\tTime  0.152 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7731)\tD(fake) 0.2318 (0.2283)\tgrad(D) penalty 0.0171 (0.0160)\tRec loss 3541.9260 (3750.8473)\tnorm 0.8779 (0.8698)\n",
            "Epoch: [176][130/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2166 (0.2275)\tgrad(D) penalty 0.0176 (0.0161)\tRec loss 3725.0903 (3744.9853)\tnorm 0.8739 (0.8697)\n",
            "Epoch: [176][140/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7731)\tD(fake) 0.2224 (0.2275)\tgrad(D) penalty 0.0146 (0.0161)\tRec loss 3719.2388 (3748.8416)\tnorm 0.8815 (0.8701)\n",
            "Epoch: [176][150/195]\tTime  0.168 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2315 (0.2281)\tgrad(D) penalty 0.0136 (0.0160)\tRec loss 3745.7034 (3753.5104)\tnorm 0.8853 (0.8704)\n",
            "Epoch: [176][160/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7731)\tD(fake) 0.2300 (0.2281)\tgrad(D) penalty 0.0117 (0.0159)\tRec loss 4055.0703 (3754.2139)\tnorm 0.8625 (0.8702)\n",
            "Epoch: [176][170/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2184 (0.2278)\tgrad(D) penalty 0.0120 (0.0158)\tRec loss 3781.7163 (3755.0981)\tnorm 0.8729 (0.8699)\n",
            "Epoch: [176][180/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2310 (0.2279)\tgrad(D) penalty 0.0140 (0.0157)\tRec loss 3757.2466 (3755.3785)\tnorm 0.8634 (0.8702)\n",
            "Epoch: [176][190/195]\tTime  0.155 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7731)\tD(fake) 0.2254 (0.2279)\tgrad(D) penalty 0.0146 (0.0156)\tRec loss 3649.1348 (3756.6294)\tnorm 0.8649 (0.8701)\n",
            "Epoch: [177][  0/195]\tTime  0.459 ( 0.459)\tData  0.263 ( 0.263)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3685.3884 (3685.3884)\tnorm 0.8799 (0.8799)\n",
            "Epoch: [177][ 10/195]\tTime  0.150 ( 0.180)\tData  0.000 ( 0.024)\tD(real) 0.7727 (0.7729)\tD(fake) 0.2137 (0.2144)\tgrad(D) penalty 0.0166 (0.0153)\tRec loss 3613.1575 (3723.8153)\tnorm 0.8733 (0.8720)\n",
            "Epoch: [177][ 20/195]\tTime  0.148 ( 0.166)\tData  0.000 ( 0.013)\tD(real) 0.7729 (0.7729)\tD(fake) 0.1978 (0.2237)\tgrad(D) penalty 0.0150 (0.0151)\tRec loss 3435.3357 (3731.9369)\tnorm 0.8767 (0.8746)\n",
            "Epoch: [177][ 30/195]\tTime  0.151 ( 0.161)\tData  0.000 ( 0.009)\tD(real) 0.7730 (0.7729)\tD(fake) 0.2472 (0.2242)\tgrad(D) penalty 0.0161 (0.0157)\tRec loss 3793.6421 (3745.3142)\tnorm 0.8699 (0.8726)\n",
            "Epoch: [177][ 40/195]\tTime  0.149 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2084 (0.2239)\tgrad(D) penalty 0.0200 (0.0163)\tRec loss 4065.6411 (3744.3340)\tnorm 0.8848 (0.8725)\n",
            "Epoch: [177][ 50/195]\tTime  0.174 ( 0.157)\tData  0.000 ( 0.005)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2363 (0.2261)\tgrad(D) penalty 0.0118 (0.0161)\tRec loss 3517.3650 (3746.0292)\tnorm 0.8668 (0.8720)\n",
            "Epoch: [177][ 60/195]\tTime  0.150 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2436 (0.2263)\tgrad(D) penalty 0.0176 (0.0161)\tRec loss 3515.9114 (3743.7381)\tnorm 0.8613 (0.8716)\n",
            "Epoch: [177][ 70/195]\tTime  0.153 ( 0.156)\tData  0.000 ( 0.004)\tD(real) 0.7731 (0.7730)\tD(fake) 0.1873 (0.2244)\tgrad(D) penalty 0.0141 (0.0159)\tRec loss 3708.3716 (3748.8845)\tnorm 0.8685 (0.8718)\n",
            "Epoch: [177][ 80/195]\tTime  0.150 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7730 (0.7730)\tD(fake) 0.2020 (0.2260)\tgrad(D) penalty 0.0158 (0.0157)\tRec loss 3991.9785 (3748.8036)\tnorm 0.8813 (0.8720)\n",
            "Epoch: [177][ 90/195]\tTime  0.151 ( 0.155)\tData  0.000 ( 0.003)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2523 (0.2267)\tgrad(D) penalty 0.0131 (0.0157)\tRec loss 3738.4663 (3744.0018)\tnorm 0.8714 (0.8720)\n",
            "Epoch: [177][100/195]\tTime  0.166 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7728 (0.7730)\tD(fake) 0.2125 (0.2266)\tgrad(D) penalty 0.0183 (0.0157)\tRec loss 3785.3379 (3745.6002)\tnorm 0.8788 (0.8719)\n",
            "Epoch: [177][110/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2320 (0.2271)\tgrad(D) penalty 0.0167 (0.0158)\tRec loss 3466.1392 (3748.7355)\tnorm 0.8770 (0.8721)\n",
            "Epoch: [177][120/195]\tTime  0.152 ( 0.154)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2601 (0.2272)\tgrad(D) penalty 0.0144 (0.0159)\tRec loss 3713.0337 (3748.3650)\tnorm 0.8686 (0.8721)\n",
            "Epoch: [177][130/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7729 (0.7730)\tD(fake) 0.2402 (0.2272)\tgrad(D) penalty 0.0183 (0.0159)\tRec loss 3493.1951 (3744.1244)\tnorm 0.8667 (0.8718)\n",
            "Epoch: [177][140/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2172 (0.2269)\tgrad(D) penalty 0.0167 (0.0157)\tRec loss 3625.2249 (3745.6409)\tnorm 0.8690 (0.8715)\n",
            "Epoch: [177][150/195]\tTime  0.168 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7728 (0.7730)\tD(fake) 0.2279 (0.2275)\tgrad(D) penalty 0.0146 (0.0156)\tRec loss 3988.1843 (3746.7696)\tnorm 0.8658 (0.8710)\n",
            "Epoch: [177][160/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7730)\tD(fake) 0.2314 (0.2273)\tgrad(D) penalty 0.0136 (0.0155)\tRec loss 3843.7532 (3749.8241)\tnorm 0.8735 (0.8706)\n",
            "Epoch: [177][170/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2289 (0.2270)\tgrad(D) penalty 0.0148 (0.0156)\tRec loss 3713.5676 (3746.0196)\tnorm 0.8655 (0.8703)\n",
            "Epoch: [177][180/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7730)\tD(fake) 0.2138 (0.2266)\tgrad(D) penalty 0.0141 (0.0154)\tRec loss 3723.2021 (3750.0109)\tnorm 0.8702 (0.8701)\n",
            "Epoch: [177][190/195]\tTime  0.152 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7730)\tD(fake) 0.2288 (0.2266)\tgrad(D) penalty 0.0156 (0.0154)\tRec loss 4036.4854 (3750.2270)\tnorm 0.8495 (0.8700)\n",
            "Epoch: [178][  0/195]\tTime  0.450 ( 0.450)\tData  0.262 ( 0.262)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3942.7520 (3942.7520)\tnorm 0.8743 (0.8743)\n",
            "Epoch: [178][ 10/195]\tTime  0.151 ( 0.179)\tData  0.000 ( 0.024)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2322 (0.2374)\tgrad(D) penalty 0.0130 (0.0128)\tRec loss 3942.1826 (3819.5007)\tnorm 0.8708 (0.8700)\n",
            "Epoch: [178][ 20/195]\tTime  0.151 ( 0.165)\tData  0.000 ( 0.013)\tD(real) 0.7731 (0.7732)\tD(fake) 0.2439 (0.2275)\tgrad(D) penalty 0.0123 (0.0141)\tRec loss 3812.7878 (3789.2544)\tnorm 0.8610 (0.8685)\n",
            "Epoch: [178][ 30/195]\tTime  0.155 ( 0.161)\tData  0.000 ( 0.009)\tD(real) 0.7731 (0.7732)\tD(fake) 0.2177 (0.2265)\tgrad(D) penalty 0.0157 (0.0144)\tRec loss 3872.4961 (3787.0702)\tnorm 0.8596 (0.8685)\n",
            "Epoch: [178][ 40/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.007)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2325 (0.2300)\tgrad(D) penalty 0.0161 (0.0152)\tRec loss 3684.1768 (3762.4407)\tnorm 0.8796 (0.8688)\n",
            "Epoch: [178][ 50/195]\tTime  0.165 ( 0.157)\tData  0.000 ( 0.005)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2335 (0.2292)\tgrad(D) penalty 0.0126 (0.0151)\tRec loss 3959.2217 (3778.8841)\tnorm 0.8651 (0.8687)\n",
            "Epoch: [178][ 60/195]\tTime  0.154 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2336 (0.2272)\tgrad(D) penalty 0.0132 (0.0149)\tRec loss 3620.6724 (3780.3199)\tnorm 0.8657 (0.8682)\n",
            "Epoch: [178][ 70/195]\tTime  0.153 ( 0.156)\tData  0.000 ( 0.004)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2107 (0.2269)\tgrad(D) penalty 0.0151 (0.0147)\tRec loss 3533.4170 (3778.5094)\tnorm 0.8672 (0.8687)\n",
            "Epoch: [178][ 80/195]\tTime  0.150 ( 0.155)\tData  0.000 ( 0.003)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2298 (0.2267)\tgrad(D) penalty 0.0184 (0.0148)\tRec loss 3720.0872 (3768.5906)\tnorm 0.8625 (0.8685)\n",
            "Epoch: [178][ 90/195]\tTime  0.152 ( 0.155)\tData  0.000 ( 0.003)\tD(real) 0.7730 (0.7732)\tD(fake) 0.2213 (0.2264)\tgrad(D) penalty 0.0164 (0.0148)\tRec loss 3725.2554 (3768.1712)\tnorm 0.8607 (0.8679)\n",
            "Epoch: [178][100/195]\tTime  0.167 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2165 (0.2270)\tgrad(D) penalty 0.0144 (0.0147)\tRec loss 3679.3159 (3766.1039)\tnorm 0.8758 (0.8680)\n",
            "Epoch: [178][110/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2386 (0.2278)\tgrad(D) penalty 0.0145 (0.0147)\tRec loss 3802.5125 (3755.6428)\tnorm 0.8590 (0.8680)\n",
            "Epoch: [178][120/195]\tTime  0.152 ( 0.154)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2533 (0.2276)\tgrad(D) penalty 0.0150 (0.0149)\tRec loss 3765.5693 (3757.3112)\tnorm 0.8777 (0.8682)\n",
            "Epoch: [178][130/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2165 (0.2280)\tgrad(D) penalty 0.0170 (0.0149)\tRec loss 3655.5991 (3752.6595)\tnorm 0.8751 (0.8682)\n",
            "Epoch: [178][140/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7731 (0.7732)\tD(fake) 0.2205 (0.2279)\tgrad(D) penalty 0.0117 (0.0148)\tRec loss 3671.2122 (3752.6120)\tnorm 0.8742 (0.8686)\n",
            "Epoch: [178][150/195]\tTime  0.167 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2390 (0.2285)\tgrad(D) penalty 0.0145 (0.0149)\tRec loss 4047.9648 (3753.9379)\tnorm 0.8732 (0.8690)\n",
            "Epoch: [178][160/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2339 (0.2280)\tgrad(D) penalty 0.0111 (0.0148)\tRec loss 3635.2285 (3751.5879)\tnorm 0.8748 (0.8690)\n",
            "Epoch: [178][170/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2218 (0.2278)\tgrad(D) penalty 0.0138 (0.0147)\tRec loss 4073.7812 (3752.3266)\tnorm 0.8746 (0.8691)\n",
            "Epoch: [178][180/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2295 (0.2278)\tgrad(D) penalty 0.0148 (0.0146)\tRec loss 3844.3511 (3755.0938)\tnorm 0.8579 (0.8689)\n",
            "Epoch: [178][190/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7732)\tD(fake) 0.2254 (0.2277)\tgrad(D) penalty 0.0145 (0.0146)\tRec loss 3638.9441 (3750.4885)\tnorm 0.8718 (0.8690)\n",
            "Epoch: [179][  0/195]\tTime  0.419 ( 0.419)\tData  0.241 ( 0.241)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3853.0393 (3853.0393)\tnorm 0.8674 (0.8674)\n",
            "Epoch: [179][ 10/195]\tTime  0.147 ( 0.173)\tData  0.000 ( 0.022)\tD(real) 0.7732 (0.7732)\tD(fake) 0.2274 (0.2230)\tgrad(D) penalty 0.0115 (0.0152)\tRec loss 3808.7610 (3697.1031)\tnorm 0.8614 (0.8645)\n",
            "Epoch: [179][ 20/195]\tTime  0.151 ( 0.161)\tData  0.000 ( 0.012)\tD(real) 0.7734 (0.7733)\tD(fake) 0.2418 (0.2252)\tgrad(D) penalty 0.0138 (0.0150)\tRec loss 3691.5532 (3690.3849)\tnorm 0.8658 (0.8646)\n",
            "Epoch: [179][ 30/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7733 (0.7733)\tD(fake) 0.1955 (0.2227)\tgrad(D) penalty 0.0144 (0.0145)\tRec loss 3620.2437 (3683.1827)\tnorm 0.8608 (0.8632)\n",
            "Epoch: [179][ 40/195]\tTime  0.153 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7734 (0.7733)\tD(fake) 0.2125 (0.2245)\tgrad(D) penalty 0.0154 (0.0148)\tRec loss 3553.9595 (3694.8423)\tnorm 0.8671 (0.8642)\n",
            "Epoch: [179][ 50/195]\tTime  0.173 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7734 (0.7733)\tD(fake) 0.2432 (0.2243)\tgrad(D) penalty 0.0172 (0.0154)\tRec loss 3608.4766 (3716.2395)\tnorm 0.8699 (0.8664)\n",
            "Epoch: [179][ 60/195]\tTime  0.151 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2241 (0.2245)\tgrad(D) penalty 0.0133 (0.0151)\tRec loss 3922.9470 (3716.8584)\tnorm 0.8543 (0.8665)\n",
            "Epoch: [179][ 70/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2258 (0.2252)\tgrad(D) penalty 0.0127 (0.0149)\tRec loss 3499.6262 (3712.6018)\tnorm 0.8780 (0.8667)\n",
            "Epoch: [179][ 80/195]\tTime  0.153 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2230 (0.2251)\tgrad(D) penalty 0.0128 (0.0147)\tRec loss 3675.7886 (3708.1228)\tnorm 0.8553 (0.8673)\n",
            "Epoch: [179][ 90/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7736 (0.7734)\tD(fake) 0.2240 (0.2251)\tgrad(D) penalty 0.0143 (0.0147)\tRec loss 3633.1487 (3699.7854)\tnorm 0.8653 (0.8666)\n",
            "Epoch: [179][100/195]\tTime  0.167 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2225 (0.2250)\tgrad(D) penalty 0.0168 (0.0147)\tRec loss 3692.6772 (3702.7934)\tnorm 0.8682 (0.8667)\n",
            "Epoch: [179][110/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2196 (0.2256)\tgrad(D) penalty 0.0145 (0.0147)\tRec loss 3581.9458 (3709.9819)\tnorm 0.8721 (0.8672)\n",
            "Epoch: [179][120/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2453 (0.2259)\tgrad(D) penalty 0.0148 (0.0148)\tRec loss 4090.6904 (3724.2623)\tnorm 0.8731 (0.8674)\n",
            "Epoch: [179][130/195]\tTime  0.155 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2082 (0.2254)\tgrad(D) penalty 0.0178 (0.0149)\tRec loss 3678.0508 (3732.1681)\tnorm 0.8834 (0.8685)\n",
            "Epoch: [179][140/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2233 (0.2254)\tgrad(D) penalty 0.0176 (0.0150)\tRec loss 3502.6226 (3731.2301)\tnorm 0.8589 (0.8686)\n",
            "Epoch: [179][150/195]\tTime  0.164 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7734)\tD(fake) 0.2359 (0.2254)\tgrad(D) penalty 0.0154 (0.0151)\tRec loss 3756.1182 (3735.5424)\tnorm 0.8722 (0.8690)\n",
            "Epoch: [179][160/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7734)\tD(fake) 0.2296 (0.2258)\tgrad(D) penalty 0.0129 (0.0151)\tRec loss 3689.3274 (3738.6429)\tnorm 0.8733 (0.8692)\n",
            "Epoch: [179][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2208 (0.2257)\tgrad(D) penalty 0.0120 (0.0150)\tRec loss 3884.2183 (3743.1025)\tnorm 0.8700 (0.8694)\n",
            "Epoch: [179][180/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7734)\tD(fake) 0.2060 (0.2256)\tgrad(D) penalty 0.0120 (0.0148)\tRec loss 4009.1421 (3744.1642)\tnorm 0.8745 (0.8695)\n",
            "Epoch: [179][190/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2352 (0.2263)\tgrad(D) penalty 0.0109 (0.0147)\tRec loss 4027.6575 (3747.3230)\tnorm 0.8676 (0.8697)\n",
            "Epoch: [180][  0/195]\tTime  0.437 ( 0.437)\tData  0.244 ( 0.244)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3771.7705 (3771.7705)\tnorm 0.8780 (0.8780)\n",
            "Epoch: [180][ 10/195]\tTime  0.147 ( 0.174)\tData  0.000 ( 0.022)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2129 (0.2412)\tgrad(D) penalty 0.0118 (0.0115)\tRec loss 3853.8809 (3786.5023)\tnorm 0.8763 (0.8754)\n",
            "Epoch: [180][ 20/195]\tTime  0.147 ( 0.161)\tData  0.000 ( 0.012)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2329 (0.2368)\tgrad(D) penalty 0.0119 (0.0128)\tRec loss 3471.2263 (3786.3366)\tnorm 0.8698 (0.8747)\n",
            "Epoch: [180][ 30/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7736 (0.7735)\tD(fake) 0.2508 (0.2353)\tgrad(D) penalty 0.0143 (0.0141)\tRec loss 3712.8647 (3763.7909)\tnorm 0.8634 (0.8734)\n",
            "Epoch: [180][ 40/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.006)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2266 (0.2324)\tgrad(D) penalty 0.0171 (0.0147)\tRec loss 3474.1707 (3759.4475)\tnorm 0.8819 (0.8730)\n",
            "Epoch: [180][ 50/195]\tTime  0.166 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2327 (0.2348)\tgrad(D) penalty 0.0118 (0.0145)\tRec loss 3486.8022 (3748.5835)\tnorm 0.8701 (0.8730)\n",
            "Epoch: [180][ 60/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7734 (0.7735)\tD(fake) 0.2548 (0.2335)\tgrad(D) penalty 0.0138 (0.0150)\tRec loss 3720.3223 (3751.5379)\tnorm 0.8762 (0.8727)\n",
            "Epoch: [180][ 70/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7735 (0.7735)\tD(fake) 0.2207 (0.2319)\tgrad(D) penalty 0.0135 (0.0150)\tRec loss 3815.9834 (3749.1242)\tnorm 0.8748 (0.8731)\n",
            "Epoch: [180][ 80/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7736 (0.7735)\tD(fake) 0.2253 (0.2312)\tgrad(D) penalty 0.0116 (0.0147)\tRec loss 3801.5476 (3744.5681)\tnorm 0.8582 (0.8732)\n",
            "Epoch: [180][ 90/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7735 (0.7735)\tD(fake) 0.1993 (0.2301)\tgrad(D) penalty 0.0137 (0.0146)\tRec loss 3594.9600 (3740.4339)\tnorm 0.8947 (0.8736)\n",
            "Epoch: [180][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7736 (0.7735)\tD(fake) 0.2357 (0.2306)\tgrad(D) penalty 0.0118 (0.0144)\tRec loss 3667.6206 (3734.4703)\tnorm 0.8702 (0.8735)\n",
            "Epoch: [180][110/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7735)\tD(fake) 0.2647 (0.2305)\tgrad(D) penalty 0.0153 (0.0145)\tRec loss 3528.7354 (3731.3506)\tnorm 0.8756 (0.8729)\n",
            "Epoch: [180][120/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7735)\tD(fake) 0.2059 (0.2294)\tgrad(D) penalty 0.0232 (0.0147)\tRec loss 3851.0347 (3738.7292)\tnorm 0.8671 (0.8729)\n",
            "Epoch: [180][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7735)\tD(fake) 0.2578 (0.2318)\tgrad(D) penalty 0.0133 (0.0145)\tRec loss 3735.2673 (3741.0502)\tnorm 0.8703 (0.8723)\n",
            "Epoch: [180][140/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7735)\tD(fake) 0.2736 (0.2312)\tgrad(D) penalty 0.0183 (0.0148)\tRec loss 4090.3884 (3739.1631)\tnorm 0.8698 (0.8722)\n",
            "Epoch: [180][150/195]\tTime  0.165 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7735)\tD(fake) 0.2044 (0.2316)\tgrad(D) penalty 0.0167 (0.0147)\tRec loss 3666.1851 (3741.1422)\tnorm 0.8656 (0.8718)\n",
            "Epoch: [180][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7735)\tD(fake) 0.2340 (0.2317)\tgrad(D) penalty 0.0135 (0.0148)\tRec loss 3644.0325 (3741.5876)\tnorm 0.8551 (0.8716)\n",
            "Epoch: [180][170/195]\tTime  0.144 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7735)\tD(fake) 0.2350 (0.2314)\tgrad(D) penalty 0.0122 (0.0148)\tRec loss 3625.2053 (3745.0732)\tnorm 0.8633 (0.8710)\n",
            "Epoch: [180][180/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7735)\tD(fake) 0.2130 (0.2308)\tgrad(D) penalty 0.0131 (0.0147)\tRec loss 3684.8108 (3740.5567)\tnorm 0.8579 (0.8706)\n",
            "Epoch: [180][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7735)\tD(fake) 0.2341 (0.2309)\tgrad(D) penalty 0.0123 (0.0147)\tRec loss 3920.7979 (3742.2027)\tnorm 0.8614 (0.8706)\n",
            "Epoch: [181][  0/195]\tTime  0.445 ( 0.445)\tData  0.264 ( 0.264)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3487.0088 (3487.0088)\tnorm 0.8604 (0.8604)\n",
            "Epoch: [181][ 10/195]\tTime  0.147 ( 0.176)\tData  0.000 ( 0.024)\tD(real) 0.7736 (0.7735)\tD(fake) 0.2446 (0.2413)\tgrad(D) penalty 0.0131 (0.0134)\tRec loss 3775.6416 (3667.5340)\tnorm 0.8714 (0.8661)\n",
            "Epoch: [181][ 20/195]\tTime  0.146 ( 0.162)\tData  0.000 ( 0.013)\tD(real) 0.7735 (0.7735)\tD(fake) 0.2214 (0.2300)\tgrad(D) penalty 0.0153 (0.0144)\tRec loss 3864.1123 (3682.1327)\tnorm 0.8819 (0.8708)\n",
            "Epoch: [181][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.009)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2120 (0.2291)\tgrad(D) penalty 0.0131 (0.0140)\tRec loss 3688.5107 (3695.9943)\tnorm 0.8879 (0.8709)\n",
            "Epoch: [181][ 40/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.007)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2217 (0.2293)\tgrad(D) penalty 0.0120 (0.0137)\tRec loss 3964.1641 (3722.0415)\tnorm 0.8695 (0.8712)\n",
            "Epoch: [181][ 50/195]\tTime  0.165 ( 0.153)\tData  0.000 ( 0.005)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2232 (0.2263)\tgrad(D) penalty 0.0188 (0.0142)\tRec loss 3866.3403 (3738.7711)\tnorm 0.8784 (0.8719)\n",
            "Epoch: [181][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.005)\tD(real) 0.7733 (0.7734)\tD(fake) 0.2091 (0.2277)\tgrad(D) penalty 0.0212 (0.0146)\tRec loss 3843.2859 (3744.0756)\tnorm 0.8675 (0.8714)\n",
            "Epoch: [181][ 70/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2330 (0.2283)\tgrad(D) penalty 0.0153 (0.0148)\tRec loss 3970.3228 (3752.8155)\tnorm 0.8692 (0.8714)\n",
            "Epoch: [181][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2294 (0.2273)\tgrad(D) penalty 0.0177 (0.0151)\tRec loss 3802.5098 (3748.2258)\tnorm 0.8837 (0.8720)\n",
            "Epoch: [181][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2052 (0.2267)\tgrad(D) penalty 0.0111 (0.0151)\tRec loss 3869.2810 (3744.7621)\tnorm 0.8667 (0.8718)\n",
            "Epoch: [181][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2171 (0.2272)\tgrad(D) penalty 0.0146 (0.0151)\tRec loss 3564.7964 (3739.9734)\tnorm 0.8750 (0.8721)\n",
            "Epoch: [181][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2544 (0.2278)\tgrad(D) penalty 0.0126 (0.0151)\tRec loss 3348.7314 (3741.4254)\tnorm 0.8681 (0.8719)\n",
            "Epoch: [181][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7732 (0.7734)\tD(fake) 0.2427 (0.2277)\tgrad(D) penalty 0.0142 (0.0151)\tRec loss 3849.2563 (3746.4434)\tnorm 0.8700 (0.8725)\n",
            "Epoch: [181][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7734)\tD(fake) 0.1994 (0.2264)\tgrad(D) penalty 0.0132 (0.0150)\tRec loss 3816.5947 (3748.6847)\tnorm 0.8496 (0.8719)\n",
            "Epoch: [181][140/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2194 (0.2270)\tgrad(D) penalty 0.0145 (0.0149)\tRec loss 3785.9238 (3751.4890)\tnorm 0.8689 (0.8716)\n",
            "Epoch: [181][150/195]\tTime  0.167 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2277 (0.2271)\tgrad(D) penalty 0.0136 (0.0149)\tRec loss 3830.7466 (3746.1626)\tnorm 0.8602 (0.8711)\n",
            "Epoch: [181][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2217 (0.2271)\tgrad(D) penalty 0.0140 (0.0148)\tRec loss 3733.4741 (3743.9185)\tnorm 0.8652 (0.8709)\n",
            "Epoch: [181][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2427 (0.2268)\tgrad(D) penalty 0.0118 (0.0147)\tRec loss 3704.9446 (3745.0894)\tnorm 0.8673 (0.8705)\n",
            "Epoch: [181][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2171 (0.2267)\tgrad(D) penalty 0.0144 (0.0146)\tRec loss 3898.6536 (3744.2535)\tnorm 0.8798 (0.8708)\n",
            "Epoch: [181][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2328 (0.2270)\tgrad(D) penalty 0.0134 (0.0146)\tRec loss 3537.3369 (3744.6188)\tnorm 0.8699 (0.8707)\n",
            "Epoch: [182][  0/195]\tTime  0.458 ( 0.458)\tData  0.261 ( 0.261)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3803.3789 (3803.3789)\tnorm 0.8656 (0.8656)\n",
            "Epoch: [182][ 10/195]\tTime  0.147 ( 0.177)\tData  0.000 ( 0.024)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2460 (0.2444)\tgrad(D) penalty 0.0121 (0.0138)\tRec loss 4221.5293 (3740.1401)\tnorm 0.8792 (0.8676)\n",
            "Epoch: [182][ 20/195]\tTime  0.150 ( 0.163)\tData  0.000 ( 0.013)\tD(real) 0.7733 (0.7733)\tD(fake) 0.2549 (0.2318)\tgrad(D) penalty 0.0151 (0.0147)\tRec loss 3611.8049 (3722.1769)\tnorm 0.8643 (0.8719)\n",
            "Epoch: [182][ 30/195]\tTime  0.147 ( 0.158)\tData  0.000 ( 0.009)\tD(real) 0.7733 (0.7733)\tD(fake) 0.2016 (0.2262)\tgrad(D) penalty 0.0182 (0.0149)\tRec loss 3905.7896 (3736.3826)\tnorm 0.8796 (0.8707)\n",
            "Epoch: [182][ 40/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7733 (0.7733)\tD(fake) 0.2213 (0.2291)\tgrad(D) penalty 0.0134 (0.0147)\tRec loss 3761.7988 (3723.1015)\tnorm 0.8808 (0.8718)\n",
            "Epoch: [182][ 50/195]\tTime  0.164 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7734 (0.7733)\tD(fake) 0.2338 (0.2262)\tgrad(D) penalty 0.0168 (0.0151)\tRec loss 3653.9282 (3714.7846)\tnorm 0.8722 (0.8713)\n",
            "Epoch: [182][ 60/195]\tTime  0.145 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2192 (0.2262)\tgrad(D) penalty 0.0195 (0.0151)\tRec loss 3806.9258 (3710.8147)\tnorm 0.8669 (0.8702)\n",
            "Epoch: [182][ 70/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2326 (0.2296)\tgrad(D) penalty 0.0127 (0.0149)\tRec loss 3681.9585 (3713.9778)\tnorm 0.8732 (0.8697)\n",
            "Epoch: [182][ 80/195]\tTime  0.146 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2600 (0.2307)\tgrad(D) penalty 0.0109 (0.0149)\tRec loss 3755.5232 (3720.8649)\tnorm 0.8723 (0.8690)\n",
            "Epoch: [182][ 90/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2096 (0.2291)\tgrad(D) penalty 0.0191 (0.0152)\tRec loss 3560.9050 (3721.7947)\tnorm 0.8671 (0.8692)\n",
            "Epoch: [182][100/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2237 (0.2296)\tgrad(D) penalty 0.0153 (0.0151)\tRec loss 3716.6985 (3723.6496)\tnorm 0.8615 (0.8692)\n",
            "Epoch: [182][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7732 (0.7734)\tD(fake) 0.2471 (0.2290)\tgrad(D) penalty 0.0119 (0.0152)\tRec loss 3666.0828 (3720.6130)\tnorm 0.8659 (0.8693)\n",
            "Epoch: [182][120/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2314 (0.2289)\tgrad(D) penalty 0.0164 (0.0151)\tRec loss 3854.8511 (3725.8764)\tnorm 0.8857 (0.8695)\n",
            "Epoch: [182][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2222 (0.2293)\tgrad(D) penalty 0.0137 (0.0150)\tRec loss 3961.6282 (3727.4842)\tnorm 0.8720 (0.8701)\n",
            "Epoch: [182][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7734)\tD(fake) 0.2136 (0.2290)\tgrad(D) penalty 0.0134 (0.0149)\tRec loss 3651.2949 (3731.8683)\tnorm 0.8564 (0.8697)\n",
            "Epoch: [182][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2394 (0.2291)\tgrad(D) penalty 0.0123 (0.0149)\tRec loss 3649.9814 (3739.1435)\tnorm 0.8618 (0.8698)\n",
            "Epoch: [182][160/195]\tTime  0.153 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7734)\tD(fake) 0.2197 (0.2282)\tgrad(D) penalty 0.0124 (0.0147)\tRec loss 3792.1050 (3742.5128)\tnorm 0.8615 (0.8701)\n",
            "Epoch: [182][170/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7734)\tD(fake) 0.2278 (0.2278)\tgrad(D) penalty 0.0153 (0.0148)\tRec loss 3714.1377 (3740.2563)\tnorm 0.8647 (0.8700)\n",
            "Epoch: [182][180/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7734)\tD(fake) 0.2236 (0.2277)\tgrad(D) penalty 0.0161 (0.0148)\tRec loss 3656.3447 (3742.0037)\tnorm 0.8789 (0.8699)\n",
            "Epoch: [182][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7734)\tD(fake) 0.2046 (0.2271)\tgrad(D) penalty 0.0189 (0.0150)\tRec loss 3779.6372 (3738.9043)\tnorm 0.8740 (0.8699)\n",
            "Epoch: [183][  0/195]\tTime  0.438 ( 0.438)\tData  0.242 ( 0.242)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3714.8210 (3714.8210)\tnorm 0.8687 (0.8687)\n",
            "Epoch: [183][ 10/195]\tTime  0.145 ( 0.175)\tData  0.000 ( 0.022)\tD(real) 0.7735 (0.7735)\tD(fake) 0.2322 (0.2296)\tgrad(D) penalty 0.0143 (0.0152)\tRec loss 3809.3579 (3656.0631)\tnorm 0.8728 (0.8646)\n",
            "Epoch: [183][ 20/195]\tTime  0.147 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7736 (0.7735)\tD(fake) 0.2269 (0.2252)\tgrad(D) penalty 0.0148 (0.0151)\tRec loss 3944.1924 (3693.5353)\tnorm 0.8650 (0.8668)\n",
            "Epoch: [183][ 30/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7735 (0.7735)\tD(fake) 0.2310 (0.2268)\tgrad(D) penalty 0.0151 (0.0150)\tRec loss 3753.1782 (3681.7612)\tnorm 0.8602 (0.8664)\n",
            "Epoch: [183][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7737 (0.7735)\tD(fake) 0.2324 (0.2244)\tgrad(D) penalty 0.0166 (0.0154)\tRec loss 3911.3853 (3717.2814)\tnorm 0.8755 (0.8679)\n",
            "Epoch: [183][ 50/195]\tTime  0.165 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7731 (0.7735)\tD(fake) 0.2229 (0.2254)\tgrad(D) penalty 0.0152 (0.0152)\tRec loss 3807.5640 (3725.7898)\tnorm 0.8746 (0.8690)\n",
            "Epoch: [183][ 60/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7735 (0.7735)\tD(fake) 0.2226 (0.2263)\tgrad(D) penalty 0.0143 (0.0148)\tRec loss 3491.4558 (3718.3834)\tnorm 0.8818 (0.8694)\n",
            "Epoch: [183][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7737 (0.7735)\tD(fake) 0.2302 (0.2266)\tgrad(D) penalty 0.0130 (0.0146)\tRec loss 3999.3291 (3717.7722)\tnorm 0.8853 (0.8693)\n",
            "Epoch: [183][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2160 (0.2265)\tgrad(D) penalty 0.0126 (0.0144)\tRec loss 3860.8054 (3729.2058)\tnorm 0.8748 (0.8703)\n",
            "Epoch: [183][ 90/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2526 (0.2271)\tgrad(D) penalty 0.0153 (0.0144)\tRec loss 3778.3611 (3729.6712)\tnorm 0.8816 (0.8710)\n",
            "Epoch: [183][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7734 (0.7736)\tD(fake) 0.2224 (0.2267)\tgrad(D) penalty 0.0181 (0.0145)\tRec loss 3834.4485 (3731.5685)\tnorm 0.8729 (0.8709)\n",
            "Epoch: [183][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2231 (0.2275)\tgrad(D) penalty 0.0136 (0.0144)\tRec loss 3654.1858 (3736.3049)\tnorm 0.8720 (0.8707)\n",
            "Epoch: [183][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2241 (0.2267)\tgrad(D) penalty 0.0144 (0.0146)\tRec loss 3762.7737 (3739.5298)\tnorm 0.8543 (0.8709)\n",
            "Epoch: [183][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7736)\tD(fake) 0.2407 (0.2274)\tgrad(D) penalty 0.0113 (0.0145)\tRec loss 3770.2751 (3734.3930)\tnorm 0.8614 (0.8707)\n",
            "Epoch: [183][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2232 (0.2264)\tgrad(D) penalty 0.0140 (0.0145)\tRec loss 3860.9341 (3736.4337)\tnorm 0.8663 (0.8707)\n",
            "Epoch: [183][150/195]\tTime  0.164 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2174 (0.2263)\tgrad(D) penalty 0.0163 (0.0145)\tRec loss 3875.0916 (3739.4378)\tnorm 0.8694 (0.8706)\n",
            "Epoch: [183][160/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7735)\tD(fake) 0.2148 (0.2264)\tgrad(D) penalty 0.0135 (0.0145)\tRec loss 3666.8276 (3737.0463)\tnorm 0.8616 (0.8705)\n",
            "Epoch: [183][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7735)\tD(fake) 0.2343 (0.2265)\tgrad(D) penalty 0.0149 (0.0146)\tRec loss 3817.7563 (3738.1264)\tnorm 0.8706 (0.8702)\n",
            "Epoch: [183][180/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7735)\tD(fake) 0.2261 (0.2267)\tgrad(D) penalty 0.0141 (0.0145)\tRec loss 3573.4905 (3738.9667)\tnorm 0.8672 (0.8702)\n",
            "Epoch: [183][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7735 (0.7735)\tD(fake) 0.2257 (0.2266)\tgrad(D) penalty 0.0113 (0.0145)\tRec loss 3884.9819 (3745.7315)\tnorm 0.8631 (0.8703)\n",
            "Epoch: [184][  0/195]\tTime  0.469 ( 0.469)\tData  0.269 ( 0.269)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3734.5430 (3734.5430)\tnorm 0.8725 (0.8725)\n",
            "Epoch: [184][ 10/195]\tTime  0.150 ( 0.178)\tData  0.000 ( 0.025)\tD(real) 0.7734 (0.7735)\tD(fake) 0.2077 (0.2188)\tgrad(D) penalty 0.0154 (0.0136)\tRec loss 3579.8691 (3735.9959)\tnorm 0.8677 (0.8734)\n",
            "Epoch: [184][ 20/195]\tTime  0.149 ( 0.164)\tData  0.000 ( 0.013)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2392 (0.2279)\tgrad(D) penalty 0.0134 (0.0144)\tRec loss 3525.2456 (3714.9827)\tnorm 0.8767 (0.8726)\n",
            "Epoch: [184][ 30/195]\tTime  0.147 ( 0.158)\tData  0.000 ( 0.009)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2120 (0.2224)\tgrad(D) penalty 0.0173 (0.0151)\tRec loss 3702.6479 (3728.9873)\tnorm 0.8696 (0.8711)\n",
            "Epoch: [184][ 40/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2115 (0.2252)\tgrad(D) penalty 0.0162 (0.0152)\tRec loss 3655.9316 (3721.9152)\tnorm 0.8667 (0.8710)\n",
            "Epoch: [184][ 50/195]\tTime  0.167 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7735 (0.7736)\tD(fake) 0.2445 (0.2266)\tgrad(D) penalty 0.0122 (0.0152)\tRec loss 3691.7231 (3722.6356)\tnorm 0.8647 (0.8710)\n",
            "Epoch: [184][ 60/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.005)\tD(real) 0.7737 (0.7736)\tD(fake) 0.2416 (0.2270)\tgrad(D) penalty 0.0142 (0.0152)\tRec loss 3678.8169 (3723.0397)\tnorm 0.8636 (0.8701)\n",
            "Epoch: [184][ 70/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2240 (0.2269)\tgrad(D) penalty 0.0140 (0.0149)\tRec loss 3792.4976 (3721.2840)\tnorm 0.8453 (0.8695)\n",
            "Epoch: [184][ 80/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7737 (0.7736)\tD(fake) 0.2357 (0.2272)\tgrad(D) penalty 0.0133 (0.0149)\tRec loss 3676.6211 (3728.2560)\tnorm 0.8727 (0.8688)\n",
            "Epoch: [184][ 90/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7736)\tD(fake) 0.2133 (0.2257)\tgrad(D) penalty 0.0159 (0.0150)\tRec loss 3615.1475 (3728.5730)\tnorm 0.8682 (0.8688)\n",
            "Epoch: [184][100/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7736)\tD(fake) 0.2201 (0.2260)\tgrad(D) penalty 0.0165 (0.0149)\tRec loss 3774.0405 (3725.7201)\tnorm 0.8715 (0.8690)\n",
            "Epoch: [184][110/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7736)\tD(fake) 0.2235 (0.2260)\tgrad(D) penalty 0.0179 (0.0151)\tRec loss 3552.9407 (3729.1708)\tnorm 0.8666 (0.8693)\n",
            "Epoch: [184][120/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7736)\tD(fake) 0.2242 (0.2256)\tgrad(D) penalty 0.0129 (0.0149)\tRec loss 3685.4163 (3730.2519)\tnorm 0.8687 (0.8693)\n",
            "Epoch: [184][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2296 (0.2258)\tgrad(D) penalty 0.0130 (0.0148)\tRec loss 3777.6509 (3727.2090)\tnorm 0.8809 (0.8696)\n",
            "Epoch: [184][140/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7736)\tD(fake) 0.2146 (0.2255)\tgrad(D) penalty 0.0186 (0.0147)\tRec loss 3685.3691 (3732.4570)\tnorm 0.8804 (0.8695)\n",
            "Epoch: [184][150/195]\tTime  0.162 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2213 (0.2259)\tgrad(D) penalty 0.0136 (0.0145)\tRec loss 3836.5903 (3731.1654)\tnorm 0.8586 (0.8690)\n",
            "Epoch: [184][160/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7736)\tD(fake) 0.2083 (0.2256)\tgrad(D) penalty 0.0149 (0.0146)\tRec loss 3877.1875 (3732.0713)\tnorm 0.8752 (0.8687)\n",
            "Epoch: [184][170/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2482 (0.2263)\tgrad(D) penalty 0.0137 (0.0145)\tRec loss 4014.0703 (3733.1117)\tnorm 0.8725 (0.8687)\n",
            "Epoch: [184][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7733 (0.7736)\tD(fake) 0.2416 (0.2264)\tgrad(D) penalty 0.0143 (0.0146)\tRec loss 3773.1853 (3734.3041)\tnorm 0.8755 (0.8689)\n",
            "Epoch: [184][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7736)\tD(fake) 0.2299 (0.2265)\tgrad(D) penalty 0.0141 (0.0146)\tRec loss 3926.6692 (3739.2742)\tnorm 0.8882 (0.8690)\n",
            "Epoch: [185][  0/195]\tTime  0.439 ( 0.439)\tData  0.243 ( 0.243)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3702.6382 (3702.6382)\tnorm 0.8826 (0.8826)\n",
            "Epoch: [185][ 10/195]\tTime  0.147 ( 0.175)\tData  0.000 ( 0.022)\tD(real) 0.7736 (0.7737)\tD(fake) 0.2403 (0.2350)\tgrad(D) penalty 0.0148 (0.0163)\tRec loss 3890.6099 (3753.0772)\tnorm 0.8700 (0.8732)\n",
            "Epoch: [185][ 20/195]\tTime  0.147 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2283 (0.2261)\tgrad(D) penalty 0.0158 (0.0172)\tRec loss 3746.0117 (3716.7478)\tnorm 0.8502 (0.8704)\n",
            "Epoch: [185][ 30/195]\tTime  0.148 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2013 (0.2207)\tgrad(D) penalty 0.0164 (0.0167)\tRec loss 3439.0679 (3753.7990)\tnorm 0.8817 (0.8706)\n",
            "Epoch: [185][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7739 (0.7737)\tD(fake) 0.2059 (0.2232)\tgrad(D) penalty 0.0137 (0.0163)\tRec loss 3446.7827 (3722.4534)\tnorm 0.8629 (0.8717)\n",
            "Epoch: [185][ 50/195]\tTime  0.162 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2567 (0.2247)\tgrad(D) penalty 0.0130 (0.0163)\tRec loss 3822.2231 (3729.5782)\tnorm 0.8607 (0.8708)\n",
            "Epoch: [185][ 60/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2338 (0.2245)\tgrad(D) penalty 0.0151 (0.0160)\tRec loss 3822.5442 (3735.0080)\tnorm 0.8679 (0.8710)\n",
            "Epoch: [185][ 70/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2176 (0.2249)\tgrad(D) penalty 0.0165 (0.0159)\tRec loss 3496.5488 (3734.3101)\tnorm 0.8707 (0.8711)\n",
            "Epoch: [185][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2338 (0.2265)\tgrad(D) penalty 0.0134 (0.0154)\tRec loss 3948.8237 (3735.0095)\tnorm 0.8520 (0.8709)\n",
            "Epoch: [185][ 90/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2395 (0.2260)\tgrad(D) penalty 0.0122 (0.0153)\tRec loss 3643.6531 (3730.2816)\tnorm 0.8722 (0.8712)\n",
            "Epoch: [185][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2359 (0.2260)\tgrad(D) penalty 0.0120 (0.0150)\tRec loss 3659.4836 (3735.6863)\tnorm 0.8771 (0.8710)\n",
            "Epoch: [185][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2263 (0.2258)\tgrad(D) penalty 0.0153 (0.0150)\tRec loss 4048.4702 (3739.5121)\tnorm 0.8585 (0.8710)\n",
            "Epoch: [185][120/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2296 (0.2254)\tgrad(D) penalty 0.0160 (0.0150)\tRec loss 3883.5361 (3739.4213)\tnorm 0.8524 (0.8708)\n",
            "Epoch: [185][130/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2319 (0.2257)\tgrad(D) penalty 0.0132 (0.0149)\tRec loss 3752.7773 (3739.2357)\tnorm 0.8665 (0.8705)\n",
            "Epoch: [185][140/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2309 (0.2260)\tgrad(D) penalty 0.0133 (0.0148)\tRec loss 3734.1973 (3740.8928)\tnorm 0.8587 (0.8707)\n",
            "Epoch: [185][150/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7738)\tD(fake) 0.2049 (0.2252)\tgrad(D) penalty 0.0152 (0.0148)\tRec loss 3701.6304 (3740.0300)\tnorm 0.8680 (0.8706)\n",
            "Epoch: [185][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2249 (0.2259)\tgrad(D) penalty 0.0143 (0.0148)\tRec loss 3653.2349 (3741.2427)\tnorm 0.8887 (0.8707)\n",
            "Epoch: [185][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2234 (0.2257)\tgrad(D) penalty 0.0162 (0.0148)\tRec loss 3952.1162 (3738.6640)\tnorm 0.8693 (0.8708)\n",
            "Epoch: [185][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2234 (0.2263)\tgrad(D) penalty 0.0159 (0.0148)\tRec loss 3820.2310 (3737.9802)\tnorm 0.8726 (0.8705)\n",
            "Epoch: [185][190/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2077 (0.2257)\tgrad(D) penalty 0.0175 (0.0148)\tRec loss 3957.1860 (3737.9741)\tnorm 0.8716 (0.8703)\n",
            "Epoch: [186][  0/195]\tTime  0.423 ( 0.423)\tData  0.245 ( 0.245)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3501.4966 (3501.4966)\tnorm 0.8706 (0.8706)\n",
            "Epoch: [186][ 10/195]\tTime  0.147 ( 0.173)\tData  0.000 ( 0.023)\tD(real) 0.7738 (0.7739)\tD(fake) 0.2354 (0.2318)\tgrad(D) penalty 0.0120 (0.0152)\tRec loss 3687.2314 (3798.8775)\tnorm 0.8657 (0.8677)\n",
            "Epoch: [186][ 20/195]\tTime  0.148 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7733 (0.7737)\tD(fake) 0.2368 (0.2286)\tgrad(D) penalty 0.0112 (0.0139)\tRec loss 3618.7056 (3722.6092)\tnorm 0.8482 (0.8646)\n",
            "Epoch: [186][ 30/195]\tTime  0.152 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2143 (0.2229)\tgrad(D) penalty 0.0128 (0.0139)\tRec loss 3783.6301 (3729.0869)\tnorm 0.8863 (0.8666)\n",
            "Epoch: [186][ 40/195]\tTime  0.152 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2305 (0.2230)\tgrad(D) penalty 0.0119 (0.0139)\tRec loss 3977.7971 (3755.6464)\tnorm 0.8791 (0.8683)\n",
            "Epoch: [186][ 50/195]\tTime  0.165 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2170 (0.2214)\tgrad(D) penalty 0.0150 (0.0141)\tRec loss 3735.9780 (3755.6133)\tnorm 0.8730 (0.8691)\n",
            "Epoch: [186][ 60/195]\tTime  0.152 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7738 (0.7738)\tD(fake) 0.1949 (0.2215)\tgrad(D) penalty 0.0185 (0.0142)\tRec loss 3617.0317 (3743.6837)\tnorm 0.8624 (0.8687)\n",
            "Epoch: [186][ 70/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2415 (0.2242)\tgrad(D) penalty 0.0137 (0.0141)\tRec loss 3817.6362 (3736.5575)\tnorm 0.8696 (0.8686)\n",
            "Epoch: [186][ 80/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2442 (0.2242)\tgrad(D) penalty 0.0156 (0.0143)\tRec loss 3588.4631 (3727.1737)\tnorm 0.8804 (0.8688)\n",
            "Epoch: [186][ 90/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2134 (0.2239)\tgrad(D) penalty 0.0147 (0.0143)\tRec loss 3785.1147 (3737.8523)\tnorm 0.8656 (0.8684)\n",
            "Epoch: [186][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2269 (0.2241)\tgrad(D) penalty 0.0186 (0.0147)\tRec loss 3845.2334 (3735.0146)\tnorm 0.8779 (0.8683)\n",
            "Epoch: [186][110/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2109 (0.2239)\tgrad(D) penalty 0.0147 (0.0147)\tRec loss 3694.5588 (3734.9182)\tnorm 0.8654 (0.8684)\n",
            "Epoch: [186][120/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2466 (0.2248)\tgrad(D) penalty 0.0135 (0.0148)\tRec loss 3698.8135 (3734.6028)\tnorm 0.8753 (0.8687)\n",
            "Epoch: [186][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2291 (0.2246)\tgrad(D) penalty 0.0146 (0.0148)\tRec loss 3338.1768 (3732.8686)\tnorm 0.8697 (0.8686)\n",
            "Epoch: [186][140/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2165 (0.2247)\tgrad(D) penalty 0.0152 (0.0148)\tRec loss 3550.1702 (3731.4833)\tnorm 0.8665 (0.8684)\n",
            "Epoch: [186][150/195]\tTime  0.170 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7738)\tD(fake) 0.2105 (0.2244)\tgrad(D) penalty 0.0142 (0.0147)\tRec loss 3491.9233 (3731.9658)\tnorm 0.8714 (0.8683)\n",
            "Epoch: [186][160/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2069 (0.2241)\tgrad(D) penalty 0.0131 (0.0147)\tRec loss 3848.4365 (3733.1263)\tnorm 0.8804 (0.8685)\n",
            "Epoch: [186][170/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2440 (0.2244)\tgrad(D) penalty 0.0130 (0.0147)\tRec loss 3587.7749 (3735.5154)\tnorm 0.8545 (0.8683)\n",
            "Epoch: [186][180/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2286 (0.2244)\tgrad(D) penalty 0.0117 (0.0147)\tRec loss 3704.2192 (3735.3913)\tnorm 0.8664 (0.8680)\n",
            "Epoch: [186][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2429 (0.2245)\tgrad(D) penalty 0.0157 (0.0147)\tRec loss 3716.6440 (3734.4506)\tnorm 0.8743 (0.8683)\n",
            "Epoch: [187][  0/195]\tTime  0.433 ( 0.433)\tData  0.250 ( 0.250)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3802.9148 (3802.9148)\tnorm 0.8843 (0.8843)\n",
            "Epoch: [187][ 10/195]\tTime  0.150 ( 0.176)\tData  0.000 ( 0.023)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2589 (0.2507)\tgrad(D) penalty 0.0133 (0.0139)\tRec loss 3521.1177 (3749.4862)\tnorm 0.8829 (0.8701)\n",
            "Epoch: [187][ 20/195]\tTime  0.148 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2183 (0.2344)\tgrad(D) penalty 0.0152 (0.0143)\tRec loss 3619.3521 (3788.9789)\tnorm 0.8610 (0.8702)\n",
            "Epoch: [187][ 30/195]\tTime  0.147 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7736 (0.7738)\tD(fake) 0.2230 (0.2317)\tgrad(D) penalty 0.0161 (0.0147)\tRec loss 3749.6121 (3765.0014)\tnorm 0.8815 (0.8705)\n",
            "Epoch: [187][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2274 (0.2294)\tgrad(D) penalty 0.0159 (0.0150)\tRec loss 3883.6680 (3739.0270)\tnorm 0.8612 (0.8694)\n",
            "Epoch: [187][ 50/195]\tTime  0.161 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2184 (0.2280)\tgrad(D) penalty 0.0147 (0.0152)\tRec loss 3425.5791 (3722.2008)\tnorm 0.8566 (0.8697)\n",
            "Epoch: [187][ 60/195]\tTime  0.154 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2475 (0.2279)\tgrad(D) penalty 0.0128 (0.0152)\tRec loss 3932.5034 (3724.3168)\tnorm 0.8632 (0.8694)\n",
            "Epoch: [187][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2313 (0.2265)\tgrad(D) penalty 0.0175 (0.0152)\tRec loss 3668.1365 (3724.9182)\tnorm 0.8722 (0.8697)\n",
            "Epoch: [187][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2134 (0.2266)\tgrad(D) penalty 0.0210 (0.0154)\tRec loss 3447.6816 (3728.8590)\tnorm 0.8678 (0.8691)\n",
            "Epoch: [187][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2243 (0.2283)\tgrad(D) penalty 0.0136 (0.0151)\tRec loss 3851.3936 (3737.5287)\tnorm 0.8646 (0.8691)\n",
            "Epoch: [187][100/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2424 (0.2275)\tgrad(D) penalty 0.0101 (0.0148)\tRec loss 3797.1504 (3737.3816)\tnorm 0.8877 (0.8690)\n",
            "Epoch: [187][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2060 (0.2256)\tgrad(D) penalty 0.0152 (0.0148)\tRec loss 3671.2202 (3733.2983)\tnorm 0.8740 (0.8695)\n",
            "Epoch: [187][120/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2058 (0.2268)\tgrad(D) penalty 0.0215 (0.0147)\tRec loss 3805.7573 (3732.8343)\tnorm 0.8693 (0.8691)\n",
            "Epoch: [187][130/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2394 (0.2282)\tgrad(D) penalty 0.0144 (0.0147)\tRec loss 3795.9731 (3730.6379)\tnorm 0.8758 (0.8694)\n",
            "Epoch: [187][140/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2438 (0.2275)\tgrad(D) penalty 0.0130 (0.0148)\tRec loss 3644.7095 (3727.8749)\tnorm 0.8724 (0.8696)\n",
            "Epoch: [187][150/195]\tTime  0.166 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2291 (0.2280)\tgrad(D) penalty 0.0159 (0.0148)\tRec loss 3664.1284 (3728.6758)\tnorm 0.8675 (0.8696)\n",
            "Epoch: [187][160/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2317 (0.2277)\tgrad(D) penalty 0.0157 (0.0149)\tRec loss 3711.4102 (3730.3102)\tnorm 0.8755 (0.8700)\n",
            "Epoch: [187][170/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2178 (0.2272)\tgrad(D) penalty 0.0177 (0.0150)\tRec loss 3589.7283 (3730.6831)\tnorm 0.8703 (0.8699)\n",
            "Epoch: [187][180/195]\tTime  0.152 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7738)\tD(fake) 0.2136 (0.2273)\tgrad(D) penalty 0.0145 (0.0150)\tRec loss 3818.4370 (3729.6184)\tnorm 0.8761 (0.8703)\n",
            "Epoch: [187][190/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2286 (0.2271)\tgrad(D) penalty 0.0181 (0.0150)\tRec loss 3784.0017 (3731.3825)\tnorm 0.8765 (0.8705)\n",
            "Epoch: [188][  0/195]\tTime  0.455 ( 0.455)\tData  0.254 ( 0.254)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3735.3467 (3735.3467)\tnorm 0.8739 (0.8739)\n",
            "Epoch: [188][ 10/195]\tTime  0.148 ( 0.178)\tData  0.000 ( 0.023)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2480 (0.2335)\tgrad(D) penalty 0.0148 (0.0148)\tRec loss 3813.9363 (3722.9867)\tnorm 0.8649 (0.8681)\n",
            "Epoch: [188][ 20/195]\tTime  0.147 ( 0.164)\tData  0.000 ( 0.012)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2154 (0.2266)\tgrad(D) penalty 0.0170 (0.0148)\tRec loss 3970.4407 (3735.2026)\tnorm 0.8748 (0.8691)\n",
            "Epoch: [188][ 30/195]\tTime  0.148 ( 0.159)\tData  0.000 ( 0.008)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2069 (0.2261)\tgrad(D) penalty 0.0126 (0.0142)\tRec loss 3719.9714 (3711.3184)\tnorm 0.8782 (0.8707)\n",
            "Epoch: [188][ 40/195]\tTime  0.149 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2190 (0.2269)\tgrad(D) penalty 0.0151 (0.0147)\tRec loss 3554.3052 (3689.2019)\tnorm 0.8657 (0.8709)\n",
            "Epoch: [188][ 50/195]\tTime  0.163 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2473 (0.2262)\tgrad(D) penalty 0.0135 (0.0147)\tRec loss 3684.6917 (3682.3973)\tnorm 0.8677 (0.8713)\n",
            "Epoch: [188][ 60/195]\tTime  0.145 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2255 (0.2260)\tgrad(D) penalty 0.0138 (0.0142)\tRec loss 3509.6851 (3685.5325)\tnorm 0.8710 (0.8711)\n",
            "Epoch: [188][ 70/195]\tTime  0.145 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2324 (0.2272)\tgrad(D) penalty 0.0148 (0.0142)\tRec loss 3684.7646 (3701.1549)\tnorm 0.8770 (0.8707)\n",
            "Epoch: [188][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2266 (0.2287)\tgrad(D) penalty 0.0153 (0.0143)\tRec loss 3721.0610 (3722.7649)\tnorm 0.8770 (0.8710)\n",
            "Epoch: [188][ 90/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2334 (0.2281)\tgrad(D) penalty 0.0180 (0.0146)\tRec loss 3699.5117 (3726.1670)\tnorm 0.8603 (0.8707)\n",
            "Epoch: [188][100/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2115 (0.2280)\tgrad(D) penalty 0.0187 (0.0146)\tRec loss 3569.0576 (3718.8190)\tnorm 0.8606 (0.8699)\n",
            "Epoch: [188][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2462 (0.2301)\tgrad(D) penalty 0.0136 (0.0146)\tRec loss 3652.5024 (3728.5709)\tnorm 0.8622 (0.8703)\n",
            "Epoch: [188][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2439 (0.2294)\tgrad(D) penalty 0.0151 (0.0148)\tRec loss 3940.9727 (3727.3954)\tnorm 0.8765 (0.8702)\n",
            "Epoch: [188][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2314 (0.2289)\tgrad(D) penalty 0.0161 (0.0149)\tRec loss 3427.8718 (3731.4804)\tnorm 0.8625 (0.8700)\n",
            "Epoch: [188][140/195]\tTime  0.144 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2250 (0.2291)\tgrad(D) penalty 0.0150 (0.0150)\tRec loss 3711.9204 (3729.9458)\tnorm 0.8654 (0.8701)\n",
            "Epoch: [188][150/195]\tTime  0.166 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2186 (0.2289)\tgrad(D) penalty 0.0139 (0.0149)\tRec loss 3699.6243 (3732.5875)\tnorm 0.8732 (0.8702)\n",
            "Epoch: [188][160/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.1984 (0.2286)\tgrad(D) penalty 0.0157 (0.0148)\tRec loss 3591.7239 (3728.8232)\tnorm 0.8721 (0.8698)\n",
            "Epoch: [188][170/195]\tTime  0.154 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2528 (0.2296)\tgrad(D) penalty 0.0122 (0.0147)\tRec loss 3645.7920 (3727.1184)\tnorm 0.8627 (0.8695)\n",
            "Epoch: [188][180/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7738)\tD(fake) 0.2310 (0.2295)\tgrad(D) penalty 0.0107 (0.0146)\tRec loss 3435.8091 (3729.2826)\tnorm 0.8740 (0.8691)\n",
            "Epoch: [188][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2302 (0.2288)\tgrad(D) penalty 0.0162 (0.0148)\tRec loss 4042.9346 (3731.9417)\tnorm 0.8603 (0.8691)\n",
            "Epoch: [189][  0/195]\tTime  0.432 ( 0.432)\tData  0.238 ( 0.238)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3639.6003 (3639.6003)\tnorm 0.8626 (0.8626)\n",
            "Epoch: [189][ 10/195]\tTime  0.149 ( 0.174)\tData  0.000 ( 0.022)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2587 (0.2419)\tgrad(D) penalty 0.0147 (0.0181)\tRec loss 3555.6084 (3732.0364)\tnorm 0.8792 (0.8706)\n",
            "Epoch: [189][ 20/195]\tTime  0.148 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7736 (0.7737)\tD(fake) 0.2152 (0.2247)\tgrad(D) penalty 0.0186 (0.0179)\tRec loss 3686.0056 (3732.3370)\tnorm 0.8896 (0.8734)\n",
            "Epoch: [189][ 30/195]\tTime  0.150 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2215 (0.2248)\tgrad(D) penalty 0.0176 (0.0177)\tRec loss 3783.1157 (3734.4910)\tnorm 0.8721 (0.8750)\n",
            "Epoch: [189][ 40/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2248 (0.2238)\tgrad(D) penalty 0.0162 (0.0174)\tRec loss 3540.0762 (3738.1227)\tnorm 0.8724 (0.8746)\n",
            "Epoch: [189][ 50/195]\tTime  0.164 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2289 (0.2257)\tgrad(D) penalty 0.0124 (0.0166)\tRec loss 3507.9688 (3726.7200)\tnorm 0.8604 (0.8734)\n",
            "Epoch: [189][ 60/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2220 (0.2257)\tgrad(D) penalty 0.0130 (0.0162)\tRec loss 3517.9375 (3716.4885)\tnorm 0.8616 (0.8721)\n",
            "Epoch: [189][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2573 (0.2264)\tgrad(D) penalty 0.0127 (0.0159)\tRec loss 3698.1147 (3719.7714)\tnorm 0.8763 (0.8716)\n",
            "Epoch: [189][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2809 (0.2276)\tgrad(D) penalty 0.0100 (0.0155)\tRec loss 3683.3225 (3722.0503)\tnorm 0.8697 (0.8708)\n",
            "Epoch: [189][ 90/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2007 (0.2268)\tgrad(D) penalty 0.0188 (0.0155)\tRec loss 3725.3081 (3723.1992)\tnorm 0.8719 (0.8707)\n",
            "Epoch: [189][100/195]\tTime  0.163 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2129 (0.2282)\tgrad(D) penalty 0.0139 (0.0153)\tRec loss 3773.9463 (3722.4176)\tnorm 0.8602 (0.8705)\n",
            "Epoch: [189][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2660 (0.2296)\tgrad(D) penalty 0.0134 (0.0154)\tRec loss 3556.6274 (3720.4487)\tnorm 0.8678 (0.8704)\n",
            "Epoch: [189][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2329 (0.2289)\tgrad(D) penalty 0.0157 (0.0155)\tRec loss 3544.4436 (3722.2547)\tnorm 0.8767 (0.8701)\n",
            "Epoch: [189][130/195]\tTime  0.144 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2354 (0.2297)\tgrad(D) penalty 0.0152 (0.0155)\tRec loss 3430.3242 (3717.2766)\tnorm 0.8465 (0.8700)\n",
            "Epoch: [189][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2385 (0.2296)\tgrad(D) penalty 0.0159 (0.0155)\tRec loss 3673.8970 (3716.4010)\tnorm 0.8858 (0.8700)\n",
            "Epoch: [189][150/195]\tTime  0.162 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2221 (0.2295)\tgrad(D) penalty 0.0120 (0.0155)\tRec loss 3774.9805 (3720.9204)\tnorm 0.8738 (0.8701)\n",
            "Epoch: [189][160/195]\tTime  0.145 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2527 (0.2297)\tgrad(D) penalty 0.0121 (0.0154)\tRec loss 3625.5706 (3721.6344)\tnorm 0.8822 (0.8704)\n",
            "Epoch: [189][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2181 (0.2287)\tgrad(D) penalty 0.0141 (0.0154)\tRec loss 4048.6741 (3723.3103)\tnorm 0.8634 (0.8703)\n",
            "Epoch: [189][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2329 (0.2283)\tgrad(D) penalty 0.0162 (0.0154)\tRec loss 3826.3030 (3726.8294)\tnorm 0.8792 (0.8703)\n",
            "Epoch: [189][190/195]\tTime  0.153 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2061 (0.2282)\tgrad(D) penalty 0.0150 (0.0153)\tRec loss 3864.8867 (3729.3207)\tnorm 0.8730 (0.8703)\n",
            "Epoch: [190][  0/195]\tTime  0.429 ( 0.429)\tData  0.254 ( 0.254)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3585.4243 (3585.4243)\tnorm 0.8656 (0.8656)\n",
            "Epoch: [190][ 10/195]\tTime  0.148 ( 0.175)\tData  0.000 ( 0.023)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2223 (0.2267)\tgrad(D) penalty 0.0140 (0.0131)\tRec loss 3444.8320 (3691.8053)\tnorm 0.8739 (0.8726)\n",
            "Epoch: [190][ 20/195]\tTime  0.147 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2115 (0.2223)\tgrad(D) penalty 0.0125 (0.0130)\tRec loss 3794.9121 (3710.4417)\tnorm 0.8648 (0.8748)\n",
            "Epoch: [190][ 30/195]\tTime  0.146 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2618 (0.2271)\tgrad(D) penalty 0.0117 (0.0135)\tRec loss 3634.0518 (3703.1579)\tnorm 0.8822 (0.8759)\n",
            "Epoch: [190][ 40/195]\tTime  0.147 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7736 (0.7737)\tD(fake) 0.2355 (0.2262)\tgrad(D) penalty 0.0161 (0.0137)\tRec loss 3807.3550 (3697.9654)\tnorm 0.8625 (0.8744)\n",
            "Epoch: [190][ 50/195]\tTime  0.163 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7739 (0.7737)\tD(fake) 0.2243 (0.2283)\tgrad(D) penalty 0.0186 (0.0140)\tRec loss 3683.4321 (3687.8522)\tnorm 0.8621 (0.8728)\n",
            "Epoch: [190][ 60/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2332 (0.2272)\tgrad(D) penalty 0.0166 (0.0148)\tRec loss 3765.1250 (3703.6041)\tnorm 0.8535 (0.8711)\n",
            "Epoch: [190][ 70/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7739 (0.7737)\tD(fake) 0.2227 (0.2265)\tgrad(D) penalty 0.0177 (0.0150)\tRec loss 3737.5815 (3726.3500)\tnorm 0.8811 (0.8713)\n",
            "Epoch: [190][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2258 (0.2242)\tgrad(D) penalty 0.0140 (0.0154)\tRec loss 3536.8604 (3717.5749)\tnorm 0.8646 (0.8707)\n",
            "Epoch: [190][ 90/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2188 (0.2244)\tgrad(D) penalty 0.0161 (0.0155)\tRec loss 3615.9937 (3724.2714)\tnorm 0.8673 (0.8706)\n",
            "Epoch: [190][100/195]\tTime  0.170 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2396 (0.2240)\tgrad(D) penalty 0.0131 (0.0154)\tRec loss 3549.1792 (3718.6208)\tnorm 0.8670 (0.8708)\n",
            "Epoch: [190][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7740 (0.7738)\tD(fake) 0.2152 (0.2238)\tgrad(D) penalty 0.0132 (0.0153)\tRec loss 3956.3477 (3716.5945)\tnorm 0.8651 (0.8707)\n",
            "Epoch: [190][120/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2026 (0.2228)\tgrad(D) penalty 0.0174 (0.0153)\tRec loss 3521.5427 (3711.7254)\tnorm 0.8708 (0.8708)\n",
            "Epoch: [190][130/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2484 (0.2235)\tgrad(D) penalty 0.0141 (0.0153)\tRec loss 3551.2200 (3712.2118)\tnorm 0.8685 (0.8708)\n",
            "Epoch: [190][140/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7738)\tD(fake) 0.2313 (0.2238)\tgrad(D) penalty 0.0109 (0.0152)\tRec loss 3989.5769 (3716.6795)\tnorm 0.8758 (0.8704)\n",
            "Epoch: [190][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2362 (0.2237)\tgrad(D) penalty 0.0146 (0.0151)\tRec loss 3649.7695 (3723.6715)\tnorm 0.8781 (0.8706)\n",
            "Epoch: [190][160/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7738)\tD(fake) 0.2292 (0.2245)\tgrad(D) penalty 0.0165 (0.0151)\tRec loss 3767.3374 (3726.3743)\tnorm 0.8596 (0.8703)\n",
            "Epoch: [190][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2398 (0.2252)\tgrad(D) penalty 0.0140 (0.0151)\tRec loss 3724.1863 (3726.7139)\tnorm 0.8680 (0.8699)\n",
            "Epoch: [190][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7738)\tD(fake) 0.2258 (0.2251)\tgrad(D) penalty 0.0166 (0.0150)\tRec loss 3446.5378 (3726.9435)\tnorm 0.8729 (0.8700)\n",
            "Epoch: [190][190/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2278 (0.2255)\tgrad(D) penalty 0.0130 (0.0150)\tRec loss 3666.6714 (3725.3789)\tnorm 0.8799 (0.8698)\n",
            "Epoch: [191][  0/195]\tTime  0.425 ( 0.425)\tData  0.252 ( 0.252)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3530.1951 (3530.1951)\tnorm 0.8784 (0.8784)\n",
            "Epoch: [191][ 10/195]\tTime  0.147 ( 0.175)\tData  0.000 ( 0.023)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2435 (0.2318)\tgrad(D) penalty 0.0148 (0.0172)\tRec loss 3775.7617 (3617.5986)\tnorm 0.8659 (0.8682)\n",
            "Epoch: [191][ 20/195]\tTime  0.146 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2009 (0.2229)\tgrad(D) penalty 0.0189 (0.0165)\tRec loss 3739.4709 (3671.4136)\tnorm 0.8631 (0.8694)\n",
            "Epoch: [191][ 30/195]\tTime  0.149 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7740 (0.7738)\tD(fake) 0.2208 (0.2246)\tgrad(D) penalty 0.0181 (0.0165)\tRec loss 3609.9424 (3684.0009)\tnorm 0.8701 (0.8705)\n",
            "Epoch: [191][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2221 (0.2254)\tgrad(D) penalty 0.0147 (0.0164)\tRec loss 3522.1313 (3705.1273)\tnorm 0.8667 (0.8716)\n",
            "Epoch: [191][ 50/195]\tTime  0.163 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7736 (0.7738)\tD(fake) 0.2483 (0.2277)\tgrad(D) penalty 0.0131 (0.0162)\tRec loss 3686.6970 (3710.6185)\tnorm 0.8834 (0.8715)\n",
            "Epoch: [191][ 60/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7740 (0.7739)\tD(fake) 0.2401 (0.2277)\tgrad(D) penalty 0.0130 (0.0158)\tRec loss 3867.3127 (3715.7565)\tnorm 0.8706 (0.8715)\n",
            "Epoch: [191][ 70/195]\tTime  0.154 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7737 (0.7739)\tD(fake) 0.2289 (0.2273)\tgrad(D) penalty 0.0162 (0.0161)\tRec loss 3702.8560 (3710.9961)\tnorm 0.8612 (0.8707)\n",
            "Epoch: [191][ 80/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2091 (0.2266)\tgrad(D) penalty 0.0144 (0.0158)\tRec loss 3740.2102 (3720.0130)\tnorm 0.8803 (0.8704)\n",
            "Epoch: [191][ 90/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2506 (0.2281)\tgrad(D) penalty 0.0149 (0.0157)\tRec loss 4026.5869 (3720.9985)\tnorm 0.8689 (0.8701)\n",
            "Epoch: [191][100/195]\tTime  0.171 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2381 (0.2284)\tgrad(D) penalty 0.0155 (0.0158)\tRec loss 3885.6963 (3720.9765)\tnorm 0.8715 (0.8700)\n",
            "Epoch: [191][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2202 (0.2278)\tgrad(D) penalty 0.0173 (0.0158)\tRec loss 3975.0234 (3725.3918)\tnorm 0.8669 (0.8698)\n",
            "Epoch: [191][120/195]\tTime  0.145 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7738)\tD(fake) 0.2168 (0.2277)\tgrad(D) penalty 0.0146 (0.0157)\tRec loss 3661.2324 (3725.1589)\tnorm 0.8547 (0.8696)\n",
            "Epoch: [191][130/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7738)\tD(fake) 0.2388 (0.2278)\tgrad(D) penalty 0.0183 (0.0159)\tRec loss 3806.1250 (3728.0153)\tnorm 0.8747 (0.8695)\n",
            "Epoch: [191][140/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2356 (0.2283)\tgrad(D) penalty 0.0153 (0.0159)\tRec loss 3667.1450 (3732.2768)\tnorm 0.8662 (0.8693)\n",
            "Epoch: [191][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2308 (0.2286)\tgrad(D) penalty 0.0152 (0.0157)\tRec loss 4159.7300 (3735.8281)\tnorm 0.8691 (0.8695)\n",
            "Epoch: [191][160/195]\tTime  0.144 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7738)\tD(fake) 0.2094 (0.2285)\tgrad(D) penalty 0.0131 (0.0156)\tRec loss 3684.0269 (3731.1184)\tnorm 0.8568 (0.8690)\n",
            "Epoch: [191][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7739)\tD(fake) 0.2675 (0.2287)\tgrad(D) penalty 0.0102 (0.0154)\tRec loss 3802.9131 (3726.4760)\tnorm 0.8707 (0.8688)\n",
            "Epoch: [191][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7739)\tD(fake) 0.2352 (0.2284)\tgrad(D) penalty 0.0153 (0.0154)\tRec loss 3868.5938 (3725.6304)\tnorm 0.8656 (0.8686)\n",
            "Epoch: [191][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2055 (0.2282)\tgrad(D) penalty 0.0152 (0.0154)\tRec loss 3371.3589 (3722.8007)\tnorm 0.8632 (0.8684)\n",
            "Epoch: [192][  0/195]\tTime  0.429 ( 0.429)\tData  0.234 ( 0.234)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3621.3298 (3621.3298)\tnorm 0.8640 (0.8640)\n",
            "Epoch: [192][ 10/195]\tTime  0.147 ( 0.174)\tData  0.000 ( 0.022)\tD(real) 0.7739 (0.7740)\tD(fake) 0.2184 (0.2189)\tgrad(D) penalty 0.0144 (0.0143)\tRec loss 3495.1741 (3632.4957)\tnorm 0.8694 (0.8706)\n",
            "Epoch: [192][ 20/195]\tTime  0.148 ( 0.162)\tData  0.000 ( 0.011)\tD(real) 0.7740 (0.7740)\tD(fake) 0.2238 (0.2239)\tgrad(D) penalty 0.0155 (0.0150)\tRec loss 3373.9065 (3631.7617)\tnorm 0.8684 (0.8689)\n",
            "Epoch: [192][ 30/195]\tTime  0.146 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2324 (0.2251)\tgrad(D) penalty 0.0166 (0.0157)\tRec loss 3617.4014 (3666.7041)\tnorm 0.8669 (0.8695)\n",
            "Epoch: [192][ 40/195]\tTime  0.148 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2409 (0.2248)\tgrad(D) penalty 0.0188 (0.0164)\tRec loss 3801.2056 (3679.4137)\tnorm 0.8645 (0.8698)\n",
            "Epoch: [192][ 50/195]\tTime  0.164 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7740 (0.7739)\tD(fake) 0.2132 (0.2252)\tgrad(D) penalty 0.0184 (0.0166)\tRec loss 3813.2061 (3685.4434)\tnorm 0.8693 (0.8696)\n",
            "Epoch: [192][ 60/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7738 (0.7739)\tD(fake) 0.2392 (0.2254)\tgrad(D) penalty 0.0145 (0.0164)\tRec loss 3808.2434 (3689.5839)\tnorm 0.8715 (0.8708)\n",
            "Epoch: [192][ 70/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2135 (0.2248)\tgrad(D) penalty 0.0131 (0.0163)\tRec loss 3480.6602 (3703.3732)\tnorm 0.8744 (0.8704)\n",
            "Epoch: [192][ 80/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2057 (0.2258)\tgrad(D) penalty 0.0160 (0.0161)\tRec loss 3819.1406 (3716.1256)\tnorm 0.8721 (0.8703)\n",
            "Epoch: [192][ 90/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7740 (0.7739)\tD(fake) 0.2489 (0.2275)\tgrad(D) penalty 0.0113 (0.0159)\tRec loss 3792.3440 (3712.6145)\tnorm 0.8674 (0.8704)\n",
            "Epoch: [192][100/195]\tTime  0.162 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7739)\tD(fake) 0.2348 (0.2276)\tgrad(D) penalty 0.0102 (0.0157)\tRec loss 3651.1406 (3721.1006)\tnorm 0.8635 (0.8703)\n",
            "Epoch: [192][110/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7739)\tD(fake) 0.2482 (0.2271)\tgrad(D) penalty 0.0143 (0.0156)\tRec loss 3524.7639 (3719.8103)\tnorm 0.8453 (0.8704)\n",
            "Epoch: [192][120/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2313 (0.2269)\tgrad(D) penalty 0.0175 (0.0156)\tRec loss 3727.7671 (3722.0959)\tnorm 0.8648 (0.8703)\n",
            "Epoch: [192][130/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2205 (0.2273)\tgrad(D) penalty 0.0142 (0.0155)\tRec loss 3681.9102 (3721.8111)\tnorm 0.8805 (0.8698)\n",
            "Epoch: [192][140/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2295 (0.2280)\tgrad(D) penalty 0.0165 (0.0154)\tRec loss 3886.6172 (3726.4396)\tnorm 0.8688 (0.8700)\n",
            "Epoch: [192][150/195]\tTime  0.161 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7739)\tD(fake) 0.2160 (0.2275)\tgrad(D) penalty 0.0178 (0.0155)\tRec loss 3545.1025 (3721.6011)\tnorm 0.8594 (0.8698)\n",
            "Epoch: [192][160/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2367 (0.2275)\tgrad(D) penalty 0.0153 (0.0154)\tRec loss 3561.2109 (3722.8899)\tnorm 0.8659 (0.8692)\n",
            "Epoch: [192][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7739)\tD(fake) 0.2180 (0.2279)\tgrad(D) penalty 0.0142 (0.0153)\tRec loss 3844.8691 (3727.8798)\tnorm 0.8795 (0.8692)\n",
            "Epoch: [192][180/195]\tTime  0.150 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2383 (0.2281)\tgrad(D) penalty 0.0159 (0.0153)\tRec loss 3786.4126 (3731.6799)\tnorm 0.8696 (0.8695)\n",
            "Epoch: [192][190/195]\tTime  0.151 ( 0.150)\tData  0.000 ( 0.001)\tD(real) 0.7737 (0.7739)\tD(fake) 0.2366 (0.2283)\tgrad(D) penalty 0.0163 (0.0153)\tRec loss 3663.5342 (3726.8995)\tnorm 0.8645 (0.8695)\n",
            "Epoch: [193][  0/195]\tTime  0.449 ( 0.449)\tData  0.253 ( 0.253)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3991.3418 (3991.3418)\tnorm 0.8693 (0.8693)\n",
            "Epoch: [193][ 10/195]\tTime  0.146 ( 0.178)\tData  0.000 ( 0.023)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2335 (0.2313)\tgrad(D) penalty 0.0158 (0.0157)\tRec loss 3778.7412 (3678.8513)\tnorm 0.8610 (0.8688)\n",
            "Epoch: [193][ 20/195]\tTime  0.146 ( 0.163)\tData  0.000 ( 0.012)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2242 (0.2255)\tgrad(D) penalty 0.0160 (0.0169)\tRec loss 3875.9736 (3716.5793)\tnorm 0.8598 (0.8674)\n",
            "Epoch: [193][ 30/195]\tTime  0.147 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2101 (0.2190)\tgrad(D) penalty 0.0173 (0.0168)\tRec loss 3745.7302 (3711.5403)\tnorm 0.8802 (0.8703)\n",
            "Epoch: [193][ 40/195]\tTime  0.150 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2033 (0.2213)\tgrad(D) penalty 0.0182 (0.0169)\tRec loss 3634.5884 (3700.1276)\tnorm 0.8659 (0.8720)\n",
            "Epoch: [193][ 50/195]\tTime  0.166 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2264 (0.2235)\tgrad(D) penalty 0.0144 (0.0164)\tRec loss 3367.4082 (3700.0103)\tnorm 0.8687 (0.8715)\n",
            "Epoch: [193][ 60/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7736 (0.7738)\tD(fake) 0.2352 (0.2240)\tgrad(D) penalty 0.0131 (0.0161)\tRec loss 3609.7842 (3699.9116)\tnorm 0.8821 (0.8715)\n",
            "Epoch: [193][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2349 (0.2234)\tgrad(D) penalty 0.0132 (0.0160)\tRec loss 3444.3135 (3692.7166)\tnorm 0.8709 (0.8711)\n",
            "Epoch: [193][ 80/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7740 (0.7739)\tD(fake) 0.2270 (0.2236)\tgrad(D) penalty 0.0156 (0.0159)\tRec loss 3706.4177 (3697.9433)\tnorm 0.8762 (0.8715)\n",
            "Epoch: [193][ 90/195]\tTime  0.146 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2328 (0.2246)\tgrad(D) penalty 0.0159 (0.0158)\tRec loss 3687.8501 (3702.4876)\tnorm 0.8665 (0.8717)\n",
            "Epoch: [193][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2232 (0.2254)\tgrad(D) penalty 0.0151 (0.0158)\tRec loss 3448.5635 (3705.9069)\tnorm 0.8637 (0.8714)\n",
            "Epoch: [193][110/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2260 (0.2253)\tgrad(D) penalty 0.0167 (0.0158)\tRec loss 3934.0093 (3705.3014)\tnorm 0.8664 (0.8708)\n",
            "Epoch: [193][120/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7739)\tD(fake) 0.2094 (0.2255)\tgrad(D) penalty 0.0181 (0.0158)\tRec loss 3555.8711 (3712.1614)\tnorm 0.8624 (0.8703)\n",
            "Epoch: [193][130/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7739)\tD(fake) 0.2370 (0.2268)\tgrad(D) penalty 0.0120 (0.0156)\tRec loss 3750.5610 (3714.2819)\tnorm 0.8866 (0.8704)\n",
            "Epoch: [193][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2280 (0.2263)\tgrad(D) penalty 0.0142 (0.0155)\tRec loss 3394.1594 (3713.8030)\tnorm 0.8575 (0.8706)\n",
            "Epoch: [193][150/195]\tTime  0.166 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7739)\tD(fake) 0.2106 (0.2259)\tgrad(D) penalty 0.0146 (0.0154)\tRec loss 3820.1738 (3717.5484)\tnorm 0.8556 (0.8706)\n",
            "Epoch: [193][160/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7739)\tD(fake) 0.2233 (0.2263)\tgrad(D) penalty 0.0144 (0.0154)\tRec loss 3778.2737 (3718.1514)\tnorm 0.8611 (0.8703)\n",
            "Epoch: [193][170/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2153 (0.2260)\tgrad(D) penalty 0.0160 (0.0154)\tRec loss 3916.3892 (3721.1088)\tnorm 0.8688 (0.8700)\n",
            "Epoch: [193][180/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2299 (0.2263)\tgrad(D) penalty 0.0139 (0.0153)\tRec loss 3645.1079 (3719.9793)\tnorm 0.8594 (0.8697)\n",
            "Epoch: [193][190/195]\tTime  0.153 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7739)\tD(fake) 0.2259 (0.2264)\tgrad(D) penalty 0.0141 (0.0153)\tRec loss 4106.3394 (3721.7981)\tnorm 0.8775 (0.8696)\n",
            "Epoch: [194][  0/195]\tTime  0.461 ( 0.461)\tData  0.263 ( 0.263)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3667.2930 (3667.2930)\tnorm 0.8815 (0.8815)\n",
            "Epoch: [194][ 10/195]\tTime  0.145 ( 0.177)\tData  0.000 ( 0.024)\tD(real) 0.7740 (0.7737)\tD(fake) 0.2032 (0.2191)\tgrad(D) penalty 0.0139 (0.0152)\tRec loss 3671.4080 (3702.4864)\tnorm 0.8607 (0.8688)\n",
            "Epoch: [194][ 20/195]\tTime  0.148 ( 0.164)\tData  0.000 ( 0.013)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2318 (0.2271)\tgrad(D) penalty 0.0164 (0.0152)\tRec loss 3757.0977 (3705.0368)\tnorm 0.8625 (0.8687)\n",
            "Epoch: [194][ 30/195]\tTime  0.148 ( 0.159)\tData  0.000 ( 0.009)\tD(real) 0.7739 (0.7739)\tD(fake) 0.2186 (0.2262)\tgrad(D) penalty 0.0125 (0.0149)\tRec loss 3743.4185 (3712.0222)\tnorm 0.8729 (0.8696)\n",
            "Epoch: [194][ 40/195]\tTime  0.146 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7738 (0.7739)\tD(fake) 0.2212 (0.2273)\tgrad(D) penalty 0.0152 (0.0147)\tRec loss 3627.2253 (3700.3899)\tnorm 0.8606 (0.8700)\n",
            "Epoch: [194][ 50/195]\tTime  0.164 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2228 (0.2258)\tgrad(D) penalty 0.0161 (0.0148)\tRec loss 3954.2109 (3718.5147)\tnorm 0.8752 (0.8694)\n",
            "Epoch: [194][ 60/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2196 (0.2267)\tgrad(D) penalty 0.0190 (0.0152)\tRec loss 3830.3142 (3717.3465)\tnorm 0.8709 (0.8693)\n",
            "Epoch: [194][ 70/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2291 (0.2282)\tgrad(D) penalty 0.0128 (0.0151)\tRec loss 3708.3347 (3714.5795)\tnorm 0.8602 (0.8693)\n",
            "Epoch: [194][ 80/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2278 (0.2283)\tgrad(D) penalty 0.0111 (0.0149)\tRec loss 3653.2998 (3716.1783)\tnorm 0.8583 (0.8689)\n",
            "Epoch: [194][ 90/195]\tTime  0.145 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2380 (0.2287)\tgrad(D) penalty 0.0138 (0.0150)\tRec loss 3465.4836 (3715.2095)\tnorm 0.8740 (0.8689)\n",
            "Epoch: [194][100/195]\tTime  0.163 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2432 (0.2279)\tgrad(D) penalty 0.0133 (0.0150)\tRec loss 3941.3782 (3717.6229)\tnorm 0.8721 (0.8691)\n",
            "Epoch: [194][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2132 (0.2274)\tgrad(D) penalty 0.0133 (0.0148)\tRec loss 3704.8108 (3720.3306)\tnorm 0.8697 (0.8691)\n",
            "Epoch: [194][120/195]\tTime  0.144 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2330 (0.2286)\tgrad(D) penalty 0.0145 (0.0149)\tRec loss 3748.4707 (3719.1800)\tnorm 0.8577 (0.8688)\n",
            "Epoch: [194][130/195]\tTime  0.156 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7738)\tD(fake) 0.2524 (0.2286)\tgrad(D) penalty 0.0153 (0.0152)\tRec loss 4103.5000 (3726.5262)\tnorm 0.8746 (0.8685)\n",
            "Epoch: [194][140/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2262 (0.2286)\tgrad(D) penalty 0.0182 (0.0152)\tRec loss 3753.1699 (3729.9872)\tnorm 0.8726 (0.8689)\n",
            "Epoch: [194][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2116 (0.2286)\tgrad(D) penalty 0.0162 (0.0153)\tRec loss 3708.0537 (3729.7724)\tnorm 0.8726 (0.8689)\n",
            "Epoch: [194][160/195]\tTime  0.154 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2266 (0.2280)\tgrad(D) penalty 0.0149 (0.0153)\tRec loss 3437.3423 (3729.9909)\tnorm 0.8590 (0.8689)\n",
            "Epoch: [194][170/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2367 (0.2278)\tgrad(D) penalty 0.0146 (0.0152)\tRec loss 3717.0063 (3726.2965)\tnorm 0.8652 (0.8686)\n",
            "Epoch: [194][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2402 (0.2278)\tgrad(D) penalty 0.0146 (0.0152)\tRec loss 3646.4644 (3723.5464)\tnorm 0.8691 (0.8682)\n",
            "Epoch: [194][190/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7738)\tD(fake) 0.2159 (0.2278)\tgrad(D) penalty 0.0153 (0.0151)\tRec loss 3603.0938 (3721.1430)\tnorm 0.8578 (0.8682)\n",
            "Epoch: [195][  0/195]\tTime  0.442 ( 0.442)\tData  0.247 ( 0.247)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3452.7266 (3452.7266)\tnorm 0.8777 (0.8777)\n",
            "Epoch: [195][ 10/195]\tTime  0.148 ( 0.175)\tData  0.000 ( 0.023)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2474 (0.2353)\tgrad(D) penalty 0.0128 (0.0129)\tRec loss 3582.6326 (3539.2421)\tnorm 0.8739 (0.8665)\n",
            "Epoch: [195][ 20/195]\tTime  0.145 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7733 (0.7736)\tD(fake) 0.2533 (0.2344)\tgrad(D) penalty 0.0125 (0.0140)\tRec loss 3711.1677 (3627.2818)\tnorm 0.8752 (0.8678)\n",
            "Epoch: [195][ 30/195]\tTime  0.148 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2116 (0.2288)\tgrad(D) penalty 0.0140 (0.0141)\tRec loss 3770.1687 (3664.5199)\tnorm 0.8622 (0.8707)\n",
            "Epoch: [195][ 40/195]\tTime  0.153 ( 0.155)\tData  0.000 ( 0.006)\tD(real) 0.7732 (0.7736)\tD(fake) 0.2224 (0.2276)\tgrad(D) penalty 0.0182 (0.0143)\tRec loss 3496.5061 (3668.9241)\tnorm 0.8687 (0.8716)\n",
            "Epoch: [195][ 50/195]\tTime  0.162 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7734 (0.7736)\tD(fake) 0.2301 (0.2267)\tgrad(D) penalty 0.0171 (0.0146)\tRec loss 3651.4707 (3689.2979)\tnorm 0.8815 (0.8706)\n",
            "Epoch: [195][ 60/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2282 (0.2280)\tgrad(D) penalty 0.0159 (0.0146)\tRec loss 3840.8159 (3700.5139)\tnorm 0.8620 (0.8717)\n",
            "Epoch: [195][ 70/195]\tTime  0.151 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2325 (0.2283)\tgrad(D) penalty 0.0147 (0.0147)\tRec loss 3903.0593 (3703.2430)\tnorm 0.8684 (0.8718)\n",
            "Epoch: [195][ 80/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2217 (0.2286)\tgrad(D) penalty 0.0169 (0.0149)\tRec loss 3743.5986 (3702.8289)\tnorm 0.8627 (0.8714)\n",
            "Epoch: [195][ 90/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2393 (0.2295)\tgrad(D) penalty 0.0137 (0.0148)\tRec loss 3548.3267 (3702.4699)\tnorm 0.8747 (0.8717)\n",
            "Epoch: [195][100/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7735 (0.7737)\tD(fake) 0.2228 (0.2295)\tgrad(D) penalty 0.0146 (0.0149)\tRec loss 3804.4067 (3714.9353)\tnorm 0.8729 (0.8715)\n",
            "Epoch: [195][110/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2306 (0.2286)\tgrad(D) penalty 0.0142 (0.0149)\tRec loss 3626.9360 (3711.2777)\tnorm 0.8636 (0.8705)\n",
            "Epoch: [195][120/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2388 (0.2286)\tgrad(D) penalty 0.0134 (0.0149)\tRec loss 3921.1284 (3718.4968)\tnorm 0.8768 (0.8709)\n",
            "Epoch: [195][130/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2256 (0.2282)\tgrad(D) penalty 0.0165 (0.0150)\tRec loss 3633.5552 (3717.4414)\tnorm 0.8712 (0.8706)\n",
            "Epoch: [195][140/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2109 (0.2277)\tgrad(D) penalty 0.0185 (0.0151)\tRec loss 4001.2466 (3719.9672)\tnorm 0.8786 (0.8706)\n",
            "Epoch: [195][150/195]\tTime  0.164 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2299 (0.2281)\tgrad(D) penalty 0.0158 (0.0151)\tRec loss 3698.6265 (3717.8979)\tnorm 0.8661 (0.8703)\n",
            "Epoch: [195][160/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2591 (0.2278)\tgrad(D) penalty 0.0145 (0.0153)\tRec loss 3810.4717 (3716.5034)\tnorm 0.8674 (0.8698)\n",
            "Epoch: [195][170/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7737)\tD(fake) 0.2112 (0.2272)\tgrad(D) penalty 0.0130 (0.0152)\tRec loss 3694.1628 (3712.2822)\tnorm 0.8734 (0.8696)\n",
            "Epoch: [195][180/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2028 (0.2275)\tgrad(D) penalty 0.0158 (0.0152)\tRec loss 3759.8977 (3715.6164)\tnorm 0.8715 (0.8692)\n",
            "Epoch: [195][190/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7737)\tD(fake) 0.2648 (0.2285)\tgrad(D) penalty 0.0123 (0.0152)\tRec loss 3720.3762 (3717.6215)\tnorm 0.8660 (0.8692)\n",
            "Epoch: [196][  0/195]\tTime  0.448 ( 0.448)\tData  0.254 ( 0.254)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3607.8931 (3607.8931)\tnorm 0.8792 (0.8792)\n",
            "Epoch: [196][ 10/195]\tTime  0.148 ( 0.175)\tData  0.000 ( 0.023)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2471 (0.2488)\tgrad(D) penalty 0.0149 (0.0140)\tRec loss 3845.7566 (3782.6444)\tnorm 0.8808 (0.8677)\n",
            "Epoch: [196][ 20/195]\tTime  0.146 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2585 (0.2331)\tgrad(D) penalty 0.0164 (0.0167)\tRec loss 3821.9998 (3781.6713)\tnorm 0.8752 (0.8708)\n",
            "Epoch: [196][ 30/195]\tTime  0.149 ( 0.158)\tData  0.000 ( 0.009)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2035 (0.2300)\tgrad(D) penalty 0.0163 (0.0164)\tRec loss 3663.0554 (3746.6812)\tnorm 0.8786 (0.8711)\n",
            "Epoch: [196][ 40/195]\tTime  0.148 ( 0.156)\tData  0.000 ( 0.007)\tD(real) 0.7736 (0.7737)\tD(fake) 0.1984 (0.2293)\tgrad(D) penalty 0.0144 (0.0159)\tRec loss 3647.3142 (3722.0861)\tnorm 0.8676 (0.8709)\n",
            "Epoch: [196][ 50/195]\tTime  0.167 ( 0.155)\tData  0.000 ( 0.005)\tD(real) 0.7736 (0.7737)\tD(fake) 0.2422 (0.2313)\tgrad(D) penalty 0.0119 (0.0157)\tRec loss 3596.3857 (3709.8876)\tnorm 0.8704 (0.8699)\n",
            "Epoch: [196][ 60/195]\tTime  0.154 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2492 (0.2310)\tgrad(D) penalty 0.0148 (0.0157)\tRec loss 3751.8667 (3710.8930)\tnorm 0.8530 (0.8688)\n",
            "Epoch: [196][ 70/195]\tTime  0.147 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7736 (0.7737)\tD(fake) 0.2141 (0.2290)\tgrad(D) penalty 0.0160 (0.0157)\tRec loss 3741.9265 (3701.6616)\tnorm 0.8484 (0.8688)\n",
            "Epoch: [196][ 80/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2275 (0.2286)\tgrad(D) penalty 0.0163 (0.0155)\tRec loss 3509.0122 (3693.4009)\tnorm 0.8723 (0.8690)\n",
            "Epoch: [196][ 90/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2142 (0.2278)\tgrad(D) penalty 0.0141 (0.0154)\tRec loss 3889.5867 (3695.5029)\tnorm 0.8748 (0.8691)\n",
            "Epoch: [196][100/195]\tTime  0.167 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7734 (0.7737)\tD(fake) 0.2236 (0.2280)\tgrad(D) penalty 0.0151 (0.0153)\tRec loss 3647.0308 (3696.6652)\tnorm 0.8787 (0.8692)\n",
            "Epoch: [196][110/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2323 (0.2277)\tgrad(D) penalty 0.0182 (0.0155)\tRec loss 3652.8960 (3696.8847)\tnorm 0.8642 (0.8691)\n",
            "Epoch: [196][120/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7737)\tD(fake) 0.2138 (0.2276)\tgrad(D) penalty 0.0201 (0.0156)\tRec loss 3638.6465 (3704.1609)\tnorm 0.8541 (0.8688)\n",
            "Epoch: [196][130/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2230 (0.2274)\tgrad(D) penalty 0.0155 (0.0157)\tRec loss 3480.7827 (3696.2485)\tnorm 0.8592 (0.8689)\n",
            "Epoch: [196][140/195]\tTime  0.151 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7730 (0.7737)\tD(fake) 0.2372 (0.2282)\tgrad(D) penalty 0.0139 (0.0157)\tRec loss 3753.0686 (3692.4976)\tnorm 0.8653 (0.8690)\n",
            "Epoch: [196][150/195]\tTime  0.165 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2501 (0.2286)\tgrad(D) penalty 0.0152 (0.0156)\tRec loss 3656.3809 (3694.5781)\tnorm 0.8712 (0.8689)\n",
            "Epoch: [196][160/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2307 (0.2288)\tgrad(D) penalty 0.0176 (0.0156)\tRec loss 3441.0498 (3697.2507)\tnorm 0.8617 (0.8693)\n",
            "Epoch: [196][170/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7737)\tD(fake) 0.2013 (0.2286)\tgrad(D) penalty 0.0134 (0.0154)\tRec loss 3935.9182 (3707.8252)\tnorm 0.8761 (0.8693)\n",
            "Epoch: [196][180/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7737)\tD(fake) 0.2452 (0.2291)\tgrad(D) penalty 0.0155 (0.0154)\tRec loss 3926.1279 (3715.1371)\tnorm 0.8768 (0.8695)\n",
            "Epoch: [196][190/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2227 (0.2286)\tgrad(D) penalty 0.0160 (0.0155)\tRec loss 3811.4536 (3715.8908)\tnorm 0.8669 (0.8695)\n",
            "Epoch: [197][  0/195]\tTime  0.426 ( 0.426)\tData  0.236 ( 0.236)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3677.4189 (3677.4189)\tnorm 0.8700 (0.8700)\n",
            "Epoch: [197][ 10/195]\tTime  0.151 ( 0.175)\tData  0.000 ( 0.022)\tD(real) 0.7737 (0.7735)\tD(fake) 0.2371 (0.2196)\tgrad(D) penalty 0.0178 (0.0172)\tRec loss 3505.2842 (3651.1101)\tnorm 0.8632 (0.8698)\n",
            "Epoch: [197][ 20/195]\tTime  0.146 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7736 (0.7735)\tD(fake) 0.2107 (0.2232)\tgrad(D) penalty 0.0200 (0.0177)\tRec loss 4088.5645 (3677.6762)\tnorm 0.8666 (0.8716)\n",
            "Epoch: [197][ 30/195]\tTime  0.148 ( 0.158)\tData  0.000 ( 0.008)\tD(real) 0.7737 (0.7735)\tD(fake) 0.2426 (0.2292)\tgrad(D) penalty 0.0140 (0.0171)\tRec loss 3784.0212 (3707.3146)\tnorm 0.8732 (0.8713)\n",
            "Epoch: [197][ 40/195]\tTime  0.147 ( 0.156)\tData  0.000 ( 0.006)\tD(real) 0.7737 (0.7735)\tD(fake) 0.2299 (0.2287)\tgrad(D) penalty 0.0134 (0.0164)\tRec loss 3596.9521 (3712.4201)\tnorm 0.8821 (0.8708)\n",
            "Epoch: [197][ 50/195]\tTime  0.167 ( 0.154)\tData  0.000 ( 0.005)\tD(real) 0.7738 (0.7736)\tD(fake) 0.2321 (0.2286)\tgrad(D) penalty 0.0163 (0.0164)\tRec loss 3731.6306 (3724.4534)\tnorm 0.8816 (0.8707)\n",
            "Epoch: [197][ 60/195]\tTime  0.149 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7738 (0.7736)\tD(fake) 0.2415 (0.2261)\tgrad(D) penalty 0.0137 (0.0161)\tRec loss 3576.0330 (3719.2530)\tnorm 0.8616 (0.8705)\n",
            "Epoch: [197][ 70/195]\tTime  0.148 ( 0.153)\tData  0.000 ( 0.004)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2155 (0.2249)\tgrad(D) penalty 0.0150 (0.0159)\tRec loss 3803.5566 (3715.4556)\tnorm 0.8692 (0.8701)\n",
            "Epoch: [197][ 80/195]\tTime  0.153 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2073 (0.2246)\tgrad(D) penalty 0.0146 (0.0155)\tRec loss 3597.4551 (3711.6338)\tnorm 0.8779 (0.8702)\n",
            "Epoch: [197][ 90/195]\tTime  0.151 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7736)\tD(fake) 0.2409 (0.2253)\tgrad(D) penalty 0.0133 (0.0154)\tRec loss 3620.2485 (3711.0913)\tnorm 0.8690 (0.8702)\n",
            "Epoch: [197][100/195]\tTime  0.166 ( 0.152)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7736)\tD(fake) 0.2611 (0.2256)\tgrad(D) penalty 0.0154 (0.0157)\tRec loss 3691.0706 (3714.4951)\tnorm 0.8648 (0.8703)\n",
            "Epoch: [197][110/195]\tTime  0.155 ( 0.153)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7736)\tD(fake) 0.2159 (0.2262)\tgrad(D) penalty 0.0163 (0.0157)\tRec loss 3695.5913 (3715.8195)\tnorm 0.8793 (0.8704)\n",
            "Epoch: [197][120/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7737)\tD(fake) 0.2534 (0.2280)\tgrad(D) penalty 0.0125 (0.0157)\tRec loss 3934.2534 (3721.4140)\tnorm 0.8621 (0.8701)\n",
            "Epoch: [197][130/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2110 (0.2268)\tgrad(D) penalty 0.0188 (0.0159)\tRec loss 3901.9414 (3721.1624)\tnorm 0.8832 (0.8703)\n",
            "Epoch: [197][140/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7734 (0.7737)\tD(fake) 0.2169 (0.2279)\tgrad(D) penalty 0.0166 (0.0159)\tRec loss 4004.3584 (3724.4885)\tnorm 0.8671 (0.8702)\n",
            "Epoch: [197][150/195]\tTime  0.165 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7736 (0.7737)\tD(fake) 0.2397 (0.2273)\tgrad(D) penalty 0.0152 (0.0160)\tRec loss 3703.1299 (3720.3336)\tnorm 0.8827 (0.8700)\n",
            "Epoch: [197][160/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2184 (0.2274)\tgrad(D) penalty 0.0173 (0.0160)\tRec loss 3346.7271 (3718.7279)\tnorm 0.8658 (0.8698)\n",
            "Epoch: [197][170/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2229 (0.2277)\tgrad(D) penalty 0.0155 (0.0159)\tRec loss 3900.1155 (3717.6437)\tnorm 0.8684 (0.8695)\n",
            "Epoch: [197][180/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2193 (0.2275)\tgrad(D) penalty 0.0165 (0.0159)\tRec loss 3655.8066 (3718.1041)\tnorm 0.8719 (0.8694)\n",
            "Epoch: [197][190/195]\tTime  0.152 ( 0.151)\tData  0.000 ( 0.001)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2473 (0.2273)\tgrad(D) penalty 0.0154 (0.0160)\tRec loss 3677.5508 (3716.6466)\tnorm 0.8734 (0.8697)\n",
            "Epoch: [198][  0/195]\tTime  0.451 ( 0.451)\tData  0.253 ( 0.253)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3734.4080 (3734.4080)\tnorm 0.8643 (0.8643)\n",
            "Epoch: [198][ 10/195]\tTime  0.147 ( 0.179)\tData  0.000 ( 0.023)\tD(real) 0.7731 (0.7734)\tD(fake) 0.2232 (0.2256)\tgrad(D) penalty 0.0136 (0.0144)\tRec loss 3786.5234 (3659.2604)\tnorm 0.8650 (0.8682)\n",
            "Epoch: [198][ 20/195]\tTime  0.148 ( 0.165)\tData  0.000 ( 0.012)\tD(real) 0.7735 (0.7736)\tD(fake) 0.2159 (0.2198)\tgrad(D) penalty 0.0161 (0.0163)\tRec loss 3344.9624 (3627.2406)\tnorm 0.8662 (0.8671)\n",
            "Epoch: [198][ 30/195]\tTime  0.150 ( 0.160)\tData  0.000 ( 0.008)\tD(real) 0.7736 (0.7736)\tD(fake) 0.2158 (0.2171)\tgrad(D) penalty 0.0199 (0.0171)\tRec loss 3542.5615 (3629.9259)\tnorm 0.8651 (0.8669)\n",
            "Epoch: [198][ 40/195]\tTime  0.146 ( 0.157)\tData  0.000 ( 0.006)\tD(real) 0.7739 (0.7737)\tD(fake) 0.2039 (0.2187)\tgrad(D) penalty 0.0181 (0.0172)\tRec loss 3609.3267 (3633.1244)\tnorm 0.8609 (0.8674)\n",
            "Epoch: [198][ 50/195]\tTime  0.170 ( 0.156)\tData  0.000 ( 0.005)\tD(real) 0.7739 (0.7737)\tD(fake) 0.2474 (0.2200)\tgrad(D) penalty 0.0144 (0.0172)\tRec loss 3960.0876 (3658.6777)\tnorm 0.8628 (0.8664)\n",
            "Epoch: [198][ 60/195]\tTime  0.155 ( 0.155)\tData  0.000 ( 0.004)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2464 (0.2219)\tgrad(D) penalty 0.0149 (0.0171)\tRec loss 3513.8679 (3660.4347)\tnorm 0.8638 (0.8663)\n",
            "Epoch: [198][ 70/195]\tTime  0.148 ( 0.154)\tData  0.000 ( 0.004)\tD(real) 0.7740 (0.7738)\tD(fake) 0.2210 (0.2228)\tgrad(D) penalty 0.0173 (0.0169)\tRec loss 3723.4561 (3670.3447)\tnorm 0.8746 (0.8672)\n",
            "Epoch: [198][ 80/195]\tTime  0.150 ( 0.154)\tData  0.000 ( 0.003)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2213 (0.2240)\tgrad(D) penalty 0.0135 (0.0166)\tRec loss 3566.2671 (3681.7528)\tnorm 0.8651 (0.8673)\n",
            "Epoch: [198][ 90/195]\tTime  0.150 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2515 (0.2251)\tgrad(D) penalty 0.0127 (0.0164)\tRec loss 3681.9814 (3684.9115)\tnorm 0.8615 (0.8675)\n",
            "Epoch: [198][100/195]\tTime  0.162 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2188 (0.2250)\tgrad(D) penalty 0.0144 (0.0163)\tRec loss 3757.7446 (3689.6991)\tnorm 0.8719 (0.8677)\n",
            "Epoch: [198][110/195]\tTime  0.149 ( 0.153)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2148 (0.2241)\tgrad(D) penalty 0.0186 (0.0164)\tRec loss 3749.8643 (3694.4466)\tnorm 0.8610 (0.8673)\n",
            "Epoch: [198][120/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7738)\tD(fake) 0.2142 (0.2227)\tgrad(D) penalty 0.0179 (0.0166)\tRec loss 3759.6689 (3700.8739)\tnorm 0.8718 (0.8674)\n",
            "Epoch: [198][130/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7738)\tD(fake) 0.1982 (0.2220)\tgrad(D) penalty 0.0205 (0.0168)\tRec loss 3751.9304 (3706.9811)\tnorm 0.8655 (0.8678)\n",
            "Epoch: [198][140/195]\tTime  0.148 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7738)\tD(fake) 0.2052 (0.2212)\tgrad(D) penalty 0.0181 (0.0169)\tRec loss 3795.4497 (3707.3451)\tnorm 0.8615 (0.8679)\n",
            "Epoch: [198][150/195]\tTime  0.163 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7741 (0.7738)\tD(fake) 0.2418 (0.2222)\tgrad(D) penalty 0.0156 (0.0169)\tRec loss 3853.0132 (3705.4235)\tnorm 0.8535 (0.8677)\n",
            "Epoch: [198][160/195]\tTime  0.149 ( 0.152)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7738)\tD(fake) 0.2358 (0.2225)\tgrad(D) penalty 0.0168 (0.0168)\tRec loss 3439.7708 (3703.2320)\tnorm 0.8709 (0.8673)\n",
            "Epoch: [198][170/195]\tTime  0.146 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7738)\tD(fake) 0.2272 (0.2227)\tgrad(D) penalty 0.0174 (0.0168)\tRec loss 3789.6875 (3708.6331)\tnorm 0.8672 (0.8674)\n",
            "Epoch: [198][180/195]\tTime  0.150 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7740 (0.7738)\tD(fake) 0.2421 (0.2232)\tgrad(D) penalty 0.0126 (0.0167)\tRec loss 3896.2622 (3711.4753)\tnorm 0.8768 (0.8677)\n",
            "Epoch: [198][190/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7739 (0.7738)\tD(fake) 0.2393 (0.2237)\tgrad(D) penalty 0.0166 (0.0167)\tRec loss 3932.1230 (3709.5420)\tnorm 0.8838 (0.8679)\n",
            "Epoch: [199][  0/195]\tTime  0.421 ( 0.421)\tData  0.244 ( 0.244)\tD(real) 0.0000 (0.0000)\tD(fake) 0.0000 (0.0000)\tgrad(D) penalty 0.0000 (0.0000)\tRec loss 3574.2114 (3574.2114)\tnorm 0.8690 (0.8690)\n",
            "Epoch: [199][ 10/195]\tTime  0.151 ( 0.174)\tData  0.000 ( 0.022)\tD(real) 0.7738 (0.7739)\tD(fake) 0.2546 (0.2430)\tgrad(D) penalty 0.0180 (0.0166)\tRec loss 3716.6836 (3727.7228)\tnorm 0.8645 (0.8682)\n",
            "Epoch: [199][ 20/195]\tTime  0.148 ( 0.162)\tData  0.000 ( 0.012)\tD(real) 0.7738 (0.7738)\tD(fake) 0.2374 (0.2322)\tgrad(D) penalty 0.0148 (0.0150)\tRec loss 3498.4033 (3721.6971)\tnorm 0.8785 (0.8702)\n",
            "Epoch: [199][ 30/195]\tTime  0.147 ( 0.157)\tData  0.000 ( 0.008)\tD(real) 0.7736 (0.7738)\tD(fake) 0.2269 (0.2333)\tgrad(D) penalty 0.0163 (0.0151)\tRec loss 3736.4597 (3700.5469)\tnorm 0.8746 (0.8709)\n",
            "Epoch: [199][ 40/195]\tTime  0.147 ( 0.154)\tData  0.000 ( 0.006)\tD(real) 0.7736 (0.7737)\tD(fake) 0.2263 (0.2299)\tgrad(D) penalty 0.0157 (0.0154)\tRec loss 3624.4922 (3685.8279)\tnorm 0.8809 (0.8706)\n",
            "Epoch: [199][ 50/195]\tTime  0.162 ( 0.153)\tData  0.000 ( 0.005)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2269 (0.2299)\tgrad(D) penalty 0.0148 (0.0154)\tRec loss 3757.4834 (3687.4242)\tnorm 0.8582 (0.8694)\n",
            "Epoch: [199][ 60/195]\tTime  0.147 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2222 (0.2264)\tgrad(D) penalty 0.0171 (0.0153)\tRec loss 3789.3362 (3691.9696)\tnorm 0.8649 (0.8693)\n",
            "Epoch: [199][ 70/195]\tTime  0.150 ( 0.152)\tData  0.000 ( 0.004)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2135 (0.2271)\tgrad(D) penalty 0.0136 (0.0152)\tRec loss 3830.8516 (3686.0941)\tnorm 0.8741 (0.8688)\n",
            "Epoch: [199][ 80/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2061 (0.2271)\tgrad(D) penalty 0.0157 (0.0152)\tRec loss 3828.4309 (3693.0531)\tnorm 0.8683 (0.8686)\n",
            "Epoch: [199][ 90/195]\tTime  0.149 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2486 (0.2282)\tgrad(D) penalty 0.0167 (0.0154)\tRec loss 3763.3821 (3695.8438)\tnorm 0.8736 (0.8684)\n",
            "Epoch: [199][100/195]\tTime  0.168 ( 0.151)\tData  0.000 ( 0.003)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2471 (0.2271)\tgrad(D) penalty 0.0178 (0.0157)\tRec loss 3620.9268 (3694.1925)\tnorm 0.8638 (0.8680)\n",
            "Epoch: [199][110/195]\tTime  0.148 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2010 (0.2269)\tgrad(D) penalty 0.0152 (0.0157)\tRec loss 3900.1255 (3704.6998)\tnorm 0.8724 (0.8680)\n",
            "Epoch: [199][120/195]\tTime  0.147 ( 0.151)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2346 (0.2278)\tgrad(D) penalty 0.0154 (0.0162)\tRec loss 3660.7913 (3711.3139)\tnorm 0.8721 (0.8679)\n",
            "Epoch: [199][130/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7735 (0.7737)\tD(fake) 0.2232 (0.2267)\tgrad(D) penalty 0.0240 (0.0165)\tRec loss 3805.6707 (3715.8266)\tnorm 0.8768 (0.8685)\n",
            "Epoch: [199][140/195]\tTime  0.149 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2182 (0.2282)\tgrad(D) penalty 0.0188 (0.0166)\tRec loss 3486.6914 (3709.2736)\tnorm 0.8556 (0.8682)\n",
            "Epoch: [199][150/195]\tTime  0.165 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2458 (0.2293)\tgrad(D) penalty 0.0121 (0.0164)\tRec loss 3705.4868 (3705.3580)\tnorm 0.8629 (0.8678)\n",
            "Epoch: [199][160/195]\tTime  0.148 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2333 (0.2298)\tgrad(D) penalty 0.0155 (0.0163)\tRec loss 3558.5076 (3705.7398)\tnorm 0.8660 (0.8673)\n",
            "Epoch: [199][170/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2268 (0.2297)\tgrad(D) penalty 0.0146 (0.0162)\tRec loss 3684.2478 (3704.8548)\tnorm 0.8631 (0.8673)\n",
            "Epoch: [199][180/195]\tTime  0.146 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7737 (0.7737)\tD(fake) 0.2342 (0.2294)\tgrad(D) penalty 0.0131 (0.0161)\tRec loss 3639.4106 (3703.7020)\tnorm 0.8767 (0.8675)\n",
            "Epoch: [199][190/195]\tTime  0.147 ( 0.150)\tData  0.000 ( 0.002)\tD(real) 0.7738 (0.7737)\tD(fake) 0.2230 (0.2289)\tgrad(D) penalty 0.0152 (0.0161)\tRec loss 3667.0203 (3707.5773)\tnorm 0.8779 (0.8675)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGlzirju6hEY"
      },
      "source": [
        "### Check results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n96NtwfOvtT2"
      },
      "source": [
        "vidname = os.path.join(GANSIAM_DIR, \"results/test.mp4\")\n",
        "create_progress_animation(GENERATED_GRIDS, vidname)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuuGi0nh6i5S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "71e32619-36c0-4c11-9e75-23ec4b5e7b8a"
      },
      "source": [
        "%matplotlib inline\n",
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "\n",
        "def show_sample():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.sample_latent(16).cuda(args.gpu)\n",
        "        im_tensor = model.decoder(model.G(z))\n",
        "        im_tensor = inv_normalize(im_tensor)\n",
        "        im_grid = vutils.make_grid(im_tensor.cpu(), padding=2, nrow=4)\n",
        "        plt.imshow(im_grid.permute(1,2,0))\n",
        "\n",
        "show_sample()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-a00f3aeff945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mshow_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-a00f3aeff945>\u001b[0m in \u001b[0;36mshow_sample\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mim_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mim_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minv_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mim_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-0b324b6d06e7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;31m# Add H and W dimensions, infer channels dim (add if none)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-0b324b6d06e7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    923\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    924\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [224, 512, 4, 4], expected input[16, 2048, 1, 1] to have 224 channels, but got 2048 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTi2aOol6zr6"
      },
      "source": [
        "# Recreate a batch of images from the dataset\n",
        "for x, y in train_loader:\n",
        "    with torch.no_grad():\n",
        "        #x_rec = model.decoder(simsiam.encoder(x.cuda(args.gpu)))\n",
        "        x = x.cuda(args.gpu)\n",
        "        x_rec, _ = model.decoder_with_proj(simsiam.encoder(x), model.encoder(x))\n",
        "    break\n",
        "\n",
        "# Actual\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.imshow(vutils.make_grid(\n",
        "    inv_normalize(x)[:16].cpu(), padding=2, nrow=4).permute(1,2,0))\n",
        "# Reconstruction\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.imshow(vutils.make_grid(\n",
        "    inv_normalize(x_rec)[:16].cpu(), padding=2, nrow=4).permute(1,2,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgbPq2U4w2Sv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfJZoegxwZvc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}